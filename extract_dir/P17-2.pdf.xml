<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="date" content="2017-07-26T04:37:28Z" />
<meta name="pdf:PDFVersion" content="1.5" />
<meta name="pdf:docinfo:title" content="Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics" />
<meta name="xmp:CreatorTool" content="LaTeX with hyperref package" />
<meta name="access_permission:can_print_degraded" content="true" />
<meta name="subject" content="" />
<meta name="dc:format" content="application/pdf; version=1.5" />
<meta name="pdf:docinfo:creator_tool" content="LaTeX with hyperref package" />
<meta name="access_permission:fill_in_form" content="true" />
<meta name="pdf:encrypted" content="false" />
<meta name="dc:title" content="Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics" />
<meta name="modified" content="2017-07-26T04:37:28Z" />
<meta name="cp:subject" content="" />
<meta name="pdf:docinfo:subject" content="" />
<meta name="pdf:docinfo:creator" content="Association for Computational Linguistics" />
<meta name="PTEX.Fullbanner" content="This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3" />
<meta name="meta:author" content="Association for Computational Linguistics" />
<meta name="trapped" content="False" />
<meta name="meta:creation-date" content="2017-07-26T04:37:28Z" />
<meta name="created" content="Wed Jul 26 00:37:28 EDT 2017" />
<meta name="access_permission:extract_for_accessibility" content="true" />
<meta name="Creation-Date" content="2017-07-26T04:37:28Z" />
<meta name="resourceName" content="P17-2.pdf" />
<meta name="tika:file_ext" content="pdf" />
<meta name="Author" content="Association for Computational Linguistics" />
<meta name="producer" content="pdfTeX-1.40.18" />
<meta name="pdf:docinfo:producer" content="pdfTeX-1.40.18" />
<meta name="Keywords" content="" />
<meta name="access_permission:modify_annotations" content="true" />
<meta name="dc:creator" content="Association for Computational Linguistics" />
<meta name="tika_batch_fs:relative_path" content="P17-2.pdf" />
<meta name="dcterms:created" content="2017-07-26T04:37:28Z" />
<meta name="Last-Modified" content="2017-07-26T04:37:28Z" />
<meta name="dcterms:modified" content="2017-07-26T04:37:28Z" />
<meta name="Last-Save-Date" content="2017-07-26T04:37:28Z" />
<meta name="pdf:docinfo:keywords" content="" />
<meta name="pdf:docinfo:modified" content="2017-07-26T04:37:28Z" />
<meta name="meta:save-date" content="2017-07-26T04:37:28Z" />
<meta name="pdf:docinfo:custom:PTEX.Fullbanner" content="This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3" />
<meta name="Content-Length" content="27118192" />
<meta name="X-TIKA:digest:MD5" content="a2a2cadf4d16c90d03edad10e8255f74" />
<meta name="Content-Type" content="application/pdf" />
<meta name="X-Parsed-By" content="org.apache.tika.parser.DefaultParser" />
<meta name="X-Parsed-By" content="org.apache.tika.parser.pdf.PDFParser" />
<meta name="creator" content="Association for Computational Linguistics" />
<meta name="dc:subject" content="" />
<meta name="access_permission:assemble_document" content="true" />
<meta name="xmpTPg:NPages" content="732" />
<meta name="access_permission:extract_content" content="true" />
<meta name="access_permission:can_print" content="true" />
<meta name="pdf:docinfo:trapped" content="False" />
<meta name="meta:keyword" content="" />
<meta name="access_permission:can_modify" content="true" />
<meta name="pdf:docinfo:created" content="2017-07-26T04:37:28Z" />
<title>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
</head>
<body><div class="page"><p />
<p>ACL 2017
</p>
<p>The 55th Annual Meeting of the
Association for Computational Linguistics
</p>
<p>Proceedings of the Conference, Vol. 2 (Short Papers)
</p>
<p>July 30 - August 4, 2017
Vancouver, Canada</p>
<p />
</div>
<div class="page"><p />
<p>Platinum Sponsors:
</p>
<p>Gold Sponsors:
</p>
<p>ii</p>
<p />
</div>
<div class="page"><p />
<p>Silver Sponsors:
</p>
<p>Bronze Sponsors:
</p>
<p>Supporters:
</p>
<p>iii</p>
<p />
</div>
<div class="page"><p />
<p>c©2017 The Association for Computational Linguistics
</p>
<p>Order copies of this and other ACL proceedings from:
</p>
<p>Association for Computational Linguistics (ACL)
209 N. Eighth Street
Stroudsburg, PA 18360
USA
Tel: +1-570-476-8006
Fax: +1-570-476-0860
acl@aclweb.org
</p>
<p>ISBN 978-1-945626-75-3 (Volume 1)
ISBN 978-1-945626-76-0 (Volume 2)
</p>
<p>iv</p>
<p />
</div>
<div class="page"><p />
<p>Preface: General Chair
</p>
<p>Welcome to ACL 2017 in Vancouver, Canada! This is the 55th annual meeting of the Association for
Computational Linguistics. A tremendous amount of knowledge has been presented at more than half
a century’s worth of our conferences. Hopefully, some of it is still relevant now that deep learning has
solved language. We are anticipating one of the largest ACL conferences ever. We had a record number
of papers submitted to the conference, and a record number of industry partners joining us as sponsors of
the conference. We are on track to be one of the best attended ACL conferences to date. I hope that this
year’s conference is intellectually stimulating and that you take home many new ideas and techniques
that will help extend your own research.
</p>
<p>Each year, the ACL conference is organized by a dedicated team of volunteers. Please thank this year’s
organizers for their service to the community when you see them at the conference. Without these peo-
ple, this conference would not happen: Regina Barzilay and Min-Yen Kan (Program Co-Chairs), Priscilla
Rasmussen and Anoop Sarkar (Local Organizing Committee), Wei Xu and Jonathan Berant (Workshop
Chairs), Maja Popović and Jordan Boyd-Graber (Tutorial Chairs), Wei Lu, Sameer Singh and Mar-
garet Mitchell (Publication Chairs), Heng Ji and Mohit Bansal (Demonstration Chairs), Spandana Gella,
Allyson Ettinger, and Matthieu Labeau (Student Research Workshop Organizers), Cecilia Ovesdotter
Alm, Mark Dredze, and Marine Carpuat (Faculty Advisors to the Student Research Workshop), Charley
Chan (Publicity Chair), Christian Federmann (Conference Handbook Chair), Maryam Siahbani (Student
Volunteer Coordinator), and Nitin Madnani (Webmaster and Appmaster).
</p>
<p>The organizers have been working for more than a year to put together the conference. Far more than
a year in advance, the ACL 2017 Coordinating Committee helped to select the venue and to pick the
General Chair and the Program Co-Chairs. This consisted of members from NAACL and ACL executive
boards. Representing NAACL we had Hal Daumé III, Michael White, Joel Tetreault, and Emily Bender.
Representing ACL we had Pushpak Bhattacharyya, Dragomir Radev, Graeme Hirst, Yejin Choi, and
Priscilla Rasmussen. I would like to extend a personal thanks to Graeme and Priscilla who often serve
as the ACL’s institutional memory, and who have helped fill in many details along the way.
</p>
<p>I would like to extend a special thanks to our Program Co-Chairs, Regina Barzilay and Min-Yen Kan.
They documented their work creating the program by running a blog. They used their blog as a plat-
form for engaging the ACL community in many of the decision making processes including soliciting
suggestions for the conference’s area chairs and invited speakers. They hosted discussions with Marti
Hearst and Joakim Nivre about the value of publishing pre-prints of submitted paper on arXiv and how
they relate to double blind reviewing. They even invited several prominent members of our community
to provide last-minute writing advice. If you weren’t following the blog in the lead-up to the conference,
I highly recommend taking a look through it now. You can find it linked from the ACL 2017 web page.
</p>
<p>This year’s program looks like it will be excellent! We owe a huge thank you to Regina Barzilay and Min-
Yen Kan. They selected this year’s papers from 1,318 submissions with the help of 44 area chairs and
more than 1,200 reviewers. Thanks to Regina, Min, the area chairs, the reviewers and the authors. Be-
yond the papers, we have talks by luminaries in the field of NLP, including ACL President Joakim Nivre,
invited speakers Mirella Lapata and Noah Smith, and the recipient of this year’s Lifetime Achievement
Award. We also have an excellent set of workshops and tutorials. On the tutorial day, there will also be a
special workshop on Women and Underrepresented Minorities in Natural Language Processing. Thank
you to our workshop organizers and tutorial presenters.
</p>
<p>This year’s conference features two outreach activities that I would like to highlight. First, on Sunday,
July 30, 2017, there will be a workshop on Women and Underrepresented Minorities in Natural Lan-
guage Processing organized by Libby Barak, Isabelle Augenstein, Chloé Braud, He He, and Margaret
Mitchell. The goals of the workshop are to increase awareness of the work women and underrepresented
</p>
<p>v</p>
<p />
</div>
<div class="page"><p />
<p>groups do, support women and underrepresented groups in continuing to pursue their research, and mo-
tivate long-term resources for underrepresented groups within ACL. Second, for the first time ever, ACL
is offering subsidized on-site childcare at the conference hotel. The goal of this is to allow ACL partic-
ipants with children to more readily be able to attend the conference. Since childcare duties often fall
disproportionately on women, our hope is that by having professional childcare on-site that we will allow
more women to participate, and therefore to help promote their careers. My hope is that the childcare
will be continued in future conferences.
</p>
<p>I would like to thank our many sponsors for their generous contributions. Our platinum sponsors are Al-
ibaba, Amazon, Apple, Baidu, Bloomberg, Facebook, Google, Samsung and Tencent. Our gold sponsors
are eBay, Elsevier, IBM Research, KPMG, Maluuba, Microsoft, Naver Line, NEC, Recruit Institute of
Technology, and SAP. Our silver sponsors are Adobe, Bosch, CVTE, Duolingo, Huawei, Nuance, Oracle,
and Sogou. Our bronze sponsors are Grammarly, Toutiao, and Yandex. Our supporters include Newsela
and four professional master’s degree programs from Brandeis, Columbia, NYU and the University of
Washington. We would like to acknowledge the generous support of the National Science Foundation
which has awarded a $15,000 grant to the ACL Student Research Workshop. Finally, NVIDIA donated
several Titan X GPU cards for us to raffle off during the conference.
</p>
<p>Lastly, I would like to thank everyone else who helped to make this conference a success. Thank you
to our area chairs, our army of reviewers, our workshop organizers, our tutorial presenters, our invited
speakers, and our authors. Best regards to all of you.
</p>
<p>Welcome to ACL 2017!
</p>
<p>Chris Callison-Burch
General Chair
</p>
<p>vi</p>
<p />
</div>
<div class="page"><p />
<p>vii</p>
<p />
</div>
<div class="page"><p />
<p>Preface: Program Committee Co-Chairs
</p>
<p>Welcome to the 55th Annual Meeting of the Association for Computational Linguistics! This year,
ACL received 751 long paper submissions and 567 short paper submissions1. Of the long papers, 195
were accepted for presentation at ACL — 117 as oral presentations and 78 as poster presentations (25%
acceptance rate). 107 short papers were accepted — 34 as oral and 73 as poster presentations (acceptance
rate of 18%). In addition, ACL will also feature 21 presentations of papers accepted in the Transactions
of the Association for Computational Linguistics (TACL). Including the student research workshop and
software demonstrations, the ACL program swells to a massive total of 367 paper presentations on the
scientific program, representing the largest ACL program to date.
</p>
<p>ACL 2017 will have two distinguished invited speakers: Noah A. Smith (Associate Professor of Com-
puter Science and Engineering at the University of Washington) and Mirella Lapata (Professor in the
School of Informatics at the University of Edinburgh). Both are well-renowned for their contributions to
the field of computational linguistics and are excellent orators. We are honored that they have accepted
our invitation to address the membership at this exciting juncture in our field’s history, addressing key
issues in representation learning and multimodal machine translation.
</p>
<p>To manage the tremendous growth of our field, we introduced some changes to the conference. With the
rotation of the annual meeting to the Americas, we anticipated a heavy load of submissions and early
on we decided to have both the long and short paper deadlines merged to reduce reviewing load and to
force authors to take a stand on their submissions’ format. The joint deadline allowed us to only load
our reviewers once, and also enabled us to have an extended period for more lengthy dialogue among
authors, reviewers and area chairs.
</p>
<p>In addition, oral presentations were shortened to fourteen (twelve) minutes for long (short) papers, plus
time for questions. While this places a greater demand on speakers to be concise, we believe it is worth
the effort, allowing far more work to be presented orally. We also took advantage of the many halls
available and expanded the number of parallel talks to five during most of the conference sessions.
</p>
<p>In keeping with changes introduced in the ACL community from last year, we continued the practice of
recognizing outstanding papers at ACL. The 22 outstanding papers (15 long, 7 short, 1.6% of submis-
sions) represent a broad spectrum of exciting contributions and have been specially placed on the final
day of the main conference where the program is focused into two parallel sessions of these outstanding
contributions. From these, a best paper and a best short paper those will be announced in the awards
session on Wednesday afternoon.
</p>
<p>Chris has already mentioned our introduction of the chairs’ blog2, where we strove to make the selec-
tion process of the internal workings of the scientific committee more transparent. We have publicly
documented our calls for area chairs, reviewers and accepted papers selection process. Via the blog,
we communicated several innovations in the conference organization workflow, of which we would call
attention to two key ones here.
</p>
<p>In the review process, we pioneered the use of the Toronto Paper Matching System, a topic model based
approach to the assignment of reviewers to papers. We hope this decision will spur other program
chairs to adopt the system, as increased coverage will better the reviewer/submission matching process,
ultimately leading to a higher quality program.
</p>
<p>For posterity, we also introduced the usage of hyperlinks in the bibliography reference sections of papers,
1These numbers exclude papers that were not reviewed due to formatting, anonymity, or double submission violations or
</p>
<p>that were withdrawn prior to review, which was unfortunately a substantial number.
2https://chairs-blog.acl2017.org/
</p>
<p>viii</p>
<p />
</div>
<div class="page"><p />
<p>and have worked with the ACL Anthology to ensure that digital object identifiers (DOIs) appear in the
footer of each paper. These steps will help broaden the long-term impact of the work that our community
has on the scientific world at large.
</p>
<p>There are many individuals we wish to thank for their contributions to ACL 2017, some multiple times:
</p>
<p>• The 61 area chairs who volunteered for our extra duty. They recruited reviewers, led discussions
on each paper, replied to authors’ direct comments to them and carefully assessed each submission.
Their input was instrumental in guiding the final decisions on papers and selecting the outstanding
papers.
</p>
<p>• Our full program committee of BUG hard-working individuals who reviewed the conference’s
1,318 submissions (including secondary reviewers).
</p>
<p>• TACL editors-in-chief Mark Johnson, Lillian Lee, and Kristina Toutanova, for coordinating with
us on TACL presentations at ACL.
</p>
<p>• Noah Smith and Katrin Erk, program co-chairs of ACL 2016 and Ani Nenkova and Owen Rambow,
program co-chairs of NAACL 2016, who we consulted several times on short order for help and
advice.
</p>
<p>• Wei Lu and Sameer Singh, our well-organized publication chairs, with direction and oversight
from publication chair mentor Meg Mitchell. Also, Christian Federmann who helped with the
local handbook.
</p>
<p>• The responsive team at Softconf led by Rich Gerber, who worked quickly to resolve problems and
who strove to integrate the use of the Toronto Paper Matching System (TPMS) for our use.
</p>
<p>• Priscilla Rasmussen and Anoop Sarkar and the local organization team, especially webmaster Nitin
Madnani.
</p>
<p>• Christopher Calliston-Burch, our general chair, who kept us coordinated with the rest of the ACL
2017 team and helped us free our time to concentrate on the key duty of organizing the scientific
program.
</p>
<p>• Key-Sun Choi, Jing Jiang, Graham Neubig, Emily Pitler, and Bonnie Webber who carefully re-
viewed papers under consideration for best paper recognition.
</p>
<p>• Our senior correspondents for the blog, who contributed guest posts and advice for writing and
reviewing: Waleed Ammar, Yoav Artzi, Tim Baldwin, Marco Baroni, Claire Cardie, Xavier Car-
reras, Hal Daumé, Kevin Duh, Chris Dyer, Marti Hearst, Mirella Lapata, Emily M. Bender, Au-
rélien Max, Kathy McKeown, Ray Mooney, Ani Nenkova, Joakim Nivre, Philip Resnik, and Joel
Tetreault. Without them, the participation of the community through the productive comments, and
without you the readership, our blog for disseminating information about the decision processes
would not have been possible and a success.
</p>
<p>We hope that you enjoy ACL 2017 in Vancouver!
</p>
<p>ACL 2017 program co-chairs
Regina Barzilay, Massachusetts Institute of Technology
Min-Yen Kan, National University of Singapore
</p>
<p>ix</p>
<p />
</div>
<div class="page"><p />
</div>
<div class="page"><p />
<p>Organizing Committee
</p>
<p>General Chair:
</p>
<p>Chris Callison-Burch, University of Pennsylvania
</p>
<p>Program Co-Chairs:
</p>
<p>Regina Barzilay, Massachusetts Institute of Technology
Min-Yen Kan, National University of Singapore
</p>
<p>Local Organizing Committee:
</p>
<p>Priscilla Rasmussen, ACL
Anoop Sarkar, Simon Fraser University
</p>
<p>Workshop Chairs:
</p>
<p>Wei Xu, Ohio State University
Jonathan Berant, Tel Aviv University
</p>
<p>Tutorial Chairs:
</p>
<p>Maja Popović, Humboldt-Universität zu Berlin
Jordan Boyd-Graber, University of Colorado, Boulder
</p>
<p>Publication Chairs:
</p>
<p>Wei Lu, Singapore University of Technology and Design
Sameer Singh, University of California, Irvine
Margaret Mitchell, Google (Advisory)
</p>
<p>Demonstration Chairs:
</p>
<p>Heng Ji, Rensselaer Polytechnic Institute
Mohit Bansal, University of North Carolina, Chapel Hill
</p>
<p>Student Research Workshop Organizers:
</p>
<p>Spandana Gella, University of Edinburgh
Allyson Ettinger, University of Maryland, College Park
Matthieu Labeau, Laboratoire d’Informatique pour la Mécanique et les Sciences de l’Ingénieur
(LIMSI)
</p>
<p>Faculty Advisors to the Student Research Workshop:
</p>
<p>Cecilia Ovesdotter Alm, Rochester Institute of Technology
Mark Dredze, Johns Hopkins University
Marine Carpuat, University of Maryland, College Park
</p>
<p>xi</p>
<p />
</div>
<div class="page"><p />
<p>Publicity Chair:
</p>
<p>Charley Chan, Bloomberg
</p>
<p>Conference Handbook Chair:
</p>
<p>Christian Federmann, Microsoft
</p>
<p>Student Volunteer Coordinator:
</p>
<p>Maryam Siahbani, University of the Fraser Valley
</p>
<p>Webmaster and Appmaster:
</p>
<p>Nitin Madnani, Educational Testing Service
</p>
<p>xii</p>
<p />
</div>
<div class="page"><p />
<p>Program Committee
</p>
<p>Program Committee Co-Chairs
</p>
<p>Regina Barzilay, Massachusetts Institute of Technology
Min-Yen Kan, National University of Singapore
</p>
<p>Area Chairs
</p>
<p>Mausam (Information Extraction and NLP Applications Area)
Omri Abend (Multilingual Area)
Eugene Agichtein (Information Extraction and NLP Applications Area)
Ron Artstein (Dialogue and Interactive Systems Area)
Alexandra Balahur (Sentiment Analysis and Opinion Mining Area)
Mohit Bansal (Vision, Robotics and Grounding Area)
Chia-Hui Chang (Information Extraction and NLP Applications Area)
Grzegorz Chrupała (Machine Learning Area)
Mona Diab (Multilingual Area)
Jason Eisner (Phonology, Morphology and Word Segmentation Area)
Manaal Faruqui (Semantics Area)
Raquel Fernandez (Dialogue and Interactive Systems Area)
Karën Fort (Multidisciplinary Area)
Amir Globerson (Machine Learning Area)
Hannaneh Hajishirzi (Semantics Area)
Chiori Hori (Speech Area)
Tommi Jaakkola (Machine Learning Area)
Yangfeng Ji (Discourse and Pragmatics Area)
Jing Jiang (Information Extraction and NLP Applications Area)
Sarvnaz Karimi (Information Extraction and NLP Applications Area)
Anna Korhonen (Semantics Area)
Zornitsa Kozareva (Information Extraction and NLP Applications Area)
Lun-Wei Ku (Sentiment Analysis and Opinion Mining Area)
Nate Kushman (Vision, Robotics and Grounding Area)
Chia-ying Lee (Speech Area)
Oliver Lemon (Dialogue and Interactive Systems Area)
Roger Levy (Cognitive Modeling and Psycholinguistics Area)
Sujian Li (Discourse and Pragmatics Area)
Wenjie Li (Summarization and Generation Area)
Kang Liu (Information Extraction and NLP Applications Area)
Tie-Yan Liu (Information Extraction and NLP Applications Area)
Yang Liu (Machine Translation Area)
Zhiyuan Liu (Social Media Area)
Minh-Thang Luong (Machine Translation Area)
Saif M Mohammad (Sentiment Analysis and Opinion Mining Area)
Alexander M Rush (Summarization and Generation Area)
Haitao Mi (Machine Translation Area)
Alessandro Moschitti (Information Extraction and NLP Applications Area)
Smaranda Muresan (Information Extraction and NLP Applications Area)
Preslav Nakov (Semantics Area)
Graham Neubig (Machine Translation Area)
</p>
<p>xiii</p>
<p />
</div>
<div class="page"><p />
<p>Aurélie Névéol (Biomedical Area)
Shimei Pan (Social Media Area)
Michael Piotrowski (Multidisciplinary Area)
Emily Pitler (Tagging, Chunking, Syntax and Parsing Area)
Barbara Plank (Tagging, Chunking, Syntax and Parsing Area)
Sujith Ravi (Machine Learning Area)
Verena Rieser (Summarization and Generation Area)
Sophie Rosset (Resources and Evaluation Area)
Mehroosh Sadrzadeh (Semantics Area)
Hinrich Schütze (Phonology, Morphology and Word Segmentation Area)
Anders Søgaard (Cognitive Modeling and Psycholinguistics Area)
Karin Verspoor (Biomedical Area)
Aline Villavicencio (Semantics Area)
Svitlana Volkova (Social Media Area)
Bonnie Webber (Discourse and Pragmatics Area)
Deyi Xiong (Machine Translation Area)
William Yang Wang (Machine Learning Area)
Wajdi Zaghouani (Resources and Evaluation Area)
Yue Zhang (Tagging, Chunking, Syntax and Parsing Area)
Hai Zhao (Tagging, Chunking, Syntax and Parsing Area)
</p>
<p>Primary Reviewers
</p>
<p>Reviewers who are acknowledged by the program committee for providing one or more outstand-
ing reviews are marked with “*”.
</p>
<p>Balamurali A R, Mourad Abbas, Omri Abend, Amjad Abu-Jbara, Gilles Adda, Heike Adel, Ster-
gos Afantenos, Apoorv Agarwal, Eneko Agirre, Željko Agić, Alan Akbik, Ahmet Aker, Mo-
hammed Alam, Hanan Aldarmaki, Enrique Alfonseca, Afra Alishahi, Laura Alonso Alemany,
David Alvarez-Melis, Maxime Amblard, Maryam Aminian, Silvio Amir, Waleed Ammar, Daniel
Andrade, Jacob Andreas∗, Nicholas Andrews∗, Ion Androutsopoulos, Galia Angelova, Jean-Yves
Antoine∗, Emilia Apostolova, Jun Araki, Yuki Arase, Lora Aroyo, Philip Arthur, Yoav Artzi∗,
Masayuki Asahara, Giuseppe Attardi, AiTi Aw, Ahmed Hassan Awadallah, Wilker Aziz
</p>
<p>Collin Baker, Alexandra Balahur, Niranjan Balasubramanian, Timothy Baldwin, Tyler Baldwin,
Miguel Ballesteros, David Bamman, Rafael E. Banchs, Carmen Banea, Ritwik Banerjee, Srini-
vas Bangalore, Libby Barak, Alistair Baron, Marco Baroni, Alberto Barrón-Cedeño, Roberto
Basili, David Batista, Daniel Bauer, Timo Baumann, Daniel Beck, Srikanta Bedathur, Beata
Beigman Klebanov, Kedar Bellare, Charley Beller, Islam Beltagy, Anja Belz, Yassine Bena-
jiba, Fabrício Benevenuto, Luciana Benotti∗, Jonathan Berant, Taylor Berg-Kirkpatrick, Sabine
Bergler∗, Robert Berwick, Laurent Besacier, Steven Bethard, Archna Bhatia, Chris Biemann,
Joachim Bingel, Or Biran, Alexandra Birch, Arianna Bisazza, Yonatan Bisk, Prakhar Biyani,
Johannes Bjerva, Anders Björkelund, Philippe Blache, Frédéric Blain, Eduardo Blanco, Nate
Blaylock, Bernd Bohnet, Gemma Boleda, Danushka Bollegala, Claire Bonial, Francesca Bonin,
Kalina Bontcheva, Benjamin Börschinger, Johan Bos, Elizabeth Boschee, Florian Boudin, Fethi
Bougares, Samuel Bowman, Johan Boye, Kristy Boyer, Cem Bozsahin, David Bracewell, S.R.K.
Branavan, Pavel Braslavski, Adrian Brasoveanu, Ted Briscoe, Chris Brockett, Julian Brooke,
Elia Bruni, William Bryce, Marco Büchler, Christian Buck, Paul Buitelaar, Harry Bunt, Manuel
Burghardt, David Burkett, Hendrik Buschmeier, Miriam Butt
</p>
<p>xiv</p>
<p />
</div>
<div class="page"><p />
<p>José G. C. de Souza, Deng Cai, Jose Camacho-Collados, Berkant Barla Cambazoglu, Erik Cam-
bria, Burcu Can, Marie Candito, Hailong Cao, Kris Cao∗, Yuan Cao, Ziqiang Cao, Cornelia
Caragea, Jesus Cardeńosa, Giuseppe Carenini, Marine Carpuat, Xavier Carreras, John Carroll,
Paula Carvalho, Francisco Casacuberta, Helena Caseli, Tommaso Caselli∗, Taylor Cassidy, Vitto-
rio Castelli, Giuseppe Castellucci, Asli Celikyilmaz∗, Daniel Cer, Özlem Çetinoğlu, Mauro Cet-
tolo, Arun Chaganty, Joyce Chai, Soumen Chakrabarti, Gaël de Chalendar, Yllias Chali, Nathanael
Chambers, Jane Chandlee, Muthu Kumar Chandrasekaran, Angel Chang∗, Baobao Chang, Kai-
Wei Chang, Ming-Wei Chang, Snigdha Chaturvedi, Wanxiang Che, Ciprian Chelba, Bin Chen,
Boxing Chen, Chen Chen, Hsin-Hsi Chen, John Chen, Kehai Chen, Kuang-hua Chen, Qingcai
Chen, Tao Chen, Wenliang Chen, Xinchi Chen, Yubo Chen, Yun-Nung Chen, Zhiyuan Chen,
Jianpeng Cheng, Colin Cherry, Sean Chester, Jackie Chi Kit Cheung∗, David Chiang, Jen-Tzung
Chien, Hai Leong Chieu, Laura Chiticariu, Eunsol Choi, Kostadin Cholakov, Shamil Chollampatt,
Jan Chorowski, Christos Christodoulopoulos, Tagyoung Chung, Kenneth Church, Mark Cieliebak∗,
Philipp Cimiano, Alina Maria Ciobanu∗, Alexander Clark∗, Jonathan Clark, Stephen Clark, Ann
Clifton, Maximin Coavoux, Kevin Cohen, Nigel Collier, Michael Collins, Sandra Collovini, Miriam
Connor, John Conroy∗, Matthieu Constant, Danish Contractor, Mark Core, Ryan Cotterell, Benoit
Crabbé, Danilo Croce∗, Fabien Cromieres, Montse Cuadros, Heriberto Cuayahuitl, Silviu-Petru
Cucerzan, Aron Culotta∗
</p>
<p>Luis Fernando D’Haro, Giovanni Da San Martino, Walter Daelemans, Daniel Dahlmeier, Amitava
Das, Dipanjan Das, Rajarshi Das, Pradeep Dasigi, Johannes Daxenberger, Munmun De Choud-
hury, Eric De La Clergerie, Thierry Declerck, Luciano Del Corro, Louise Deléger, Felice Dell’Orletta,
Claudio Delli Bovi, Li Deng, Lingjia Deng, Pascal Denis, Michael Denkowski, Tejaswini Deoskar,
Leon Derczynski, Nina Dethlefs, Ann Devitt, Jacob Devlin, Lipika Dey, Barbara Di Eugenio,
Giuseppe Di Fabbrizio, Gaël Dias, Fernando Diaz, Georgiana Dinu, Liviu P. Dinu, Stefanie Dip-
per, Dmitriy Dligach, Simon Dobnik, Ellen Dodge, Jesse Dodge, Daxiang Dong, Li Dong, Doug
Downey, Gabriel Doyle, A. Seza Doğruöz, Eduard Dragut, Mark Dras∗, Markus Dreyer, Lan Du,
Nan Duan, Xiangyu Duan, Kevin Duh∗, Long Duong, Emmanuel Dupoux, Nadir Durrani, Greg
Durrett, Ondřej Dušek∗, Marc Dymetman
</p>
<p>Judith Eckle-Kohler, Steffen Eger∗, Markus Egg, Koji Eguchi, Patrick Ehlen, Maud Ehrmann∗,
Vladimir Eidelman, Andreas Eisele, Jacob Eisenstein∗, Heba Elfardy, Michael Elhadad∗, Desmond
Elliott∗, Micha Elsner, Nikos Engonopoulos, Messina Enza, Katrin Erk, Arash Eshghi, Miquel
Esplà-Gomis
</p>
<p>James Fan, Federico Fancellu, Licheng Fang, Benamara Farah, Stefano Faralli, Richárd Farkas,
Afsaneh Fazly, Geli Fei, Anna Feldman, Minwei Feng, Yansong Feng, Olivier Ferret, Oliver Fer-
schke, Simone Filice, Denis Filimonov, Katja Filippova∗, Andrew Finch, Nicolas Fiorini, Orhan
Firat∗, Radu Florian, Evandro Fonseca, Markus Forsberg, Eric Fosler-Lussier, George Foster,
James Foulds∗, Marc Franco-Salvador, Alexander Fraser, Dayne Freitag, Lea Frermann, An-
nemarie Friedrich, Piotr W. Fuglewicz, Akinori Fujino, Fumiyo Fukumoto, Robert Futrelle
</p>
<p>Robert Gaizauskas, Olivier Galibert∗, Irina Galinskaya, Michel Galley∗, Michael Gamon, Kuz-
man Ganchev, Siva Reddy Gangireddy, Jianfeng Gao, Claire Gardent∗, Matt Gardner, Guillermo
Garrido, Justin Garten, Milica Gasic, Eric Gaussier, Tao Ge, Georgi Georgiev, Kallirroi Georgila,
Pablo Gervás∗, George Giannakopoulos, C Lee Giles, Kevin Gimpel∗, Maite Giménez∗, Roxana
Girju, Adrià de Gispert, Dimitra Gkatzia∗, Goran Glavaš, Amir Globerson, Yoav Goldberg, Dan
Goldwasser, Carlos Gómez-Rodríguez∗, Graciela Gonzalez, Edgar Gonzàlez Pellicer, Kyle Gor-
man, Matthew R. Gormley, Isao Goto, Cyril Goutte, Amit Goyal, Kartik Goyal, Pawan Goyal,
Joao Graca, Yvette Graham, Roger Granada, Stephan Greene, Jiatao Gu, Bruno Guillaume, Liane
Guillou, Curry Guinn, Hongyu Guo, James Gung, Jiang Guo, Weiwei Guo, Yufan Guo, Yuhong
Guo, Abhijeet Gupta, Rahul Gupta, Yoan Gutiérrez, Francisco Guzmán,
</p>
<p>xv</p>
<p />
</div>
<div class="page"><p />
<p>Thanh-Le Ha, Christian Hadiwinoto, Gholamreza Haffari, Matthias Hagen, Udo Hahn, Jörg Hak-
enberg, Dilek Hakkani-Tur, Keith Hall, William L. Hamilton, Michael Hammond, Xianpei Han,
Sanda Harabagiu, Christian Hardmeier, Kazi Saidul Hasan, Sadid A. Hasan, Saša Hasan, Eva
Hasler, Hany Hassan, Helen Hastie, Claudia Hauff, He He∗, Hua He, Luheng He, Shizhu He,
Xiaodong He, Yulan He, Peter Heeman, Carmen Heger, Serge Heiden, Georg Heigold, Michael
Heilman, James Henderson, Matthew Henderson, Aron Henriksson, Aurélie Herbelot∗, Ulf Her-
mjakob, Daniel Hershcovich, Jack Hessel, Kristina Hettne, Felix Hieber, Ryuichiro Higashinaka,
Erhard Hinrichs, Tsutomu Hirao, Keikichi Hirose, Julian Hitschler, Cong Duy Vu Hoang, Julia
Hockenmaier, Kai Hong∗, Yu Hong, Ales Horak, Andrea Horbach, Takaaki Hori, Yufang Hou,
Julian Hough, Dirk Hovy∗, Eduard Hovy, Chun-Nan Hsu, Baotian Hu, Yuening Hu, Yuheng Hu,
Hen-Hsen Huang, Hongzhao Huang, Liang Huang, Lifu Huang, Minlie Huang, Ruihong Huang,
Songfang Huang, Xuanjing Huang, Yi-Ting Huang, Luwen Huangfu, Mans Hulden, Tim Hunter,
Seung-won Hwang
</p>
<p>Ignacio Iacobacci, Nancy Ide, Marco Idiart, Gonzalo Iglesias, Ryu Iida, Kenji Imamura, Diana
Inkpen, Naoya Inoue, Hitoshi Isahara, Mohit Iyyer
</p>
<p>Tommi Jaakkola, Cassandra L. Jacobs, Guillaume Jacquet, Evan Jaffe, Jagadeesh Jagarlamudi,
Siddharth Jain, Aren Jansen, Sujay Kumar Jauhar, Laura Jehl, Minwoo Jeong, Yacine Jernite,
Rahul Jha, Donghong Ji, Guoliang Ji, Sittichai Jiampojamarn, Hui Jiang, Antonio Jimeno Yepes,
Salud María Jiménez-Zafra, Richard Johansson, Kyle Johnson, Melvin Johnson Premkumar, Kris-
tiina Jokinen, Arne Jonsson, Aditya Joshi, Mahesh Joshi, Shafiq Joty, Dan Jurafsky∗, David Jur-
gens
</p>
<p>Besim Kabashi, Ákos Kádár, Sylvain Kahane∗, Juliette Kahn, Herman Kamper, Jaap Kamps, Hi-
roshi Kanayama, Hung-Yu Kao, Justine Kao, Mladen Karan, Dimitri Kartsaklis, Arzoo Katiyar,
David Kauchak, Daisuke Kawahara, Anna Kazantseva, Hideto Kazawa, Andrew Kehler, Simon
Keizer, Frank Keller, Casey Kennington, Mitesh M. Khapra, Douwe Kiela, Halil Kilicoglu∗,
Jin-Dong Kim, Jooyeon Kim, Seokhwan Kim, Suin Kim, Yoon Kim, Young-Bum Kim, Irwin
King, Brian Kingsbury, Svetlana Kiritchenko, Dietrich Klakow, Alexandre Klementiev, Sigrid
Klerke, Roman Klinger, Julien Kloetzer, Simon Kocbek, Arne Köhn∗, Daniël de Kok, Prasanth
Kolachina, Varada Kolhatkar, Mamoru Komachi, Kazunori Komatani, Rik Koncel-Kedziorski,
Fang Kong, Lingpeng Kong, Ioannis Konstas∗, Selcuk Kopru, Valia Kordoni, Yannis Korkontze-
los, Bhushan Kotnis, Alexander Kotov, Mikhail Kozhevnikov, Martin Krallinger, Julia Kreutzer∗,
Jayant Krishnamurthy∗, Kriste Krstovski, Canasai Kruengkrai, Germán Kruszewski, Mark Kröll,
Lun-Wei Ku∗, Marco Kuhlmann, Jonas Kuhn, Roland Kuhn, Shankar Kumar, Jonathan K. Kum-
merfeld, Sadao Kurohashi, Polina Kuznetsova, Tom Kwiatkowski,
</p>
<p>Igor Labutov, Wai Lam, Patrik Lambert, Man Lan, Ian Lane, Ni Lao, Mirella Lapata, Shalom
Lappin, Romain Laroche, Kornel Laskowski, Jey Han Lau, Alon Lavie, Angeliki Lazaridou, Phong
Le∗, Joseph Le Roux, Robert Leaman, Kenton Lee, Lung-Hao Lee, Moontae Lee, Sungjin Lee,
Yoong Keok Lee, Young-Suk Lee, Els Lefever, Tao Lei, Jochen L. Leidner, Alessandro Lenci,
Yves Lepage∗, Johannes Leveling, Tomer Levinboim, Gina-Anne Levow∗, Omer Levy∗, Roger
Levy, Dave Lewis, Mike Lewis, Binyang Li, Chen Li, Cheng-Te Li, Chenliang Li, Fangtao Li,
Haizhou Li, Hang Li, Jiwei Li, Junhui Li, Junyi Jessy Li, Lishuang Li, Peifeng Li, Peng Li, Qi Li,
Qing Li, Shaohua Li, Sheng Li, Shoushan Li, Xiaoli Li, Yanran Li, Yunyao Li, Zhenghua Li, Maria
Liakata∗, Kexin Liao∗, Xiangwen Liao, Chin-Yew Lin, Chu-Cheng Lin, Chuan-Jie Lin, Shou-de
Lin, Victoria Lin, Ziheng Lin, Wang Ling, Xiao Ling, Tal Linzen, Christina Lioma, Pierre Lison,
Marina Litvak, Bing Liu, Fei Liu, Hongfang Liu, Jiangming Liu, Lemao Liu, Qian Liu, Qun Liu,
Tie-Yan Liu, Ting Liu, Xiaobing Liu, Yang Liu, Nikola Ljubešić, Chi-kiu Lo, Henning Lobin,
Varvara Logacheva, Lucelene Lopes, Adam Lopez, Oier Lopez de Lacalle, Aurelio Lopez-Lopez,
Annie Louis, Bin Lu, Yi Luan, Andy Luecking, Michal Lukasik, Xiaoqiang Luo, Anh Tuan Luu
</p>
<p>xvi</p>
<p />
</div>
<div class="page"><p />
<p>Ji Ma, Qingsong Ma, Shuming Ma, Xuezhe Ma, Wolfgang Macherey, Nitin Madnani, Saad Ma-
hamood, Cerstin Mahlow, Wolfgang Maier, Prodromos Malakasiotis, Andreas Maletti, Shervin
Malmasi, Titus von der Malsburg, Suresh Manandhar, Gideon Mann, Diego Marcheggiani, Daniel
Marcu, David Mareček, Matthew Marge, Benjamin Marie, Katja Markert, Marie-Catherine de
Marneffe, Erwin Marsi, Patricio Martinez-Barco, André F. T. Martins∗, Sebastian Martschat,
Héctor Martínez Alonso, Eugenio Martínez-Cámara∗, Fernando Martínez-Santiago, Yann Ma-
thet, Shigeki Matsubara, Yuichiroh Matsubayashi, Yuji Matsumoto, Takuya Matsuzaki, Austin
Matthews, Jonathan May, David McClosky, Tara McIntosh, Kathy McKeown, Michael McTear,
Yashar Mehdad, Sameep Mehta, Hongyuan Mei∗, Yelena Mejova, Oren Melamud, Fandong Meng,
Adam Meyers, Yishu Miao, Rada Mihalcea, Todor Mihaylov, Timothy Miller, Tristan Miller∗,
David Mimno, Bonan Min, Zhao-Yan Ming, Shachar Mirkin, Seyed Abolghasem Mirroshan-
del, Abhijit Mishra, Prasenjit Mitra, Makoto Miwa, Daichi Mochihashi, Ashutosh Modi, Marie-
Francine Moens, Samaneh Moghaddam, Abdelrahman Mohamed, Behrang Mohit, Mitra Mo-
htarami, Karo Moilanen, Luis Gerardo Mojica de la Vega, Manuel Montes, Andres Montoyo, Tae-
sun Moon, Michael Moortgat, Roser Morante, Hajime Morita, Lili Mou, Dana Movshovitz-Attias,
Arjun Mukherjee, Philippe Muller, Yugo Murawaki, Brian Murphy, Gabriel Murray∗, Reinhard
Muskens, Sung-Hyon Myaeng
</p>
<p>Masaaki Nagata, Ajay Nagesh, Vinita Nahar, Iftekhar Naim, Tetsuji Nakagawa, Mikio Nakano,
Yukiko Nakano, Ndapandula Nakashole, Ramesh Nallapati, Courtney Napoles, Jason Naradowsky,
Karthik Narasimhan∗, Shashi Narayan, Alexis Nasr, Vivi Nastase, Borja Navarro, Roberto Navigli,
Adeline Nazarenko∗, Mark-Jan Nederhof, Arvind Neelakantan, Sapna Negi, Matteo Negri, Aida
Nematzadeh, Guenter Neumann, Mariana Neves, Denis Newman-Griffis, Dominick Ng, Hwee
Tou Ng, Jun-Ping Ng, Vincent Ng, Dong Nguyen∗, Thien Huu Nguyen, Truc-Vien T. Nguyen,
Viet-An Nguyen, Garrett Nicolai, Massimo Nicosia, Vlad Niculae∗, Jian-Yun Nie, Jan Niehues,
Luis Nieto Piña∗, Ivelina Nikolova, Malvina Nissim∗, Joakim Nivre∗, Hiroshi Noji, Gertjan van
Noord, Joel Nothman
</p>
<p>Brendan O’Connor, Timothy O’Donnell, Yusuke Oda, Stephan Oepen, Kemal Oflazer∗, Alice Oh∗,
Jong-Hoon Oh, Tomoko Ohta, Kiyonori Ohtake, Hidekazu Oiwa, Naoaki Okazaki, Manabu Oku-
mura, Hiroshi G. Okuno, Constantin Orasan, Vicente Ordonez, Petya Osenova, Mari Ostendorf∗,
Myle Ott, Katja Ovchinnikova, Cecilia Ovesdotter Alm
</p>
<p>Muntsa Padró, Valeria de Paiva, Alexis Palmer, Martha Palmer, Alessio Palmero Aprosio, Sinno
Jialin Pan∗, Xiaoman Pan, Denis Paperno, Ankur Parikh, Cecile Paris, Seong-Bae Park, Tommaso
Pasini, Marco Passarotti∗, Peyman Passban, Panupong Pasupat, Siddharth Patwardhan, Michael
J. Paul∗, Adam Pauls, Ellie Pavlick∗, Adam Pease, Pavel Pecina, Ted Pedersen, Nanyun Peng,
Xiaochang Peng, Gerald Penn, Marco Pennacchiotti, Bryan Perozzi, Casper Petersen, Slav Petrov,
Eva Pettersson, Anselmo Peñas, Hieu Pham, Nghia The Pham, Lawrence Phillips, Davide Picca,
Karl Pichotta, Olivier Pietquin, Mohammad Taher Pilehvar, Yuval Pinter, Paul Piwek, Thierry
Poibeau, Tamara Polajnar, Heather Pon-Barry, Simone Paolo Ponzetto, Andrei Popescu-Belis,
Maja Popović, Fred Popowich, François Portet∗, Matt Post∗, Christopher Potts, Vinodkumar Prab-
hakaran, Daniel Preoţiuc-Pietro, Emily Prud’hommeaux∗, Laurent Prévot, Jay Pujara, Matthew
Purver, James Pustejovsky
</p>
<p>Juan Antonio Pérez-Ortiz, Ashequl Qadir, Peng Qi, Longhua Qian, Xian Qian, Lu Qin, Long
Qiu, Xipeng Qiu, Lizhen Qu, Ariadna Quattoni, Chris Quirk∗ Alexandre Rademaker, Will Rad-
ford, Alessandro Raganato, Afshin Rahimi∗, Altaf Rahman, Maya Ramanath, Rohan Ramanath,
A Ramanathan, Arti Ramesh, Gabriela Ramirez-de-la-Rosa, Carlos Ramisch, Anita Ramm, Vivek
Kumar Rangarajan Sridhar, Ari Rappoport, Mohammad Sadegh Rasooli, Pushpendre Rastogi, An-
</p>
<p>xvii</p>
<p />
</div>
<div class="page"><p />
<p>toine Raux, Sravana Reddy, Ines Rehbein∗, Georg Rehm, Roi Reichart, Ehud Reiter, Zhaochun
Ren, Corentin Ribeyre, Matthew Richardson, Martin Riedl, Jason Riesa, German Rigau, Ellen
Riloff∗, Laura Rimell∗, Alan Ritter, Brian Roark∗, Molly Roberts, Tim Rocktäschel, Oleg Rokhlenko,
Salvatore Romeo, Andrew Rosenberg, Sara Rosenthal∗, Paolo Rosso, Benjamin Roth, Michael
Roth, Sascha Rothe, Masoud Rouhizadeh, Mickael Rouvier, Alla Rozovskaya, Josef Ruppenhofer,
Delia Rusu, Attapol Rutherford
</p>
<p>Mrinmaya Sachan, Kugatsu Sadamitsu, Fatiha Sadat, Mehrnoosh Sadrzadeh, Markus Saers, Kenji
Sagae, Horacio Saggion, Saurav Sahay, Magnus Sahlgren, Patrick Saint-dizier, Hassan Sajjad,
Sakriani Sakti, Mohammad Salameh, Bahar Salehi, Avneesh Saluja, Rajhans Samdani, Mark
Sammons, Germán Sanchis-Trilles, Ryohei Sasano, Agata Savary∗, Asad Sayeed, Carolina Scar-
ton, Tatjana Scheffler, Christian Scheible, David Schlangen, Natalie Schluter, Allen Schmaltz∗,
Helmut Schmid, Alexandra Schofield, William Schuler, Sebastian Schuster, Lane Schwartz, Roy
Schwartz∗, Christof Schöch, Diarmuid Ó Séaghdha, Djamé Seddah, Abigail See, Nina Seemann,
Satoshi Sekine, Mark Seligman, Minjoon Seo, Burr Settles, Lei Sha, Kashif Shah, Rebecca Sharp,
Shiqi Shen, Shuming Shi, Hiroyuki Shindo, Koichi Shinoda, Chaitanya Shivade, Eyal Shnarch∗,
Milad Shokouhi, Ekaterina Shutova, Advaith Siddharthan, Avirup Sil, Carina Silberer, Yanchuan
Sim, Patrick Simianer, Kiril Simov, Kairit Sirts, Amy Siu, Gabriel Skantze, Kevin Small, Noah
A. Smith, Pavel Smrz, Richard Socher, Artem Sokolov, Thamar Solorio, Swapna Somasundaran,
Hyun-Je Song, Min Song, Sa-kwang Song, Yang Song, Yangqiu Song, Radu Soricut, Aitor Soroa,
Matthias Sperber, Caroline Sporleder, Vivek Srikumar, Somayajulu Sripada, Shashank Srivastava,
Edward Stabler, Jan Šnajder∗, Sanja Štajner, Gabriel Stanovsky∗, Manfred Stede, Mark Steed-
man, Josef Steinberger, Amanda Stent, Mark Stevenson, Brandon Stewart, Matthew Stone, Svet-
lana Stoyanchev, Veselin Stoyanov, Carlo Strapparava, Karl Stratos, Kristina Striegnitz∗, Emma
Strubell, Tomek Strzalkowski, Sara Stymne, Maik Stührenberg, Jinsong Su, Keh-Yih Su, Yu Su,
L V Subramaniam, Katsuhito Sudoh, Ang Sun, Huan Sun, Le Sun, Weiwei Sun, Xu Sun, Simon
Suster, Hisami Suzuki, Jun Suzuki, Yoshimi Suzuki, Swabha Swayamdipta, Stan Szpakowicz, Idan
Szpektor, Felipe Sánchez-Martínez, Pascale Sébillot, Anders Søgaard
</p>
<p>Prasad Tadepalli, Kaveh Taghipour, Hiroya Takamura, David Talbot, Partha Talukdar, Aleš Tam-
chyna, Akihiro Tamura, Chenhao Tan∗, Liling Tan, Niket Tandon, Duyu Tang, Jiliang Tang,
Christoph Teichmann, Serra Sinem Tekiroglu, Irina Temnikova, Joel Tetreault, Kapil Thadani∗,
Sam Thomson, Jörg Tiedemann, Ivan Titov, Katrin Tomanek, Gaurav Singh Tomar, Marc Tom-
linson, Sara Tonelli, Antonio Toral, Kentaro Torisawa, Ke M. Tran, Isabel Trancoso, Ming-Feng
Tsai, Richard Tzong-Han Tsai, Reut Tsarfaty∗, Oren Tsur, Yoshimasa Tsuruoka, Yulia Tsvetkov,
Cunchao Tu, Zhaopeng Tu, Gokhan Tur, Marco Turchi, Ferhan Ture, Oscar Täckström
</p>
<p>Raghavendra Udupa, Stefan Ultes, Lyle Ungar, Shyam Upadhyay, L. Alfonso Urena Lopez, Olga
Uryupina
</p>
<p>Alessandro Valitutti∗, Benjamin Van Durme∗, Tim Van de Cruys, Lucy Vanderwende∗, Vasudeva
Varma, Paola Velardi, Sumithra Velupillai, Sriram Venkatapathy, Yannick Versley, Tim Vieira,
David Vilar, Martín Villalba∗, Veronika Vincze, Sami Virpioja∗, Andreas Vlachos, Rob Voigt,
Ngoc Thang Vu, Ivan Vulić, Yogarshi Vyas, V.G.Vinod Vydiswaran, Ekaterina Vylomova
</p>
<p>Houfeng Wang, Henning Wachsmuth, Joachim Wagner, Matthew Walter∗, Stephen Wan, Xiaojun
Wan, Baoxun Wang, Chang Wang, Chong Wang, Dingquan Wang, Hongning Wang, Lu Wang,
Mingxuan Wang, Pidong Wang, Rui Wang, Shaojun Wang, Tong Wang, Yu-Chun Wang, Wei
Wang, Wenya Wang, William Yang Wang, Xiaolin Wang, Xiaolong Wang, Yiou Wang, Zhiguo
Wang, Zhongqing Wang∗, Leo Wanner, Nigel Ward, Shinji Watanabe, Taro Watanabe, Aleksander
Wawer, Bonnie Webber, Ingmar Weber, Julie Weeds, Furu Wei, Zhongyu Wei, Gerhard Weikum,
David Weir, Michael White, Antoine Widlöcher, Michael Wiegand∗, Jason D Williams∗, Shuly
Wintner, Sam Wiseman, Michael Witbrock, Silke Witt-Ehsani, Travis Wolfe, Kam-Fai Wong, Jian
Wu, Yuanbin Wu, Joern Wuebker
</p>
<p>xviii</p>
<p />
</div>
<div class="page"><p />
<p>Aris Xanthos, Rui Xia, Yunqing Xia, Bing Xiang, Min Xiao, Tong Xiao, Xinyan Xiao, Boyi Xie,
Pengtao Xie, Shasha Xie, Chenyan Xiong, Feiyu Xu, Hua Xu, Ruifeng Xu, Wei Xu, Wenduan Xu,
Huichao Xue, Nianwen Xue
</p>
<p>Yadollah Yaghoobzadeh, Ichiro Yamada, Bishan Yang, Cheng Yang, Diyi Yang, Grace Hui Yang,
Min Yang, Yaqin Yang, Yi Yang, Roman Yangarber, Mark Yatskar, Meliha Yetisgen, Wen-tau Yih,
Pengcheng Yin, Wenpeng Yin, Anssi Yli-Jyrä, Dani Yogatama, Naoki Yoshinaga, Bei Yu, Dian
Yu, Dianhai Yu, Kai Yu, Liang-Chih Yu, Mo Yu, Zhou Yu, François Yvon
</p>
<p>Marcos Zampieri, Menno van Zaanen, Fabio Massimo Zanzotto, Amir Zeldes∗, Daojian Zeng,
Xiaodong Zeng, Kalliopi Zervanou∗, Luke Zettlemoyer, Deniz Zeyrek, Feifei Zhai, Congle Zhang,
Dongdong Zhang, Guchun Zhang, Jiajun Zhang, Jian Zhang, Jianwen Zhang, Lei Zhang, Meishan
Zhang, Min Zhang, Qi Zhang, Renxian Zhang, Sicong Zhang, Wei Zhang∗, Zhisong Zhang, Bing
Zhao, Dongyan Zhao, Jun Zhao, Shiqi Zhao, Tiejun Zhao, Wayne Xin Zhao, Alisa Zhila, Guodong
Zhou, Xinjie Zhou, Muhua Zhu, Xiaodan Zhu, Xiaoning Zhu, Ayah Zirikly, Chengqing Zong,
Bowei Zou
</p>
<p>Secondary Reviewers
</p>
<p>Naveed Afzal, Yamen Ajjour
</p>
<p>Jeremy Barnes, Joost Bastings, Joachim Bingel, Luana Bulat
</p>
<p>Iacer Calixto, Lea Canales, Kai Chen, Tongfei Chen, Hao Cheng, Jianpeng Cheng, Yiming Cui
</p>
<p>Marco Damonte, Saman Daneshvar, Tobias Domhan, Daxiang Dong, Li Dong
</p>
<p>Mohamed Eldesouki
</p>
<p>Stefano Faralli, Bin Fu
</p>
<p>Srinivasa P. Gadde, Qiaozi Gao, Luca Gilardi, Sujatha Das Gollapalli, J. Manuel Gomez, Stig-Arne
Grönroos, Lin Gui
</p>
<p>Casper Hansen, Lihong He, Martin Horn
</p>
<p>Oana Inel
</p>
<p>Gongye Jin
</p>
<p>Roman Kern, Vaibhav Kesarwani, Joo-Kyung Kim, Seongchan Kim, Christine Köhn, Santosh
Kosgi
</p>
<p>Ronja Laarmann-Quante, Egoitz Laparra, Anais Lefeuvre-Halftermeyer, Guanlin Li, Jing Li, Min-
glei Li, Xiang Li, Xiaolong Li, Chen Liang, Ming Liao, Sijia Liu, Pranay Lohia
</p>
<p>Chunpeng Ma, Shuming Ma, Tengfei Ma, Ana Marasovic
</p>
<p>Toan Nguyen, Eric Nichols, Sergiu Nisioi
</p>
<p>Gözde Özbal
</p>
<p>Alexis Palmer, Suraj Pandey, Nikolaos Pappas, José M. Perea-Ortega, Marten Postma
</p>
<p>Longhua Qian
</p>
<p>Masoud Rouhizadeh Abeed Sarker, Andrew Schneider, Roxane Segers, Pararth Shah, Samiulla
Shaikh, Xing Shi, Tomohide Shibata, Samiulla Shiekh, Miikka Silfverberg
</p>
<p>Bo Wang∗, Boli Wang, Jianxiang Wang, Jingjing Wang, Rui Wang, Shuai Wang, Shuting Wang,
Tsung-Hsien Wen, John Wieting
</p>
<p>Nan Yang, Yi Yang, Mark Yatskar, Yichun Yin
</p>
<p>Sheng Zhang, Kai Zhao, Imed Zitouni
</p>
<p>xix</p>
<p />
</div>
<div class="page"><p />
<p>Outstanding Papers
</p>
<p>With twin upward trends in the interest in computational linguistics and natural language processing
and the size of our annual meeting, ACL has begun the practice of recognizing outstanding papers that
represent a select cross-section of the entire field, as nominated by reviewers and vetted by the area chairs
and program co-chairs. These papers have been centrally located in the program, on the last day of our
meeting, in a more focused two parallel tracks format.
</p>
<p>This year, we have nominated 15 long papers and 7 short papers, representing 1.8% of all submissions
and approximately 5% of the accepted ACL program. Congratulations, authors!
</p>
<p>(in alphabetical order by first author surname)
</p>
<p>Long Papers
</p>
<p>• Jan Buys and Phil Blunsom. Robust Incremental Neural Semantic Graph Parsing.
• Xinchi Chen, Zhan Shi, Xipeng Qiu and Xuanjing Huang. Adversarial Multi-Criteria Learn-
</p>
<p>ing for Chinese Word Segmentation.
</p>
<p>• Ryan Cotterell and Jason Eisner. Probabilistic Typology: Deep Generative Models of Vowel
Inventories.
</p>
<p>• Yanzhuo Ding, Yang Liu, Huanbo Luan and Maosong Sun. Visualizing and Understanding
Neural Machine Translation.
</p>
<p>• Milan Gritta, Mohammad Taher Pilehvar, Nut Limsopatham and Nigel Collier. Vancouver
Welcomes You! Minimalist Location Metonymy Resolution.
</p>
<p>• Daniel Hershcovich, Omri Abend and Ari Rappoport. A Transition-Based Directed Acyclic
Graph Parser for UCCA.
</p>
<p>• Shuhei Kurita, Daisuke Kawahara and Sadao Kurohashi. Neural Joint Model for Transition-
based Chinese Syntactic Analysis.
</p>
<p>• Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua
Bengio and Joelle Pineau. Towards an Automatic Turing Test: Learning to Evaluate Dialogue
Responses.
</p>
<p>• Yasuhide Miura, Motoki Taniguchi, Tomoki Taniguchi and Tomoko Ohkuma. Unifying Text,
Metadata, and User Network Representations with a Neural Network for Geolocation Pre-
diction.
</p>
<p>• Ramakanth Pasunuru and Mohit Bansal. Multi-Task Video Captioning with Visual and En-
tailment Generation.
</p>
<p>• Maxim Rabinovich, Mitchell Stern and Dan Klein. Abstract Syntax Networks for Code Gen-
eration and Semantic Parsing.
</p>
<p>• Ines Rehbein and Josef Ruppenhofer. Detecting annotation noise in automatically labelled
data.
</p>
<p>• Jiwei Tan, Xiaojun Wan and Jianguo Xiao. Abstractive Document Summarization with a
Graph-Based Attentional Neural Model.
</p>
<p>• Mingbin Xu, Hui Jiang and Sedtawut Watcharawittayakul. A Local Detection Approach for
Named Entity Recognition and Mention Detection.
</p>
<p>• Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou and Bo Xu. Joint
Extraction of Entities and Relations Based on a Novel Tagging Scheme.
</p>
<p>xx</p>
<p />
</div>
<div class="page"><p />
<p>Short Papers
</p>
<p>• Xinyu Hua and Lu Wang. Understanding and Detecting Diverse Supporting Arguments on
Controversial Issues.
</p>
<p>• Jindřich Libovický and Jindřich Helcl. Attention Strategies for Multi-Source Sequence-to-
Sequence Learning.
</p>
<p>• Bogdan Ludusan, Reiko Mazuka, Mathieu Bernard, Alejandrina Cristia and Emmanuel Dupoux.
The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling
Perspective.
</p>
<p>• Afshin Rahimi, Trevor Cohn and Timothy Baldwin. A Neural Model for User Geolocation
and Lexical Dialectology.
</p>
<p>• Keisuke Sakaguchi, Matt Post and Benjamin Van Durme. Error-repair Dependency Parsing
for Ungrammatical Texts.
</p>
<p>• Alane Suhr, Mike Lewis, James Yeh and Yoav Artzi. A Corpus of Compositional Language
for Visual Reasoning.
</p>
<p>• Yizhong Wang, Sujian Li and Houfeng Wang. A Two-stage Parsing Method for Text-level
Discourse Analysis.
</p>
<p>xxi</p>
<p />
</div>
<div class="page"><p />
</div>
<div class="page"><p />
<p>Table of Contents
</p>
<p>Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths
Fei Cheng and Yusuke Miyao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>AMR-to-text Generation with Synchronous Node Replacement Grammar
Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea . . . . . . . . . . . . . . . . . . 7
</p>
<p>Lexical Features in Coreference Resolution: To be Used With Caution
Nafise Sadat Moosavi and Michael Strube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
</p>
<p>Alternative Objective Functions for Training MT Evaluation Metrics
Miloš Stanojević and Khalil Sima’an . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
</p>
<p>A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against
Human Judgments
</p>
<p>Maxime Peyrard and Judith Eckle-Kohler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
</p>
<p>Vector space models for evaluating semantic fluency in autism
Emily Prud’hommeaux, Jan van Santen and Douglas Gliner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
</p>
<p>Neural Architectures for Multilingual Semantic Parsing
Raymond Hendy Susanto and Wei Lu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
</p>
<p>Incorporating Uncertainty into Deep Learning for Spoken Language Assessment
Andrey Malinin, Anton Ragni, Kate Knill and Mark Gales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>Incorporating Dialectal Variability for Socially Equitable Language Identification
David Jurgens, Yulia Tsvetkov and Dan Jurafsky . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
</p>
<p>Evaluating Compound Splitters Extrinsically with Textual Entailment
Glorianna Jagfeld, Patrick Ziering and Lonneke van der Plas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
</p>
<p>An Analysis of Action Recognition Datasets for Language and Vision Tasks
Spandana Gella and Frank Keller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
</p>
<p>Learning to Parse and Translate Improves Neural Machine Translation
Akiko Eriguchi, Yoshimasa Tsuruoka and Kyunghyun Cho . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
</p>
<p>On the Distribution of Lexical Features at Multiple Levels of Analysis
Fatemeh Almodaresi, Lyle Ungar, Vivek Kulkarni, Mohsen Zakeri, Salvatore Giorgi and H. Andrew
</p>
<p>Schwartz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
</p>
<p>Exploring Neural Text Simplification Models
Sergiu Nisioi, Sanja Štajner, Simone Paolo Ponzetto and Liviu P. Dinu . . . . . . . . . . . . . . . . . . . . . . . 85
</p>
<p>On the Challenges of Translating NLP Research into Commercial Products
Daniel Dahlmeier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
</p>
<p>Sentence Alignment Methods for Improving Text Simplification Systems
Sanja Štajner, Marc Franco-Salvador, Simone Paolo Ponzetto, Paolo Rosso and Heiner Stucken-
</p>
<p>schmidt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .97
</p>
<p>Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection
Youxuan Jiang, Jonathan K. Kummerfeld and Walter S. Lasecki . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
</p>
<p>xxiii</p>
<p />
</div>
<div class="page"><p />
<p>Arc-swift: A Novel Transition System for Dependency Parsing
Peng Qi and Christopher D. Manning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>A Generative Parser with a Discriminative Recognition Algorithm
Jianpeng Cheng, Adam Lopez and Mirella Lapata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Transla-
tion
</p>
<p>Weiyue Wang, Tamer Alkhouli, Derui Zhu and Hermann Ney . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>Towards String-To-Tree Neural Machine Translation
Roee Aharoni and Yoav Goldberg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
</p>
<p>Learning Lexico-Functional Patterns for First-Person Affect
Lena Reed, Jiaqi Wu, Shereen Oraby, Pranav Anand and Marilyn Walker . . . . . . . . . . . . . . . . . . . .141
</p>
<p>Lifelong Learning CRF for Supervised Aspect Extraction
Lei Shu, Hu Xu and Bing Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
</p>
<p>Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization
Ye Zhang, Matthew Lease and Byron C. Wallace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
</p>
<p>Improving Neural Parsing by Disentangling Model Combination and Reranking Effects
Daniel Fried, Mitchell Stern and Dan Klein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
</p>
<p>Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function
Oren Melamud and Jacob Goldberger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
</p>
<p>Implicitly-Defined Neural Networks for Sequence Labeling
Michaeel Kazi and Brian Thompson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
</p>
<p>The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective
Bogdan Ludusan, Reiko Mazuka, Mathieu Bernard, Alejandrina Cristia and Emmanuel Dupoux178
</p>
<p>A Two-Stage Parsing Method for Text-Level Discourse Analysis
Yizhong Wang, Sujian Li and Houfeng Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
</p>
<p>Error-repair Dependency Parsing for Ungrammatical Texts
Keisuke Sakaguchi, Matt Post and Benjamin Van Durme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
</p>
<p>Attention Strategies for Multi-Source Sequence-to-Sequence Learning
Jindřich Libovický and Jindřich Helcl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
</p>
<p>Understanding and Detecting Diverse Supporting Arguments on Controversial Issues
Xinyu Hua and Lu Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
</p>
<p>A Neural Model for User Geolocation and Lexical Dialectology
Afshin Rahimi, Trevor Cohn and Timothy Baldwin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
</p>
<p>A Corpus of Natural Language for Visual Reasoning
Alane Suhr, Mike Lewis, James Yeh and Yoav Artzi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
</p>
<p>Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative
Containers
</p>
<p>Julien Tourille, Olivier Ferret, Aurelie Neveol and Xavier Tannier . . . . . . . . . . . . . . . . . . . . . . . . . . 224
</p>
<p>xxiv</p>
<p />
</div>
<div class="page"><p />
<p>How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Mod-
els
</p>
<p>Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yansong Feng and Dongyan Zhao . . . . . . . . . . . 231
</p>
<p>Cross-lingual and cross-domain discourse segmentation of entire documents
Chloé Braud, Ophélie Lacroix and Anders Søgaard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?
Beata Beigman Klebanov, Binod Gyawali and Yi Song . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
</p>
<p>Argumentation Quality Assessment: Theory vs. Practice
Henning Wachsmuth, Nona Naderi, Ivan Habernal, Yufang Hou, Graeme Hirst, Iryna Gurevych and
</p>
<p>Benno Stein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
</p>
<p>A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations
Samuel Rönnqvist, Niko Schenk and Christian Chiarcos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
</p>
<p>Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure The-
ory Framework
</p>
<p>Xinhao Wang, James Bruno, Hillary Molloy, Keelan Evanini and Klaus Zechner . . . . . . . . . . . . . 263
</p>
<p>Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings
Changxing Wu, Xiaodong Shi, Yidong Chen, Jinsong Su and Boli Wang . . . . . . . . . . . . . . . . . . . . 269
</p>
<p>Oracle Summaries of Compressive Summarization
Tsutomu Hirao, Masaaki Nishino and Masaaki Nagata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
</p>
<p>Japanese Sentence Compression with a Large Training Dataset
Shun Hasegawa, Yuta Kikuchi, Hiroya Takamura and Manabu Okumura . . . . . . . . . . . . . . . . . . . . 281
</p>
<p>A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes
Pablo Loyola, Edison Marrese-Taylor and Yutaka Matsuo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .287
</p>
<p>English Event Detection With Translated Language Features
Sam Wei, Igor Korostil, Joel Nothman and Ben Hachey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
</p>
<p>EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering
Denis Savenkov and Eugene Agichtein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
</p>
<p>Pocket Knowledge Base Population
Travis Wolfe, Mark Dredze and Benjamin Van Durme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
</p>
<p>Answering Complex Questions Using Open Information Extraction
Tushar Khot, Ashish Sabharwal and Peter Clark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
</p>
<p>Bootstrapping for Numerical Open IE
Swarnadeep Saha, Harinder Pal and Mausam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
</p>
<p>Feature-Rich Networks for Knowledge Base Completion
Alexandros Komninos and Suresh Manandhar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
</p>
<p>Fine-Grained Entity Typing with High-Multiplicity Assignments
Maxim Rabinovich and Dan Klein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
</p>
<p>Group Sparse CNNs for Question Classification with Answer Sets
Mingbo Ma, Liang Huang, Bing Xiang and Bowen Zhou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
</p>
<p>xxv</p>
<p />
</div>
<div class="page"><p />
<p>Multi-Task Learning of Keyphrase Boundary Classification
Isabelle Augenstein and Anders Søgaard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
</p>
<p>Cardinal Virtues: Extracting Relation Cardinalities from Text
Paramita Mirza, Simon Razniewski, Fariz Darari and Gerhard Weikum . . . . . . . . . . . . . . . . . . . . . 347
</p>
<p>Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets
Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy Puzikov, Ido Dagan and Iryna Gurevych . . . 352
</p>
<p>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks
Rajarshi Das, Manzil Zaheer, Siva Reddy and Andrew McCallum . . . . . . . . . . . . . . . . . . . . . . . . . . 358
</p>
<p>Differentiable Scheduled Sampling for Credit Assignment
Kartik Goyal, Chris Dyer and Taylor Berg-Kirkpatrick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
</p>
<p>A Deep Network with Visual Text Composition Behavior
Hongyu Guo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
</p>
<p>Neural System Combination for Machine Translation
Long Zhou, Wenpeng Hu, Jiajun Zhang and Chengqing Zong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
</p>
<p>An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation
Chenhui Chu, Raj Dabre and Sadao Kurohashi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
</p>
<p>Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings
Benjamin Marie and Atsushi Fujita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
</p>
<p>Feature Hashing for Language and Dialect Identification
Shervin Malmasi and Mark Dras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
</p>
<p>Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM
Yow-Ting Shiue, Hen-Hsen Huang and Hsin-Hsi Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
</p>
<p>Automatic Compositor Attribution in the First Folio of Shakespeare
Maria Ryskina, Hannah Alpert-Abrams, Dan Garrette and Taylor Berg-Kirkpatrick . . . . . . . . . . 411
</p>
<p>STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset
Yuya Yoshikawa, Yutaro Shigeto and Akikazu Takeuchi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
</p>
<p>"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection
William Yang Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
</p>
<p>English Multiword Expression-aware Dependency Parsing Including Named Entities
Akihiko Kato, Hiroyuki Shindo and Yuji Matsumoto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
</p>
<p>Improving Semantic Composition with Offset Inference
Thomas Kober, Julie Weeds, Jeremy Reffin and David Weir . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
</p>
<p>Learning Topic-Sensitive Word Representations
Marzieh Fadaee, Arianna Bisazza and Christof Monz. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .441
</p>
<p>Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings
Terrence Szymanski . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
</p>
<p>Methodical Evaluation of Arabic Word Embeddings
Mohammed Elrazzaz, Shady Elbassuoni, Khaled Shaban and Chadi Helwe . . . . . . . . . . . . . . . . . . 454
</p>
<p>xxvi</p>
<p />
</div>
<div class="page"><p />
<p>Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and
Forecast
</p>
<p>Hannah Rashkin, Eric Bell, Yejin Choi and Svitlana Volkova . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
</p>
<p>Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation
Svetlana Kiritchenko and Saif Mohammad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
</p>
<p>Demographic Inference on Twitter using Recursive Neural Networks
Sunghwan Mac Kim, Qiongkai Xu, Lizhen Qu, Stephen Wan and Cecile Paris . . . . . . . . . . . . . . . 471
</p>
<p>Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning
Prashanth Vijayaraghavan, Soroush Vosoughi and Deb Roy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
</p>
<p>A Network Framework for Noisy Label Aggregation in Social Media
Xueying Zhan, Yaowei Wang, Yanghui Rao, Haoran Xie, Qing Li, Fu Lee Wang and Tak-Lam
</p>
<p>Wong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
</p>
<p>Parser Adaptation for Social Media by Integrating Normalization
Rob van der Goot and Gertjan van Noord . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
</p>
<p>AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine
Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan Chen, Weipeng Zhao, Haiqing Chen, Jun
</p>
<p>Huang and Wei Chu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498
</p>
<p>A Conditional Variational Framework for Dialog Generation
Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi Niu, Yang Zhao, Akiko Aizawa and Guoping
</p>
<p>Long . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
</p>
<p>Question Answering through Transfer Learning from Large Fine-grained Supervision Data
Sewon Min, Minjoon Seo and Hannaneh Hajishirzi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
</p>
<p>Self-Crowdsourcing Training for Relation Extraction
Azad Abad, Moin Nabi and Alessandro Moschitti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518
</p>
<p>A Generative Attentional Neural Network Model for Dialogue Act Classification
Quan Hung Tran, Gholamreza Haffari and Ingrid Zukerman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524
</p>
<p>Salience Rank: Efficient Keyphrase Extraction with Topic Modeling
Nedelina Teneva and Weiwei Cheng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
</p>
<p>List-only Entity Linking
Ying Lin, Chin-Yew Lin and Heng Ji . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
</p>
<p>Improving Native Language Identification by Using Spelling Errors
Lingzhen Chen, Carlo Strapparava and Vivi Nastase. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .542
</p>
<p>Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model
Paria Jamshid Lou and Mark Johnson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
</p>
<p>On the Equivalence of Holographic and Complex Embeddings for Link Prediction
Katsuhiko Hayashi and Masashi Shimbo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
</p>
<p>Sentence Embedding for Neural Machine Translation Domain Adaptation
Rui Wang, Andrew Finch, Masao Utiyama and Eiichiro Sumita . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
</p>
<p>xxvii</p>
<p />
</div>
<div class="page"><p />
<p>Data Augmentation for Low-Resource Neural Machine Translation
Marzieh Fadaee, Arianna Bisazza and Christof Monz. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .567
</p>
<p>Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary
Xing Shi and Kevin Knight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574
</p>
<p>Chunk-Based Bi-Scale Decoder for Neural Machine Translation
Hao Zhou, Zhaopeng Tu, Shujian Huang, Xiaohua Liu, Hang Li and Jiajun Chen . . . . . . . . . . . . 580
</p>
<p>Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary
Meng Fang and Trevor Cohn. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587
</p>
<p>EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text
Claudio Delli Bovi, Jose Camacho-Collados, Alessandro Raganato and Roberto Navigli . . . . . . 594
</p>
<p>Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and
Part-of-Speech Tagging
</p>
<p>Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Ahmed Abdelali, Yonatan Belinkov and Stephan Vogel
601
</p>
<p>Fast and Accurate Neural Word Segmentation for Chinese
Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu and Feiyue Huang. . . . . . . . . . . . .608
</p>
<p>Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task
Zheng Cai, Lifu Tu and Kevin Gimpel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616
</p>
<p>Neural Semantic Parsing over Multiple Knowledge-bases
Jonathan Herzig and Jonathan Berant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623
</p>
<p>Representing Sentences as Low-Rank Subspaces
Jiaqi Mu, Suma Bhat and Pramod Viswanath . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
</p>
<p>Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Sum-
marization
</p>
<p>Shuming Ma, Xu Sun, Jingjing Xu, Houfeng Wang, Wenjie Li and Qi Su. . . . . . . . . . . . . . . . . . . .635
</p>
<p>Determining Whether and When People Participate in the Events They Tweet About
Krishna Chaitanya Sanagavarapu, Alakananda Vempala and Eduardo Blanco . . . . . . . . . . . . . . . . 641
</p>
<p>Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on
Twitter
</p>
<p>Svitlana Volkova, Kyle Shaffer, Jin Yea Jang and Nathan Hodas . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647
</p>
<p>Recognizing Counterfactual Thinking in Social Media Texts
Youngseo Son, Anneke Buffone, Joe Raso, Allegra Larche, Anthony Janocko, Kevin Zembroski,
</p>
<p>H. Andrew Schwartz and Lyle Ungar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
</p>
<p>Temporal Orientation of Tweets for Predicting Income of Users
Mohammed Hasanuzzaman, Sabyasachi Kamila, Mandeep Kaur, Sriparna Saha and Asif Ekbal659
</p>
<p>Character-Aware Neural Morphological Disambiguation
Alymzhan Toleu, Gulmira Tolegen and Aibek Makazhanov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666
</p>
<p>Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Mor-
phologically Rich Languages
</p>
<p>Xiang Yu and Ngoc Thang Vu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672
</p>
<p>xxviii</p>
<p />
</div>
<div class="page"><p />
<p>How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers
Željko Agić and Natalie Schluter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679
</p>
<p>xxix</p>
<p />
</div>
<div class="page"><p />
</div>
<div class="page"><p />
<p>Conference Program
</p>
<p>Monday, July 31st
</p>
<p>11:46–11:58 Session 1A: Information Extraction 1 (NN)
</p>
<p>11:46–11:58 Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths
Fei Cheng and Yusuke Miyao
</p>
<p>11:46–11:58 Session 1B: Semantics 1
</p>
<p>11:46–11:58 AMR-to-text Generation with Synchronous Node Replacement Grammar
Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea
</p>
<p>11:46–11:58 Session 1C: Discourse 1
</p>
<p>11:46–11:58 Lexical Features in Coreference Resolution: To be Used With Caution
Nafise Sadat Moosavi and Michael Strube
</p>
<p>11:46–11:58 Session 1D: Machine Translation 1
</p>
<p>11:46–11:58 Alternative Objective Functions for Training MT Evaluation Metrics
Miloš Stanojević and Khalil Sima’an
</p>
<p>xxxi</p>
<p />
</div>
<div class="page"><p />
<p>Monday, July 31st (continued)
</p>
<p>11:46–11:58 Session 1E: Generation 1
</p>
<p>11:46–11:58 A Principled Framework for Evaluating Summarizers: Comparing Models of Sum-
mary Quality against Human Judgments
Maxime Peyrard and Judith Eckle-Kohler
</p>
<p>17:00–17:12 Session 3A: Information Extraction 2 / Biomedical 1
</p>
<p>17:00–17:12 Vector space models for evaluating semantic fluency in autism
Emily Prud’hommeaux, Jan van Santen and Douglas Gliner
</p>
<p>17:00–17:12 Session 3B: Semantics 2 (NN)
</p>
<p>17:00–17:12 Neural Architectures for Multilingual Semantic Parsing
Raymond Hendy Susanto and Wei Lu
</p>
<p>17:00–17:12 Session 3C: Speech 1 / Dialogue 1
</p>
<p>17:00–17:12 Incorporating Uncertainty into Deep Learning for Spoken Language Assessment
Andrey Malinin, Anton Ragni, Kate Knill and Mark Gales
</p>
<p>17:00–17:12 Session 3D: Multilingual 1
</p>
<p>17:00–17:12 Incorporating Dialectal Variability for Socially Equitable Language Identification
David Jurgens, Yulia Tsvetkov and Dan Jurafsky
</p>
<p>xxxii</p>
<p />
</div>
<div class="page"><p />
<p>Monday, July 31st (continued)
</p>
<p>17:00–17:12 Session 3E: Phonology 1
</p>
<p>17:00–17:12 Evaluating Compound Splitters Extrinsically with Textual Entailment
Glorianna Jagfeld, Patrick Ziering and Lonneke van der Plas
</p>
<p>Tuesday, August 1st
</p>
<p>11:46–12:04 Session 4B: Cognitive Modelling 1 / Vision 2
</p>
<p>11:46–12:04 An Analysis of Action Recognition Datasets for Language and Vision Tasks
Spandana Gella and Frank Keller
</p>
<p>11:46–12:04 Session 4D: Machine Translation 2
</p>
<p>11:46–12:04 Learning to Parse and Translate Improves Neural Machine Translation
Akiko Eriguchi, Yoshimasa Tsuruoka and Kyunghyun Cho
</p>
<p>11:46–12:04 Session 4E: Social Media 1
</p>
<p>11:46–12:04 On the Distribution of Lexical Features at Multiple Levels of Analysis
Fatemeh Almodaresi, Lyle Ungar, Vivek Kulkarni, Mohsen Zakeri, Salvatore Giorgi
and H. Andrew Schwartz
</p>
<p>xxxiii</p>
<p />
</div>
<div class="page"><p />
<p>Tuesday, August 1st (continued)
</p>
<p>13:30–15:02 Session 5A: Multidisciplinary 1
</p>
<p>13:30–13:48 Exploring Neural Text Simplification Models
Sergiu Nisioi, Sanja Štajner, Simone Paolo Ponzetto and Liviu P. Dinu
</p>
<p>14:40–15:02 On the Challenges of Translating NLP Research into Commercial Products
Daniel Dahlmeier
</p>
<p>14:27–15:02 Session 5B: Language and Resources 1
</p>
<p>14:27–14:39 Sentence Alignment Methods for Improving Text Simplification Systems
Sanja Štajner, Marc Franco-Salvador, Simone Paolo Ponzetto, Paolo Rosso and
Heiner Stuckenschmidt
</p>
<p>14:40–15:02 Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection
Youxuan Jiang, Jonathan K. Kummerfeld and Walter S. Lasecki
</p>
<p>14:27–15:02 Session 5C: Syntax 2 (NN)
</p>
<p>14:27–14:39 Arc-swift: A Novel Transition System for Dependency Parsing
Peng Qi and Christopher D. Manning
</p>
<p>14:40–15:02 A Generative Parser with a Discriminative Recognition Algorithm
Jianpeng Cheng, Adam Lopez and Mirella Lapata
</p>
<p>xxxiv</p>
<p />
</div>
<div class="page"><p />
<p>Tuesday, August 1st (continued)
</p>
<p>14:27–15:02 Session 5D: Machine Translation 3 (NN)
</p>
<p>14:27–14:39 Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical
Machine Translation
Weiyue Wang, Tamer Alkhouli, Derui Zhu and Hermann Ney
</p>
<p>14:40–15:02 Towards String-To-Tree Neural Machine Translation
Roee Aharoni and Yoav Goldberg
</p>
<p>14:08–15:02 Session 5E: Sentiment 2
</p>
<p>14:08–14:26 Learning Lexico-Functional Patterns for First-Person Affect
Lena Reed, Jiaqi Wu, Shereen Oraby, Pranav Anand and Marilyn Walker
</p>
<p>14:27–14:39 Lifelong Learning CRF for Supervised Aspect Extraction
Lei Shu, Hu Xu and Bing Liu
</p>
<p>14:40–15:02 Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text
Categorization
Ye Zhang, Matthew Lease and Byron C. Wallace
</p>
<p>16:41–17:00 Session 6A: Information Extraction 4
</p>
<p>16:41–17:00 Improving Neural Parsing by Disentangling Model Combination and Reranking Ef-
fects
Daniel Fried, Mitchell Stern and Dan Klein
</p>
<p>xxxv</p>
<p />
</div>
<div class="page"><p />
<p>Tuesday, August 1st (continued)
</p>
<p>16:22–17:00 Session 6D: Machine Learning 2
</p>
<p>16:22–16:40 Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective
Function
Oren Melamud and Jacob Goldberger
</p>
<p>16:41–17:00 Implicitly-Defined Neural Networks for Sequence Labeling
Michaeel Kazi and Brian Thompson
</p>
<p>Wednesday, August 2nd
</p>
<p>11:37–12:25 Session 7A: Outstanding Papers 1
</p>
<p>11:37–11:49 The Role of Prosody and Speech Register in Word Segmentation: A Computational
Modelling Perspective
Bogdan Ludusan, Reiko Mazuka, Mathieu Bernard, Alejandrina Cristia and Em-
manuel Dupoux
</p>
<p>11:50–12:12 A Two-Stage Parsing Method for Text-Level Discourse Analysis
Yizhong Wang, Sujian Li and Houfeng Wang
</p>
<p>12:13–12:25 Error-repair Dependency Parsing for Ungrammatical Texts
Keisuke Sakaguchi, Matt Post and Benjamin Van Durme
</p>
<p>11:18–12:25 Session 7B: Outstanding Papers 2
</p>
<p>11:18–11:36 Attention Strategies for Multi-Source Sequence-to-Sequence Learning
Jindřich Libovický and Jindřich Helcl
</p>
<p>11:37–11:49 Understanding and Detecting Diverse Supporting Arguments on Controversial Is-
sues
Xinyu Hua and Lu Wang
</p>
<p>11:50–12:12 A Neural Model for User Geolocation and Lexical Dialectology
Afshin Rahimi, Trevor Cohn and Timothy Baldwin
</p>
<p>12:13–12:25 A Corpus of Natural Language for Visual Reasoning
Alane Suhr, Mike Lewis, James Yeh and Yoav Artzi
</p>
<p>xxxvi</p>
<p />
</div>
<div class="page"><p />
<p>Monday, July 31st
</p>
<p>18:00–21:30 Session P1: Poster Session 1
</p>
<p>Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for
Detecting Narrative Containers
Julien Tourille, Olivier Ferret, Aurelie Neveol and Xavier Tannier
</p>
<p>How to Make Context More Useful? An Empirical Study on Context-Aware Neural
Conversational Models
Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yansong Feng and Dongyan Zhao
</p>
<p>Cross-lingual and cross-domain discourse segmentation of entire documents
Chloé Braud, Ophélie Lacroix and Anders Søgaard
</p>
<p>Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?
Beata Beigman Klebanov, Binod Gyawali and Yi Song
</p>
<p>Argumentation Quality Assessment: Theory vs. Practice
Henning Wachsmuth, Nona Naderi, Ivan Habernal, Yufang Hou, Graeme Hirst,
Iryna Gurevych and Benno Stein
</p>
<p>A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit
Discourse Relations
Samuel Rönnqvist, Niko Schenk and Christian Chiarcos
</p>
<p>Discourse Annotation of Non-native Spontaneous Spoken Responses Using the
Rhetorical Structure Theory Framework
Xinhao Wang, James Bruno, Hillary Molloy, Keelan Evanini and Klaus Zechner
</p>
<p>Improving Implicit Discourse Relation Recognition with Discourse-specific Word
Embeddings
Changxing Wu, Xiaodong Shi, Yidong Chen, Jinsong Su and Boli Wang
</p>
<p>Oracle Summaries of Compressive Summarization
Tsutomu Hirao, Masaaki Nishino and Masaaki Nagata
</p>
<p>Japanese Sentence Compression with a Large Training Dataset
Shun Hasegawa, Yuta Kikuchi, Hiroya Takamura and Manabu Okumura
</p>
<p>xxxvii</p>
<p />
</div>
<div class="page"><p />
<p>Monday, July 31st (continued)
</p>
<p>A Neural Architecture for Generating Natural Language Descriptions from Source
Code Changes
Pablo Loyola, Edison Marrese-Taylor and Yutaka Matsuo
</p>
<p>English Event Detection With Translated Language Features
Sam Wei, Igor Korostil, Joel Nothman and Ben Hachey
</p>
<p>EviNets: Neural Networks for Combining Evidence Signals for Factoid Question
Answering
Denis Savenkov and Eugene Agichtein
</p>
<p>Pocket Knowledge Base Population
Travis Wolfe, Mark Dredze and Benjamin Van Durme
</p>
<p>Answering Complex Questions Using Open Information Extraction
Tushar Khot, Ashish Sabharwal and Peter Clark
</p>
<p>Bootstrapping for Numerical Open IE
Swarnadeep Saha, Harinder Pal and Mausam
</p>
<p>Feature-Rich Networks for Knowledge Base Completion
Alexandros Komninos and Suresh Manandhar
</p>
<p>Fine-Grained Entity Typing with High-Multiplicity Assignments
Maxim Rabinovich and Dan Klein
</p>
<p>Group Sparse CNNs for Question Classification with Answer Sets
Mingbo Ma, Liang Huang, Bing Xiang and Bowen Zhou
</p>
<p>Multi-Task Learning of Keyphrase Boundary Classification
Isabelle Augenstein and Anders Søgaard
</p>
<p>Cardinal Virtues: Extracting Relation Cardinalities from Text
Paramita Mirza, Simon Razniewski, Fariz Darari and Gerhard Weikum
</p>
<p>Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets
Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy Puzikov, Ido Dagan and Iryna
Gurevych
</p>
<p>xxxviii</p>
<p />
</div>
<div class="page"><p />
<p>Monday, July 31st (continued)
</p>
<p>Question Answering on Knowledge Bases and Text using Universal Schema and
Memory Networks
Rajarshi Das, Manzil Zaheer, Siva Reddy and Andrew McCallum
</p>
<p>Differentiable Scheduled Sampling for Credit Assignment
Kartik Goyal, Chris Dyer and Taylor Berg-Kirkpatrick
</p>
<p>A Deep Network with Visual Text Composition Behavior
Hongyu Guo
</p>
<p>Neural System Combination for Machine Translation
Long Zhou, Wenpeng Hu, Jiajun Zhang and Chengqing Zong
</p>
<p>An Empirical Comparison of Domain Adaptation Methods for Neural Machine
Translation
Chenhui Chu, Raj Dabre and Sadao Kurohashi
</p>
<p>Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Us-
ing Word Embeddings
Benjamin Marie and Atsushi Fujita
</p>
<p>Feature Hashing for Language and Dialect Identification
Shervin Malmasi and Mark Dras
</p>
<p>Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with
Bidirectional LSTM
Yow-Ting Shiue, Hen-Hsen Huang and Hsin-Hsi Chen
</p>
<p>Automatic Compositor Attribution in the First Folio of Shakespeare
Maria Ryskina, Hannah Alpert-Abrams, Dan Garrette and Taylor Berg-Kirkpatrick
</p>
<p>STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset
Yuya Yoshikawa, Yutaro Shigeto and Akikazu Takeuchi
</p>
<p>"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection
William Yang Wang
</p>
<p>English Multiword Expression-aware Dependency Parsing Including Named Enti-
ties
Akihiko Kato, Hiroyuki Shindo and Yuji Matsumoto
</p>
<p>xxxix</p>
<p />
</div>
<div class="page"><p />
<p>Monday, July 31st (continued)
</p>
<p>Improving Semantic Composition with Offset Inference
Thomas Kober, Julie Weeds, Jeremy Reffin and David Weir
</p>
<p>Learning Topic-Sensitive Word Representations
Marzieh Fadaee, Arianna Bisazza and Christof Monz
</p>
<p>Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word
Embeddings
Terrence Szymanski
</p>
<p>Methodical Evaluation of Arabic Word Embeddings
Mohammed Elrazzaz, Shady Elbassuoni, Khaled Shaban and Chadi Helwe
</p>
<p>Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sen-
timent Analysis and Forecast
Hannah Rashkin, Eric Bell, Yejin Choi and Svitlana Volkova
</p>
<p>Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment
Intensity Annotation
Svetlana Kiritchenko and Saif Mohammad
</p>
<p>Demographic Inference on Twitter using Recursive Neural Networks
Sunghwan Mac Kim, Qiongkai Xu, Lizhen Qu, Stephen Wan and Cecile Paris
</p>
<p>Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning
Prashanth Vijayaraghavan, Soroush Vosoughi and Deb Roy
</p>
<p>A Network Framework for Noisy Label Aggregation in Social Media
Xueying Zhan, Yaowei Wang, Yanghui Rao, Haoran Xie, Qing Li, Fu Lee Wang
and Tak-Lam Wong
</p>
<p>Parser Adaptation for Social Media by Integrating Normalization
Rob van der Goot and Gertjan van Noord
</p>
<p>xl</p>
<p />
</div>
<div class="page"><p />
<p>Tuesday, August 1st
</p>
<p>19:00–22:00 Session P2: Poster Session 2
</p>
<p>AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine
Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan Chen, Weipeng Zhao,
Haiqing Chen, Jun Huang and Wei Chu
</p>
<p>A Conditional Variational Framework for Dialog Generation
Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi Niu, Yang Zhao, Akiko Aizawa
and Guoping Long
</p>
<p>Question Answering through Transfer Learning from Large Fine-grained Supervi-
sion Data
Sewon Min, Minjoon Seo and Hannaneh Hajishirzi
</p>
<p>Self-Crowdsourcing Training for Relation Extraction
Azad Abad, Moin Nabi and Alessandro Moschitti
</p>
<p>A Generative Attentional Neural Network Model for Dialogue Act Classification
Quan Hung Tran, Gholamreza Haffari and Ingrid Zukerman
</p>
<p>Salience Rank: Efficient Keyphrase Extraction with Topic Modeling
Nedelina Teneva and Weiwei Cheng
</p>
<p>List-only Entity Linking
Ying Lin, Chin-Yew Lin and Heng Ji
</p>
<p>Improving Native Language Identification by Using Spelling Errors
Lingzhen Chen, Carlo Strapparava and Vivi Nastase
</p>
<p>Disfluency Detection using a Noisy Channel Model and a Deep Neural Language
Model
Paria Jamshid Lou and Mark Johnson
</p>
<p>On the Equivalence of Holographic and Complex Embeddings for Link Prediction
Katsuhiko Hayashi and Masashi Shimbo
</p>
<p>xli</p>
<p />
</div>
<div class="page"><p />
<p>Tuesday, August 1st (continued)
</p>
<p>Sentence Embedding for Neural Machine Translation Domain Adaptation
Rui Wang, Andrew Finch, Masao Utiyama and Eiichiro Sumita
</p>
<p>Data Augmentation for Low-Resource Neural Machine Translation
Marzieh Fadaee, Arianna Bisazza and Christof Monz
</p>
<p>Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocab-
ulary
Xing Shi and Kevin Knight
</p>
<p>Chunk-Based Bi-Scale Decoder for Neural Machine Translation
Hao Zhou, Zhaopeng Tu, Shujian Huang, Xiaohua Liu, Hang Li and Jiajun Chen
</p>
<p>Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary
Meng Fang and Trevor Cohn
</p>
<p>EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel
Text
Claudio Delli Bovi, Jose Camacho-Collados, Alessandro Raganato and Roberto
Navigli
</p>
<p>Challenging Language-Dependent Segmentation for Arabic: An Application to Ma-
chine Translation and Part-of-Speech Tagging
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Ahmed Abdelali, Yonatan Belinkov
and Stephan Vogel
</p>
<p>Fast and Accurate Neural Word Segmentation for Chinese
Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu and Feiyue Huang
</p>
<p>Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task
Zheng Cai, Lifu Tu and Kevin Gimpel
</p>
<p>Neural Semantic Parsing over Multiple Knowledge-bases
Jonathan Herzig and Jonathan Berant
</p>
<p>Representing Sentences as Low-Rank Subspaces
Jiaqi Mu, Suma Bhat and Pramod Viswanath
</p>
<p>Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese So-
cial Media Text Summarization
Shuming Ma, Xu Sun, Jingjing Xu, Houfeng Wang, Wenjie Li and Qi Su
</p>
<p>xlii</p>
<p />
</div>
<div class="page"><p />
<p>Tuesday, August 1st (continued)
</p>
<p>Determining Whether and When People Participate in the Events They Tweet About
Krishna Chaitanya Sanagavarapu, Alakananda Vempala and Eduardo Blanco
</p>
<p>Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted
News Posts on Twitter
Svitlana Volkova, Kyle Shaffer, Jin Yea Jang and Nathan Hodas
</p>
<p>Recognizing Counterfactual Thinking in Social Media Texts
Youngseo Son, Anneke Buffone, Joe Raso, Allegra Larche, Anthony Janocko,
Kevin Zembroski, H. Andrew Schwartz and Lyle Ungar
</p>
<p>Temporal Orientation of Tweets for Predicting Income of Users
Mohammed Hasanuzzaman, Sabyasachi Kamila, Mandeep Kaur, Sriparna Saha and
Asif Ekbal
</p>
<p>Character-Aware Neural Morphological Disambiguation
Alymzhan Toleu, Gulmira Tolegen and Aibek Makazhanov
</p>
<p>Character Composition Model with Convolutional Neural Networks for Depen-
dency Parsing on Morphologically Rich Languages
Xiang Yu and Ngoc Thang Vu
</p>
<p>How (not) to train a dependency parser: The curious case of jackknifing part-of-
speech taggers
Željko Agić and Natalie Schluter
</p>
<p>xliii</p>
<p />
</div>
<div class="page"><p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1–6
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2001
</p>
<p>Classifying Temporal Relations by Bidirectional LSTM over Dependency
Paths
</p>
<p>Fei Cheng and Yusuke Miyao
Research Center for Financial Smart Data
</p>
<p>National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
{fei-cheng, yusuke}@nii.ac.jp
</p>
<p>Abstract
</p>
<p>Temporal relation classification is becom-
ing an active research field. Lots of meth-
ods have been proposed, while most of
them focus on extracting features from
external resources. Less attention has
been paid to a significant advance in a
closely related task: relation extraction.
In this work, we borrow a state-of-the-art
method in relation extraction by adopting
bidirectional long short-term memory (Bi-
LSTM) along dependency paths (DP). We
make a “common root” assumption to ex-
tend DP representations of cross-sentence
links. In the final comparison to two state-
of-the-art systems on TimeBank-Dense,
our model achieves comparable perfor-
mance, without using external knowledge
and manually annotated attributes of enti-
ties (class, tense, polarity, etc.).
</p>
<p>1 Introduction
</p>
<p>Recently, the need for extracting temporal infor-
mation from text is motivated rapidly by many
NLP tasks such as: question answering (QA), in-
formation extraction (IE), etc. Along with the
TimeBank1 (Pustejovsky et al., 2003) and other
temporal information annotated corpora, a se-
ries of temporal evaluation challenges (TempEval-
1,2,3) (Verhagen et al., 2009, 2010; UzZaman
et al., 2012) are attracting growing research ef-
forts.
</p>
<p>Temporal relation classification is a task to iden-
tify the pairs of temporal entities (events or tem-
poral expressions) that have a temporal link and
classify the temporal relations between them. For
instance, we show an event-event (E-E) link with
‘DURING’ type in (i), an event-time (E-T) link
</p>
<p>1https://catalog.ldc.upenn.edu/LDC2006T08
</p>
<p>with ‘INCLUDES’ type in (ii) and an event-DCT
(document creation time, E-D) with ‘BEFORE’
type in (iii).
</p>
<p>(i) There was no hint of trouble in the last con-
versation between controllers and TWA pilot
Steven Snyder.
</p>
<p>(ii) In Washington today, the Federal Aviation
Administration released air traffic control
tapes.
</p>
<p>(iii) The U.S. Navy has 27 ships in the maritime
barricade of Iraq.
</p>
<p>Marcu and Echihabi (2002) propose an ap-
proach considering word-based pairs as useful fea-
tures. The following researchers (Laokulrat et al.,
2013; Chambers et al., 2014; Mani et al., 2006;
D’Souza and Ng, 2013) focus on extracting lex-
ical, syntactic or semantic information from var-
ious external knowledge bases such as: Word-
Net (Miller, 1995) and VerbOcean (Chklovski and
Pantel, 2004). However, these feature based meth-
ods rely on hand-crafted efforts and external re-
sources. In addition, these works require the fea-
tures of entity attributes (class, tense, polarity,
etc.), which are manually annotated to achieve
high performance. Consequently, they are hard to
obtain in practical application scenarios.
</p>
<p>In relation extraction, there is an explosion of
the works done with the dependency path (DP)
based methods, which employ various models
along dependency paths (Bunescu and Mooney,
2005; Plank and Moschitti, 2013). In recent years,
the DP-based neural networks (Socher et al., 2011;
Xu et al., 2015a,b) show state-of-the-art perfor-
mance, with less requirements on explicit features.
Intuitively, the DP-based approaches have the po-
tential to classify temporal relations.
</p>
<p>Both relation extraction and temporal relation
classification require the identification of relation-
</p>
<p>1</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2001">https://doi.org/10.18653/v1/P17-2001</a></div>
</div>
<div class="page"><p />
<p>Figure 1: An example of the sentences with entity attributes annotated in TimeBank.
</p>
<p>ship between entities in texts. However, temporal
relation classification is more challenging, since it
includes three different type of entities: ‘event’,
‘time expression’ and DCT. Cross-sentence links
also add additional complexity into the task. Due
to the outstanding performance of DP-based neu-
ral networks revealed in relation extraction, we
borrow this state-of-the-art approach to temporal
relation classification.
</p>
<p>In Section 2 of this paper, we review related
work and introduce TimeBank-Dense. We dis-
cuss the cross-sentence link problem and the ar-
chitectures of our E-E, E-T and E-D classifiers in
Section 3. In Section 4, the experiments are per-
formed on TimeBank-Dense and we compare our
model to the baseline and two state-of-the-art sys-
tems. The final conclusion is made in Section 5.
</p>
<p>2 Background
</p>
<p>2.1 Related Work
</p>
<p>Current state-of-the-art temporal relation classi-
fiers exploit a variety of features. Laokulrat et al.
(2013); Chambers et al. (2014) extract lexical
and morphological features derived from Word-
Net synsets. Mani et al. (2006); D’Souza and
Ng (2013) incorporate semantic relations between
verbs from VerbOcean as features. In addition,
most of the systems include the entity attributes
(Figure 1) specified in TimeML 2 as basic features,
which actually need heavy human annotations.
</p>
<p>In this work, we push this work into a more
practical level by using only word, part-of-speech
(POS), dependency parsing information, without
incorporating entity attributes, as well as any other
external resources.
</p>
<p>In relation extraction, Bunescu and Mooney
(2005) propose an observation that a relation
can be captured by the shortest dependency path
</p>
<p>2http://timeml.org/
</p>
<p>Figure 2: An example of the DP representation of
a cross-sentence link between the two sentences in
Figure 1.
</p>
<p>(SDP) between the two entities in the entire depen-
dency graph. Plank and Moschitti (2013) extract
syntactic and semantic information in a tree ker-
nel. Following this line, researchers (Socher et al.,
2011; Xu et al., 2015a,b) achieve state-of-the-art
performance by building various neural networks
over dependency path.
</p>
<p>Our system is similar to the work by Xu et al.
(2015b). They perform LSTM with max pooling
separately on each feature channel along depen-
dency path. In contrast, our system adopts bidi-
rectional LSTM on the concatenation of feature
embeddings.
</p>
<p>2.2 TimeBank-Dense
</p>
<p>In the original TimeBank, temporal links have
been created on those pairs with semantic connec-
tions, which led to a sparse annotation style. Cas-
sidy et al. (2014) 3 propose a mechanism to force
annotators to create complete graphs over the enti-
ties in neighboring sentences. Compared to 6,418
links in 183 TimeBank documents, TimeBank-
Dense achieves greater density with 12,715 links
in 36 documents.
</p>
<p>We follow a similar experiment setting to the
other two systems (Mirza and Tonelli, 2016;
Chambers et al., 2014) with the same 9 documents
</p>
<p>3https://www.usna.edu/Users/cs/nchamber/caevo
</p>
<p>2</p>
<p />
</div>
<div class="page"><p />
<p>as test data and the others as training data (15%
of training data is split as validation data for early
stopping).
</p>
<p>3 The Proposed Method
</p>
<p>3.1 Cross-sentence Dependency Paths
Intuitively, the dependency path based idea can
be introduced into the temporal relation classifica-
tion task. However, around 64% E-E, E-T links in
TimeBank-Dense are with the ends in two neigh-
boring sentences, called cross-sentence links.
</p>
<p>A crucial obstacle is how to represent the depen-
dency path of a cross-sentence link. In this work,
we make a naive assumption that two neighbor-
ing sentences share a “common root”. Therefore,
a cross-sentence dependency path can be repre-
sented as two shortest dependency path branches
from the ends to the “common root”, as shown in
Figure 2.
</p>
<p>Stanford CoreNLP4 is used to parsing syntactic
structures of sentences in this work.
</p>
<p>3.2 Temporal Relation Classifiers
Long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) is a natural choice for pro-
cessing sequential dependency paths. As the re-
versed order also takes useful information, a back-
ward representation can be achieved by feed-
ing LSTM with the same input in reverse. We
adopt the concatenation of the forward and back-
ward LSTMs outputs, referred to as bidirectional
LSTM (Graves and Schmidhuber, 2005).
</p>
<p>Figure 3a shows the neural network architec-
ture of our E-E, E-T classifier. Given an E-E or
E-T temporal link, our system first generates two
SDP branches: 1) the source entity to common
root, 2) the target entity to common root. For
each word along a SDP branch, concatenation of
word, POS and dependency relation (DEP) em-
beddings (word-level) is fed into Bi-LSTM. The
forward and backward outputs of both source and
target branches are all concatenated, and fed into a
fully connected hidden units layer. The final Soft-
max layer generates multi-class predictions. Since
an E-D link contains single event SDP branch,
our system applies a similar architecture, but with
single branch Bi-LSTM with outputs fed into the
penultimate hidden layer, as shown in Figure 3b.
</p>
<p>In this work, we use word2vec5 (Mikolov et al.,
4http://stanfordnlp.github.io/CoreNLP/
5https://code.google.com/archive/p/word2vec/
</p>
<p>LINK type E-D E-E E-T
AFTER .493 .477 .350
BEFORE .552 .380 .311
SIMULTANEOUS - - -
INCLUDES .305 .185 .254
IS INCLUDED .513 .296 .204
VAGUE .482 .656 .616
Overall .491 .544 .480
</p>
<p>Table 1: The best sentence-level 5-fold CV per-
formance (Micro-average Overall F1-score).
</p>
<p>2013a,b) to train 200-dimensions word embed-
dings on English Gigaword 4th edition with skip-
gram model and other default settings. For ei-
ther of POS or DEP, we adopt the 50-dimensions
lookup table initialized randomly.
</p>
<p>4 Experiments
</p>
<p>4.1 Hyper-parameters and Cross-validation
</p>
<p>The grid search exploring a full hyper-parameter
space takes time for three classifiers (E-E, E-T and
E-D). Empirically, we set each single LSTM out-
put with the same dimensions (equal to 300) as the
concatenation of word, POS, DEP embeddings.
The hidden layer is set as 200-dimensions.
</p>
<p>Our system adopts dependency paths as input,
which means that the entities in the same sen-
tences contain highly covered word sequence in-
put. Simple cross-validation (CV) on links can not
reflect the generalization ability of our model cor-
rectly. We use a grouped 5-fold CV based on the
source entity ids (document id + sentence id) of
links. This schema can reduce bias separately in
either the source SDP or the target SDP. Although
document level CV can avoid this issue, it’s not
feasible for TimeBank-Dense because it contains
only 27 training documents.
</p>
<p>Early stopping is used to save the best model
based on the validation data. In each run of the
5-fold cross-validation, we split 80% of ‘original
training’ as ‘tentative training’ and 20% as ‘ten-
tative test’. 85% of ‘tentative training’ is used to
learning and 15% is used for validation. We also
adopt early stopping in the final system on the val-
idation data (15% of ‘original training’). The pa-
tience is set as 10.
</p>
<p>Dropout (Srivastava et al., 2014) recently is
proved to be an useful approach to prevent neu-
ral networks from over-fitting. We adopt dropout
</p>
<p>3</p>
<p />
</div>
<div class="page"><p />
<p>(a) E-E, E-T classifier (b) E-D classifier
</p>
<p>Figure 3: The DP-based Bi-LSTM temporal relation classifier.
</p>
<p>Our Mirza Our Mirza
LINK type E-D E-D E-E E-E
AFTER .582 .466 .440 .430
BEFORE .634 .671 .460 .471
SIMULTA. - - - -
INCLUDES .056 .250 .025 .049
IS INCLUD. .595 .600 .170 .250
VAGUE .526 .502 .624 .613
Overall .546 .534 .529 .519
</p>
<p>Table 2: The detailed comparison of E-E and E-T
against relation types to Mirza and Tonelli (2016)
(Micro-average Overall F1-score) on test data.
</p>
<p>separately after the following layers: embeddings,
LSTM, and hidden layer to investigate the impact
of dropout on performance. Table 1 shows the best
CV results recorded in tuning dropout. The hyper-
parameter setting with the best CV performance is
adopted in the final system.
</p>
<p>4.2 Overall Performance
</p>
<p>Recently, Mirza and Tonelli (2016) report state-of-
the-art performance on TimeBank-Dense. They
show the new attempt to mine the value of low-
dimensions word embeddings by concatenating
them with sparse traditional features. Their tra-
ditional features include entity attributes, tempo-
ral signals, semantic information of WordNet, etc.,
which means it’s a hard setting for challenging
their performance. In Table 2 and 3, ‘Mirza’ de-
notes their system.
</p>
<p>Table 2 shows the detailed comparison to
</p>
<p>Systems E-D E-E E-T Overall
Baseline .471 .502 .437 .486
Proposed .546 .529 .471 .520
Mirza .534 .519 .468 .512
CAEVO .553 .494 .494 .502
</p>
<p>Table 3: The final comparison of E-E, E-T and E-
D to the baseline and two state-of-the-art systems
on test data.
</p>
<p>their work. Our system achieves higher perfor-
mance on ‘AFTER’, ‘VAGUE’, while lower on
‘BEFORE’, ‘INCLUDES’ (5% of all data) and
‘IS INCLUDED’ (4% of all data). It is likely that
their rich traditional features help the classifiers to
capture more minority-class links. On the whole,
our system reaches better ‘Overall’ on both E-E
and E-D. As their E-T classifier does not include
word embeddings, the E-T results are not listed.
</p>
<p>The final comparison is shown in Table 3. An
one-layer fully connected hidden units baseline
(200-dimensions) with word, POS embeddings as
input (without any dependency information) is
provided. The significant out-performance of our
proposed model over the baseline indicates the ef-
fectiveness of the dependency path information
and our Bi-LSTM in classifying temporal links.
As a hybrid system, ‘CAEVO’ (Chambers et al.,
2014) includes hand-crafted rules for their E-T and
E-D classifiers. For instance, the temporal prepo-
sitions in, on, over, during, and within indicate
‘IN INCLUDED’ relations. Their system is supe-
rior in E-T and E-D. ’Miza’ takes the pure feature-
</p>
<p>4</p>
<p />
</div>
<div class="page"><p />
<p>based methods and performs slightly better in E-
E and overall, compared to ‘CAEVO’. Our sys-
tem shows the highest scores in E-E and overall
among the four systems. In general, our system
achieves comparable performance to two state-of-
the-art systems, without using any hand-crafted
features, rules, or external resources.
</p>
<p>5 Conclusion
</p>
<p>We borrow the idea of the dependency path based
neural networks into temporal relation classifica-
tion. A “common root” assumption adapts our
model to cross-sentence links. Our model adopts
bidirectional LSTM for capturing both forward
and backward orders information. We observe
the significant benefit of the DP-based Bi-LSTM
model by comparing it to the baseline. Our model
achieves comparable performance to two state-of-
the-art systems without using any explicit features
(class, tense, polarity, etc.) or external resources,
which indicates that our model can capture such
information automatically.
</p>
<p>6 Acknowledgments
</p>
<p>We thank the anonymous reviewers for the insight-
ful comments.
</p>
<p>References
Razvan Bunescu and Raymond Mooney. 2005. A
</p>
<p>shortest path dependency kernel for relation ex-
traction. In Proceedings of Human Language
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Van-
couver, British Columbia, Canada, pages 724–
731. http://www.aclweb.org/anthology/H/H05/H05-
1091.
</p>
<p>Taylor Cassidy, Bill McDowell, Nathanael Cham-
bers, and Steven Bethard. 2014. An annotation
framework for dense event ordering. In Pro-
ceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume
2: Short Papers). Association for Computational
Linguistics, Baltimore, Maryland, pages 501–506.
http://www.aclweb.org/anthology/P14-2082.
</p>
<p>Nathanael Chambers, Taylor Cassidy, Bill McDowell,
and Steven Bethard. 2014. Dense event ordering
with a multi-pass architecture. Transactions of the
Association for Computational Linguistics 2:273–
284. http://aclweb.org/anthology/Q/Q14/Q14-
1022.pdf.
</p>
<p>Timothy Chklovski and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained se-
</p>
<p>mantic verb relations. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP
2004. Association for Computational Lin-
guistics, Barcelona, Spain, pages 33–40.
https://www.aclweb.org/anthology/W/W04/W04-
3205.pdf.
</p>
<p>Jennifer D’Souza and Vincent Ng. 2013. Classify-
ing temporal relations with rich linguistic knowl-
edge. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 918–927.
http://www.aclweb.org/anthology/N13-1112.
</p>
<p>Alex Graves and Jürgen Schmidhuber. 2005.
Framewise phoneme classification with bidi-
rectional lstm and other neural network ar-
chitectures. Neural Networks 18(5):602–610.
https://doi.org/10.1016/j.neunet.2005.06.042.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhu-
ber. 1997. Long short-term memory.
Neural computation 9(8):1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735.
</p>
<p>Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Uttime: Tem-
poral relation classification using deep syntactic fea-
tures. In Second Joint Conference on Lexical and
Computational Semantics (* SEM). volume 2, pages
88–92. http://aclweb.org/anthology/S/S13/S13-
2015.pdf.
</p>
<p>Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In Pro-
ceedings of the 21st International Conference
on Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, Sydney, Australia, pages 753–760.
https://doi.org/10.3115/1220175.1220270.
</p>
<p>Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of 40th Annual Meet-
ing of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA, pages 368–375.
https://doi.org/10.3115/1073083.1073145.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013a. Efficient es-
timation of word representations in vec-
tor space. arXiv preprint arXiv:1301.3781
http://arxiv.org/pdf/1301.3781v3.pdf.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In Advances in neural in-
formation processing systems. pages 3111–3119.
https://arxiv.org/pdf/1310.4546.pdf.
</p>
<p>5</p>
<p />
</div>
<div class="page"><p />
<p>George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41. https://doi.org/10.1145/219717.219748.
</p>
<p>Paramita Mirza and Sara Tonelli. 2016. On the con-
tribution of word embeddings to temporal relation
classification. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers. The COLING 2016
Organizing Committee, Osaka, Japan, pages 2818–
2828. http://aclweb.org/anthology/C16-1265.
</p>
<p>Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Sofia, Bulgaria, pages 1498–1507.
http://www.aclweb.org/anthology/P13-1147.
</p>
<p>James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day,
Lisa Ferro, et al. 2003. The timebank cor-
pus. In Corpus linguistics. volume 2003, page 40.
https://catalog.ldc.upenn.edu/LDC2006T08.
</p>
<p>Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Edinburgh, Scotland, UK., pages
151–161. http://www.aclweb.org/anthology/D11-
1014.
</p>
<p>Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov.
2014. Dropout: a simple way to prevent
neural networks from overfitting. Journal of
Machine Learning Research 15(1):1929–1958.
http://jmlr.org/papers/v15/srivastava14a.html.
</p>
<p>Naushad UzZaman, Hector Llorens, James Allen,
Leon Derczynski, Marc Verhagen, and James
Pustejovsky. 2012. Tempeval-3: Evaluat-
ing events, time expressions, and temporal
relations. arXiv preprint arXiv:1206.5333
http://aclweb.org/anthology/S/S13/S13-2001.pdf.
</p>
<p>Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The tempeval challenge:
identifying temporal relations in text. Lan-
guage Resources and Evaluation 43(2):161–179.
https://doi.org/10.1007/s10579-009-9086-z.
</p>
<p>Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th interna-
tional workshop on semantic evaluation. Associa-
tion for Computational Linguistics, pages 57–62.
http://www.aclweb.org/anthology/S10-1010.
</p>
<p>Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation clas-
sification via convolutional neural networks with
simple negative sampling. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Lisbon, Portugal, pages 536–540.
http://aclweb.org/anthology/D15-1062.
</p>
<p>Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao
Peng, and Zhi Jin. 2015b. Classifying relations
via long short term memory networks along short-
est dependency paths. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 1785–1794.
http://aclweb.org/anthology/D15-1206.
</p>
<p>6</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2002
</p>
<p>AMR-to-text Generation with Synchronous Node Replacement Grammar
</p>
<p>Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea
Department of Computer Science, University of Rochester, Rochester, NY 14627
</p>
<p>IBM T.J. Watson Research Center, Yorktown Heights, NY 10598
Singapore University of Technology and Design
</p>
<p>Abstract
</p>
<p>This paper addresses the task of AMR-to-
text generation by leveraging synchronous
node replacement grammar. During train-
ing, graph-to-string rules are learned us-
ing a heuristic extraction algorithm. At
test time, a graph transducer is applied to
collapse input AMRs and generate output
sentences. Evaluated on a standard bench-
mark, our method gives the state-of-the-art
result.
</p>
<p>1 Introduction
</p>
<p>Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013) is a semantic formalism en-
coding the meaning of a sentence as a rooted,
directed graph. AMR uses a graph to represent
meaning, where nodes (such as “boy”, “want-01”)
represent concepts, and edges (such as “ARG0”,
“ARG1”) represent relations between concepts.
Encoding many semantic phenomena into a graph
structure, AMR is useful for NLP tasks such as
machine translation (Jones et al., 2012; Tamchyna
et al., 2015), question answering (Mitra and Baral,
2015), summarization (Takase et al., 2016) and
event detection (Li et al., 2015).
</p>
<p>AMR-to-text generation is challenging as func-
tion words and syntactic structures are abstracted
away, making an AMR graph correspond to mul-
tiple realizations. Despite much literature so far
on text-to-AMR parsing (Flanigan et al., 2014;
Wang et al., 2015; Peng et al., 2015; Vanderwende
et al., 2015; Pust et al., 2015; Artzi et al., 2015;
Groschwitz et al., 2015; Goodman et al., 2016;
Zhou et al., 2016; Peng et al., 2017), there has been
little work on AMR-to-text generation (Flanigan
et al., 2016; Song et al., 2016; Pourdamghani et al.,
2016).
</p>
<p>#X1#
</p>
<p>the boy   wants     to go
</p>
<p>#X1#
</p>
<p>ARG1
</p>
<p>go-01boy
</p>
<p>want-01ARG0
</p>
<p>ARG0
</p>
<p>ARG1
</p>
<p>#X2#
</p>
<p>#X3#
</p>
<p>go-01#X2#
ARG0
</p>
<p>#X3#
</p>
<p>ARG1
</p>
<p>go-01
</p>
<p>want-01ARG0
</p>
<p>ARG0
</p>
<p>Figure 1: Graph-to-string derivation.
</p>
<p>Flanigan et al. (2016) transform a given AMR
graph into a spanning tree, before translating it
to a sentence using a tree-to-string transducer.
Their method leverages existing machine transla-
tion techniques, capturing hierarchical correspon-
dences between the spanning tree and the surface
string. However, it suffers from error propagation
since the output is constrained given a spanning
tree due to the projective correspondence between
them. Information loss in the graph-to-tree trans-
formation step cannot be recovered. Song et al.
(2016) directly generate sentences using graph-
fragment-to-string rules. They cast the task of
finding a sequence of disjoint rules to transduce
an AMR graph into a sentence as a traveling sales-
man problem, using local features and a language
model to rank candidate sentences. However, their
method does not learn hierarchical structural cor-
respondences between AMR graphs and strings.
</p>
<p>We propose to leverage the advantages of hier-
archical rules without suffering from graph-to-tree
errors by directly learning graph-to-string rules.
As shown in Figure 1, we learn a synchronous
node replacement grammar (NRG) from a cor-
pus of aligned AMR and sentence pairs. At test
time, we apply a graph transducer to collapse input
</p>
<p>7</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2002">https://doi.org/10.18653/v1/P17-2002</a></div>
</div>
<div class="page"><p />
<p>go-01
</p>
<p>boy
</p>
<p>want-01
ARG1
</p>
<p>ARG0
</p>
<p>ARG0
</p>
<p>go-01#X2#
</p>
<p>ARG1
</p>
<p>ARG0
</p>
<p>(root)
</p>
<p>go-01
</p>
<p>#X3#
</p>
<p>want-01
ARG1
</p>
<p>ARG0
</p>
<p>ARG0
</p>
<p>#S# #X1#
</p>
<p>{#S#}                      {#X1#}                            {#X2# to go}                               {#X3# wants to go}                        {the boy wants to go}
</p>
<p>(a)
AMR:
</p>
<p>String:
</p>
<p>(b)(c)
</p>
<p>Figure 2: Example deduction procedure
</p>
<p>ID. F E
(a) (b / boy) the boy
</p>
<p>(b) (w / want-01 #X# wants:ARG0 (X / #X#))
</p>
<p>(c)
(X / #X#
</p>
<p>#X# to go:ARG1 (g / go-01
:ARG0 X))
</p>
<p>(d) (w / want-01 the boy wants:ARG0 (b / boy))
</p>
<p>Table 1: Example rule set
</p>
<p>AMR graphs and generate output strings accord-
ing to the learned grammar. Our system makes
use of a log-linear model with real-valued fea-
tures, tuned using MERT (Och, 2003), and beam
search decoding. It gives a BLEU score of 25.62
on LDC2015E86, which is the state-of-the-art on
this dataset.
</p>
<p>2 Synchronous Node Replacement
Grammar
</p>
<p>2.1 Grammar Definition
A synchronous node replacement grammar (NRG)
is a rewriting formalism: G = 〈N,Σ,∆, P, S〉,
where N is a finite set of nonterminals, Σ and ∆
are finite sets of terminal symbols for the source
and target sides, respectively. S ∈ N is the start
symbol, and P is a finite set of productions. Each
instance of P takes the form Xi → (〈F,E〉,∼),
where Xi ∈ N is a nonterminal node, F is a
rooted, connected AMR fragment with edge labels
over Σ and node labels over N ∪ Σ, E is a corre-
sponding target string over N ∪∆ and ∼ denotes
the alignment of nonterminal symbols between F
and E. A classic NRG (Engelfriet and Rozenberg,
1997, Chapter 1) also defines C, which is an em-
bedding mechanism defining how F is connected
to the rest of the graph when replacing Xi with
F on the graph. Here we omit defining C and
allow arbitrary connections.1 Following Chiang
</p>
<p>1This may over generate, but does not affect our case, as
in our bottom-up decoding procedure (section 3) when F is
replaced with Xi, nodes previously connected to F are re-
connected to Xi
</p>
<p>Data: training corpus C
Result: rule instances R
</p>
<p>1 R← [];
2 for (Sent,AMR,∼) in C do
3 Rcur ← FRAGMENTEXTRACT(Sent,AMR,∼);
4 for ri in Rcur do
5 R.APPEND(ri) ;
6 for rj in Rcur/{ri} do
7 if ri.CONTAINS(rj) then
8 rij ← ri.COLLAPSE(rj);
9 R.APPEND(rij) ;
</p>
<p>10 end
11 end
12 end
13 end
</p>
<p>Algorithm 1: Rule extraction
</p>
<p>(2005), we use only one nonterminal X in addi-
tion to S, and use subscripts to distinguish differ-
ent non-terminal instances.
</p>
<p>Figure 2 shows an example derivation process
for the sentence “the boy wants to go” given the
rule set in Table 1. Given the start symbol S,
which is first replaced with X1, rule (c) is applied
to generate “X2 to go” and its AMR counterpart.
Then rule (b) is used to generate “X3 wants” and
its AMR counterpart from X2. Finally, rule (a)
is used to generate “the boy” and its AMR coun-
terpart from X3. Our graph-to-string rules are
inspired by synchronous grammars for machine
translation (Wu, 1997; Yamada and Knight, 2002;
Gildea, 2003; Chiang, 2005; Huang et al., 2006;
Liu et al., 2006; Shen et al., 2008; Xie et al., 2011;
Meng et al., 2013).
</p>
<p>2.2 Induced Rules
There are three types of rules in our system,
namely induced rules, concept rules and graph
glue rules. Here we first introduce induced rules,
which are obtained by a two-step procedure on a
training corpus. Shown in Algorithm 1, the first
step is to extract a set of initial rules from train-
ing 〈sentence, AMR, ∼〉2 pairs (Line 2) using the
phrase-to-graph-fragment extraction algorithm of
Peng et al. (2015) (Line 3). Here an initial rule
</p>
<p>2∼ denotes alignment between words and AMR labels.
</p>
<p>8</p>
<p />
</div>
<div class="page"><p />
<p>contains only terminal symbols in both F and E.
As a next step, we match between pairs of initial
rules ri and rj , and generate rij by collapsing ri
with rj , if ri contains rj (Line 6-8). Here ri con-
tains rj , if rj .F is a subgraph of ri.F and rj .E
is a sub-phrase of ri.E. When collapsing ri with
rj , we replace the corresponding subgraph in ri.F
with a new non-terminal node, and the sub-phrase
in ri.E with the same non-terminal. For example,
we obtain rule (b) by collapsing (d) with (a) in Ta-
ble 1. All initial and generated rules are stored in
a rule list R (Lines 5 and 9), which will be further
normalized to obtain the final induced rule set.
</p>
<p>2.3 Concept Rules and Glue Rules
</p>
<p>In addition to induced rules, we adopt concept
rules (Song et al., 2016) and graph glue rules to en-
sure existence of derivations. For a concept rule, F
is a single node in the input AMR graph, andE is a
morphological string of the node concept. A con-
cept rule is used in case no induced rule can cover
the node. We refer to the verbalization list3 and
AMR guidelines4 for creating more complex con-
cept rules. For example, one concept rule created
from the verbalization list is “(k / keep-01 :ARG1
(p / peace)) ||| peacekeeping”.
</p>
<p>Inspired by Chiang (2005), we define graph
glue rules to concatenate non-terminal nodes con-
nected with an edge, when no induced rules can be
applied. Three glue rules are defined for each type
of edge label. Taking the edge label “ARG0” as an
example, we create the following glue rules:
</p>
<p>ID. F E
r1 (X1 / #X1# :ARG0 (X2 / #X2#)) #X1# #X2#
r2 (X1 / #X1# :ARG0 (X2 / #X2#)) #X2# #X1#
r3 (X1 / #X1# :ARG0 X1) #X1#
</p>
<p>where for both r1 and r2, F contains two non-
terminal nodes with a directed edge connecting
them, and E is the concatenation the two non-
terminals in either the monotonic or the inverse
order. For r3, F contains one non-terminal node
with a self-pointing edge, and E is the non-
terminal. With concept rules and glue rules in our
final rule set, it is easily guaranteed that there are
legal derivations for any input AMR graph.
</p>
<p>3 Model
</p>
<p>We adopt a log-linear model for scoring search hy-
potheses. Given an input AMR graph, we find
</p>
<p>3http://amr.isi.edu/download/lists/verbalization-list-
v1.06.txt
</p>
<p>4https://github.com/amrisi/amr-guidelines
</p>
<p>the highest scored derivation t∗ from all possible
derivations t:
</p>
<p>t∗ = argmax
t
</p>
<p>exp
∑
</p>
<p>i
</p>
<p>wifi(g, t), (1)
</p>
<p>where g denotes the input AMR, fi(·, ·) and wi
represent a feature and the corresponding weight,
respectively. The feature set that we adopt in-
cludes phrase-to-graph and graph-to-phrase trans-
lation probabilities and their corresponding lexi-
calized translation probabilities (section 3.1), lan-
guage model score, word count, rule count, re-
ordering model score (section 3.2) and moving
distance (section 3.3). The language model score,
word count and phrase count features are adopted
from SMT (Koehn et al., 2003; Chiang, 2005).
</p>
<p>We perform bottom-up search to transduce in-
put AMRs to surface strings. Each hypothesis con-
tains the current AMR graph, translations of col-
lapsed subgraphs, the feature vector and the cur-
rent model score. Beam search is adopted, where
hypotheses with the same number of collapsed
edges and nodes are put into the same beam.
</p>
<p>3.1 Translation Probabilities
Production rules serve as a basis for scoring hy-
potheses. We associate each synchronous NRG
rule n → (〈F,E〉,∼) with a set of probabilities.
First, phrase-to-fragment translation probabilities
are defined based on maximum likelihood estima-
tion (MLE), as shown in Equation 2, where c〈F,E〉
is the fractional count of 〈F,E〉.
</p>
<p>p(F |E) =
c〈F,E〉∑
F ′ c〈F ′,E〉
</p>
<p>(2)
</p>
<p>In addition, lexicalized translation probabilities
are defined as:
</p>
<p>pw(F |E) =
∏
</p>
<p>l∈F
</p>
<p>∑
</p>
<p>w∈E
p(l|w) (3)
</p>
<p>Here l is a label (including both edge labels such
as “ARG0” and concept labels such as “want-01”)
in the AMR fragment F , and w is a word in
the phrase E. Equation 3 can be regarded as a
“soft” version of the lexicalized translation prob-
abilities adopted by SMT, which picks the align-
ment yielding the maximum lexicalized probabil-
ity for each translation rule. In addition to p(F |E)
and pw(F |E), we use features in the reverse direc-
tion, namely p(E|F ) and pw(E|F ), the definitions
of which are omitted as they are consistent with
</p>
<p>9</p>
<p />
</div>
<div class="page"><p />
<p>Equations 2 and 3, respectively. The probabilities
associated with concept rules and glue rules are
manually set to 0.0001.
</p>
<p>3.2 Reordering Model
Although the word order is defined for induced
rules, it is not the case for glue rules. We learn a
reordering model that helps to decide whether the
translations of the nodes should be monotonic or
inverse given the directed connecting edge label.
The probabilistic model using smoothed counts is
defined as:
</p>
<p>p(M |h, l, t) =
1.0 +
</p>
<p>∑
h
</p>
<p>∑
t c(h, l, t,M)
</p>
<p>2.0 +
∑
</p>
<p>o∈{M,I}
∑
</p>
<p>h
</p>
<p>∑
t c(h, l, t, o)
</p>
<p>(4)
</p>
<p>c(h, l, t,M) is the count of monotonic translations
of head h and tail t, connected by edge l.
</p>
<p>3.3 Moving Distance
The moving distance feature captures the dis-
tances between the subgraph roots of two consec-
utive rule matches in the decoding process, which
controls a bias towards collapsing nearby sub-
graphs consecutively.
</p>
<p>4 Experiments
</p>
<p>4.1 Setup
We use LDC2015E86 as our experimental dataset,
which contains 16833 training, 1368 dev and 1371
test instances. Each instance contains a sentence,
an AMR graph and the alignment generated by
a heuristic aligner. Rules are extracted from the
training data, and model parameters are tuned on
the dev set. For tuning and testing, we filter out
sentences with more than 30 words, resulting in
1103 dev instances and 1055 test instances. We
train a 4-gram language model (LM) on gigaword
(LDC2011T07), and use BLEU (Papineni et al.,
2002) as the evaluation metric. MERT is used
(Och, 2003) to tune model parameters on k-best
outputs on the devset, where k is set 50.
</p>
<p>We investigate the effectiveness of rules and
features by ablation tests: “NoInducedRule” does
not adopt induced rules, “NoConceptRule” does
not adopt concept rules, “NoMovingDistance”
does not adopt the moving distance feature,
and “NoReorderModel” disables the reordering
model. Given an AMR graph, if NoConceptRule
cannot produce a legal derivation, we concatenate
</p>
<p>System Dev Test
TSP-gen 21.12 22.44
JAMR-gen 23.00 23.00
All 25.24 25.62
NoInducedRule 16.75 17.43
NoConceptRule 23.99 24.86
NoMovingDistance 23.48 24.06
NoReorderModel 25.09 25.43
</p>
<p>Table 2: Main results.
</p>
<p>existing translation fragments into a final transla-
tion, and if a subgraph can not be translated, the
empty string is used as the output. We also com-
pare our method with previous works, in particu-
lar JAMR-gen (Flanigan et al., 2016) and TSP-gen
(Song et al., 2016), on the same dataset.
</p>
<p>4.2 Main results
</p>
<p>The results are shown in Table 2. First, All out-
performs all baselines. NoInducedRule leads to
the greatest performance drop compared with All,
demonstrating that induced rules play a very im-
portant role in our system. On the other hand, No-
ConceptRule does not lead to much performance
drop. This observation is consistent with the ob-
servation of Song et al. (2016) for their TSP-based
system. NoMovingDistance leads to a significant
performance drop, empirically verifying the fact
that the translations of nearby subgraphs are also
close. Finally, NoReorderingModel does not af-
fect the performance significantly, which can be
because the most important reordering patterns are
already covered by the hierarchical induced rules.
Compared with TSP-gen and JAMR-gen, our fi-
nal model All improves the BLEU from 22.44
and 23.00 to 25.62, showing the advantage of our
model. To our knowledge, this is the best result
reported so far on the task.
</p>
<p>4.3 Grammar analysis
</p>
<p>We have shown the effectiveness of our syn-
chronous node replacement grammar (SNRG) on
the AMR-to-text generation task. Here we further
analyze our grammar as it is relatively less studied
than the hyperedge replacement grammar (HRG)
(Drewes et al., 1997).
</p>
<p>Statistics on the whole rule set
We first categorize our rule set by the number of
terminals and nonterminals in the AMR fragment
F , and show the percentages of each type in Fig-
ure 3. Each rule contains at most 1 nonterminal,
as we collapse each initial rule only once. First
</p>
<p>10</p>
<p />
</div>
<div class="page"><p />
<p>0 1 2 3 4 &gt;4
Number of terminals
</p>
<p>0
</p>
<p>1
N
</p>
<p>u
m
</p>
<p>b
e
r 
</p>
<p>o
f 
</p>
<p>n
o
n
-t
</p>
<p>e
rm
</p>
<p>in
a
ls
</p>
<p>0.00 0.03 0.06 0.09 0.12 0.15 0.18 0.21 0.24
</p>
<p>Figure 3: Statistics on the right-hand side.
</p>
<p>Glue Nonterminal Terminal
1-best 30.0% 30.1% 39.9%
</p>
<p>Table 3: Rules used for decoding.
</p>
<p>of all, the percentage of rules containing nonter-
minals are much more than those without nonter-
minals, as we collapse each pair of initial rules (in
Algorithm 1) and the results can be quadratic the
number of initial rules. In addition, most rules are
small containing 1 to 3 terminals, meaning that
they represent small pieces of meaning and are
easier to matched on a new AMR graph. Finally,
there are a few large rules, which represent com-
plex meaning.
</p>
<p>Statistics on the rules used for decoding
In addition, we collect the rules that our well-tuned
system used for generating the 1-best output on
the testset, and categorize them into 3 types: (1)
glue rules, (2) nonterminal rules, which are not
glue rules but contain nonterminals on the right-
hand side and (3) terminal rules, whose right-hand
side only contain terminals. Over the rules used on
the 1-best result, more than 30% are non-terminal
rules, showing that the induced rules play an im-
portant role. On the other hand, 30% are glue
rules. The reason is that the data sparsity for graph
grammars is more severe than string-based gram-
mars (such as CFG), as the graph structures are
more complex than strings. Finally, terminal rules
take the largest percentage, while most are induced
rules, but not concept rules.
</p>
<p>Rule examples
Finally, we show some rules in Table 4, where F
and E are the right-hand-side AMR fragment and
phrase, respectively. For the first rule, the root of
F is a verb (“give-01”) whose subject is a nonter-
minal and object is a AMR fragment “(p / person
:ARG0-of (u / use-01))”, which means “user”. So
it is easy to see that the corresponding phrase E
conveys the same meaning. For the second rule,
“(s3 / stay-01 :accompanier (i / i))” means “stay
</p>
<p>F :
</p>
<p>(g / give-01
:ARG0 (X1 / #X1#)
:ARG2 (p / person
</p>
<p>:ARG0-of (u / use-01)))
E: #X1# has given users an
</p>
<p>F :
(X1 / #X1#
</p>
<p>:ARG2 (s3 / stay-01 :ARG1 X1
:accompanier (i / i)))
</p>
<p>E: #X1# staying with me
</p>
<p>Table 4: Example rules.
</p>
<p>(u / understand-01
:ARG0 (y / you)
:ARG1 (t2 / thing
</p>
<p>:ARG1-of (f2 / feel-01
:ARG0 (p2 / person
</p>
<p>:example (p / person :wiki -
:name (t / name :op1 “TMT”)
:location (c / city :wiki “Fairfax, Virginia”
</p>
<p>:name (f / name :op1 “Fairfax”))))))
:time (n / now))
</p>
<p>Trans: now, you have to understand that people feel about
such as tmt fairfax
Ref: now you understand how people like tmt in fairfax
feel .
</p>
<p>Table 5: Generation example.
</p>
<p>with me”, which is also covered by its phrase.
</p>
<p>4.4 Generation example
Finally, we show an example in Table 5, where the
top is the input AMR graph, and the bottom is the
generation result. Generally, most of the meaning
of the input AMR are correctly translated, such as
“:example”, which means “such as”, and “thing”,
which is an abstract concept and should not be
translated, while there are a few errors, such as
“that” in the result should be “what”, and there
should be an “in” between “tmt” and “fairfax”.
</p>
<p>5 Conclusion
</p>
<p>We showed that synchronous node replacement
grammar is useful for AMR-to-text generation by
developing a system that learns a synchronous
NRG in the training time, and applies a graph
transducer to collapse input AMR graphs and gen-
erate output strings according to the learned gram-
mar at test time. Our method performs better than
the previous systems, empirically proving the ad-
vantages of our graph-to-string rules.
</p>
<p>Acknowledgement
</p>
<p>This work was funded by a Google Faculty
Research Award. Yue Zhang is funded by
NSFC61572245 and T2MOE201301 from Singa-
pore Ministry of Education.
</p>
<p>11</p>
<p />
</div>
<div class="page"><p />
<p>References
Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
</p>
<p>Broad-coverage CCG semantic parsing with AMR.
In Conference on Empirical Methods in Natural
Language Processing (EMNLP-15). pages 1699–
1710.
</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse. pages 178–186.
</p>
<p>David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-05). Ann
Arbor, Michigan, pages 263–270.
</p>
<p>Frank Drewes, Hans-Jörg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement, graph gram-
mars. Handbook of Graph Grammars 1:95–162.
</p>
<p>J. Engelfriet and G. Rozenberg. 1997. Node replace-
ment graph grammars. In Grzegorz Rozenberg, edi-
tor, Handbook of Graph Grammars and Computing
by Graph Transformation, World Scientific Publish-
ing Co., Inc., River Edge, NJ, USA, pages 1–94.
</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and
Jaime Carbonell. 2016. Generation from abstract
meaning representation using tree transducers. In
Proceedings of the 2016 Meeting of the North Amer-
ican chapter of the Association for Computational
Linguistics (NAACL-16). pages 731–739.
</p>
<p>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL-14). pages 1426–1436.
</p>
<p>Daniel Gildea. 2003. Loosely tree-based alignment
for machine translation. In Proceedings of the 41th
Annual Conference of the Association for Computa-
tional Linguistics (ACL-03). Sapporo, Japan, pages
80–87.
</p>
<p>James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016. Noise reduction and targeted explo-
ration in imitation learning for abstract meaning rep-
resentation parsing. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (ACL-16). Berlin, Germany, pages 1–11.
</p>
<p>Jonas Groschwitz, Alexander Koller, and Christoph Te-
ichmann. 2015. Graph parsing with s-graph gram-
mars. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-15). Beijing, China, pages 1481–1490.
</p>
<p>Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of Association
for Machine Translation in the Americas (AMTA-
2006). pages 66–73.
</p>
<p>Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proceedings of
the International Conference on Computational
Linguistics (COLING-12). pages 1359–1376.
</p>
<p>Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Lin-
guistics (NAACL-03). pages 48–54.
</p>
<p>Xiang Li, Thien Huu Nguyen, Kai Cao, and Ralph Gr-
ishman. 2015. Improving event detection with ab-
stract meaning representation. In Proceedings of the
First Workshop on Computing News Storylines. Bei-
jing, China, pages 11–15.
</p>
<p>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-06). Sydney, Australia, pages 609–616.
</p>
<p>Fandong Meng, Jun Xie, Linfeng Song, Yajuan Lü,
and Qun Liu. 2013. Translation with source con-
stituency and dependency trees. In Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-13). Seattle, Washington, USA, pages
1066–1076.
</p>
<p>Arindam Mitra and Chitta Baral. 2015. Addressing a
question answering challenge by combining statisti-
cal methods with inductive rule learning and reason-
ing. In Proceedings of the National Conference on
Artificial Intelligence (AAAI-16).
</p>
<p>Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL-03). Sapporo,
Japan, pages 160–167.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-02). pages 311–318.
</p>
<p>Xiaochang Peng, Linfeng Song, and Daniel Gildea.
2015. A synchronous hyperedge replacement gram-
mar based approach for AMR parsing. In Pro-
ceedings of the Nineteenth Conference on Compu-
tational Natural Language Learning (CoNLL-15).
pages 731–739.
</p>
<p>Xiaochang Peng, Chuan Wang, Daniel Gildea, and Ni-
anwen Xue. 2017. Addressing the data sparsity is-
sue in neural amr parsing. In Proceedings of the
</p>
<p>12</p>
<p />
</div>
<div class="page"><p />
<p>15th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-17).
Valencia, Spain, pages 366–375.
</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Herm-
jakob. 2016. Generating English from abstract
meaning representations. In International Confer-
ence on Natural Language Generation (INLG-16).
Edinburgh, UK, pages 21–25.
</p>
<p>Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel
Marcu, and Jonathan May. 2015. Parsing English
into abstract meaning representation using syntax-
based machine translation. In Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-15). pages 1143–1154.
</p>
<p>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08).
Columbus, Ohio, pages 577–585.
</p>
<p>Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo
Wang, and Daniel Gildea. 2016. AMR-to-text gen-
eration as a traveling salesman problem. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-16). Austin, Texas, pages
2084–2089.
</p>
<p>Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu
Hirao, and Masaaki Nagata. 2016. Neural head-
line generation on abstract meaning representation.
In Conference on Empirical Methods in Natural
Language Processing (EMNLP-16). Austin, Texas,
pages 1054–1059.
</p>
<p>Aleš Tamchyna, Chris Quirk, and Michel Galley.
2015. A discriminative model for semantics-
to-string translation. In Proceedings of the 1st
Workshop on Semantics-Driven Statistical Machine
Translation (S2MT 2015). Beijing, China, pages 30–
36.
</p>
<p>Lucy Vanderwende, Arul Menezes, and Chris Quirk.
2015. An AMR parser for English, French, German,
Spanish and Japanese and a new AMR-annotated
corpus. In Proceedings of the 2015 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-15). pages 26–30.
</p>
<p>Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015. A transition-based algorithm for AMR pars-
ing. In Proceedings of the 2015 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-15). pages 366–375.
</p>
<p>Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics 23(3):377–403.
</p>
<p>Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Conference on Empirical Methods in
Natural Language Processing (EMNLP-11). Edin-
burgh, Scotland, UK., pages 216–226.
</p>
<p>Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-02). Philadelphia, Penn-
sylvania, USA, pages 303–310.
</p>
<p>Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang
QU, Ran Li, and Yanhui Gu. 2016. AMR parsing
with an incremental joint model. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP-16). Austin, Texas, pages 680–689.
</p>
<p>13</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 14–19
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2003
</p>
<p>Lexical Features in Coreference Resolution: To be Used With Caution
</p>
<p>Nafise Sadat Moosavi and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
</p>
<p>Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
</p>
<p>{nafise.moosavi|michael.strube}@h-its.org
</p>
<p>Abstract
</p>
<p>Lexical features are a major source of in-
formation in state-of-the-art coreference
resolvers. Lexical features implicitly
model some of the linguistic phenomena
at a fine granularity level. They are es-
pecially useful for representing the con-
text of mentions. In this paper we in-
vestigate a drawback of using many lexi-
cal features in state-of-the-art coreference
resolvers. We show that if coreference
resolvers mainly rely on lexical features,
they can hardly generalize to unseen do-
mains. Furthermore, we show that the
current coreference resolution evaluation
is clearly flawed by only evaluating on a
specific split of a specific dataset in which
there is a notable overlap between the
training, development and test sets.
</p>
<p>1 Introduction
</p>
<p>Similar to many other tasks, lexical features are
a major source of information in current corefer-
ence resolvers. Coreference resolution is a set par-
titioning problem in which each resulting partition
refers to an entity. As shown by Durrett and Klein
(2013), lexical features implicitly model some lin-
guistic phenomena, which were previously mod-
eled by heuristic features, but at a finer level of
granularity. However, we question whether the
knowledge that is mainly captured by lexical fea-
tures can be generalized to other domains.
</p>
<p>The introduction of the CoNLL dataset en-
abled a significant boost in the performance of
coreference resolvers, i.e. about 10 percent differ-
ence between the CoNLL score of the currently
best coreference resolver, deep-coref by Clark and
Manning (2016b), and the winner of the CoNLL
2011 shared task, the Stanford rule-based system
</p>
<p>by Lee et al. (2013). However, this substantial im-
provement does not seem to be visible in down-
stream tasks. Worse, the difference between state-
of-the-art coreference resolvers and the rule-based
system drops significantly when they are applied
on a new dataset, even with consistent definitions
of mentions and coreference relations (Ghaddar
and Langlais, 2016a).
</p>
<p>In this paper, we show that if we mainly rely
on lexical features, as it is the case in state-of-the-
art coreference resolvers, overfitting become more
sever. Overfitting to the training dataset is a prob-
lem that cannot be completely avoided. However,
there is a notable overlap between the CoNLL
training, development and test sets that encour-
ages overfitting. Therefore, the current corefer-
ence evaluation scheme is flawed by only evalu-
ating on this overlapped validation set. To ensure
meaningful improvements in coreference resolu-
tion, we believe an out-of-domain evaluation is a
must in the coreference literature.
</p>
<p>2 Lexical Features
</p>
<p>The large difference in performance between
coreference resolvers that use lexical features and
ones which do not, implies the importance of lex-
ical features. Durrett and Klein (2013) show that
lexical features implicitly capture some phenom-
ena, e.g. definiteness and syntactic roles, which
were previously modeled by heuristic features.
Durrett and Klein (2013) use exact surface forms
as lexical features. However, when word embed-
dings are used instead of surface forms, the use
of lexical features is even more beneficial. Word
embeddings are an efficient way of capturing se-
mantic relatedness. Especially, they provide an ef-
ficient way for describing the context of mentions.
</p>
<p>Durrett and Klein (2013) show that the addi-
tion of some heuristic features like gender, num-
</p>
<p>14</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2003">https://doi.org/10.18653/v1/P17-2003</a></div>
</div>
<div class="page"><p />
<p>MUC B3 CEAFe CoNLL LEA
R P F1 R P F1 R P F1 Avg. F1 R P F1
</p>
<p>CoNLL test set
rule-based 64.29 65.19 64.74 49.18 56.79 52.71 52.45 46.58 49.34 55.60 43.72 51.53 47.30
berkeley 67.56 74.09 70.67 53.93 63.50 58.33 53.29 56.22 54.72 61.24 49.66 59.17 54.00
cort 67.83 78.35 72.71 54.34 68.42 60.57 53.10 61.10 56.82 63.37 50.40 64.46 56.57
deep-coref [conll] 70.55 79.13 74.59 58.17 69.01 63.13 54.20 63.44 58.45 65.39 54.55 65.35 59.46
deep-coref [lea] 70.43 79.57 74.72 58.08 69.26 63.18 54.43 64.17 58.90 65.60 54.55 65.68 59.60
</p>
<p>WikiCoref
rule-based 60.42 61.56 60.99 43.34 53.53 47.90 50.89 42.70 46.44 51.77 38.79 48.92 43.27
berkeley 68.52 55.96 61.61 59.08 39.72 47.51 48.06 40.44 43.92 51.01 - - -
cort 70.39 53.63 60.88 60.81 37.58 46.45 47.88 38.18 42.48 49.94 - - -
deep-coref [conll] 58.59 66.63 62.35 44.40 54.87 49.08 42.47 51.47 46.54 52.65 40.36 50.73 44.95
deep-coref [lea] 57.48 70.55 63.35 42.12 60.13 49.54 41.40 53.08 46.52 53.14 38.22 55.98 45.43
deep-coref− 55.07 71.81 62.33 38.05 61.82 47.11 38.46 50.31 43.60 51.01 34.11 57.15 42.72
</p>
<p>Table 1: Comparison of the results on the CoNLL test set and WikiCoref.
</p>
<p>ber, person and animacy agreements and syntactic
roles on top of their lexical features does not result
in a significant improvement.
</p>
<p>deep-coref, the state-of-the-art coreference re-
solver, follows the same approach. Clark and
Manning (2016b) capture the required information
for resolving coreference relations by using a large
number of lexical features and a small set of non-
lexical features including string match, distance,
mention type, speaker and genre features. The
main difference is that Clark and Manning (2016b)
use word embeddings instead of the exact surface
forms that are used by Durrett and Klein (2013).
</p>
<p>Based on the error analysis by cort (Martschat
and Strube, 2014), in comparison to systems that
do not use word embeddings, deep-coref has fewer
recall and precision errors especially for pro-
nouns. For example, deep-coref correctly recog-
nizes around 83 percent of non-anaphoric “it” in
the CoNLL development set. This could be a di-
rect result of a better context representation by
word embeddings.
</p>
<p>3 Out-of-Domain Evaluation
</p>
<p>Aside from the evident success of lexical features,
it is debatable how well the knowledge that is
mainly captured by the lexical information of the
training data can be generalized to other domains.
As reported by Ghaddar and Langlais (2016b),
state-of-the-art coreference resolvers trained on
the CoNLL dataset perform poorly, i.e. worse than
the rule-based system (Lee et al., 2013), on the
new dataset, WikiCoref (Ghaddar and Langlais,
2016b), even though WikiCoref is annotated with
the same annotation guidelines as the CoNLL
dataset. The results of some of recent coreference
resolvers on this dataset are listed in Table 1.
</p>
<p>The results are reported using MUC (Vilain
</p>
<p>et al., 1995), B3 (Bagga and Baldwin, 1998),
CEAFe (Luo, 2005), the average F1 score of
these three metrics, i.e. CoNLL score, and LEA
(Moosavi and Strube, 2016).
</p>
<p>berkeley is the mention-ranking model of Dur-
rett and Klein (2013) with the FINAL feature set
including the head, first, last, preceding and fol-
lowing words of a mention, the ancestry, length,
gender and number of a mention, distance of two
mentions, whether the anaphor and antecedent are
nested, same speaker and a small set of string
match features.
</p>
<p>cort is the mention-ranking model of Martschat
and Strube (2015). cort uses the following set of
features: the head, first, last, preceding and fol-
lowing words of a mention, the ancestry, length,
gender, number, type, semantic class, dependency
relation and dependency governor of a mention,
the named entity type of the head word, distance of
two mentions, same speaker, whether the anaphor
and antecedent are nested, and a set of string
match features. berkeley and cort scores in Table 1
are taken from Ghaddar and Langlais (2016a).
</p>
<p>deep-coref is the mention-ranking model of
Clark and Manning (2016b). deep-coref incorpo-
rates a large set of embeddings, i.e. embeddings of
the head, first, last, two previous/following words,
and the dependency governor of a mention in ad-
dition to the averaged embeddings of the five pre-
vious/following words, all words of the mention,
sentence words, and document words. deep-coref
also incorporates type, length, and position of a
mention, whether the mention is nested in any
other mention, distance of two mentions, speaker
features and a small set of string match features.
</p>
<p>For deep-coref [conll] the averaged CoNLL
score is used to select the best trained model on the
development set. deep-coref [lea] uses the LEA
</p>
<p>15</p>
<p />
</div>
<div class="page"><p />
<p>genre
bc bn mz nw pt tc wb
</p>
<p>train+dev
43% 50% 51% 45% 77% 38% 39%
</p>
<p>train
41% 49% 39% 44% 76% 37% 38%
</p>
<p>Table 2: Ratio of non-pronominal coreferent men-
tions in the test set that are seen as coreferent in
the training data.
</p>
<p>metric (Moosavi and Strube, 2016) for choosing
the best model. It is worth noting that the results
of deep-coref ’s ranking model may be slightly dif-
ferent at various experiments. However, the per-
formance of deep-coref [lea] is always higher than
that of deep-coref [conll].
</p>
<p>We add WikiCoref’s words to deep-coref ’s dic-
tionary for both deep-coref [conll] and deep-coref
[lea]. deep-coref− reports the performance of
deep-coref [lea] in which WikiCoref’s words are
not incorporated into the dictionary. Therefore,
for deep-coref−, WikiCoref’s words that do not
exist in CoNLL will be initialized randomly in-
stead of using pre-trained word2vec word embed-
dings. The performance gain of deep-coref [lea]
in comparison to deep-coref− indicates the bene-
fit of using pre-trained word embeddings and word
embeddings in general. Henceforth, we refer to
deep-coref [lea] as deep-coref.
</p>
<p>4 Why do Improvements Fade Away?
</p>
<p>In this section, we investigate how much lexical
features contribute to the fact that current improve-
ments in coreference resolution do not properly
apply to a new domain.
</p>
<p>Table 2 shows the ratio of non-pronominal
coreferent mentions in the CoNLL test set that also
appear as coreferent mentions in the training data.
These high ratios indicate a high degree of overlap
between the mentions of the CoNLL datasets.
</p>
<p>The highest overlap between the training and
test sets exists in genre pt (Bible). The tc (tele-
phone conversation) genre has the lowest over-
lap for non-pronominal mentions. However, this
genre includes a large number of pronouns. We
choose wb (weblog) and pt for our analysis as two
genres with low and high degree of overlap.
</p>
<p>Table 3 shows the results of the examined coref-
erence resolvers when the test set only includes
one genre, i.e. pt or wb, in two different settings:
(1) the training set includes all genres (in-domain
</p>
<p>evaluation), and (2) the corresponding genre of the
test set is excluded from the training and develop-
ment sets (out-of-domain evaluation).
</p>
<p>berkeley-final is the coreference resolver of
Durrett and Klein (2013) with the FINAL feature
set explained in Section 3. berkeley-surface is the
same coreference resolver with only surface fea-
tures, i.e. ancestry, gender, number, same speaker
and nested features are excluded from the FINAL
feature set.
</p>
<p>cort−lexical is a version of cort in which no
lexical feature is used, i.e. the head, first, last, gov-
ernor, preceding and following words of a mention
are excluded.
</p>
<p>For in-domain evaluations we train deep-coref ’s
ranking model for 100 iterations, i.e. the setting
used by Clark and Manning (2016a). However,
based on the performance on the development set,
we only train the model for 50 iterations in out-of-
domain evaluations.
</p>
<p>The results of the pt genre show that when
there is a high overlap between the training and
test datasets, the performance of all learning-based
classifiers significantly improves. deep-coref has
the largest gain from including pt in the training
data that is more than 13% based on the LEA score.
cort uses both lexical and a relatively large num-
ber of non-lexical features while berkeley-surface
is a pure lexicalized system. However, the differ-
ence between the berkeley-surface’s performances
when pt is included or excluded from the train-
ing data is lower than that of cort. berkeley uses
feature-value pruning so lexical features that occur
fewer than 20 times are pruned from the training
data. Maybe, this is the reason that berkeley’s per-
formance difference is less than other lexicalized
systems in highly overlapping datasets.
</p>
<p>For a less overlapping genre, i.e. wb, the perfor-
mance gain of including the genre in the training
data is significantly lower for all lexicalized sys-
tems. Interestingly, the performance of berkeley-
final, cort and cort−lexical increases for the wb
genre when this genre is excluded from the train-
ing set. deep-coref, which uses a complex deep
neural network and mainly lexical features, has the
highest gain from the redundancy in the training
and test datasets. As we use more complex neu-
ral networks, there is more capacity for brute-force
memorization of the training dataset.
</p>
<p>It is also worth noting that the performance
gains and drops in out-of-domain evaluations are
</p>
<p>16</p>
<p />
</div>
<div class="page"><p />
<p>CoNLL LEA CoNLL LEA
Avg. F1 R P F1 Avg. F1 R P F1
</p>
<p>pt
in-domain out-of-domain
</p>
<p>rule-based - - - - 65.01 50.58 65.02 56.90
berkeley-surface 69.15 58.57 65.24 61.73 63.01 46.56 62.13 53.23
berkeley-final 70.71 60.48 67.29 63.70 64.24 47.10 65.77 54.89
cort 72.56 61.82 70.70 65.96 64.60 46.85 67.69 55.37
cort−lexical 69.48 54.26 70.33 61.26 64.32 45.63 68.51 54.77
deep-coref 75.61 68.48 73.70 71.00 66.06 52.44 63.84 57.58
</p>
<p>wb
in-domain out-of-domain
</p>
<p>rule-based - - - - 53.80 45.19 44.98 45.08
berkeley-surface 56.37 45.72 47.20 46.45 55.14 45.94 44.59 45.26
berkeley-final 56.08 44.20 50.45 47.12 57.31 50.33 46.17 48.16
cort 59.29 50.37 51.56 50.96 58.87 51.47 50.96 51.21
cort−lexical 56.83 51.00 47.34 49.10 57.10 51.50 47.83 49.60
deep-coref 61.46 48.04 60.99 53.75 57.17 50.29 47.27 48.74
</p>
<p>Table 3: In-domain and out-of-domain evaluations for a high and a low overlapped genres.
</p>
<p>Anaphor
Antecedent Proper Nominal Pronominal
</p>
<p>Proper seen 80% 85% 77%all 3221 261 1200
</p>
<p>Nominal seen 75% 93% 95%all 69 1673 1315
</p>
<p>Pronominal seen 58% 99% 100%all 85 74 4737
</p>
<p>Table 4: Ratio of links created by deep-coref for
which the head-pair is seen in the training data.
</p>
<p>not entirely because of lexical features, as the per-
formance of cort−lexical also drops significantly
in pt out-of-domain evaluation. The classifier may
also memorize other properties of the seen men-
tions in the training data. However, in compari-
son to features like gender and number agreement
or syntactic roles, lexical features have the highest
potential for overfitting.
</p>
<p>We further analyze the output of deep-coref on
the development set. The all rows in Table 4 show
the number of pairwise links that are created by
deep-coref on the development set for different
mention types. The seen rows show the ratio of
each category of links for which the (antecedent
head, anaphor head) pair is seen in the training set.
All ratios are surprisingly high. The most worri-
some cases are those in which both mentions are
either a proper name or a common noun.
</p>
<p>Table 5 further divides the links of Table 4 based
on whether they are correct coreferent links. The
results of Table 5 show that most of the incorrect
links are also made between the mentions that are
both seen in the training data.
</p>
<p>The high ratios indicate that (1) there is a high
</p>
<p>Anaphor
Proper Nominal Pronominal
</p>
<p>Antecedent Correct decisions
</p>
<p>Proper seen 82% 85% 78%all 2603 150 921
</p>
<p>Nominal seen 76% 94% 96%all 42 1058 890
</p>
<p>Pronominal seen 63% 98% 100%all 49 44 3998
Incorrect decisions
</p>
<p>Proper seen 73% 85% 76%a11 618 111 279
</p>
<p>Nominal sen 74% 92% 94%all 27 615 425
</p>
<p>Pronominal seen 50% 100% 100%all 36 30 739
</p>
<p>Table 5: Ratio of links created by deep-coref for
which the head-pair is seen in the training data.
</p>
<p>overlap between the mention pairs of the training
and development sets, and (2) even though that
deep-coref uses generalized word embeddings in-
stead of exact surface forms, it is strongly biased
towards the seen mentions.
</p>
<p>We analyze the links that are created by Stan-
ford’s rule-based system and compute the ratio of
the links that exist in the training set. All corre-
sponding ratios are lower than those of deep-coref
in Table 5. However, the ratios are surprisingly
high for a system that does not use the training
data. This analysis emphasizes the overlap in the
CoNLL datasets. Because of this high overlap, it
is not easy to assess the generalizability of a coref-
erence resolver to unseen mentions on the CoNLL
dataset given its official split.
</p>
<p>We also compute the ratios of Table 5 for the
missing links that are associated with the recall er-
</p>
<p>17</p>
<p />
</div>
<div class="page"><p />
<p>Anaphor
Antecedent Proper Nominal Pronominal
</p>
<p>Proper seen 63% 51% 75%all 818 418 278
</p>
<p>Nominal seen 44% 73% 90%all 168 892 538
</p>
<p>Pronominal seen 82% 90% 100%all 49 59 444
</p>
<p>Table 6: Ratio of deep-coref’s recall errors for
which the head-pair exists in the training data.
</p>
<p>rors of deep-coref. We compute the recall errors
by cort error analysis tool (Martschat and Strube,
2014). Table 6 shows the corresponding ratios for
recall errors. The lower ratios of Table 6 in com-
parison to those of Table 4 emphasize the bias of
deep-coref towards the seen mentions.
</p>
<p>For example, the deep-coref links include 31
cases in which both mentions are either proper
names or common nouns and the head of one of
the mentions is “country”. For all these links,
“country” is linked to a mention that is seen in the
training data. Therefore, this raises the question
how the classifier would perform on a text about
countries not mentioned in the training data.
</p>
<p>Memorizing the pairs in which one of them is a
common noun could help the classifier to capture
world knowledge to some extent. From the seen
pairs like (Haiti, his country), and (Guangzhou,
the city) the classifier could learn that “Haiti” is
a country and “Guangzhou” is a city. However, it
is questionable how useful word knowledge is if it
is mainly based on the training data.
</p>
<p>The coreference relation of two nominal noun
phrases with no head match can be very hard to
resolve. The resolution of such pairs has been re-
ferred to as capturing semantic similarity (Clark
and Manning, 2016b). deep-coref links 49 such
pairs on the development set. Among all these
links, only 5 pairs are unseen on the training set
and all of them are incorrect links.
</p>
<p>The effect of lexical features is also analyzed
by Levy et al. (2015) for tasks like hypernymy and
entailment. They show that state-of-the-art classi-
fiers memorize words from the training data. The
classifiers benefit from this lexical memorization
when there are common words between the train-
ing and test sets.
</p>
<p>5 Discussion
</p>
<p>We show the extensive use of lexical features bi-
ases coreference resolvers towards seen mentions.
</p>
<p>This bias holds us back from developing more ro-
bust and generalizable coreference resolvers. Af-
ter all, while coreference resolution is an impor-
tant step for text understanding, it is not an end-
task. Coreference resolvers are going to be used
in tasks and domains for which coreference an-
notated corpora may not be available. Therefore,
generalizability should be brought into attention in
developing coreference resolvers.
</p>
<p>Moreover, we show that there is a significant
overlap between the training and validation sets in
the CoNLL dataset. The LEA metric (Moosavi and
Strube, 2016) is introduced as an attempt to make
coreference evaluations more reliable. However,
in order to ensure valid developments on corefer-
ence resolution, it is not enough to have reliable
evaluation metrics. The validation set on which
the evaluations are performed also needs to be re-
liable. A dataset is reliable for evaluations if a con-
siderable improvement on this dataset indicates a
better solution for the coreference problem instead
of a better exploitation of the dataset itself.
</p>
<p>This paper is not intended to argue against the
use of lexical features. Especially, when word em-
beddings are used as lexical features. The incorpo-
ration of word embeddings is an efficient way for
capturing semantic relatedness. Maybe we should
use them more for describing the context and less
for describing the mentions themselves. Pruning
rare lexical features plus incorporating more gen-
eralizable features could also help to prevent over-
fitting.
</p>
<p>To ensure more meaningful improvements, we
ask to incorporate out-of-domain evaluations in
the current coreference evaluation scheme. Out-
of-domain evaluations could be performed by us-
ing either the existing genres of the CoNLL dataset
or by using other existing coreference annotated
datasets like WikiCoref, MUC or ACE.
</p>
<p>Acknowledgments
</p>
<p>The authors would like to thank Kevin Clark for
answering all of our questions regarding deep-
coref. We would also like to thank the three anony-
mous reviewers for their thoughtful comments.
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany. The first au-
thor has been supported by a Heidelberg Institute
for Theoretical Studies PhD. scholarship.
</p>
<p>18</p>
<p />
</div>
<div class="page"><p />
<p>References
Amit Bagga and Breck Baldwin. 1998. Algorithms
</p>
<p>for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28–30
May 1998. pages 563–566.
</p>
<p>Kevin Clark and Christopher D. Manning. 2016a.
Deep reinforcement learning for mention-
ranking coreference models. In Proceedings
of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, Austin,
Tex., 1–5 November 2016. pages 2256–2262.
https://www.aclweb.org/anthology/D16-1245.pdf.
</p>
<p>Kevin Clark and Christopher D. Manning. 2016b. Im-
proving coreference resolution by learning entity-
level distributed representations. In Proceedings
of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), Berlin, Germany, 7–12 August 2016.
http://www.aclweb.org/anthology/P16-1061.pdf.
</p>
<p>Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In
Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, Seat-
tle, Wash., 18–21 October 2013. pages 1971–1982.
http://www.aclweb.org/anthology/D13-1203.pdf.
</p>
<p>Abbas Ghaddar and Philippe Langlais. 2016a. Coref-
erence in Wikipedia: Main concept resolution.
In Proceedings of the 20th Conference on Com-
putational Natural Language Learning, Berlin,
Germany, 11–12 August 2016. pages 229–238.
https://www.aclweb.org/anthology/K16-1023.pdf.
</p>
<p>Abbas Ghaddar and Philippe Langlais. 2016b.
WikiCoref: An English coreference-annotated
corpus of Wikipedia articles. In Proceedings
of the 10th International Conference on Lan-
guage Resources and Evaluation, Portorož,
Slovenia, 23–28 May 2016. http://www.lrec-
conf.org/proceedings/lrec2016/pdf/192 Paper.pdf.
</p>
<p>Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2013. Deterministic coreference res-
olution based on entity-centric, precision-ranked
rules. Computational Linguistics 39(4):885–916.
https://www.aclweb.org/anthology/J13-4004.pdf.
</p>
<p>Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Den-
ver, Col., 31 May – 5 June 2015. pages 970–976.
http://www.aclweb.org/anthology/N15-1098.
</p>
<p>Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
</p>
<p>Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6–8
October 2005. pages 25–32.
</p>
<p>Sebastian Martschat and Michael Strube. 2014. Re-
call error analysis for coreference resolution. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, Doha,
Qatar, 25–29 October 2014. pages 2070–2081.
http://www.aclweb.org/anthology/D14-1221.pdf.
</p>
<p>Sebastian Martschat and Michael Strube. 2015. Latent
structures for coreference resolution. Transactions
of the Association for Computational Linguistics
3:405–418. http://www.aclweb.org/anthology/Q15-
1029.pdf.
</p>
<p>Nafise Sadat Moosavi and Michael Strube. 2016.
Which coreference evaluation metric do you
trust? A proposal for a link-based entity aware
metric. In Proceedings of the 54th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), Berlin,
Germany, 7–12 August 2016. pages 632–642.
http://www.aclweb.org/anthology/P16-1060.pdf.
</p>
<p>Sameer Pradhan, Xiaoqiang Luo, Marta Recasens,
Eduard Hovy, Vincent Ng, and Michael Strube.
2014. Scoring coreference partitions of predicted
mentions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short
Papers), Baltimore, Md., 22–27 June 2014. pages
30–35. http://www.aclweb.org/anthology/P14-
2006.pdf.
</p>
<p>Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Confer-
ence (MUC-6). Morgan Kaufmann, San Mateo, Cal.,
pages 45–52.
</p>
<p>19</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 20–25
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2004
</p>
<p>Alternative Objective Functions
for Training MT Evaluation Metrics
</p>
<p>Miloš Stanojević
ILLC
</p>
<p>University of Amsterdam
m.stanojevic@uva.nl
</p>
<p>Khalil Sima’an
ILLC
</p>
<p>University of Amsterdam
k.simaan@uva.nl
</p>
<p>Abstract
</p>
<p>MT evaluation metrics are tested for cor-
relation with human judgments either at
the sentence- or the corpus-level. Trained
metrics ignore corpus-level judgments and
are trained for high sentence-level correla-
tion only. We show that training only for
one objective (sentence or corpus level),
can not only harm the performance on the
other objective, but it can also be subopti-
mal for the objective being optimized. To
this end we present a metric trained for
corpus-level and show empirical compar-
ison against a metric trained for sentence-
level exemplifying how their performance
may vary per language pair, type and level
of judgment. Subsequently we propose a
model trained to optimize both objectives
simultaneously and show that it is far more
stable than–and on average outperforms–
both models on both objectives.
</p>
<p>1 Introduction
</p>
<p>Ever since BLEU (Papineni et al., 2002) many
proposals for an improved automatic evaluation
metric for Machine Translation (MT) have been
made. Some proposals use additional information
for extracting quality indicators, like paraphrasing
(Denkowski and Lavie, 2011), syntactic trees (Liu
and Gildea, 2005; Stanojević and Sima’an, 2015)
or shallow semantics (Rios et al., 2011; Lo et al.,
2012) etc. Whereas others use different match-
ing strategies, like n-grams (Papineni et al., 2002),
treelets (Liu and Gildea, 2005) and skip-bigrams
(Lin and Och, 2004). Most metrics use several
indicators of translation quality which are often
combined in a linear model whose weights are es-
timated on a training set of human judgments.
</p>
<p>Because the most widely available type of hu-
man judgments are relative ranking (RR) judg-
ments, the main machine learning method used for
training the metrics were based on the learning-
to-rank framework (Li, 2011). While the effec-
tiveness of this framework for training evaluation
metrics has been confirmed many times, e.g., (Ye
et al., 2007; Duh, 2008; Stanojević and Sima’an,
2014; Ma et al., 2016), so far there is no prior work
exploring alternative objective functions for train-
ing learning-to-rank models. Without exception,
all existing learning-to-rank models are trained to
rank sentences while completely ignoring the cor-
pora judgments, likely because human judgments
come in the form of sentence rankings.
</p>
<p>It might seem that sentence and corpus level
tasks are very similar but that is not the case. Em-
pirically it has been shown that many metrics that
perform well on the sentence level do not perform
well on the corpus level and vice versa. By train-
ing to rank sentences the model does not necessar-
ily learn to give scores that are well scaled, but
only to give higher scores to better translations.
Training for the corpus level score would force the
metric to give well scaled scores on the sentence
level.
</p>
<p>Human judgments of sentences can be aggre-
gated in different ways to hypothesize human
judgments of full corpora. However, this fact has
not been used so far to train learning-to-rank mod-
els that are good for ranking different corpora.
</p>
<p>This work fills-in this gap by exploring the mer-
its of different objective functions that take corpus
level judgments into consideration. We first create
a learning-to-rank model for ranking corpora and
compare it to the standard learning-to-rank model
that is trained for ranking sentences. This com-
parison shows that performance of these two ob-
jectives can vary radically depending on the cho-
sen meta-evaluation method. To tackle this prob-
</p>
<p>20</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2004">https://doi.org/10.18653/v1/P17-2004</a></div>
</div>
<div class="page"><p />
<p>Φswin Φslos Φcwin Φclos
</p>
<p>forward forward corpScore corpScore
</p>
<p>margin loss margin loss
</p>
<p>LossCorp
average
</p>
<p>LossSent
average
</p>
<p>LossJoint
</p>
<p>Figure 1: Computation Graph
</p>
<p>lem we contribute a new objective function, in-
spired by multi-task learning, in which we train
for both objectives simultaneously. This multi-
objective model behaves a lot more stable over all
methods of meta-evaluation and achieves a higher
correlation than both single objective models.
</p>
<p>2 Models
</p>
<p>All the models that we define have one basic func-
tion in common, we call it a forward(·) function,
that maps the features of any sentence to a sin-
gle real number. That function can be any differ-
entiable function including multi-layer neural net-
works as in (Ma et al., 2016), but here we will stick
with the standard linear model:
</p>
<p>forward(φ) = φTw + b
</p>
<p>Here φ is a vector with feature values of a sen-
tence, w is a weight vector and b is a bias term.
Usually in training we would like to process a
mini-batch of feature vectors Φ, where Φ is a ma-
trix in which each column is a feature vector of
individual sentence in the mini-batch or in the cor-
pus. By using broadcasting we can rewrite the pre-
vious definition of the forward(·) function as:
</p>
<p>forward(Φ) = ΦTw + b
</p>
<p>Now we can define the score of a sentence as a
sigmoid function applied over the output of the
forward(·) function because we want to get a
score between 0 and 1:
</p>
<p>sentScore(φ) = σ(forward(φ))
</p>
<p>As the corpus level score we will use just the av-
erage of sentence level scores:
</p>
<p>corpScore(Φ) =
1
</p>
<p>m
</p>
<p>∑
sentScore(Φ)
</p>
<p>where m is the number of sentences in the corpus.
Next we present several objective functions that
</p>
<p>are illustrated by the computation graph in Fig-
ure 1.
</p>
<p>2.1 Training for Sentence Level Accuracy
Here we use the training objective very similar to
BEER (Stanojević and Sima’an, 2014) which is
a learning-to-rank framework that finds a separat-
ing hyper-plane between “good” and “bad” trans-
lations. Unlike BEER, we use a max-margin ob-
jective instead of logistic regression.
</p>
<p>For each mini-batch we randomly select m hu-
man relative ranking pairwise judgments and after
extracting features for all the sentences taking part
in these judgments we put features in two matrices
Φswin and Φslos. These matrices are structured in
such a way that for judgment i the column i in
Φswin contains the features of the “good” transla-
tion in the judgment and the column i in Φslos the
features of the “bad” translation.
</p>
<p>We would like to maximize the average mar-
gin that would separate sentence level scores of
pairs of translations in each judgment. Because
the squashing sigmoid function does not influence
the ranking we can directly optimize on the un-
squashed forward pass and require that the margin
between “good” and “bad” translation is at least 1:
</p>
<p>∆sent = forward(Φswin)− forward(Φslos)
</p>
<p>LossSent =
1
</p>
<p>m
</p>
<p>∑
max(0, 1−∆sent)
</p>
<p>2.2 Training for Corpus Level Accuracy
At the corpus level we would like to do a simi-
lar thing as on the sentence level: maximize the
distance between the scores of “good” and “bad”
corpora. In this case we have additional informa-
tion that is not present on the sentence level: we
know not only which corpus is (according to hu-
mans) better, but also by how much it is better. For
</p>
<p>21</p>
<p />
</div>
<div class="page"><p />
<p>that we can use one of the heuristics such as the
Expected Wins (Koehn, 2012). We can use this
information to guide the learning model by how
much it should separate the scores of two corpora.
</p>
<p>For doing this we use an approach similar
to Max-Margin Markov Networks (Taskar et al.,
2003) where for each training instance we dynami-
cally scale the margin that should be enforced. We
want the margin between the scores ∆corp to be
at least as big as the margin between the human
scores ∆human assigned to these systems. In one
mini-batch we will use only a randomly chosen
pair of corpora with feature matrices Φcwin and
Φclos for which we have a human comparison. The
corpus level loss function is given by:
</p>
<p>∆corp = corpScore(Φcwin)− corpScore(Φclos)
LossCorp = max(0,∆human −∆corp)
</p>
<p>2.3 Training Jointly for Sentence and Corpus
Level Accuracy
</p>
<p>In this model we optimize both objectives jointly
in the style of multi-task learning (Caruana, 1997).
Here we employ the simplest approach of just
tasking the interpolation of the previously intro-
duced loss functions.
</p>
<p>LossJoint = α · LossSent + (1− α) · LossCorp
</p>
<p>The interpolation is controlled by the hyper-
parameter α which could in principle be tuned for
good performance, but here we just fix it to 0.5 to
give both objectives equal importance.
</p>
<p>2.4 Feature Functions
</p>
<p>The feature functions that are used are reimple-
mentation of many (but not all) feature functions
of BEER. Because the point of this paper is about
the exploration of different objective functions we
did not try to experiment with more complex fea-
ture functions based on paraphrasing, function
words or permutation trees.
</p>
<p>We use just simple precision, recall and 3 types
of F-score (with β parameters 1, 2 and 0.5) over
different “pieces” of translation:
</p>
<p>• character n-grams of orders 1,2,3,4 and 5
• word n-grams of orders 1,2,3 and 4
• skip-bigrams of maximum skip 2 and ∞
</p>
<p>(similar to ROUGE-S2 and ROUGE-S* (Lin
and Och, 2004))
</p>
<p>One final feature deals with length-disbalance.
If the length of the system and reference trans-
lation are a and b respectively then this feature
is computed as max(a,b)−min(a,b)min(a,b) . It is computed
both for word and character length.
</p>
<p>3 Experiments
</p>
<p>Experiments are conducted on WMT13
(Macháček and Bojar, 2013), WMT14 (Machacek
and Bojar, 2014) and WMT16 (Bojar et al., 2016)
datasets which were used as training, validation
and testing datasets respectively.
</p>
<p>All of the models are implemented using Ten-
sorFlow1 and trained with L2 regularization λ =
0.001 and ADAM optimizer with learning rate
0.001. The mini-batch size for sentence level
judgments is 2000 and for the corpus level is one
comparison. Each model is trained for 200 epochs
out of which the one performing best on the val-
idation set for the objective function being opti-
mized is used during the test time.
</p>
<p>We show the results for the relative ranking
(RR) judgments correlation in Table 1. For all lan-
guage pairs that are of the form en-X we show it
under the column X and for all the language pairs
that have English on the target side we present
their average under the column en.
</p>
<p>RR corpus vs. sentence objective The corpus-
objective is better than the sentence-objective for
both corpus and sentence level RR judgments on 5
out of 7 languages and also on average correlation.
</p>
<p>RR joint vs. single-objectives Training for the
joint objective improves even more on both lev-
els of RR correlation and outperforms both single-
objective models on average and on 4 out of 7 lan-
guages.
</p>
<p>Making confident conclusions from these re-
sults is difficult because, to the best of our knowl-
edge, there is no principled way of measuring sta-
tistical significance on the RR judgments. That
is why we also tested on direct assessment (DA)
judgments available from WMT16. On DA we
can measure statistical significance on the sen-
tence level using Williams test (Graham et al.,
2015) and on the corpus level using combination
of hybrid-supersampling and Williams test (Gra-
ham and Liu, 2016). The results of correlation
with human judgment are for sentence and corpus
level are shown in Table 2.
</p>
<p>1https://www.tensorflow.org/
</p>
<p>22</p>
<p />
</div>
<div class="page"><p />
<p>Objective en cs de fi ro ru tr Average
sent 0.963 0.977 0.737 0.938 0.922 0.905 0.937 0.912
corpus 0.944 0.982 0.765 0.940 0.917 0.907 0.954 0.916
joint 0.963 0.983 0.748 0.951 0.933 0.905 0.946 0.918
</p>
<p>(a) Corpus level
</p>
<p>Objective en cs de fi ro ru tr Average
sent 0.347 0.405 0.345 0.304 0.293 0.382 0.304 0.340
corpus 0.337 0.414 0.349 0.307 0.292 0.385 0.325 0.344
joint 0.350 0.410 0.356 0.296 0.299 0.396 0.312 0.346
</p>
<p>(b) Sentence level
</p>
<p>Table 1: Relative Ranking (RR) Correlation. The corpus level correlation is measured with Pearson r
and sentence level with Kendall τ
</p>
<p>Objective en-ru cs-en de-en fi-en ro-en ru-en tr-en Average
sent 0.9113CJ 0.9839
</p>
<p>C 0.8483C 0.9556CJ 0.8348
C 0.8888C 0.9706CJ 0.9133
</p>
<p>corpus 0.9086 0.9790 0.8032 0.9121 0.7933 0.8857 0.9011 0.8833
joint 0.9111C 0.9844CS 0.8488
</p>
<p>C
S 0.9545
</p>
<p>C 0.8399CS 0.8935
C
S 0.9647
</p>
<p>C 0.9138
(a) Corpus level
</p>
<p>Objective en-ru cs-en de-en fi-en ro-en ru-en tr-en Average
sent 0.6655C 0.6478C 0.4930C 0.4608C 0.5066C 0.5535C 0.5800C 0.5582
corpus 0.5632 0.5676 0.3913 0.3644 0.3771 0.4306 0.4579 0.4503
joint 0.6668C 0.6631SC 0.5019
</p>
<p>S
C 0.4608
</p>
<p>C 0.5276SC 0.5564
C 0.5830C 0.5657
</p>
<p>(b) Sentence level
</p>
<p>Table 2: Direct Assessment (DA) Pearson r Correlation. Super- and sub-scripts S, C and J signify that
the model outperforms with statistical significance (p &lt; 0.05) the model trained for sentence, corpus or
joint objective respectively. Bold marks that the system has outperformed both other models significantly.
</p>
<p>DA corpus vs. other objectives On DA judg-
ments the results for corpus level objective are
completely different than on the RR judgments.
On DA judgments the corpus-objective model is
significantly outperformed on both levels and on
all languages by both of the other objectives.
</p>
<p>This shows that gambling on one objective
function (being that sentence or corpus level ob-
jective) could give unpredictable results. This
is precisely the motivation for creating the joint
model with multi-objective training.
</p>
<p>DA joint vs. single objectives By choosing to
jointly optimize both objectives we get a much
more stable model that performs well both on DA
and RR judgments and on both levels of judgment.
On the DA sentence level, the joint model was not
outperformed by any other model and on 3 out of 7
language pairs it significantly outperforms both al-
ternative objectives. On the corpus level results are
</p>
<p>a bit mixed, but still joint objective outperforms
both other models on 4 out of 7 language pairs and
also it gives higher correlation on average.
</p>
<p>4 Conclusion
</p>
<p>In this work we found that altering the objective
function for training MT metrics can have radi-
cal effects on performance. Also the effects of
the objective functions can sometimes be unex-
pected: the sentence objective might not be good
for sentence level correlation (in case of RR judg-
ments) and the corpus objective might not be good
for corpus level correlation (in case of DA judg-
ments). The difference among objectives is better
explained by different types of human judgments:
the corpus objective is better for RR while sen-
tence objective is better for DA judgments.
</p>
<p>Finally, the best results are achieved by training
for both objectives at the same time. This gives
</p>
<p>23</p>
<p />
</div>
<div class="page"><p />
<p>an evaluation metric that is far more stable in its
performance over all methods of meta-evaluation.
</p>
<p>Acknowledgments
</p>
<p>This work is supported by NWO VICI grant
nr. 277-89-002, DatAptor project STW grant nr.
12271 and QT21 project H2020 nr. 645452.
</p>
<p>References
Ondřej Bojar, Yvette Graham, Amir Kamran,
</p>
<p>and Miloš Stanojević. 2016. Results of the
wmt16 metrics shared task. In Proceedings
of the First Conference on Machine Trans-
lation. Association for Computational Lin-
guistics, Berlin, Germany, pages 199–231.
http://www.aclweb.org/anthology/W/W16/W16-
2302.
</p>
<p>Rich Caruana. 1997. Multitask learn-
ing. Machine Learning 28(1):41–75.
https://doi.org/10.1023/A:1007379606734.
</p>
<p>Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
</p>
<p>Kevin Duh. 2008. Ranking vs. Regression in Machine
Translation Evaluation. In Proceedings of the
Third Workshop on Statistical Machine Transla-
tion. Association for Computational Linguistics,
Stroudsburg, PA, USA, StatMT ’08, pages 191–194.
http://dl.acm.org/citation.cfm?id=1626394.1626425.
</p>
<p>Yvette Graham and Qun Liu. 2016. Achieving accu-
rate conclusions in evaluation of automatic machine
translation metrics. In Proceedings of the 15th An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Com-
putational Linguistics, San Diego, CA.
</p>
<p>Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2015. Accurate evaluation of segment-level ma-
chine translation metrics. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics Hu-
man Language Technologies. Denver, Colorado.
</p>
<p>Philipp Koehn. 2012. Simulating human judg-
ment in machine translation evaluation campaigns.
In Proceedings of International Workshop on
Spoken Language Translation. http://www.mt-
archive.info/IWSLT-2012-Koehn.pdf.
</p>
<p>Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Mor-
gan &amp; Claypool Publishers.
</p>
<p>Chin-Yew Lin and Franz Josef Och. 2004. Au-
tomatic Evaluation of Machine Translation Qual-
ity Using Longest Common Subsequence and
Skip-bigram Statistics. In Proceedings of the
42Nd Annual Meeting on Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Stroudsburg, PA, USA, ACL ’04.
https://doi.org/10.3115/1218955.1219032.
</p>
<p>Ding Liu and Daniel Gildea. 2005. Syntactic
features for evaluation of machine transla-
tion. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summa-
rization. Association for Computational Lin-
guistics, Ann Arbor, Michigan, pages 25–32.
http://www.aclweb.org/anthology/W/W05/W05-
0904.
</p>
<p>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. 2012. Fully automatic semantic mt evalu-
ation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation. Associa-
tion for Computational Linguistics, Strouds-
burg, PA, USA, WMT ’12, pages 243–252.
http://dl.acm.org/citation.cfm?id=2393015.2393048.
</p>
<p>Qingsong Ma, Fandong Meng, Daqi Zheng, Mingxuan
Wang, Yvette Graham, Wenbin Jiang, and Qun Liu.
2016. Maxsd: A neural machine translation evalua-
tion metric optimized by maximizing similarity dis-
tance. In Chin-Yew Lin, Nianwen Xue, Dongyan
Zhao, Xuanjing Huang, and Yansong Feng, editors,
Natural Language Understanding and Intelligent
Applications: 5th CCF Conference on Natural Lan-
guage Processing and Chinese Computing and 24th
International Conference on Computer Processing
of Oriental Languages. Springer International Pub-
lishing, Kunming, China, pages 153–161.
</p>
<p>Matous Machacek and Ondrej Bojar. 2014. Results of
the wmt14 metrics shared task. In Proceedings of
the Ninth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics,
Baltimore, Maryland, USA, pages 293–301.
http://www.aclweb.org/anthology/W/W14/W14-
3336.
</p>
<p>Matouš Macháček and Ondřej Bojar. 2013. Re-
sults of the WMT13 metrics shared task. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation. Association for Compu-
tational Linguistics, Sofia, Bulgaria, pages 45–51.
http://www.aclweb.org/anthology/W13-2202.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Au-
tomatic Evaluation of Machine Translation. In
Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, ACL ’02, pages 311–318.
https://doi.org/10.3115/1073083.1073135.
</p>
<p>24</p>
<p />
</div>
<div class="page"><p />
<p>Miguel Rios, Wilker Aziz, and Lucia Specia. 2011.
Tine: A metric to assess mt adequacy. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation. Association for Computational
Linguistics, Edinburgh, Scotland, pages 116–122.
http://www.aclweb.org/anthology/W11-2112.
</p>
<p>Miloš Stanojević and Khalil Sima’an. 2014. Fit-
ting Sentence Level Translation Evaluation with
Many Dense Features. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 202–206.
http://www.aclweb.org/anthology/D14-1025.
</p>
<p>Miloš Stanojević and Khalil Sima’an. 2015. BEER 1.1:
ILLC UvA submission to metrics and tuning task.
In Proceedings of the Tenth Workshop on Statisti-
cal Machine Translation. Association for Computa-
tional Linguistics, Lisbon, Portugal, pages 396–401.
http://aclweb.org/anthology/W15-3050.
</p>
<p>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-Margin Markov Networks. In NIPS 2014 -
Advances in Neural Information Processing Systems
27.
</p>
<p>Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007.
Sentence Level Machine Translation Evalua-
tion As a Ranking Problem: One Step Aside
from BLEU. In Proceedings of the Second
Workshop on Statistical Machine Translation.
Association for Computational Linguistics, Strouds-
burg, PA, USA, StatMT ’07, pages 240–247.
http://dl.acm.org/citation.cfm?id=1626355.1626391.
</p>
<p>25</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 26–31
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2005
</p>
<p>A Principled Framework for Evaluating Summarizers: Comparing
Models of Summary Quality against Human Judgments
</p>
<p>Maxime Peyrard and Judith Eckle-Kohler
Research Training Group AIPHES and UKP Lab
</p>
<p>Computer Science Department, Technische Universität Darmstadt
www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de
</p>
<p>Abstract
</p>
<p>We present a new framework for evaluat-
ing extractive summarizers, which is based
on a principled representation as optimiza-
tion problem. We prove that every ex-
tractive summarizer can be decomposed
into an objective function and an opti-
mization technique. We perform a com-
parative analysis and evaluation of sev-
eral objective functions embedded in well-
known summarizers regarding their corre-
lation with human judgments. Our com-
parison of these correlations across two
datasets yields surprising insights into the
role and performance of objective func-
tions in the different summarizers.
</p>
<p>1 Introduction
</p>
<p>The task of extractive summarization (ES) can nat-
urally be cast as a discrete optimization problem
where the text source is considered as a set of sen-
tences and the summary is created by selecting an
optimal subset of the sentences under a length con-
straint (McDonald, 2007; Lin and Bilmes, 2011).
</p>
<p>In this work, we go one step further and mathe-
matically prove that ES is equivalent to the prob-
lem of choosing (i) an objective function θ for
scoring system summaries, and (ii) an optimizer
O. We use (θ, O) to denote the resulting decompo-
sition of any extractive summarizer. Our proposed
decomposition enables a principled analysis and
evaluation of existing summarizers, and addresses
a major issue in the current evaluation of ES.
</p>
<p>This issue concerns the traditional “intrinsic”
evaluation comparing system summaries against
human reference summaries. This kind of evalu-
ation is actually an end-to-end evaluation of sum-
marization systems which is performed after θ has
been optimized by O. This is highly problematic
</p>
<p>from an evaluation point of view, because first,
θ is typically not optimized exactly, and second,
there might be side-effects caused by the particu-
lar optimization technique O, e.g., a sentence ex-
tracted to maximize θ might be suitable because of
other properties not included in θ. Moreover, the
commonly used evaluation metric ROUGE yields
a noisy surrogate evaluation (despite its good cor-
relation with human judgments) compared to the
much more meaningful evaluation based on hu-
man judgments. As a result, the current end-to-
end evaluation does not provide any insights into
the task of automatic summarization.
</p>
<p>The (θ,O) decomposition we propose addresses
this issue: it enables a well-defined and principled
evaluation of extractive summarizers on the level
of their components θ and O. In this work, we fo-
cus on the analysis and evaluation of θ, because
θ is a model of the quality indicators of a sum-
mary, and thus crucial in order to understand the
properties of “good” summaries. Specifically, we
compare θ functions of different summarizers by
measuring the correlation of their θ functions with
human judgments.
</p>
<p>Our goal is to provide an evaluation framework
which the research community could build upon
in future research to identify the best possible θ
and use it in optimization-based systems. We be-
lieve that the identification of such a θ is the cen-
tral question of summarization, because this op-
timal θ would represent an optimal definition of
summary quality both from an algorithmic point
of view and from the human perspective.
</p>
<p>In summary, our contribution is twofold: (i) We
present a novel and principled evaluation frame-
work for ES which allows evaluating the objec-
tive function and the optimization technique sep-
arately and independently. (ii) We compare well-
known summarization systems regarding their im-
plicit choices of θ by measuring the correlation
</p>
<p>26</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2005">https://doi.org/10.18653/v1/P17-2005</a></div>
</div>
<div class="page"><p />
<p>of their θ functions with human judgments on
two datasets from the Text Analysis Conference
(TAC). Our comparative evaluation yields surpris-
ing results and shows that extractive summariza-
tion is not solved yet.
</p>
<p>The code used in our experiments, includ-
ing a general evaluation tool is available at
github.com/UKPLab/acl2017-theta_
evaluation_summarization.
</p>
<p>2 Evaluation Framework
</p>
<p>2.1 (θ,O) decomposition
Let D = {si} be a document collection consid-
ered as a set of sentences. A summary S is then a
subset of D, or we can say that S is an element of
P(D), the power set of D.
Objective function We define an objective func-
tion to be a function that takes a summary of the
document collection D and outputs a score:
</p>
<p>θ : P(D) → R
S 7→ θD(S) (1)
</p>
<p>Optimizer Then the task of ES is to select the
set of sentences S∗ with maximal θ(S∗) under a
length constraint:
</p>
<p>S∗ = argmax
S
</p>
<p>θ(S)
</p>
<p>len(S) =
∑
</p>
<p>s∈S
len(s) ≤ c (2)
</p>
<p>We use O to denote the technique which solves
this optimization problem. O is an operator which
takes an objective function θ from the set of all
objective functions Θ and a document collection
D from the set of all document collections D, and
outputs a summary S∗:
</p>
<p>O : Θ×D → S
(θ,D) 7→ S∗ (3)
</p>
<p>Decomposition Theorem Now we show that the
problem of ES is equivalent to the problem of
choosing a decomposition (θ, O).
</p>
<p>We formalize an extractive summarizer σ as a
set function which takes a document collection
D ∈ D and outputs a summary SD,σ ∈ P(D).
With this formalism, it is clear that every (θ,O) tu-
ple forms a summarizer because O(θ, ·) produces
a summary from a document collection.
</p>
<p>But the other direction is also true: for every ex-
tractive summarizer there exists at least one tuple
(θ, O) which perfectly describes the summarizer:
</p>
<p>Theorem 1 ∀σ, ∃(θ,O) such that:
∀D ∈ D, σ(D) = O(θ,D)
</p>
<p>This theorem is quite intuitive, especially since
it is common to use a similar decomposition in
optimization-based summarization systems. In the
next section we illustrate the theorem by way of
several examples, and provide a rigorous proof of
the existence in the supplemental material.
</p>
<p>2.2 Examples of θ
We analyze a range of different summarizers re-
garding their (mostly implicit) θ.
ICSI (Gillick and Favre, 2009) is a global linear
optimization that extracts a summary by solving a
maximum coverage problem considering the most
frequent bigrams in the source documents. ICSI
has been among the best systems in a classical
ROUGE evaluation (Hong et al., 2014). For ICSI,
the identification of θ is trivial because it was for-
mulated as an optimization task. If ci is the i-th
bigram selected in the summary and wi its weight
computed from D, then:
</p>
<p>θICSI(S) =
∑
</p>
<p>ci∈S
ci ∗ wi (4)
</p>
<p>LexRank (Erkan and Radev, 2004) is a well-
known graph-based approach. A similarity graph
G(V,E) is constructed where V is the set of sen-
tences and an edge eij is drawn between sentences
vi and vj if and only if the cosine similarity be-
tween them is above a given threshold. Sentences
are scored according to their PageRank score inG.
We observe that θLexRank is given by:
</p>
<p>θLexRank(S) =
∑
</p>
<p>s∈S
PRG(s) (5)
</p>
<p>where PR is the PageRank score of sentence s.
KL-Greedy (Haghighi and Vanderwende, 2009)
minimizes the Kullback Leibler (KL) divergence
between the word distributions in the summary
and D (i.e θKL = −KL). Recently, Peyrard and
Eckle-Kohler (2016) optimized KL and Jensen
Shannon (JS) divergence with a genetic algorithm.
In this work, we use KL and JS for both unigram
and bigram distributions.
LSA (Steinberger and Jezek, 2004) is an approach
involving a dimensionality reduction of the term-
document matrix via Singular Value Decomposi-
tion (SVD). The sentences extracted should cover
the most important latent topics:
</p>
<p>θLSA =
∑
</p>
<p>t∈S
λt (6)
</p>
<p>27</p>
<p />
</div>
<div class="page"><p />
<p>where t is a latent topic identified by SVD on the
term-document matrix and λt the associated sin-
gular value.
Edmundson (Edmundson, 1969) is an older
heuristic method which scores sentences accord-
ing to cue-phrases, overlap with title, term fre-
quency and sentence position. θEdmundson is sim-
ply a weighted sum of these heuristics.
TF?IDF (Luhn, 1958) scores sentences with the
TF*IDF of their terms. The best sentences are then
greedily extracted. We use both the unigram and
bigram versions in our experiments.
</p>
<p>3 Experiments
</p>
<p>Now we compare the summarizers analyzed above
by measuring the correlation of their θ functions
with human judgments.
</p>
<p>Datasets We use two multi-document summa-
rization datasets from the Text Analysis Confer-
ence (TAC) shared task: TAC-2008 and TAC-
2009.1 TAC-2008 and TAC-2009 contain 48 and
44 topics, respectively. Each topic consists of 10
news articles to be summarized in a maximum of
100 words. We use only the so-called initial sum-
maries (A summaries), but not the update part.
</p>
<p>For each topic, there are 4 human reference
summaries along with a manually created Pyramid
set. In both editions, all system summaries and
the 4 reference summaries were manually evalu-
ated by NIST assessors for readability, content se-
lection (with Pyramid) and overall responsiveness.
At the time of the shared tasks, 57 systems were
submitted to TAC-2008 and 55 to TAC-2009. For
our experiments, we use the Pyramid and the re-
sponsiveness annotations.
</p>
<p>System Comparison For each θ, we compute
the scores of all system and all manual summaries
for any given topic. These scores are compared
with the human scores. We include the manual
summaries in our computation because this yields
a more diverse set of summaries with a wider
range of scores. Since an ideal summarizer would
create summaries as well as humans, an ideal θ
would also be able to correctly score human sum-
maries with high scores.
</p>
<p>For comparison, we also report the correlation
between pyramid and responsiveness.
</p>
<p>Correlations are measured with 3 metrics: Pear-
1http://tac.nist.gov/2009/
</p>
<p>Summarization/, http://tac.nist.gov/2008/
Summarization/
</p>
<p>son’s r, Spearman’s ρ and Normalized Discounted
Cumulative Gain (Ndcg). Pearson’s r is a value
correlation metric which depicts linear relation-
ships between the scores produced by θ and the
human judgments. Spearman’s ρ is a rank correla-
tion metric which compares the ordering of sys-
tems induced by θ and the ordering of systems
induced by human judgments. Ndcg is a metric
that compares ranked lists and puts more emphasis
on the top elements by logarithmic decay weight-
ing. Intuitively, it captures how well θ can rec-
ognize the best summaries. The optimization sce-
nario benefits from high Ndcg scores because only
summaries with high θ scores are extracted.
</p>
<p>Previous work on correlation analysis averaged
scores over topics for each system and then com-
puted the correlation between averaged scores
(Louis and Nenkova, 2013; Nenkova et al., 2007).
An alternative and more natural option which we
use here is to compute the correlation for each
topic and average these correlations over topics
(CORRELATION-AVERAGE). Since we want to
estimate how well θ functions measure the quality
of summaries, we find the summary level averag-
ing more meaningful.
</p>
<p>Analysis The results of our correlation analysis
are presented in Table 1.
</p>
<p>In our (θ,O) formulation, the end-to-end ap-
proach maps a set of documents to exactly one
summary selected by the system. We call the (clas-
sical and well known) evaluation of this single
summary end-to-end evaluation because it mea-
sures the end product of the system. This is in con-
trast to our proposed evaluation of the assumption
made by individual summarizers shown in Table 1.
A system summary was extracted by a given sys-
tem because it was high scoring using its θ, but we
ask the question whether optimizing this θ made
sense in the first place.
</p>
<p>We first observe that scores are relatively low.
Summarization is not a solved problem and the
systems we investigated can not identify correctly
what makes a good summary. This is in contrast
to the picture in the classical end-to-end evaluation
with ROUGE where state-of-the-art systems score
relatively high. Some Ndcg scores are higher (for
TAC-2008) which explains why these systems can
extract relatively good summaries in the end-to-
end evaluation. In this classical evaluation, only
the single best summary is evaluated, which means
that a system does not need to be able to rank all
</p>
<p>28</p>
<p />
</div>
<div class="page"><p />
<p>TAC-2008 TAC-2009
responsiveness Pyramid responsiveness Pyramid
</p>
<p>θ r ρ Ndcg r ρ Ndcg r ρ Ndcg r ρ Ndcg
</p>
<p>TF∗IDF-1 .1777 .2257 .5031 .1850 .2386 .3575 .1996 .2282 .3826 .2514 .2890 .2280
TF∗IDF-2 .0489 .1548 .5952 .0507 .1833 .4811 .0061 .1736 .4984 .1073 .2383 .3844
ICSI .1069 .1885 .6153 .1147 .2294 .5228 .1050 .1821 .5707 .1379 .2466 .5016
JS-1 .2504 .2762 .4411 .2798 .3205 .2804 .2021 .2282 .3896 .2616 .3042 .2272
JS-2 .0383 .1698 .5873 .0410 .2038 .4804 .0284 .1475 .5646 .0021 .2084 .4734
LexRank .1995 .1821 .6618 .2498 .2168 .5935 .2831 .2585 .6028 .3714 .3421 .5764
LSA .0437 .1137 .6772 .1144 .1131 .5997 .2965 .2127 .6641 .3677 .2935 .6467
Edmunds. .2223 .2686 .6372 .2665 .3164 .5521 .2598 .2604 .5852 .3647 .3720 .5594
KL-1 .1796 .2249 .4899 .2016 .2690 .3439 .1827 .2275 .4047 .2423 .2981 .2466
KL-2 .0023 .1661 .6165 .0023 .1928 .5135 .0437 .1435 .6171 .0211 .2060 .5462
</p>
<p>Pyramid .7031 .6606 .8528 — — — .7174 .6414 .8520 — — —
</p>
<p>Table 1: Correlation of θ functions with human judgments across various systems.
</p>
<p>possible summaries correctly.
We see that systems with high end-to-end
</p>
<p>ROUGE scores (according to Hong et al. (2014))
do not necessarily have a good model of summary
quality. Indeed, the best performing θ functions
are not part of the systems performing best with
ROUGE. For example, ICSI is the best system ac-
cording to ROUGE, but it is not clear that it has
the best model of summary quality. In TAC-2009,
LexRank, LSA and the heuristic Edmundson have
better correlations with human judgments. The
difference with end-to-end evaluation might stem
from the fact that ICSI solves the optimization
problem exactly, while LexRank and Edmundson
use greedy optimizers. There might also be some
side-effects from which ICSI profits: extracting
sentences to improve θ might lead to accidentally
selecting suitable sentences, because θ can merely
correlate well with properties of good summaries,
while not modeling these properties itself.
</p>
<p>It is worth noting that systems perform differ-
ently on TAC2009 and TAC2008. There are sev-
eral differences between TAC2008 and TAC2009
like redundancy level or guidelines for annota-
tions; for example, responsiveness is scored out
of 5 in 2008 and out of 10 in 2009. The LSA sum-
marizer ranks among the best systems in TAC2009
with pearson’s r but is closer to the worst sys-
tems in TAC2008. While this is difficult to ex-
plain we hypothesize that the model of summary
quality from LSA is sensitive to the slight vari-
ations and therefore not robust. In general, any
system which claims to have a better θ than previ-
ous works should indeed report results on several
datasets to ensure robustness and generality.
</p>
<p>Interestingly, we observe that the correlation be-
tween Pyramid and responsiveness is better than in
</p>
<p>any system, but still not particularly high. Respon-
siveness is an overall annotation while Pyramid is
a manual measure of content only. These results
confirm the intuition that humans take into account
much more aspects when evaluating summaries.
</p>
<p>4 Related Work and Discussion
</p>
<p>While correlation analyses on human judgment
data have been performed in the context of validat-
ing automatic summary evaluation metrics (Louis
and Nenkova, 2013; Nenkova et al., 2007; Lin,
2004), there is no prior work which uses these data
for a principled comparison of summarizers.
</p>
<p>Much previous work focused on efficient opti-
mizers O, such as ILP, which impose constraints
on the θ function. Linear (Gillick and Favre, 2009)
and submodular (Lin and Bilmes, 2011) θ func-
tions are widespread in the summarization com-
munity because they can be optimized efficiently
and effectively via ILP (Schrijver, 1986) and the
greedy algorithm for submodularity (Fujishige,
2005). A greedy approach is often used when θ
does not have convenient properties that can be
leveraged by a classical optimizer (Haghighi and
Vanderwende, 2009).
</p>
<p>Such interdependencies of O and θ limit the ex-
pressiveness of θ. However, realistic θ functions
are unlikely to be linear or submodular, and in
the well-studied field of optimization there exist
a range of different techniques developed to tackle
difficult combinatorial problems (Schrijver, 2003;
Blum and Roli, 2003).
</p>
<p>A recent example of such a technique adapted to
extractive summarization are meta-heuristics used
to optimize non-linear, non-submodular objec-
tive functions (Peyrard and Eckle-Kohler, 2016).
</p>
<p>29</p>
<p />
</div>
<div class="page"><p />
<p>Other methods like Markov Chain Monte Carlo
(Metropolis et al., 1953) or Monte-Carlo Tree
Search (Suttner and Ertel, 1991; Silver et al.,
2016) could also be adapted to summarization and
thus become realistic choices for O. General pur-
pose optimization techniques are especially ap-
pealing, because they offer a decoupling of θ and
O and allow investigating complex θ functions
without making any assumption on their mathe-
matical properties. In particular, this supports fu-
ture work on identifying an “optimal” θ as a model
of relevant quality aspects of a summary.
</p>
<p>5 Conclusion
</p>
<p>We presented a novel evaluation framework for ES
which is based on the proof that ES is equivalent
to the problem of choosing an objective function
θ and an optimizer O. This principled and well-
defined framework allows evaluating θ and O of
any extractive summarizer – separately and inde-
pendently. We believe that our framework can
serve as a basis for future work on identifying an
“optimal” θ function, which would provide an an-
swer to the central question of what are the prop-
erties of a “good” summary.
</p>
<p>Acknowledgments
</p>
<p>This work has been supported by the German Re-
search Foundation (DFG) as part of the Research
Training Group “Adaptive Preparation of Informa-
tion from Heterogeneous Sources” (AIPHES) un-
der grant No. GRK 1994/1, and via the German-
Israeli Project Cooperation (DIP, grant No. GU
798/17-1).
</p>
<p>References
Christian Blum and Andrea Roli. 2003. Metaheuris-
</p>
<p>tics in Combinatorial Optimization: Overview and
Conceptual Comparison. ACM Computing Surveys
35(3):268–308.
</p>
<p>H. P. Edmundson. 1969. New Methods in Automatic
Extracting. Journal of the Association for Comput-
ing Machinery 16(2):264–285.
</p>
<p>Günes Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based Lexical Centrality As Salience in Text
Summarization. Journal of Artificial Intelligence
Research pages 457–479.
</p>
<p>Satoru Fujishige. 2005. Submodular functions and op-
timization. Annals of discrete mathematics. Else-
vier, Amsterdam, Boston, Paris.
</p>
<p>Dan Gillick and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Boulder, Colorado, pages 10–18.
</p>
<p>Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-document Summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Boulder, Colorado, pages 362–370.
</p>
<p>Kai Hong, John Conroy, benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A Reposi-
tory of State of the Art and Competitive Base-
line Summaries for Generic News Summarization.
In Proceedings of the Ninth International Con-
ference on Language Resources and Evaluation
(LREC’14). European Language Resources Asso-
ciation (ELRA), Reykjavik, Iceland, pages 1608–
1616.
</p>
<p>Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop. Association for Computational Linguis-
tics, Barcelona, Spain, pages 74–81.
</p>
<p>Hui Lin and Jeff A. Bilmes. 2011. A Class of Sub-
modular Functions for Document Summarization.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Portland, Oregon, USA, pages
510–520.
</p>
<p>Annie Louis and Ani Nenkova. 2013. Automati-
cally Assessing Machine Summary Content With-
out a Gold Standard. Computational Linguistics
39(2):267–300.
</p>
<p>Hans Peter Luhn. 1958. The Automatic Creation of
Literature Abstracts. IBM Journal of Research De-
velopment 2:159–165.
</p>
<p>Ryan McDonald. 2007. A Study of Global Inference
Algorithms in Multi-document Summarization. In
Proceedings of the 29th European Conference on IR
Research. Springer-Verlag, Rome, Italy, pages 557–
564.
</p>
<p>Nicholas Metropolis, Arianna Rosenbluth, Marshall
Rosenbluth, Augusta Teller, and Edward Teller.
1953. Equation of State Calculations by Fast Com-
puting Machines. Journal of Chemical Physics
21:1087 – 1092.
</p>
<p>Ani Nenkova, Rebecca Passonneau, and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing Human Content Selection Variation in Summa-
rization Evaluation. ACM Transactions on Speech
and Language Processing (TSLP) 4(2).
</p>
<p>30</p>
<p />
</div>
<div class="page"><p />
<p>Maxime Peyrard and Judith Eckle-Kohler. 2016.
A General Optimization Framework for Multi-
Document Summarization Using Genetic Algo-
rithms and Swarm Intelligence. In Proceedings of
the 26th International Conference on Computational
Linguistics (COLING 2016). The COLING 2016 Or-
ganizing Committee, Osaka, Japan, pages 247 – 257.
</p>
<p>Alexander Schrijver. 1986. Theory of Linear and In-
teger Programming. John Wiley &amp; Sons, Inc., New
York, NY, USA.
</p>
<p>Alexander Schrijver. 2003. Combinatorial Optimiza-
tion - Polyhedra and Efficiency. Springer, New
York.
</p>
<p>David Silver, Aja Huang, Chris J. Maddison, Arthur
Guez, Laurent Sifre, George van den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, Sander Dieleman, Do-
minik Grewe, John Nham, Nal Kalchbrenner, Ilya
Sutskever, Timothy Lillicrap, Madeleine Leach, Ko-
ray Kavukcuoglu, Thore Graepel, and Demis Has-
sabis. 2016. Mastering the game of Go with
deep neural networks and tree search. Nature
529(7587):484–489.
</p>
<p>Josef Steinberger and Karel Jezek. 2004. Using latent
semantic analysis in text summarization and sum-
mary evaluation. In Proceedings of the 7th Inter-
national Conference on Information Systems Imple-
mentation and Modelling (ISIM ’04). Rožnov pod
Radhoštěm, Czech Republic, pages 93–100.
</p>
<p>Christian Suttner and Wolfgang Ertel. 1991. Using
Back-Propagation Networks for Guiding the Search
of a Theorem Prover. International Journal of Neu-
ral Networks Research &amp; Applications 2(1):3–16.
</p>
<p>31</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 32–37
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2006
</p>
<p>Vector space models for evaluating semantic fluency in autism
</p>
<p>Emily Prud’hommeaux†, Jan van Santen◦, Douglas Gliner†
†Rochester Institute of Technology, ◦Oregon Health &amp; Science University
</p>
<p>{emilypx,dgg5503}@rit.edu, vansantj@ohsu.edu
</p>
<p>Abstract
</p>
<p>A common test administered during neu-
rological examination is the semantic flu-
ency test, in which the patient must list as
many examples of a given semantic cate-
gory as possible under timed conditions.
Poor performance is associated with neu-
rological conditions characterized by im-
pairments in executive function, such as
dementia, schizophrenia, and autism spec-
trum disorder (ASD). Methods for analyz-
ing semantic fluency responses at the level
of detail necessary to uncover these dif-
ferences have typically relied on subjec-
tive manual annotation. In this paper, we
explore automated approaches for scor-
ing semantic fluency responses that lever-
age ontological resources and distribu-
tional semantic models to characterize the
semantic fluency responses produced by
young children with and without ASD. Us-
ing these methods, we find significant dif-
ferences in the semantic fluency responses
of children with ASD, demonstrating the
utility of using objective methods for clin-
ical language analysis.
</p>
<p>1 Introduction
</p>
<p>Semantic fluency tasks, in which patients under-
going neuropsychological evaluation must list as
many items as possible in a particular semantic
category in a fixed, brief period of time, are widely
used by clinicians to evaluate language, develop-
ment, and cognition. Performance on such tasks
is usually measured in terms of the raw number of
appropriate items produced. A more detailed anal-
ysis of these lists, however, can reveal patterns as-
sociated with a variety of neurological conditions,
including autism, dementia, and schizophrenia.
</p>
<p>Semantic fluency responses hold particular
promise for shedding light on the language of chil-
dren with autism spectrum disorder (ASD). ASD
has been associated with atypical semantics and
pragmatic expression since the condition was was
first identified over 70 years ago (Kanner, 1943).
One linguistic feature of ASD, referenced in many
of the diagnostic instruments for the disorder, is
the use of words that are meaningful but unex-
pected (Lord et al., 2002; Rutter et al., 2003), a
phenomenon that could play an important role in
the production of semantically related words.
</p>
<p>In this paper, we present NLP-informed ap-
proaches for automatically approximating the sub-
jective manual methods described in the psychol-
ogy literature for analyzing semantic fluency re-
sponses. Applying these methods to data collected
from young children with and without ASD, we
find that none of the standard manual measures of
semantic fluency are able to distinguish children
with ASD from those without. Several compu-
tationally derived measures, however, are signifi-
cantly different between diagnostic groups. These
results indicate that computationally derived mea-
sures of semantic fluency tap into subtle differ-
ences that would be difficult to detect using stan-
dard manual metrics, lending support for the clin-
ical utility of computational linguistic analysis.
</p>
<p>2 Background
The semantic fluency task is a subtype of a more
general word-generation task commonly referred
to as verbal fluency. In such tasks, a participant
must verbally produce a list of words belonging
to some category (e.g., animals) within a predeter-
mined amount of time, usually 60 seconds. Per-
formance on verbal fluency tasks has been corre-
lated with executive function, and differences in
verbal fluency scores have been noted in a vari-
ety of neurological conditions including dementia
</p>
<p>32</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2006">https://doi.org/10.18653/v1/P17-2006</a></div>
</div>
<div class="page"><p />
<p>(Henry et al., 2004), schizophrenia (Frith et al.,
1995), and autism (Turner, 1999; Geurts et al.,
2004; Spek et al., 2009; Begeer et al., 2014).
</p>
<p>The rate at which speakers generate words in
a semantic fluency response has been observed to
vary throughout the timed period, typically with
several related words being produced in close suc-
cession followed by a pause before a new burst
of related words (Bousfield et al., 1954). Troyer
et al. (1997) proposed two cognitive processes
underlying this pattern: clustering and switching.
Clustering refers to the tendency of speakers to list
words in clusters according to their membership
in a particular subcategory of the larger seman-
tic category (e.g., pets for the larger category of
animals). Switching is the decision made by the
speaker to abandon a subcategory when it has been
exhausted and to list items in a new subcategory.
</p>
<p>Autism is associated with deficits in executive
function, and thus we should expect to see con-
sistent patterns demonstrating deficits in seman-
tic fluency performance in the ASD population.
Several studies have found overall weaker perfor-
mance, in terms of raw item count, in individu-
als with ASD (Turner, 1999; Geurts et al., 2004;
Spek et al., 2009); other more recent studies, how-
ever, have not been able to replicate this finding
(Lopez et al., 2005; Inokuchi and Kamio, 2013;
Begeer et al., 2014). Similarly conflicting results
have been reported when evaluating the semantic
relatedness of adjacent words, with some finding
smaller clusters in ASD (Turner, 1999), some find-
ing larger clusters (Begeer et al., 2014), and still
others finding no differences (Spek et al., 2009).
</p>
<p>One likely source of these discrepancies is the
subjectivity inherent in the cluster assignment
task. Troyer et al. (1997) provide examples of
common clusters and their member animals, but
they note the difficulty in assigning items to sub-
categories, explaining that their proposed subcate-
gories were not generated using any existing tax-
onomy but instead grew organically out of the pat-
terns observed in the data. An additional compli-
cation is that a word’s subcategory membership is
dependent on its context. The word camel, for in-
stance, could be assigned to any number of cate-
gories (e.g., desert animal, zoo animal), depend-
ing on the nearby words. This is particularly prob-
lematic when analyzing the responses of children,
whose semantic categories might not align with
those of an adult annotator.
</p>
<p>In response to these challenges, some recent
work has focused on modeling the cluster-switch
behavior using computational linguistic methods,
in particular, using latent semantic analysis to cal-
culate the semantic similarity between adjacent
words. Mean scores over these similarity val-
ues can capture a individual’s tendency to use
a naming strategy relying on similarity (Nicode-
mus et al., 2014; Rosenstein et al., 2015). Other
work has focused on setting thresholds over these
similarity values in order to delineate the bound-
aries between clusters or chains of related words
(Rosenstein et al., 2015; Pakhomov and Hemmy,
2014). None of these studies, however, has com-
pared the output of the automated methods to man-
ual annotations in order to determine their accu-
racy. Furthermore, the thresholds used for clus-
ter boundary identification in these studies were
set by “rule of thumb” rather than empirically or
probabilistically.
</p>
<p>To our knowledge, this is the first attempt to
use distributional semantic models to analyze se-
mantic fluency responses in children with autism
spectrum disorder. More importantly, it is the first
study that uses machine learning to validate the
utility of these models for replicating and, perhaps
improving upon, human annotation methods of se-
mantic fluency responses.
</p>
<p>3 Data
The participants in this study were 22 children
with typical development (TD) and 22 high-
functioning children with ASD, ranging in age
from 4 to 9 years. ASD was diagnosed via
clinical consensus according to the Diagnostic
and Statistical Manual of Mental Disorders, 4th
Edition (DSM-IV-TR) criteria for Autistic Dis-
order (American Psychiatric Association, 2000)
and the established thresholds on two commonly
used diagnostic instruments: the Autism Diagnos-
tic Observation Schedule (ADOS) (Lord et al.,
2002) and the Social Communication Question-
naire (SCQ) (Rutter et al., 2003). None of the
participants analyzed here met the criteria for lan-
guage impairment, and the two groups were se-
lected so that there were no statistically signifi-
cant differences (via two-tailed t-test) between the
groups in chronological age, verbal IQ, and full
scale IQ. In addition to the experimental corpus,
we had access to a development set of 55 seman-
tic fluency responses that were discarded after the
groups were matched on these three criteria.
</p>
<p>33</p>
<p />
</div>
<div class="page"><p />
<p>During administration of the task, the clinician
asked the child to name as many animals as he
could as quickly as possible. The children’s re-
sponses were timed and recorded. The audio was
then transcribed by a speech-language pathologist,
and the transcripts were reviewed to remove extra-
neous dialogue and to standardize spelling. Two
manual annotations were performed: (1) seman-
tic clusters (Troyer et al., 1997), in which a clus-
ter consists of two or more animals belonging to
same subcategory (giraffe, elephant, lion); and (2)
semantic chains (Pakhomov and Hemmy, 2014),
in which each animal shares something in com-
mon at least with the immediately preceding an-
imal (elephant, lion, cat). Inter-annotator agree-
ment for labeling cluster boundaries according to
the Troyer criteria was low (Cohen’s κ &lt; 0.4); we
therefore limit our discussion to semantic chains,
whose boundaries were labeled with more sub-
stantial agreement (κ = 0.71).
</p>
<p>4 Features
4.1 Manually derived measures
Performance on a verbal fluency task is normally
evaluated by counting the number of unique items
produced in the designated time period. Credit is
given both to a general category such as fish and to
examples of that category, such as salmon; how-
ever, a morphological or descriptive variation of
another item (e.g., doggy for dog) is considered a
repetition and does not contribute to the total. We
report this count, along with the number of seman-
tic chains and mean length of semantic chain.
</p>
<p>4.2 Semantic similarity measures
There are a number of ways to measure the seman-
tic similarity between two words, some relying on
manually curated knowledge bases and other de-
rived distributionally from large text corpora. A
high mean similarity between adjacent word pairs
in a list of words might suggest that the list con-
tains a small number of large clusters of strongly
related words (a cluster-and-switch strategy) or a
sequence of items each of which is closely re-
lated to the previous item but not necessarily to the
items before that (a chaining strategy). In either
case, the participant is tapping into semantic sub-
categories when producing his response. A lower
mean similarity should indicate that a participant
has produced a large number of small clusters or
has selected items from the larger category seem-
ingly at random.
</p>
<p>One possible way to capture relatedness is by
using a manually curated lexical ontology that im-
plicitly encodes the similarity between pairs of
words, such as WordNet (Fellbaum, 1998). Var-
ious algorithms have been proposed for assigning
similarities scores for two synsets in WordNet by
traversing the hierarchical trees connecting those
synsets. Here we calculate the mean path sim-
ilarity for each adjacent word pair in a partici-
pant’s generated wordlist. Words not appearing
in WordNet were manually replaced with equiv-
alent synsets (e.g., puppy dog was replaced with
puppy). When multiple synsets were associated
with a given item, we used the first synset whose
hypernym included the synset for animal or imag-
inary being (e.g., pegasus).
</p>
<p>One disadvantage inherent in the WordNet on-
tology of animal names is that it is derived from
the biological taxonomy of the animal kingdom;
that is, the degree to which two animals are seman-
tically related within WordNet is determined pri-
marily by their biological similarity and not by se-
mantic features (e.g., region of origin, usual habi-
tat) that a non-zoologist might use to organize an-
imals names. In order to model multiple dimen-
sions of similarity, we turn to the use of vector
space models. We explore two vector-space rep-
resentations: latent semantic analysis (LSA) (Lan-
dauer et al., 1998) and continuous space neural
word embeddings (Bengio et al., 2003). Using
the gensim Python library (Řehůřek and Sojka,
2010), we built an LSA model and a word2vec
model, both with 400 dimensions but otherwise
using default parameters settings, on the full text
of Wikipedia downloaded in November, 2016. For
each model, we take the mean of the set of cosine
similarities between each adjacent pair of items
in a participant’s response. We also calculate the
mean similarity over 100 random permutations of
a participant’s wordlist to capture “global coher-
ence”, as proposed by Nicodemus et al. (2014).
</p>
<p>4.3 Measures of identifying semantic chains
Previous work in using word embeddings to model
clustering relied on a simple cosine similarity
threshold, determined heuristically (set arbitrarily
0.9 in Rosenstein et al. (2015), and at the 75th
percentile in Pakhomov and Hemmy (2014)), in
which a cluster boundary is inserted between any
two adjacent words whose similarity did not ex-
ceed that threshold. We instead propose to empiri-
cally determine the optimal value of such a thresh-
</p>
<p>34</p>
<p />
</div>
<div class="page"><p />
<p>Feature TD ASD t
Raw count 12.0 10.2 1.043
Manual chain count 6.14 4.86 1.603
Manual chain length 2.0 2.13 -0.572
WordNet path similarity 0.169 0.1697 0.1721
LSA cosine similarity 0.365 0.308 1.636
LSA coherence 0.311 0.248 1.934∗
w2v cosine similarity 0.427 0.392 1.710∗
w2v coherence 0.409 0.375 1.530
LSA chain count 4.09 4.31 -0.316
LSA chain length 3.38 1.87 2.310∗
w2v chain count 4.14 4.41 -0.3800
w2v chain length 3.07 1.91 1.9265∗
SVM chain count 4.09 4.86 -1.0894
SVM chain length 3.66 2.19 2.4164∗
</p>
<p>Table 1: Mean values by diagnostic group for
semantic fluency metrics (∗p &lt;0.05, one-tailed).
</p>
<p>old. First, while leaving one subject out, we iter-
atively sweep through a range of possible values
for the threshold to determine the value that max-
imizes the accuracy of semantic chain boundary
identification for the rest of the participants. We
then apply that threshold to the left-out subject.
</p>
<p>In addition to thresholding over individual simi-
larity metrics, we also use three similarity metrics
(WordNet path similarity, LSA cosine similarity,
and word2vec cosine similarity) as features within
a support vector machine to classify any pair of ad-
jacent words as either containing a semantic chain
boundary or as belonging to the same semantic
chain. Using all two-word sequences found in the
children’s responses and the manual indications of
the locations of cluster boundaries, we perform
leave-one-out cross validation to predict whether
the second word in each word pair represents the
start of a new chain or a continuation of the previ-
ous chain.
</p>
<p>Although the methods all achieved reasonable
boundary identification accuracy, with AUC rang-
ing from 0.65 to 0.8, we note that the goal of de-
termining cluster boundaries in this way is not to
replicate human cluster boundary insertion, which
we know to be subjective and difficult to perform
reliably. Rather, we are attempting to develop an
objective way to insert boundaries that does not
rely on an annotator’s ability to infer another indi-
vidual’s semantic organization of the world.
</p>
<p>5 Results
Table 1 shows the mean value for each group and
the t-statistic for each of the features. In contrast to
some previous work (Turner, 1999; Geurts et al.,
2004; Spek et al., 2009), we find no between-
group differences in raw item count. These re-
</p>
<p>sults, however, support other work that did not find
such differences when comparing groups matched
on verbal ability, as our groups are (Lopez et al.,
2005; Inokuchi and Kamio, 2013).
</p>
<p>Mean cosine similarity derived using the
word2vec model is significantly different between
the two groups, with the TD group showing a
higher mean similarity between adjacent items.
We also see that the global coherence measure, de-
rived by taking the mean similarity over 100 ran-
dom orderings of each list, is significantly higher
in the TD group when derived using LSA.
</p>
<p>Although there are no between-group differ-
ences in the manually derived measures of chain
count and chain length, we find differences in
chain length when derived using both threshold-
ing over similarity measures and machine learn-
ing. In all three cases, children with typical devel-
opment have longer semantic chains than children
with ASD, suggesting that TD children employ the
semantic chaining strategy that is reportedly pre-
ferred by neurotypical adults. In short, there are
differences in the semantic fluency responses of
young children with ASD, and these differences
would be difficult to reliably detect without ap-
pealing to computational techniques.
</p>
<p>Figure 1 shows two semantic fluency responses,
one produced by a child with ASD and one by
a child with TD, with plots indicating the cosine
similarities between adjacent words derived from
both the LSA and word2vec models. Semantic
chain boundaries proposed by the SVM are indi-
cated with vertical dashed lines. Note that LSA
and word2vec similarity values are only some-
what correlated, underscoring the potential utility
of combining the two scores for chain boundary
identification. As expected given the results in Ta-
ble 1, the child with ASD has generally lower co-
sine similarity scores and many more chain bound-
aries than the typically developing child.
</p>
<p>6 Discussion and future work
One problem with applying the chaining and clus-
tering paradigms to children is that the semantic
features linking animals for a child might be very
different those of adults. Well over half of the chil-
dren in this study included the sequence cat, bear
or bear, cat, despite the lack of clear relation be-
tween the two words from an adult’s perspective.
We found, however, that our automated methods
usually grouped these two words together, recog-
nizing some similarity that adults seem to miss. At
</p>
<p>35</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Plots of successive word-pair cosine similarity values derived using LSA and word2vec models
for a child with ASD (upper panel) and a child with TD (lower panel). Vertical dashed lines indicate
semantic chain boundaries proposed by the SVM.
</p>
<p>the same time, relying on large corpora of adult-
focused texts may introduce problems: the low-
est similarity values found in our data set involved
the word turkey, suggesting a preponderance in the
data of the country rather than the bird. More
sensitive text normalization methods could likely
resolve this problem, but we also plan to build
LSA and neural word embedding models using
child language data (e.g., the CHILDES corpus
(MacWhinney, 2000)) and child-oriented texts in
the public domain.
</p>
<p>Future work will focus on improving our meth-
ods for identifying semantic chains while account-
ing for different methods of semantic organiza-
tion by combining information gained from the
rich but out-of-domain data scenarios described
here with in-domain experimental data. In addi-
tion to incorporating more child-oriented training
data, we plan to use graph-based models to cap-
ture the ways in which speakers proceed through
the semantic space (Abbott et al., 2015).
</p>
<p>As the contradictory results in the literature in-
dicate, the precise nature of the linguistic deficits
associated with ASD is somewhat unclear. Many
of the most widely reported linguistic deficits fail
to obtain when participants are carefully matched,
particularly on verbal IQ. The atypical language
features that do persist under strict matching are
usually semantic or pragmatic and, hence, more
difficult to detect using easily scored standard lan-
guage assessment instruments. Methods lever-
aging large corpora that reflect neurotypical lan-
guage use may prove to be one of the more useful
tools for identifying atypical language in ASD.
</p>
<p>Acknowledgments
This work was supported in part by NIH
grants R01DC013996, R01DC012033, and
R01DC007129. Any opinions, findings, con-
clusions or recommendations expressed in this
publication are those of the authors and do not
necessarily reflect the views of the NIH.
</p>
<p>36</p>
<p />
</div>
<div class="page"><p />
<p>References
Joshua T Abbott, Joseph L Austerweil, and Thomas L
</p>
<p>Griffiths. 2015. Random walks on semantic net-
works can resemble optimal foraging. Psychologi-
cal Review 122(3):558–569.
</p>
<p>American Psychiatric Association. 2000. DSM-IV-TR:
Diagnostic and Statistical Manual of Mental Disor-
ders. American Psychiatric Publishing, Washing-
ton, DC.
</p>
<p>Sander Begeer, Marlies Wierda, Anke M Scheeren,
Jan-Pieter Teunisse, Hans M Koot, and Hilde M
Geurts. 2014. Verbal fluency in children with autism
spectrum disorders: Clustering and switching strate-
gies. Autism 18(8):1014–1018.
</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research
3:1137–1155.
</p>
<p>Weston Ashmore Bousfield, CHW Sedgewick, and
BH Cohen. 1954. Certain temporal characteristics
of the recall of verbal associates. The American
Journal of Psychology 67(1):111–118.
</p>
<p>Christian Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
</p>
<p>CD Frith, KJ Friston, S Herold, D Silbersweig,
P Fletcher, C Cahill, RJ Dolan, RS Frackowiak, and
PF Liddle. 1995. Regional brain activity in chronic
schizophrenic patients during the performance of a
verbal fluency task. The British Journal of Psychia-
try 167(3):343–349.
</p>
<p>Hilde M Geurts, Sylvie Verté, Jaap Oosterlaan, Herbert
Roeyers, and Joseph A Sergeant. 2004. How spe-
cific are executive functioning deficits in attention
deficit hyperactivity disorder and autism? Journal
of child psychology and psychiatry 45(4):836–854.
</p>
<p>Julie D Henry, John R Crawford, and Louise H Phillips.
2004. Verbal fluency performance in dementia of
the Alzheimer’s type: A meta-analysis. Neuropsy-
chologia 42(9):1212–1222.
</p>
<p>Eiko Inokuchi and Yoko Kamio. 2013. Qualitative
analyses of verbal fluency in adolescents and young
adults with high-functioning autism spectrum dis-
order. Research in Autism Spectrum Disorders
7:1403–1410.
</p>
<p>Leo Kanner. 1943. Autistic disturbances of affective
content. Nervous Child 2:217–250.
</p>
<p>Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic anal-
ysis. Discourse processes 25(2-3):259–284.
</p>
<p>Brian Lopez, Alan Lincoln, Sally Ozonoff, and Zona
Lai. 2005. Examining the relationship between ex-
ecutive functions and restricted, repetitive symptoms
of autistic disorder. Journal of Autism and Develop-
mental Disorders 35(4).
</p>
<p>Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
</p>
<p>Brian MacWhinney. 2000. The CHILDES project: The
database, volume 2. Psychology Press.
</p>
<p>Kristin K Nicodemus, Brita Elvevåg, Peter W
Foltz, Mark Rosenstein, Catherine Diaz-Asper, and
Daniel R Weinberger. 2014. Category fluency, la-
tent semantic analysis and schizophrenia: a candi-
date gene approach. Cortex 55:182–191.
</p>
<p>Serguei V.S. Pakhomov and Laura S. Hemmy. 2014. A
computational linguistic measure of clustering be-
havior on semantic verbal fluency task predicts risk
of future dementia in the nun study. Cortex 55:97–
106.
</p>
<p>Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. pages 45–50.
</p>
<p>Mark Rosenstein, Peter W. Foltz, Anja Vaskinn, and
Brita Elvevg. 2015. Practical issues in developing
semantic frameworks for the analysis of verbal flu-
ency data: A norwegian data case study. In Proceed-
ings of the 2nd Workshop on Computational Linguis-
tics and Clinical Psychology. pages 124–133.
</p>
<p>Michael Rutter, Anthony Bailey, and Catherine Lord.
2003. Social Communication Questionnaire (SCQ).
Western Psychological Services, Los Angeles.
</p>
<p>Annelies Spek, Tjeerd Schatorjé, Evert Scholte, and
Ina van Berckelaer-Onnes. 2009. Verbal fluency in
adults with high functioning autism or asperger syn-
drome. Neuropsychologia 47(3):652–656.
</p>
<p>Angela K Troyer, Morris Moscovitch, and Gordon
Winocur. 1997. Clustering and switching as two
components of verbal fluency: evidence from
younger and older healthy adults. Neuropsychology
11(1):138–146.
</p>
<p>Michelle A Turner. 1999. Generating novel ideas: Flu-
ency performance in high-functioning and learning
disabled individuals with autism. Journal of Child
Psychology and Psychiatry 40(2):189–201.
</p>
<p>37</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 38–44
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2007
</p>
<p>Neural Architectures for Multilingual Semantic Parsing
</p>
<p>Raymond Hendy Susanto and Wei Lu
Singapore University of Technology and Design
{raymond susanto,luwei}@sutd.edu.sg
</p>
<p>Abstract
</p>
<p>In this paper, we address semantic pars-
ing in a multilingual context. We train one
multilingual model that is capable of pars-
ing natural language sentences from mul-
tiple different languages into their corre-
sponding formal semantic representations.
We extend an existing sequence-to-tree
model to a multi-task learning framework
which shares the decoder for generating
semantic representations. We report evalu-
ation results on the multilingual GeoQuery
corpus and introduce a new multilingual
version of the ATIS corpus.
</p>
<p>1 Introduction
</p>
<p>In this work, we address multilingual seman-
tic parsing – the task of mapping natural lan-
guage sentences coming from multiple different
languages into their corresponding formal seman-
tic representations. We consider two multilin-
gual scenarios: 1) the single-source setting, where
the input consists of a single sentence in a single
language, and 2) the multi-source setting, where
the input consists of parallel sentences in multi-
ple languages. Previous work handled the for-
mer by means of monolingual models (Wong and
Mooney, 2006; Lu et al., 2008; Jones et al., 2012),
while the latter has only been explored by Jie
and Lu (2014) who ensembled many monolingual
models together. Unfortunately, training a model
for each language separately ignores the shared
information among the source languages, which
may be potentially beneficial for typologically re-
lated languages. Practically, it is also inconvenient
to train, tune, and configure a new model for each
language, which can be a laborious process.
</p>
<p>In this work, we propose a parsing architec-
ture that accepts as input sentences in several
</p>
<p>languages. We extend an existing sequence-to-
tree model (Dong and Lapata, 2016) to a multi-
task learning framework, motivated by its success
in other fields, e.g., neural machine translation
(MT) (Dong et al., 2015; Firat et al., 2016). Our
model consists of multiple encoders, one for each
language, and one decoder that is shared across
source languages for generating semantic repre-
sentations. In this way, the proposed model po-
tentially benefits from having a generic decoder
that works well across languages. Intuitively, the
model encourages each source language encoder
to find a common structured representation for the
decoder. We further modify the attention mech-
anism (Bahdanau et al., 2015) to integrate multi-
source information, such that it can learn where to
focus during parsing; i.e., which input positions in
which languages.
</p>
<p>Our contributions are as follows:
</p>
<p>• We investigate semantic parsing in two mul-
tilingual scenarios that are relatively unex-
plored in past research,
</p>
<p>• We present novel extensions to the sequence-
to-tree architecture that integrates multilin-
gual information for semantic parsing, and
</p>
<p>• We release a new ATIS semantic dataset an-
notated in two new languages.
</p>
<p>2 Related Work
</p>
<p>In this section, we summarize semantic pars-
ing approaches from previous works. Wong and
Mooney (2006) created WASP, a semantic parser
based on statistical machine translation. Lu et al.
(2008) proposed generative hybrid tree structures,
which were augmented with a discriminative re-
ranker. CCG-based semantic parsing systems have
been developed, such as ZC07 (Zettlemoyer and
Collins, 2007) and UBL (Kwiatkowski et al.,
</p>
<p>38</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2007">https://doi.org/10.18653/v1/P17-2007</a></div>
</div>
<div class="page"><p />
<p>2010). Researchers have proposed sequence-to-
sequence parsing models (Jia and Liang, 2016;
Dong and Lapata, 2016; Kočiskỳ et al., 2016). Re-
cently, Susanto and Lu (2017) extended the hybrid
tree with neural features.
</p>
<p>Recent progress in multilingual NLP has moved
towards building a unified model that can work
across different languages, such as in multilingual
dependency parsing (Ammar et al., 2016), multi-
lingual MT (Firat et al., 2016), and multilingual
word embedding (Guo et al., 2016). Nonetheless,
multilingual approaches for semantic parsing are
relatively unexplored, which motivates this work.
Jones et al. (2012) evaluated an individually-
trained tree transducer on a multilingual semantic
dataset. Jie and Lu (2014) ensembled monolingual
hybrid tree models on the same dataset.
</p>
<p>3 Model
</p>
<p>In this section, we describe our approach to
multilingual semantic parsing, which extends
the sequence-to-tree model by Dong and Lap-
ata (2016). Unlike the mainstream approach that
trains one monolingual parser per source lan-
guage, our approach integrates N encoders, one
for each language, into a single model. This model
encodes a sentence from the n-th language X =
x1, x2, ..., x|X| as a vector and then uses a shared
decoder to decode the encoded vector into its cor-
responding logical form Y = y1, y2, ..., y|Y |. We
consider two types of input: 1) a single sentence
in one of N languages in the single-source setting
and 2) parallel sentences in N languages in the
multi-source setting. We elaborate on each setting
in Section 3.1 and 3.2, respectively.
</p>
<p>The encoder is implemented as a unidirectional
RNN with long short-term memory (LSTM) units
(Hochreiter and Schmidhuber, 1997), which takes
a sequence of natural language tokens as input.
Similar to previous multi-task frameworks, e.g., in
neural MT (Firat et al., 2016; Zoph and Knight,
2016), we create one encoder per source language,
i.e., {Ψnenc}Nn=1. For the n-th language, it updates
the hidden vector at time step t by:
</p>
<p>hnt = Ψ
n
enc(h
</p>
<p>n
t−1,E
</p>
<p>n
x[xt]) (1)
</p>
<p>where Ψnenc is the LSTM function and E
n
x ∈
</p>
<p>R|V |×d is an embedding matrix containing row
vectors of the source tokens in the n-th language.
Each encoder may be configured differently, such
</p>
<p>en
</p>
<p>id
</p>
<p>zh
</p>
<p>λ
</p>
<p>(a)
</p>
<p>en
</p>
<p>id
</p>
<p>zh
</p>
<p>λ
</p>
<p>(b)
</p>
<p>Figure 1: Illustration of the model with three lan-
guage encoders and a shared logical form decoder
(in λ-calculus). Two scenarios are considered: (a)
single-source and (b) multi-source with a com-
biner module (in grey color).
</p>
<p>as by the number of hidden units and the embed-
ding dimension for the source symbol.
</p>
<p>In the basic sequence-to-sequence model, the
decoder generates each target token in a linear
fashion. However, in semantic parsing, such a
model ignores the hierarchical structure of logi-
cal forms. In order to alleviate this issue, Dong
and Lapata (2016) proposed a decoder that gen-
erates logical forms in a top-down manner, where
they define a “non-terminal” token &lt;n&gt; to indi-
cate subtrees. At each depth in the tree, logical
forms are generated sequentially until the end-of-
sequence token is output.
</p>
<p>Unlike in the single language setting, here we
define a single, shared decoder Ψdec as opposed to
one decoder per source language. We augment the
parent non-terminal’s information p when com-
puting the decoder state zt, as follows:
</p>
<p>zt = Ψdec(zt−1,Ey[ỹt−1],p) (2)
</p>
<p>where Ψdec is the LSTM function and ỹt−1 is the
previous target symbol.
</p>
<p>The attention mechanism (Bahdanau et al.,
2015; Luong et al., 2015) computes a time-
dependent context vector ct (as defined later in
Section 3.1 and 3.2), which is subsequently used
for computing the probability distribution over the
next symbol, as follows:
</p>
<p>z̃t = tanh(Uzt + Vct) (3)
</p>
<p>p(yt|y&lt;t, X) ∝ exp(Wz̃t) (4)
</p>
<p>where U, V, and W are weight matrices. Finally,
the model is trained to maximize the following
conditional log-likelihood:
</p>
<p>L(θ) =
∑
</p>
<p>(X,Y )∈D
</p>
<p>|Y |∑
</p>
<p>t=1
</p>
<p>log p(yt|y&lt;t, X) (5)
</p>
<p>39</p>
<p />
</div>
<div class="page"><p />
<p>where (X,Y ) refers to a ground-truth sentence-
semantics pair in the training data D.
</p>
<p>We use the same formulation above for the en-
coders and the decoder in both multilingual set-
tings. Each setting differs in terms of: 1) the de-
coder state initialization, 2) the computation of the
context vector ct, and 3) the training procedure,
which are described in the following sections.
</p>
<p>3.1 Single-Source Setting
In this setting, the input is a source sentence com-
ing from the n-th language. Figure 1 (a) depicts
a scenario where the model is parsing Indonesian
input, with English and Chinese being non-active.
</p>
<p>The last state of the n-th encoder is used to ini-
tialize the first state of the decoder. We may need
to first project the encoder vector into a suitable
dimension for the decoder, i.e., z0 = φndec(h
</p>
<p>n
|X|),
</p>
<p>where φndec can be an affine transformation. Simi-
larly, we may do so before computing the attention
scores, i.e., h̃nk = φ
</p>
<p>n
att(h
</p>
<p>n
k). Then, we compute the
</p>
<p>context vector cnt as a weighted sum of the hidden
vectors in the n-th encoder:
</p>
<p>αnk,t =
exp(h̃nk · zt)∑|X|
</p>
<p>k′=1 exp(h̃
n
k′ · zt)
</p>
<p>(6)
</p>
<p>cnt =
</p>
<p>|X|∑
</p>
<p>k=1
</p>
<p>αnk,th̃
n
k (7)
</p>
<p>We set ct = cnt for computing Equation 3. We pro-
pose two variants of the model under this setting.
In the first version, we define separate weight ma-
trices for each language, i.e., {Un,Vn,Wn}Nn=1.
In the second version, the three weight matrices
are shared across languages, essentially reducing
the number of parameters by a factor of N .
</p>
<p>The training data consists of the union of
sentence-semantics pairs in N languages, where
the source sentences are not necessarily parallel.
We implement a scheduling mechanism that cy-
cles through all languages during training, one lan-
guage at a time. Specifically, model parameters
are updated after one batch from one language
before moving to the next one. Similar to Firat
et al. (2016), this mechanism prevents excessive
updates from a specific language.
</p>
<p>3.2 Multi-Source Setting
In this setting, the input are semantically equiva-
lent sentences in N languages. Figure 1 (b) de-
picts a scenario where the model is parsing En-
glish, Indonesian, and Chinese simultaneously. It
</p>
<p>includes a combiner module (denoted by the grey
box), which we will explain next.
</p>
<p>The decoder state at the first time step is ini-
tialized by first combining the N final states from
each encoder, i.e., z0 = φinit(h1|X|, · · · ,hN|X|),
where we implement φinit by max-pooling.
</p>
<p>We propose two ways of computing ct that inte-
grates source-side information from multiple en-
coders. First, we consider word-level combina-
tion, where we combine N encoder states at every
time step, as follows:
</p>
<p>αnk,t =
exp(h̃nk · zt)∑N
</p>
<p>n′=1
∑|X|
</p>
<p>k′=1 exp(h̃
n′
k′ · zt)
</p>
<p>(8)
</p>
<p>ct =
N∑
</p>
<p>n=1
</p>
<p>|X|∑
</p>
<p>k=1
</p>
<p>αnk,th̃
n
k (9)
</p>
<p>Alternatively, in sentence-level combination,
we first compute the context vector for each lan-
guage in the same way as Equation 6 and 7. Then,
we perform a simple concatenation of N context
vectors: ct =
</p>
<p>[
c1t ; · · · ; cNt
</p>
<p>]
.
</p>
<p>Unlike the single-source setting, the train-
ing data consists of N -way parallel sentence-
semantics pairs. That is, each training instance
consists of N semantically equivalent sentences
and their corresponding logical form.
</p>
<p>4 Experiments and Results
</p>
<p>4.1 Datasets and Settings
</p>
<p>We conduct our experiments on two multilingual
benchmark datasets, which we describe below.
Both datasets use a meaning representation based
on lambda calculus.
</p>
<p>The GeoQuery (GEO) dataset is a standard
benchmark evaluation for semantic parsing. The
multilingual version consists of 880 instances of
natural language queries related to US geography
facts in four languages (English, German, Greek,
and Thai) (Jones et al., 2012). We use the standard
split which consists of 600 training examples and
280 test examples.
</p>
<p>The ATIS dataset contains natural language
queries to a flight database. The data is split into
4,434 instances for training, 491 for development,
and 448 for evaluation, same as Zettlemoyer and
Collins (2007). The original version only includes
English. In this work, we annotate the corpus in
Indonesian and Chinese. The Chinese corpus was
</p>
<p>40</p>
<p />
</div>
<div class="page"><p />
<p>annotated (with segmentations) by hiring profes-
sional translation service. The Indonesian corpus
was annotated by a native Indonesian speaker.
</p>
<p>We use the same pre-processing as Dong and
Lapata (2016), where entities and numbers are re-
placed with their type names and unique IDs.1
</p>
<p>English words are stemmed using NLTK (Bird
et al., 2009). Each query is paired with its cor-
responding semantic representation in lambda cal-
culus (Zettlemoyer and Collins, 2005).
</p>
<p>In all experiments, following Dong and Lap-
ata (2016), we use a one-layer LSTM with 200-
dimensional cells and embeddings. We use a mini-
batch size of 20 with RMSProp updates (Tieleman
and Hinton, 2012) for a fixed number of epochs,
with gradient clipping at 5. Parameters are uni-
formly initialized at [-0.08,0.08] and regularized
using dropout (Srivastava et al., 2014). Input se-
quences are reversed. See Appendix A for detailed
experimental settings.
</p>
<p>For each model configuration, all experiments
are repeated 3 times with different random seed
values, in order to make sure that our findings
are reliable. We found empirically that the ran-
dom seed may affect SEQ2TREE performance.
This is especially important due to the relatively
small dataset. As previously done in multi-
task sequence-to-sequence learning (Luong et al.,
2016), we report the average performance for the
baseline and our model. The evaluation metric is
defined in terms of exact match accuracy with the
ground-truth logical forms. See Appendix B for
the accuracy of individual runs.
</p>
<p>4.2 Results
</p>
<p>Table 1 compares the performance of the mono-
lingual sequence-to-tree model (Dong and Lap-
ata, 2016), SINGLE, and our multilingual model,
MULTI, with separate and shared output param-
eters under the single-source setting as described
in Section 3.1. On average, both variants of the
multilingual model outperform the monolingual
model by up to 1.34% average accuracy on GEO.
Parameter sharing is shown to be helpful, in partic-
ular for GEO. We observe that the average perfor-
mance increase on ATIS mainly comes from Chi-
nese and Indonesian. We also learn that although
including English is often helpful for the other lan-
guages, it may affect its individual performance.
</p>
<p>Table 2 shows the average performance on
</p>
<p>1See Section 3.6 of (Dong and Lapata, 2016).
</p>
<p>SINGLE MULTIseparate shared
GEO
en 84.40 85.00 85.48
de 70.24 71.19 72.86
el 74.40 75.12 75.60
th 72.86 72.26 73.33
avg. 75.48 75.89 76.82
ATIS
en 81.85 81.40 81.77
id 74.85 74.03 75.45
zh 73.66 75.89 73.96
avg. 76.79 77.11 77.06
</p>
<p>Table 1: Single-source parsing results in terms of
average accuracy % over 3 runs. Best results are
in bold.
</p>
<p>RANKING MULTIword sentence
GEO
en+de+el 83.21 85.48 86.43
en+de+th 82.02 86.19 85.48
en+el+th 82.62 85.60 85.24
de+el+th 79.64 72.14 76.43
en+de+el+th 82.50 85.48 86.79
ATIS
en+id 82.81 83.93 83.78
en+zh 82.81 82.96 82.96
id+zh 78.50 76.79 77.75
en+id+zh 83.11 82.22 83.85
</p>
<p>Table 2: Multi-source parsing results in terms of
average accuracy % over 3 runs. Best results are
in bold.
</p>
<p>multi-source parsing by combining 3 to 4 lan-
guages for GEO and 2 to 3 languages for ATIS.
For RANKING, we combine the predictions from
each language by selecting the one with the high-
est probability. Indeed, we observe that system
combination at the model level is able to give bet-
ter performance on average (up to 4.29% on GEO)
than doing so at the output level. Combining at
the word level and sentence level shows compara-
ble performance on both datasets. It can be seen
that the benefit is more apparent when we include
English in the system combination.
</p>
<p>Regarding comparison to previous monolingual
works, we want to highlight that there exist two
different versions of the GeoQuery dataset anno-
tated with completely different semantic represen-
tations: semantic tree and lambda calculus. As
noted in Section 5 of Lu (2014), results obtained
from these two versions are not comparable. We
use lambda calculus same as Dong and Lapata
(2016). Under the multilingual setting, the closest
work is Jie and Lu (2014). Nonetheless, they used
the semantic tree version of GeoQuery. They eval-
</p>
<p>41</p>
<p />
</div>
<div class="page"><p />
<p>Model Input Output
</p>
<p>SINGLE (en) list the airlines with flights to or from ci0 lambda $0 e ( and ( airline $0 ) ( exists $1 ( and ( flight $1 )( or ( from $1 ci0 ) ( to $1 ci0 ) ) ( airline $1 $0 ) ) ) )
</p>
<p>SINGLE (id) daftarkan maskapai dengan penerbanganke atau dari ci0
lambda $0 e ( and ( airline $0 ) ( exists $1 ( and ( flight $1 )
( from $1 ci0 ) ( airline $1 $0 ) ) ) )
</p>
<p>SINGLE (zh) 请列出有航班起降 ci0的航空公司 lambda $0 e ( and ( airline $0 ) ( services $0 ci0 ) )
</p>
<p>MULTI (en+id+zh) lambda $0 e ( exists $1 ( and ( flight $1 ) ( or ( from $1 ci0 )( to $1 ci0 ) ) ( = ( airline:e $1 ) $0 ) ) )
</p>
<p>GOLD (en+id+zh) lambda $0 e ( exists $1 ( and ( flight $1 ) ( or ( from $1 ci0 )( to $1 ci0 ) ) ( = ( airline:e $1 ) $0 ) ) )
</p>
<p>Table 3: Example output from monolingual and multilingual models trained on ATIS.
</p>
<p>Model Number of parametersGEO ATIS
SINGLE/RANKING 3.7× 106 3.1× 106
MULTI (single)
- separate 2.3× 106 2.1× 106
- shared 2.0× 106 1.9× 106
MULTI (multi)
- word 2.0× 106 1.9× 106
- sentence 2.1× 106 1.9× 106
</p>
<p>Table 4: Model size
</p>
<p>uated extrinsically on a database query task while
we use exact match accuracy, so their work is not
directly comparable to ours.
</p>
<p>5 Analysis
</p>
<p>In this section, we report a qualitative analysis
of our multilingual model. Table 3 shows exam-
ple output from the monolingual model, SINGLE,
trained on the three languages in ATIS and the
multilingual model, MULTI, with sentence-level
combination. This example demonstrates a sce-
nario when the multilingual model successfully
parses the three input sentences into the correct
logical form, whereas the individual models are
unable to do so.
</p>
<p>Figure 2 shows the alignments produced by
MULTI (sentence) when parsing ATIS in the multi-
source setting. Each cell in the alignment matrix
corresponds to αnk,t which is computed by Equa-
tion 6. Semantically related words are strongly
aligned, such as the alignments between ground
(en), darat (id), 地面 (zh) and ground transport.
This shows that such correspondences can be
jointly learned by our multilingual model.
</p>
<p>In Table 4, we summarize the number of param-
eters in the baseline and our multilingual model.
The number of parameters in SINGLE and RANK-
ING is equal to the sum of the number of parame-
ters in their monolingual components. It can be
seen that the size of our multilingual model is
about 50-60% smaller than that of the baseline.
</p>
<p>la
m
</p>
<p>bd
a
</p>
<p>$0 e ( an
d
</p>
<p>( gr
ou
</p>
<p>nd
tr
</p>
<p>an
sp
</p>
<p>or
t
</p>
<p>$0 ) ( to
ci
</p>
<p>ty
</p>
<p>$0 ci
0
</p>
<p>) )
</p>
<p>&lt;S&gt;
</p>
<p>E
ng
</p>
<p>lis
h ci0
</p>
<p>for
</p>
<p>transport
</p>
<p>ground
</p>
<p>&lt;E&gt;
</p>
<p>&lt;S&gt;
</p>
<p>In
do
</p>
<p>ne
si
</p>
<p>an
ci0
</p>
<p>untuk
</p>
<p>darat
</p>
<p>transportasi
</p>
<p>&lt;E&gt;
</p>
<p>&lt;S&gt;
</p>
<p>C
hi
</p>
<p>ne
se 交通
</p>
<p>地面
</p>
<p>的
</p>
<p>ci0
</p>
<p>&lt;E&gt;
</p>
<p>Figure 2: Attention score matrices computed by
MULTI when parsing English, Indonesian, and
Chinese inputs from ATIS. Darker color represents
higher attention score.
</p>
<p>6 Conclusion
</p>
<p>We have presented a multilingual semantic parser
that extends the sequence-to-tree model to a multi-
task learning framework. Through experiments,
we show that our multilingual model performs bet-
ter on average than 1) monolingual models in the
single-source setting and 2) ensemble ranking in
the multi-source setting. We hope that this work
will stimulate further research in multilingual se-
mantic parsing. Our code and data is available at
http://statnlp.org/research/sp/.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank Christopher Bryant, Li
Dong, and the anonymous reviewers for helpful
comments and feedback. This work is supported
by MOE Tier 1 grant SUTDT12015008, and is
partially supported by project 61472191 under the
National Natural Science Foundation of China.
</p>
<p>42</p>
<p />
</div>
<div class="page"><p />
<p>References
Waleed Ammar, George Mulcaire, Miguel Ballesteros,
</p>
<p>Chris Dyer, and Noah Smith. 2016. Many lan-
guages, one parser. Transactions of the Association
for Computational Linguistics 4:431–444.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.
</p>
<p>Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
ing text with the natural language toolkit. ” O’Reilly
Media, Inc.”.
</p>
<p>Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In Proceedings of ACL.
https://doi.org/10.3115/v1/P15-1166.
</p>
<p>Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. In Proceedings of ACL.
https://doi.org/10.18653/v1/P16-1004.
</p>
<p>Orhan Firat, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. Multi-way, multilingual neural
machine translation with a shared atten-
tion mechanism. In Proceedings of NAACL.
https://doi.org/10.18653/v1/N16-1101.
</p>
<p>Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A representation learn-
ing framework for multi-source transfer parsing. In
Proceedings of AAAI.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of ACL.
https://doi.org/10.18653/v1/P16-1002.
</p>
<p>Zhanming Jie and Wei Lu. 2014. Multilingual se-
mantic parsing: Parsing multiple languages into se-
mantic representations. In Proceedings of COLING.
http://aclweb.org/anthology/C14-1122.
</p>
<p>Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian
tree transducers. In Proceedings of ACL.
http://aclweb.org/anthology/P12-1051.
</p>
<p>Tomáš Kočiskỳ, Gábor Melis, Edward Grefen-
stette, Chris Dyer, Wang Ling, Phil Blunsom,
and Karl Moritz Hermann. 2016. Seman-
tic parsing with semi-supervised sequential
autoencoders. In Proceedings of EMNLP.
https://doi.org/10.18653/v1/D16-1116.
</p>
<p>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of EMNLP.
http://aclweb.org/anthology/D10-1119.
</p>
<p>Wei Lu. 2014. Semantic parsing with relaxed
hybrid trees. In Proceedings of EMNLP.
http://aclweb.org/anthology/D14-1137.
</p>
<p>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and
Luke S Zettlemoyer. 2008. A generative
model for parsing natural language to mean-
ing representations. In Proceedings of EMNLP.
http://aclweb.org/anthology/D08-1082.
</p>
<p>Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In Proceedings of
ICLR.
</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
EMNLP. https://doi.org/10.18653/v1/D15-1166.
</p>
<p>Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.
</p>
<p>Raymond Hendy Susanto and Wei Lu. 2017. Semantic
parsing with neural hybrid trees. In Proceedings of
AAAI.
</p>
<p>Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
networks for machine learning 4(2).
</p>
<p>Yuk Wah Wong and Raymond J Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of NAACL.
http://aclweb.org/anthology/N06-1056.
</p>
<p>Luke S Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial gram-
mars. In Proceedings of UAI.
</p>
<p>Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
http://aclweb.org/anthology/D07-1071.
</p>
<p>Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of NAACL.
https://doi.org/10.18653/v1/N16-1004.
</p>
<p>A Hyperparameters
</p>
<p>Table 5 lists the number of training epochs and the
dropout probability used in the LSTM cell and the
hidden layers before the softmax classifiers, which
were chosen based on preliminary experiments on
a held-out dataset. We use a training schedule
where we switch to the next language after train-
ing one mini-batch for GEO and 500 for ATIS. For
</p>
<p>43</p>
<p />
</div>
<div class="page"><p />
<p>SINGLE MULTIseparate shared
1 2 3 1 2 3 1 2 3
</p>
<p>GEO
en 87.14 83.57 82.50 85.71 83.93 85.36 85.36 83.93 87.14
de 70.00 70.36 70.36 71.79 71.79 70.00 73.57 73.93 71.07
el 76.43 72.50 74.29 77.14 72.14 76.07 76.43 74.64 75.71
th 72.50 73.57 72.50 72.14 72.14 72.50 72.50 71.07 76.43
ATIS
en 84.60 79.24 81.70 82.14 81.03 81.03 82.59 80.36 82.37
id 75.67 74.55 74.33 75.67 72.54 73.88 76.56 75.45 74.33
zh 74.33 73.66 72.99 74.11 76.12 77.46 75.67 72.54 73.66
</p>
<p>Table 6: Single-source parsing results showing the accuracy of the 3 runs. Best results are in bold.
</p>
<p>RANKING MULTIword sentence
1 2 3 1 2 3 1 2 3
</p>
<p>GEO
en+de+el 85.00 82.50 82.14 87.14 84.64 84.64 87.50 85.36 86.43
en+de+th 84.29 81.07 80.71 87.86 85.00 85.71 85.71 86.43 84.29
en+el+th 84.29 82.14 81.43 87.50 84.29 85.00 84.64 85.71 85.36
de+el+th 80.00 79.29 79.64 71.07 72.86 72.50 77.86 74.64 76.79
en+de+el+th 83.93 81.79 81.79 85.71 86.07 84.64 87.50 86.79 86.07
ATIS
en+id 83.48 82.14 82.81 83.48 83.48 84.82 85.27 80.58 85.49
en+zh 84.60 80.80 83.04 83.26 82.14 83.48 85.49 80.13 83.26
id+zh 79.24 78.57 77.68 77.46 78.35 74.55 80.58 78.13 74.55
en+id+zh 84.15 81.92 83.26 82.14 81.25 83.26 85.49 81.03 85.04
</p>
<p>Table 7: Multi-source parsing results showing the accuracy of the 3 runs. Best results are in bold.
</p>
<p>all multilingual models, we initialize the encoders
using the encoder weights learned by the mono-
lingual models. For the multi-source setting, we
also initialize the decoder using the first language
in the list of the combined languages.
</p>
<p>B Additional Experimental Results
</p>
<p>In Table 6 and 7, we report the accuracy of the 3
runs for each model and dataset. In both settings,
we observe that the best accuracy on both datasets
is often achieved by MULTI. This is the same con-
clusion that we reached when averaging the results
over all runs.
</p>
<p>#epochs dropout(LSTM)
dropout
</p>
<p>(output layer)
GEO
SINGLE 90 0.1 0.4
MULTI (single) 340 0.1 0.4
MULTI (multi) 150 0.1 0.4
ATIS
SINGLE 130 0.3 0.3
MULTI (single) 390 0.3 0.3
MULTI (multi) 250 0.3 0.3
</p>
<p>Table 5: Hyperparameter values
</p>
<p>44</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 45–50
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2008
</p>
<p>Incorporating Uncertainty into Deep Learning for Spoken Language
Assessment
</p>
<p>Andrey Malinin, Anton Ragni, Kate M. Knill, Mark J. F. Gales
University of Cambridge, Department of Engineering
</p>
<p>Trumpington St, Cambridge CB2 1PZ, UK
{am969, ar527, kate.knill, mjfg}@eng.cam.ac.uk
</p>
<p>Abstract
</p>
<p>There is a growing demand for automatic
assessment of spoken English proficiency.
These systems need to handle large vari-
ations in input data owing to the wide
range of candidate skill levels and L1s, and
errors from ASR. Some candidates will
be a poor match to the training data set,
undermining the validity of the predicted
grade. For high stakes tests it is essen-
tial for such systems not only to grade
well, but also to provide a measure of
their uncertainty in their predictions, en-
abling rejection to human graders. Pre-
vious work examined Gaussian Process
(GP) graders which, though successful, do
not scale well with large data sets. Deep
Neural Networks (DNN) may also be used
to provide uncertainty using Monte-Carlo
Dropout (MCD). This paper proposes a
novel method to yield uncertainty and
compares it to GPs and DNNs with MCD.
The proposed approach explicitly teaches
a DNN to have low uncertainty on train-
ing data and high uncertainty on generated
artificial data. On experiments conducted
on data from the Business Language Test-
ing Service (BULATS), the proposed ap-
proach is found to outperform GPs and
DNNs with MCD in uncertainty-based re-
jection whilst achieving comparable grad-
ing performance.
</p>
<p>1 Introduction
</p>
<p>Systems for automatic assessment of spontaneous
spoken language proficiency (Fig. 1) are becom-
ing increasingly important to meet the demand for
English second language learning. Such systems
are able to provide throughput and consistency
</p>
<p>which are unachievable with human examiners.
This is a challenging task. There is a large vari-
</p>
<p>Audio
</p>
<p>Grade
Feature
</p>
<p>extraction
</p>
<p>Speech
</p>
<p>recogniser
Text
</p>
<p>Features Grader
</p>
<p>Figure 1: Automatic Assessment System
</p>
<p>ation in the quality of spoken English across all
proficiency levels. In addition, candidates of the
same skill level will have different accents, voices,
mispronunciations, and sentence construction er-
rors. All of which are heavily influenced by the
candidate’s L1 language and compounded by ASR
errors. It is therefore impossible in practice to ob-
serve all these variants in training. At test time, the
predicted grade’s validity will decrease the more
the candidate is mismatched to the data used to
train the system. For deployment of these systems
to high-stakes tests the performance on all candi-
dates needs to be consistent and highly correlated
with human graders. To achieve this it is impor-
tant that these systems can detect outlier speakers
who need to be examined by, for example, human
graders.
</p>
<p>Previously, separate models were used to fil-
ter out ”non-scorable” candidates (Yoon and Xie,
2014; Zechner et al., 2009; Higgins et al., 2011;
Xie et al., 2012). However, such models reject
candidates based on whether they can be scored at
all, rather than an automatic grader’s uncertainty 1
</p>
<p>in its predictions. It was shown by van Dalen et al.
(2015) that Gaussian Process (GP) graders give
</p>
<p>1Uncertainty is used in the sense of the inverse of confi-
dence to be consistent with Gal and Ghahramani (2016) and
van Dalen et al. (2015).
</p>
<p>45</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2008">https://doi.org/10.18653/v1/P17-2008</a></div>
</div>
<div class="page"><p />
<p>state-of-the-art performance for automatic assess-
ment and yield meaningful uncertainty estimates
for rejection of candidates. There are, however,
computational constraints on training set sizes for
GPs. In contrast, Deep Neural Networks (DNNs)
are able to scale to large data sets, but lack a na-
tive measure of uncertainty. However, Gal and
Ghahramani (2016) have shown that Monte-Carlo
Dropout (MCD) can be used to derive an uncer-
tainty estimate for a DNN.
</p>
<p>Alternatively, a Deep Density Network (DDN),
which is a Mixture Density Network (Bishop,
1994) with only one mixture component, may be
used to yield a mean and variance corresponding
to the predicted grade and the uncertainty in the
prediction. Similar to GP and DNNs with MCD,
a standard DDN provides an implicit modelling of
uncertainty in its prediction. This implicit model
may not be optimal for the task at hand. Hence,
a novel approach to explicitly model uncertainty is
proposed in which the DDN is trained in a multi-
task fashion to model a low variance real data dis-
tribution and a high variance artificial data dis-
tribution which represents candidates with unseen
characteristics.
</p>
<p>2 Prediction Uncertainty
</p>
<p>The principled method for dealing with uncer-
tainty in statistical modelling is the Bayesian ap-
proach, where a conditional posterior distribution
over grades, g, given inputs, x, and training data
D = {ĝ, x̂} is computed by marginalizing over all
models:
</p>
<p>p(g|x, D) =
∫
</p>
<p>p(g|x,M)p(M|D)dM (1)
</p>
<p>where p(M|D) is a prior over a model given the
data. Given the posterior, the predictive mean and
the variance (uncertainty) can be computed using:
</p>
<p>µg(x) =
</p>
<p>∫
p(g|x, D)gdg
</p>
<p>σ2g(x) =
</p>
<p>∫
p(g|x, D)g2dg − µ2g(x)
</p>
<p>(2)
</p>
<p>(3)
</p>
<p>2.1 Gaussian Processes
</p>
<p>Eq. 2, 3 can be analytically solved for a class
of models called Gaussian Processes (GP) (Ras-
mussen and Williams, 2006), a powerful non-
parametric model for regression. The GP induces
</p>
<p>a conditional posterior in the form of a normal dis-
tribution over grades g given an input x and train-
ing data D:
</p>
<p>p(g|x; D) = N (g; µg(x|D), σ2g(x|D)) (4)
</p>
<p>With mean function µg(x|D) and variance func-
tion σ2g(x|D), which is a function of the similarity
of an input x to the training data inputs x̂, where
the similarity metric is defined by a covariance
function k(., .). The nature of GP variance means
that the model is uncertain in predictions for inputs
far away from the training data, given appropriate
choice of k(., .). Unfortunately, without sparsifi-
cation approaches, the computational and mem-
ory requirements of GPs become prohibitively ex-
pensive for large data sets. Furthermore, GPs are
known to scale poorly to higher dimensional fea-
tures (Rasmussen and Williams, 2006).
</p>
<p>2.2 Monte-Carlo Dropout
Alternatively, a grader can be constructed using
Deep Neural Networks (DNNs) which have a very
flexible architecture and scale well to large data
sets. DNNs, however, lack a native measure of un-
certainty. Uncertainty estimates for DNNs can be
computed using a Monte-Carlo ensemble approx-
imation to Eq. 2, 3:
</p>
<p>µ̂g(x) =
1
</p>
<p>N
</p>
<p>N∑
</p>
<p>i=1
</p>
<p>f(x;M(i))
</p>
<p>σ̂2g(x) =
1
</p>
<p>N
</p>
<p>N∑
</p>
<p>i=1
</p>
<p>(
f(x; M(i))
</p>
<p>)2
− µ̂2g(x)
</p>
<p>(5)
</p>
<p>(6)
</p>
<p>where there are N DNN models in the ensemble,
M(i) is a DNN with a particular architecture and
parameters sampled from p(M|D) using Monte
Carlo Dropout (MCD) (Srivastava et al., 2014),
and f(x; M(i)) are the DNN predictions. Recent
work by Gal and Ghahramani (2016) showed that
MCD is equivalent to approximate variational in-
ference in GPs, and can be used to yield mean-
ingful uncertainty estimates for DNNs. Further-
more, Gal and Ghahramani (2016) show that dif-
ferent choices of DNN activation functions corre-
spond to different GP covariance functions. MCD
uncertainty assumes that for inputs further from
the training data, different subnets will produce in-
creasingly differing outputs, leading to larger vari-
ances. Unfortunately, it is difficult to know before-
hand which activation functions accomplish this in
practice.
</p>
<p>46</p>
<p />
</div>
<div class="page"><p />
<p>3 Deep Density Networks
</p>
<p>Instead of relying on a Monte Carlo approximation
to Eq. 1, a DNN can be modified to produce a
prediction of both a mean and a variance:
</p>
<p>µg(x) = fµ(x; M)
σ2g(x) = fσ2(x; M)
</p>
<p>(7)
</p>
<p>(8)
</p>
<p>parametrising a normal distribution over grades
conditioned on the input, similar to a GP. This
architecture is a Deep Density Network (DDN),
which is a Mixture Density Network (MDN)
(Bishop, 1994) with only one mixture component.
DDNs are trained by maximizing the likelihood of
the training data. The variance of the DDN rep-
resents the natural spread of grades at a given in-
put. This is an implicit measure of uncertainty, like
GP and MCD variance, because it is learned au-
tomatically as part of the model. However, this
doesn’t enforce higher variance further away from
training points in DDNs. It is possible to explic-
</p>
<p>Figure 2: Desired variance characteristic
</p>
<p>itly teach a DDN to predict a high or low vari-
ance for inputs which are unlike or similar to the
training data, respectively (Fig. 2). This requires
a novel training procedure. Two normal distribu-
tions are constructed: a low-variance real (train-
ing) data distribution pD and a high-variance arti-
ficial data distribution pN, which models data out-
side the real training data region. The DDN needs
to model both distributions in a multi-task (MT)
fashion. The loss function for training the DDN
with explicitly specified uncertainty is the expec-
tation over the training data of the KL divergence
between the distribution it parametrizes and both
the real and artificial data distributions:
</p>
<p>L = Ex̂[KL(pD||p(g|x̂; M)] +
α · Ex̃[KL(pN||p(g|x̃; M)]
</p>
<p>(9)
</p>
<p>where α is the multi-task weight.
The DDN with explicit uncertainty is trained in
</p>
<p>a two stage fashion. First, a standard DDN M0
</p>
<p>is trained, then a DDN M is instantiated using the
parameters of M0 and trained in a multi-task fash-
ion. The real data distribution pD is defined by M0
(Eq. 7, 8). The artificial data distribution pN is con-
structed by generating artificial inputs x̃ and the
associated mean and variance targets µ(x̃), σ2(x̃):
</p>
<p>pN = N (g; fµ(x̃; M0), σ2(x̃)) (10)
</p>
<p>The predictions of M0 are used as the targets for
µ(x̃). The target variance σ2(x̃) should depend
on the similarity of x̃ to the training data. Here,
this variance is modelled by the squared normal-
ized Euclidean distance from the mean of x̂, with
a diagonal covariance matrix, scaled by a hyper-
parameter λ. The artificial inputs x̃ need to be
different to, but related to the real data x̂. Ide-
ally, they should represent candidates with unseen
characteristics, such as L1, accent and proficiency.
A simple approach to generating x̃ is to use a Fac-
tor Analysis (FA) (Murphy, 2012) model trained
on x̂. The generative model of FA is:
</p>
<p>x̃ ∼ N (Wz + µ, γΨ), z ∼ N (0, γI) (11)
</p>
<p>where W is the loading matrix, Ψ the diagonal
residual noise variance, µ the mean, all derived
from x̂, and γ is used to control the distance of
the generated data from the real training data re-
gion. During training the artificial inputs are sam-
pled from the FA model.
</p>
<p>4 Experimental Results
</p>
<p>Figure 3: An example Rejection Plot
</p>
<p>AUCRR =
AUCvar
</p>
<p>AUCmax
(12)
</p>
<p>As previously stated, the operating scenario is
to use a model’s estimate of the uncertainty in
</p>
<p>47</p>
<p />
</div>
<div class="page"><p />
<p>(a) GP (b) MCD
</p>
<p>(c) DDN (d) DDN+MT
</p>
<p>Figure 4: Rejection Plots for models
</p>
<p>its prediction to reject candidates to be assessed
by human graders for high-stakes tests, maximiz-
ing the increase in performance while rejecting the
least number of candidates. The rejection process
is illustrated using a rejection plot (Fig. 3). As the
rejection fraction is increased, model predictions
are replaced with human scores in some particular
order, increasing overall correlation with human
graders. Fig. 3 has 3 curves representing differ-
ent orderings: expected random rejection, optimal
rejection and model rejection. The expected ran-
dom performance curve is a straight line from the
base predictive performance to 1.0, representing
rejection in a random order. The optimal rejec-
tion curve is constructed by rejecting predictions
in order of decreasing mean square error relative
to human graders. A rejection curve derived from
a model should sit between the random and op-
timal curves. In this work, model rejection is in
order of decreasing predicted variance.
</p>
<p>The following metrics are used to assess and
compare models: Pearson Correlation Coefficient
(PCC) with human graders, the standard perfor-
mance metric in assessment (Zechner et al., 2009;
Higgins et al., 2011); 10% rejection PCC, which
illustrates the predictive performance at a partic-
</p>
<p>ular operating point, i.e. rejecting 10% of candi-
dates; and Area under a model’s rejection curve
(AUC) (Fig 3). However, AUC is influenced by
the base PCC of a model, making it difficult to
compare the rejection performance. Thus, a metric
independent of predictive performance is needed.
The proposed metric, AUCRR (Eq. 12), is the ratio
of the areas under the actual (AUCvar) and optimal
(AUCmax) rejection curves relative to the random re-
jection curve. Ratios of 1.0 and 0.0 correspond to
perfect and random rejection, respectively.
</p>
<p>All experiments were done using 33-
dimensional pronunciation, fluency and acoustic
features derived from audio and ASR transcrip-
tions of responses to questions from the BULATS
exam (Chambers and Ingham, 2011). The ASR
system has a WER of 32% on a development
set. The training and test sets have 4300 and 224
candidates, respectively. Each candidate provided
a response to 21 questions, and the features used
are aggregated over all 21 questions into a single
feature vector. The test data was graded by expert
graders at Cambridge English. These experts
have inter-grader PCCs in the range 0.95-0.97.
Candidates are equally distributed across CEFR
grade levels (Europe, 2001).
</p>
<p>48</p>
<p />
</div>
<div class="page"><p />
<p>The input features where whitened by subtract-
ing the mean and dividing by the standard devia-
tion for each dimension computed on all training
speakers. The Adam optimizer (Kingma and Ba,
2015), dropout (Srivastava et al., 2014) regulariza-
tion with a dropout keep probability of 0.6 and an
exponentially decaying learning rate are used with
decay factor of 0.86 per epoch, batch size 50. All
networks have 2 hidden layers with 180 rectified
linear units (ReLU) in each layer. DNN and DDN
models were implemented in Tensorflow (Abadi
et al., 2015). Models were initialized using the
Xavier Initializer (Glorot and Bengio, 2010). A
validation set of 100 candidates was selected from
the training data to tune the model and hyper-
parameters. GPs were run using Scikit-Learn (Pe-
dregosa et al., 2011) using a squared exponential
covariance function.
</p>
<p>Grader PCC
10% Rej.
</p>
<p>AUC AUCRRPCC
GP 0.876 0.897 0.942 0.233
MCD 0.879 0.892 0.937 0.040
MCDtanh 0.865 0.886 0.938 0.226
DDN 0.871 0.887 0.941 0.230
</p>
<p>+MT 0.871 0.902 0.947 0.364
</p>
<p>Table 1: Grading and rejection performance
</p>
<p>The Gaussian Process grader, GP, is a com-
petitive baseline (Tab. 1). GP variance clearly
yields uncertainty which is useful for rejection. A
DNN with ReLU activation, MCD, achieves grad-
ing performance similar to the GP. However, MCD
fails to yield an informative uncertainty for rejec-
tion, with performance barely above random. If
the tanh activation function, MCDtanh, is used in-
stead, then a DNN is able to provide a meaningful
measure of uncertainty using MCD, at the cost of
grading performance. It is likely that ReLU ac-
tivations correspond to a GP covariance function
which is not suited for rejection on this data.
</p>
<p>The standard DDN has comparable grading per-
formance to the GP and DNNs. AUCRR of the DDN
is on par with the GP, but the 10% rejection PCC
is lower, indicating that the DDN is not as effec-
tive at rejecting the worst outlier candidates. The
approach proposed in this work, a DDN trained in
a multi-task fashion (DDN+MT), achieves signif-
icantly higher rejection performance, resulting in
the best AUCRR and 10% rejection PCC, showing
its better capability to detect outlier candidates.
Note, AUC reflects similar trends to AUCRR, but not
</p>
<p>as clearly, which is demonstrated by Fig. 4. The
model was found to be insensitive to the choice
of hyper-parameters α and γ, but λ needed to be
set to produce target noise variances σ2(x̃) larger
than data variances σ2(x̂).
</p>
<p>5 Conclusions and Future Work
</p>
<p>A novel method for explicitly training DDNs to
yield uncertainty estimates is proposed. A DDN
is a density estimator which is trained to model
two distributions in a multi-task fashion (1) the
low variance (uncertainty) true data distribution
and (2) a generated high variance artificial data
distribution. The model is trained by minimizing
the KL divergence between the DDN and the true
data distribution (1) and between the DDN and the
artificial data distribution (2). The DDN should
assign its prediction of low or high variance (un-
certainty) if the input is similar or dissimilar to the
true data respectively. The artificial data distribu-
tion is given by a factor analysis model trained on
the real data. During training the artificial data is
sampled from this distribution.
</p>
<p>This method outperforms GPs and Monte-Carlo
Dropout in uncertainty based rejection for auto-
matic assessment. However, the effect of the
nature of artificial data on rejection performance
should be further investigated and other data
generation methods, such as Variational Auto-
Encoders (Kingma and Welling, 2014), and met-
rics to assess similarity between artificial and real
training data should be examined. The proposed
approach must also be assessed on other tasks and
datasets.
</p>
<p>Acknowledgments
</p>
<p>This research was funded under the ALTA Insti-
tute, University of Cambridge as well as the En-
gineering and Physical Sciences Research Coun-
cil. Thanks to Cambridge English, University of
Cambridge, for support and access to the BULATS
data.
</p>
<p>49</p>
<p />
</div>
<div class="page"><p />
<p>References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene
</p>
<p>Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.
</p>
<p>C. M. Bishop. 1994. Mixture density networks. Tech-
nical Report NCRG 4288, Neural Computing Re-
search Group, Department of Computer Science, As-
ton University .
</p>
<p>Lucy Chambers and Kate Ingham. 2011. The BULATS
online speaking test. Research Notes 43:21–25.
</p>
<p>Council of Europe. 2001. Common European frame-
work of reference for languages: Learning, teach-
ing, assessment. Cambridge, U.K: Press Syndicate
of the University of Cambridge.
</p>
<p>Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a
Bayesian Approximation: Representing Model Un-
certainty in Deep Learning. In Proceedings of the
33rd International Conference on Machine Learn-
ing (ICML-16).
</p>
<p>Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Aistats. volume 9, pages 249–256.
</p>
<p>Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language
25(2):282–306.
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations (ICLR).
</p>
<p>Diederik P. Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In Proceedings of the
2nd International Conference on Learning Repre-
sentations (ICLR).
</p>
<p>Kevin P. Murphy. 2012. Machine Learning. The MIT
Press.
</p>
<p>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
</p>
<p>in Python. Journal of Machine Learning Research
12:2825–2830.
</p>
<p>Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian Processes for Machine
Learning. MIT Press.
</p>
<p>Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.
</p>
<p>Rogier C. van Dalen, Kate M. Knill, and Mark J. F.
Gales. 2015. Automatically Grading Learners’ En-
glish Using a Gaussian Process. In Proceedings of
the ISCA Workshop on Speech and Language Tech-
nology for Education (SLaTE).
</p>
<p>Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring Content Features for Automated Speech
Scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT).
</p>
<p>Su-Youn Yoon and Shasha Xie. 2014. Similarity-
Based Non-Scorable Response Detection for Auto-
mated Speech Scoring. In Proceedings of the Ninth
Workshop on Innovative Use of NLP for Building
Educational Applications.
</p>
<p>Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken
english. Speech Communication 51(10):883–895.
Spoken Language Technology for Education Spoken
Language.
</p>
<p>50</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 51–57
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2009
</p>
<p>Incorporating Dialectal Variability
for Socially Equitable Language Identification
</p>
<p>David Jurgens
Stanford University
</p>
<p>Yulia Tsvetkov
Stanford University
</p>
<p>{jurgens,tsvetkov,jurafsky}@stanford.edu
</p>
<p>Dan Jurafsky
Stanford University
</p>
<p>Abstract
</p>
<p>Language identification (LID) is a criti-
cal first step for processing multilingual
text. Yet most LID systems are not de-
signed to handle the linguistic diversity of
global platforms like Twitter, where lo-
cal dialects and rampant code-switching
lead language classifiers to systematically
miss minority dialect speakers and mul-
tilingual speakers. We propose a new
dataset and a character-based sequence-to-
sequence model for LID designed to sup-
port dialectal and multilingual language
varieties. Our model achieves state-of-the-
art performance on multiple LID bench-
marks. Furthermore, in a case study us-
ing Twitter for health tracking, our method
substantially increases the availability of
texts written by underrepresented popula-
tions, enabling the development of “so-
cially inclusive” NLP tools.
</p>
<p>1 Introduction
</p>
<p>Language identification (LID) is an essential first
step for NLP on multilingual text. In global set-
tings like Twitter, this text is written by authors
from diverse linguistic backgrounds, who may
communicate with regional dialects (Gonçalves
and Sánchez, 2014) or even include parallel trans-
lations in the same message to address different
audiences (Ling et al., 2013, 2016). Such di-
alectal variation is frequent in all languages and
even macro-dialects such as American and British
English are composed of local dialects that vary
across city and socioeconomic development level
(Labov, 1964; Orton et al., 1998). Yet current sys-
tems for broad-coverage LID—trained on dozens
of languages—have largely leveraged European-
centric corpora and not taken into account demo-
</p>
<p>1. @username R u a wizard or wat gan sef: in d mornin -
u tweet, afternoon - u tweet, nyt gan u dey tweet. beta
get ur IT placement wiv twitter
</p>
<p>2. Be the lord lantern jaysus me heart after that match!!!
3. Aku hanya mengagumimu dari jauh sekarang . RDK
</p>
<p>({}) * last tweet about you - - , maybe
</p>
<p>Figure 1: Challenges for socially-equitable LID in Twitter
include dialectal text, shown from Nigeria (#1) and Ireland
(#2), and multilingual text (Indonesian and English) in #3.
</p>
<p>graphic and dialectal variation. As a result, these
systems systematically misclassify texts from pop-
ulations with millions of speakers whose local
speech differs from the majority dialects (Hovy
and Spruit, 2016; Blodgett et al., 2016).
</p>
<p>Multiple systems have been proposed for broad-
coverage LID at the global level (McCandless,
2010; Lui and Baldwin, 2012; Brown, 2014; Jaech
et al., 2016). However, only a handful of tech-
niques have addressed the challenge of linguis-
tic variability of global data, such as the dialec-
tal variability and multilingual text seen in Fig-
ure 1. These techniques have typically focused
only on limited aspects of variability, e.g., indi-
vidual dialects like African American Vernacu-
lar English (Blodgett et al., 2016), online speech
(Nguyen and Doğruöz, 2013), similar languages
(Bergsma et al., 2012; Zampieri et al., 2014a), or
word-level code switching (Solorio et al., 2014;
Rijhwani et al., 2017).
</p>
<p>In this work, our goal is to devise a socially
equitable LID, that will enable a massively mul-
tilingual, broad-coverage identification of popu-
lations speaking underrepresented dialects, mul-
tilingual messages, and other linguistic varieties.
We first construct a large-scale dataset of Twit-
ter posts across the world (§2). Then, we intro-
duce an LID system, EQUILID, that produces per-
token language assignments and obtains state-of-
the-art performance on four LID tasks (§3), out-
performing broad-coverage LID benchmarks by
</p>
<p>51</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2009">https://doi.org/10.18653/v1/P17-2009</a></div>
</div>
<div class="page"><p />
<p>up to 300%. Finally, we present a case study on us-
ing Twitter for health monitoring and show that (1)
current widely-used systems suffer from lower re-
call rates for texts from developing countries, and
(2) our system substantially reduces this disparity
and enables socially-equitable LID.
</p>
<p>2 Curating Socially-Representative Text
</p>
<p>Despite known linguistic variation in languages,
current broad-coverage LID systems are trained
primarily on European-centric sources (e.g., Lui
and Baldwin, 2014), often due to data availabil-
ity. Further, even when training incorporates
seemingly-global texts from Wikipedia, their au-
thors are still primarily from highly-developed
countries (Graham et al., 2014). This latent bias
can significantly affect downstream applications
(as we later show in §4), since language ID is often
assumed to be a solved problem (McNamee, 2005)
and most studies employ off-the-shelf LID sys-
tems without considering how they were trained.
</p>
<p>We aim to create a socially-representative cor-
pus for LID that captures the variation within a
language, such as orthography, dialect, formality,
topic, and spelling. Motivated by the recent lan-
guage survey of Twitter (Trampus, 2016), we next
describe how we construct this corpus for 70 lan-
guages along three dimensions: geography, social
and topical diversity, and multilinguality.
Geographic Diversity We create a large-scale
dataset of geographically-diverse text by boot-
strapping with a people-centric approach (Bam-
man, 2015) that treats location and languages-
spoken as demographic attributes to be inferred
for authors. By inferring both for Twitter users
and then collecting documents from monolingual
users, we ensure that we capture regional variation
in a language, rather than focusing on a particular
aspect of linguistic variety.
</p>
<p>Individuals’ locations are inferred using the
method of Compton et al. (2014) as implemented
by Jurgens et al. (2015). The method first identi-
fies the individuals who have reliable ground truth
locations from geotagged tweets and then infers
the locations of other individuals as the geographic
center of their friends’ locations, iteratively apply-
ing this inference method to the whole social net-
work. The method is accurate to within tens of
kilometers on urban and rural users (Johnson et al.,
2017), which is sufficient for the city-level analy-
sis we use here. We use a network of 2.3B edges
</p>
<p>from reciprocal mentions to locate 132M users.
To identify monolingual users, we classify mul-
</p>
<p>tiple tweets by the same individual and consider an
author monolingual if they had at least 20 tweets
and 95% were labeled with one language `. All
tweets by that author are then treated as being `.
We use this relabeling process to automatically
identify misclassified tweets, which when aggre-
gated geographically, can potentially capture re-
gional dialects and topics.1 We construct separate
sets of monolinguals using langid.py and CLD2 as
classifiers to mitigate the biases of each.
Social and Topical Diversity Authors modulate
their writing style for different social registers
(Eisenstein, 2015; Tatman, 2015). Therefore, we
include corpora from different levels of formality
across a wide range of topics. Texts were gathered
for all of the 70 languages from (1) Wikipedia arti-
cles and their more informal Talk pages, (2) Bible
and Quran translations (3) JRC-Acquis (Stein-
berger et al., 2006), a collection of European leg-
islation, (4) the UN Declaration of Human Rights,
(5) the Watchtower online magazines, (6) the 2014
and 2015 iterations of the Distinguishing Simi-
lar Languages shared task (Zampieri et al., 2014b,
2015), and (7) the Twitter70 dataset (Trampus,
2016). We also include single-language corpora
drawn from slang websites (e.g., Urban Dictio-
nary) and the African American Vernacular En-
glish data from Blodgett et al. (2016). For all
sources, we extract instances sequentially by ag-
gregating sentences up to 140 characters.
Multilingual Diversity Authors are known to
generate multilingual texts on Twitter (Ling et al.,
2013, 2014), with Rijhwani et al. (2017) estimat-
ing that 3.5% of tweets are code-switched. To cap-
ture the potential diversity in multilingual docu-
ments, we perform data augmentation to synthet-
ically construct multilingual documents of tweet
length by (1) sampling texts for two languages
from arbitrary sources, (2) with 50% chance for
each, truncating a text at the first occurrence of
phrasal punctuation, and (3) concatenating the two
texts together and adding it to the dataset (if ≤
140 characters). We create only sentence-level
or phrase-level code-switching rather than word-
level switches to avoid classifier ambiguity for
loan words, which is known to be a significant
challenge (Çetinoğlu et al., 2016).
</p>
<p>1A manual analysis of 500 tweets confirmed that nearly
all cases (98.6%) where the classifier’s label differed from
the author’s inferred language were misclassifications.
</p>
<p>52</p>
<p />
</div>
<div class="page"><p />
<p>Corpus Summary The geographically-diverse
corpus was constructed from two Twitter datasets:
1.3B tweets drawn from a 10% sample of all
tweets from March 2014 and 14.2M tweets drawn
from 1% sample of all geotagged tweets from
November 2016. Ultimately, we collected 97.8M
tweets from 1.5M users across 197 countries and
in 53 languages. After identifying monolingual
authors in the dataset, 9.4% of the instances
(9.1M) were labeled by CLD2 or langid.py with
a different language than that spoken by its au-
thor; since nearly all are misclassifications, we
view these posts as valuable data to correct sys-
tematic bias.
</p>
<p>A total of 258M instances were collected for the
topically and socially-diverse corpora. Multilin-
gual instances were created by sampling text from
all language pairs; a total of 3.2M synthetic in-
stances were created. Full details are reported in
Supplementary Material.
</p>
<p>3 Equitable LID Classifier
</p>
<p>We introduce EQUILID, and evaluate it on mono-
lingual and multilingual tweet-length text.
Model Character-based neural network architec-
tures are particularly suitable for LID, as they
facilitate modeling nuanced orthographic and
phonological properties of languages (Jaech et al.,
2016; Samih et al., 2016), e.g., capturing regu-
lar morpheme occurrences within the words of a
language. Further, character-based methods sig-
nificantly reduce the model complexity compared
to word-based methods; the latter require sepa-
rate neural representations for each word form and
therefore are prohibitive in multilingual environ-
ments that easily contain tens of millions of unique
words. We use an encoder–decoder architecture
(Cho et al., 2014; Sutskever et al., 2014) with
an attention mechanism (Bahdanau et al., 2015).
The encoder and the decoder are 3-layer recurrent
neural networks with 512 gated recurrent units
(Chung et al., 2014). The model is trained to to-
kenize character sequence input based on white
space and output a sequence with each token’s
language, with extra token types for punctuation,
hashtags, and user mentions.
Setup The data from our socially-representative
corpus (§2) was split into training, development,
and test sets (80%/10%/10%, respectively), sepa-
rately partitioning the data from each source (e.g.,
Wikipedia). Due to different sizes, we imposed
</p>
<p>a maximum of 50K instances per source and lan-
guage to reduce training bias. A total 52.3M in-
stances were used for the final datasets. Multi-
lingual instances were generated from texts within
their respective split to prevent test-train leakage.
For the Twitter70 dataset, we use identical train-
ing, development, and test splits as Jaech et al.
(2016). The same trained model is used for all
evaluations. All parameter optimization was per-
formed on the development set using adadelta
(Zeiler, 2012) with mini-batches of size 64 to train
the models. The model was trained for 2.7M steps,
which is roughly three epochs.
</p>
<p>Comparison Systems We compare against two
broad-coverage LID systems, langid.py (Lui and
Baldwin, 2012) and CLD2 (McCandless, 2010),
both of which have been widely used for Twit-
ter within in the NLP community. CLD2 is
trained on web page text, while langid.py was
trained on newswire, JRC-Acquis, web pages, and
Wikipedia. As neither was designed for Twitter,
we preprocess text to remove user mentions, hash-
tags, and URLs for a more fair comparison. For
multilingual documents, we substitute langid.py
(Lui and Baldwin, 2012) with its extension, Poly-
glot, described in Lui et al. (2014) and designed
for that particular task.
</p>
<p>We also include the results reported in Jaech
et al. (2016), who trained separate models for two
benchmarks used here. Their architecture uses
a convolutional network to transform each input
word into a vector using its characters and then
feed the word vectors to an LSTM encoder that de-
codes to per-word soft-max distributions over lan-
guages. These word-language distributions are av-
eraged to identify the most-probable language for
the input text. In contrast, our architecture uses
only character-based representations and produces
per-token language assignments.
</p>
<p>Benchmarks We test the monolingual setting
with three datasets: (1) the test portion of the
geographically-diverse corpus from §2, which
covers 53 languages (2) the test portion of the
Twitter70 dataset, which covers 70 languages and
(3) the TweetLID shared task (Zubiaga et al.,
2016), which covers 6 languages. The Tweet-
LID data includes Galician, which is not one of
the 70 languages we include due to its relative in-
frequency. Therefore, we report results only on
the non-Galician portions of the data. Multilin-
gual LID is tested using the test data portion of the
</p>
<p>53</p>
<p />
</div>
<div class="page"><p />
<p>Geo.-Diverse Tweets Tweet 70 TweetLID† Multilingual Tweets
System Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Macro-F1 Micro-F1
</p>
<p>langid.py� 0.234 0.960 0.378 0.769 0.580 0.302 0.240
CLD2 0.217 0.930 0.497 0.741 0.544 0.360 0.629
</p>
<p>Jaech et al. (2016)‡ 0.912 0.787
EQUILID 0.598 0.982 0.920 0.905 0.796 0.886 0.853
</p>
<p>Table 1: Results on the four benchmarks. ‡ results reported in Jaech et al. (2016) are separate models optimized for each
benchmark † excludes Galician. � For multilingual tweets, we use the extension to langid.py described in Lui et al. (2014).
</p>
<p>synthetically-constructed multilingual data from
70 languages. Models are evaluated using macro-
averaged and micro-averaged F1. Macro-averaged
F1 denotes the average F1 for each language, in-
dependent of how many instances were seen for
that language. Micro-averaged F1 denotes the F1
measured from all instances and is sensitive to the
skew in the distribution of languages in the dataset.
Results EQUILID attains state-of-the-art perfor-
mance over the other broad-coverage LID systems
on all benchmarks. We attribute this increase to
more representative training data; indeed, Jaech
et al. (2016) reported langid.py obtains a substan-
tially higher F1 of 0.879 when retrained only on
Twitter70 data, underscoring the fact that broad-
coverage systems are typically not trained on data
as linguistically diverse as seen in social media.
Despite being trained for general-purpose, EQUI-
LID also outperformed the benchmark-optimized
models of Jaech et al. (2016).
</p>
<p>In the multilingual setting, EQUILID substan-
tially outperforms both Polyglot and CLD2, with
over a 300% increase in Macro-F1 over the former.
Further, because our model can also identify the
spans in each language, we view its performance
as an important step towards an all-languages
solution for detecting sentence and phrase-level
switching between languages. Indeed, in the
Twitter70 dataset, EQUILID found roughly 5% of
the test data are unmarked instances of code-
switching, one of which is the third example in
Figure 1.
Error Analysis To identify main sources of clas-
sification errors, we manually analyzed the out-
puts of EQUILID on the test set of Twitter70. The
dataset contains 9,572 test instances, 90.5% of
which were classified correctly by our system; we
discuss below sources of errors in the remaining
909 misclassified instances.
</p>
<p>Classification of closely related languages with
overlapping vocabularies written in a same script
is the biggest source of errors (374 misclassified
instances, 41.1% of all errors). Slavic languages
</p>
<p>are the most challenging, with 177 Bosnian and
65 Slovenian tweets classified as Croatian. This
is unsurprising, considering that even for a human
annotator this task is challenging (or impossible).
For example, a misclassified Bosnian tweet Sočni
čokoladni biskvit recept (“juicy chocolate biscuit
recipe”) would be the same in Croatian. Indo-
Iranian languages contribute 39 errors, with Ben-
gali, Marathi, Nepali, Punjabi, and Urdu tweets
classified as Hindi. Among Germanic languages,
Danish, Norwegian, and Swedish are frequently
confused, contributing 22 errors.
</p>
<p>Another major source of errors is due to translit-
eration and code switching with English: 328 mes-
sages in Hindi, Urdu, Tagalog, Telugu, and Pun-
jabi were classified as English, contributing 36.1%
of errors. A Hindi-labeled tweet dost tha or ra-
hega ... dont wory ... but dherya rakhe (“he was
and will remain a friend ... don’t worry ... but
have faith”) is a characteristic example, misclas-
sified by our system as English. Reducing these
types of errors is currently difficult due to the lack
of transliterated examples for these languages.
</p>
<p>4 Case Study: Health Monitoring
</p>
<p>We conclude with a real-world case study on us-
ing Twitter posts as a real-time source of infor-
mation for tracking health and well-being trends
(Paul and Dredze, 2011; Achrekar et al., 2011;
Aramaki et al., 2011). This information is es-
pecially critical for regions where local authori-
ties may not have sufficient resources to identify
trends otherwise. Commonly, trend-tracking ap-
proaches first apply language identification to se-
lect language-specific content, and then apply so-
phisticated NLP techniques to identify content re-
lated to their target phenomena, e.g., distinguish-
ing a flu comment from a hangover-related one.
This setting is where socially-inclusive LID sys-
tems can make real, practical impact: LID systems
that effectively classify languages of underrepre-
sented dialects can substantially increase the re-
</p>
<p>54</p>
<p />
</div>
<div class="page"><p />
<p>call of data for trend-tracking approaches, and thus
help reveal dangerous trends in infectious diseases
in the areas that need it most.
</p>
<p>Language varieties are associated, among other
factors, with social class (Labov, 1964; Ash, 2002)
and ethnic identity (Rose, 2006; Mendoza-Denton,
1997; Dubois and Horvath, 1998). As a case
study, we evaluate the efficacy of LID systems in
identifying English tweets containing health lex-
icons, across regions with varying Human De-
velopment Index (HDI).2 We compare EQUILID
against langid.py and CLD2.
Setup A list of health-related terms was com-
piled from lexicons for influenza (Lamb et al.,
2013); psychological well-being (Smith et al.,
2016; Preoţiuc-Pietro et al., 2015); and temporal
orientation lexica correlated with age, gender and
personality traits (Park et al., 2016). We incorpo-
rate the 100 highest-weighted alphanumeric terms
from each lexicon, for a total of 385 unique terms.
</p>
<p>To analyze the possible effect of regional lan-
guage, we selected 25 countries with English-
speaking populations and constructed 62 bounding
boxes for major cities therein for study (listed in
Supplementary Material). Using the Gnip API, a
total of 984K tweets were collected during January
2016 which used at least one term and were au-
thored within one of the bounding boxes. As these
tweets are required to contain domain-specific
terms, the vast majority are English.3 We there-
fore measure each system’s performance accord-
ing to what percent of these tweets they classify as
English, which estimates their Recall.
Results To understand how Human Development
Index relates to LID performance, we train a Logit
Regression to predict whether a tweet with one of
the target terms will be recognized as English ac-
cording to the HDI of the tweet’s origin country.
Figure 2 reveals increasing disparity in LID accu-
racy for developing countries by the two baseline
models. In contrast, EQUILID outperforms both
systems at all levels of HDI and provides 30%
more observations for countries with the lowest
development levels. This performance improve-
ment is increasingly critical in the global environ-
ment as more English text is generated from pop-
ulous developing countries such as Nigeria (HDI
</p>
<p>2HDI is a composite of life expectancy, education, and
income per capita indicators, used to rank countries into tiers
of human development.
</p>
<p>3A manual analysis of a random sample of 1000 tweets
showed that 99.4% were in English.
</p>
<p>0.4 0.5 0.6 0.7 0.8 0.9 1.0
Human Development Index
</p>
<p>of Text's Origin Country
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1.0
</p>
<p>R
ec
</p>
<p>al
l o
</p>
<p>f E
ng
</p>
<p>lis
h 
</p>
<p>Tw
ee
</p>
<p>ts
by
</p>
<p> L
an
</p>
<p>gu
ag
</p>
<p>e 
ID
</p>
<p> S
ys
</p>
<p>te
m
</p>
<p>s
</p>
<p>classifier
langid.py
CLD2
EquiLID
</p>
<p>Figure 2: Estimated recall of tweets with health-related
terms according to a logit regression on the Human Devel-
opment Index of the tweet’s origin country; bands show 95%
confidence interval.
</p>
<p>0.527) and India (HDI 0.624), which have tens of
millions of anglophones each. EQUILID provides
a 23.9% and 17.4% improvement in recall of En-
glish tweets for each country, respectively. This
study corroborates our hypothesis that socially-
equitable training corpora are an essential first step
towards socially-equitable NLP.
</p>
<p>5 Conclusion
</p>
<p>Globally-spoken languages often vary in how they
are spoken according to regional dialects, topics,
or sociolinguistic factors. However, most LID sys-
tems are not designed and trained for this linguis-
tic diversity, which has downstream consequences
for what types of text are considered a part of the
language. In this work, we introduce a socially-
equitable LID system, EQUILID, built by (1) cre-
ating a dataset representative of the types of di-
versity within languages and (2) explicitly mod-
eling multilingual and codes-switched communi-
cation for arbitrary language pairs. We demon-
strate that EQUILID significantly outperforms cur-
rent broad-coverage LID systems and, in a real-
world case study on tracking health-related con-
tent, show that EQUILID substantially reduces the
LID performance disparity between developing
and developed countries. Our work continues a
recent emphasis on NLP for social good by en-
suring NLP tools fully represent all people. The
EQUILID system is publicly available at https:
//github.com/davidjurgens/equilid and data
is available upon request.
</p>
<p>55</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgments
We thank the anonymous reviewers, the Stanford Data Sci-
ence Initiative, and Twitter and Gnip for providing access
to part of data used in this study. This work was sup-
ported by the National Science Foundation through awards
IIS-1514268, IIS-1159679, and IIS-1526745.
</p>
<p>References
Harshavardhan Achrekar, Avinash Gandhe, Ross
</p>
<p>Lazarus, Ssu-Hsin Yu, and Benyuan Liu. 2011. Pre-
dicting flu trends using Twitter data. In Proc. IEEE
Computer Communications Workshops. pages 702–
707.
</p>
<p>Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: detecting influenza
epidemics using Twitter. In Proc. EMNLP. pages
1568–1576.
</p>
<p>Sharon Ash. 2002. Social class. The handbook of lan-
guage variation and change 24:402.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR.
</p>
<p>David Bamman. 2015. People-Centric Natural Lan-
guage Processing. Ph.D. thesis, Carnegie Mellon
University.
</p>
<p>Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proc. of the Second Workshop on
Language in Social Media. pages 65–74.
</p>
<p>Su Lin Blodgett, Lisa Green, and Brendan O’Connor.
2016. Demographic dialectal variation in social me-
dia: A case study of African-American English. In
Proc. EMNLP.
</p>
<p>Ralf D Brown. 2014. Non-linear mapping for im-
proved identification of 1300+ languages. In Proc.
EMNLP. pages 627–632.
</p>
<p>Özlem Çetinoğlu, Sarah Schulz, and Ngoc Thang Vu.
2016. Challenges of computational processing of
code-switching. In Proc. of the Second Workshop
on Computational Approaches to Code Switching.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proc. EMNLP.
</p>
<p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In Proc. NIPS Deep Learning workshop.
</p>
<p>Ryan Compton, David Jurgens, and David Allen. 2014.
Geotagging one hundred million twitter accounts
with total variation minimization. In Big Data
</p>
<p>(Big Data), 2014 IEEE International Conference on.
IEEE, pages 393–401.
</p>
<p>Sylvie Dubois and Barbara M Horvath. 1998. From
accent to marker in Cajun English: A study of di-
alect formation in progress. English World-Wide
19(2):161–188.
</p>
<p>Jacob Eisenstein. 2015. Systematic patterning
in phonologically-motivated orthographic variation.
Journal of Sociolinguistics 19(2):161–188.
</p>
<p>Bruno Gonçalves and David Sánchez. 2014. Crowd-
sourcing dialect characterization through Twitter.
PloS one 9(11):e112074.
</p>
<p>Mark Graham, Bernie Hogan, Ralph K Straumann, and
Ahmed Medhat. 2014. Uneven geographies of user-
generated information: patterns of increasing in-
formational poverty. Annals of the Association of
American Geographers 104(4):746–764.
</p>
<p>Dirk Hovy and Shannon L Spruit. 2016. The social im-
pact of natural language processing. In Proc. ACL.
pages 591–598.
</p>
<p>Aaron Jaech, George Mulcaire, Shobhit Hathi, Mari
Ostendorf, and Noah A Smith. 2016. Hierarchical
character-word models for language identification.
In Proc. of the 2nd Workshop on Computational Ap-
proaches to Code Switching.
</p>
<p>I. Johnson, C. McMahon, J. Schning, and B. Hecht.
2017. The effect of population and “structural” bi-
ases on social media-based algorithms – a case study
in geolocation inference across the urban-rural spec-
trum. In Proc. CHI.
</p>
<p>David Jurgens, Tyler Finnethy, James McCorriston,
Yi Tian Xu, and Derek Ruths. 2015. Geolocation
prediction in twitter using social networks: A criti-
cal analysis and review of current practice. In Proc.
ICWSM.
</p>
<p>William Labov. 1964. The social stratification of En-
glish in New York City. Ph.D. thesis, Columbia uni-
versity.
</p>
<p>Alex Lamb, Michael J Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
Twitter. In Proc. HLT-NAACL. pages 789–795.
</p>
<p>Wang Ling, Luis Marujo, Chris Dyer, Alan Black, and
Isabel Trancoso. 2014. Crowdsourcing high-quality
parallel data extraction from Twitter. In Proc. WMT .
</p>
<p>Wang Ling, Luı́s Marujo, Chris Dyer, Alan W Black,
and Isabel Trancoso. 2016. Mining parallel corpora
from Sina Weibo and Twitter. Computational Lin-
guistics .
</p>
<p>Wang Ling, Guang Xiang, Chris Dyer, Alan W Black,
and Isabel Trancoso. 2013. Microblogs as parallel
corpora. In Proc. ACL. pages 176–186.
</p>
<p>56</p>
<p />
</div>
<div class="page"><p />
<p>Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proc.
ACL (system demonstrations). pages 25–30.
</p>
<p>Marco Lui and Timothy Baldwin. 2014. Accurate lan-
guage identification of Twitter messages. In Proc.
of the 5th Workshop on Language Analysis for So-
cial Media. pages 17–25.
</p>
<p>Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. TACL 2:27–40.
</p>
<p>Michael McCandless. 2010. Accuracy and perfor-
mance of Google’s compact language detector. Blog
post.
</p>
<p>Paul McNamee. 2005. Language identification: a
solved problem suitable for undergraduate instruc-
tion. Journal of Computing Sciences in Colleges
20(3):94–101.
</p>
<p>Norma Catalina Mendoza-Denton. 1997. Chi-
cana/Mexicana identity and linguistic variation: An
ethnographic and sociolinguistic study of gang affil-
iation in an urban high school. Ph.D. thesis, Stan-
ford University.
</p>
<p>Dong-Phuong Nguyen and A Seza Doğruöz. 2013.
Word level language identification in online mul-
tilingual communication. In Proc. EMNLP. pages
857–862.
</p>
<p>Harold Orton, Stewart Sanderson, and John Widdow-
son. 1998. The linguistic atlas of England. Psy-
chology Press.
</p>
<p>Gregory Park, H Andrew Schwartz, Maarten Sap, Mar-
garet L Kern, Evan Weingarten, Johannes C Eich-
staedt, Jonah Berger, David J Stillwell, Michal
Kosinski, Lyle H Ungar, et al. 2016. Living in the
past, present, and future: Measuring temporal orien-
tation with language. Journal of personality .
</p>
<p>Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proc. ICWSM.
</p>
<p>Daniel Preoţiuc-Pietro, Svitlana Volkova, Vasileios
Lampos, Yoram Bachrach, and Nikolaos Aletras.
2015. Studying user income through language,
behaviour and affect in social media. PloS one
10(9):e0138717.
</p>
<p>Shruti Rijhwani, Royal Sequiera, Monojit Choud-
hury, Kalika Bali, and Chandra Sekhar Maddila.
2017. Estimating code-switching on twitter with
a novel generalized word-level language detection
technique. In Proc. ACL.
</p>
<p>Mary Aleene Rose. 2006. Language, place and identity
in later life. Stanford University.
</p>
<p>Younes Samih, Suraj Maharjan, Mohammed Attia,
Laura Kallmeyer, and Thamar Solorio. 2016. Mul-
tilingual code-switching identification via LSTM re-
current neural networks. In Proc. of the 2nd Work-
shop on Computational Approaches to Code Switch-
ing.
</p>
<p>Laura K. Smith, Salvatore Giorgi, Rishi Solanki,
Johannes C. Eichstaedt, H. Andrew Schwartz,
Muhammad Abdul-Mageed, Anneke Buffone, and
Lyle H. Ungar. 2016. Does ‘well-being’ translate on
Twitter? In Proc. EMNLP.
</p>
<p>Thamar Solorio, Elizabeth Blair, Suraj Mahar-
jan, Steven Bethard, Mona Diab, Mahmoud
Gohneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-
lia Hirschberg, Alison Chang, and Pascale Fung.
2014. Overview for the first shared task on lan-
guage identification in code-switched data. In Proc.
of the First Workshop on Computational Approaches
to Code Switching. pages 62–72.
</p>
<p>Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Dániel Varga. 2006. The jrc-acquis: A multilingual
aligned parallel corpus with 20+ languages. arXiv
preprint cs/0609058 .
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Proc. NIPS.
</p>
<p>Rachael Tatman. 2015. # go awn: Sociophonetic vari-
ation in variant spellings on twitter. Working Papers
of the Linguistics Circle 25(2):97–108.
</p>
<p>Mitja Trampus. 2016. Evaluating language
identification performance. Blog post.
Https://blog.twitter.com/2015/evaluating-language-
identification-performance.
</p>
<p>Marcos Zampieri, Liling Tan, Nikola Ljubešic, and
Jörg Tiedemann. 2014a. A report on the DSL shared
task 2014. In Proc. of the First Workshop on Apply-
ing NLP Tools to Similar Languages, Varieties and
Dialects. pages 58–67.
</p>
<p>Marcos Zampieri, Liling Tan, Nikola Ljubešic, and
Jörg Tiedemann. 2014b. A report on the dsl shared
task 2014. In Proc. of the First Workshop on Apply-
ing NLP Tools to Similar Languages, Varieties and
Dialects. pages 58–67.
</p>
<p>Marcos Zampieri, Liling Tan, Nikola Ljubešic, Jörg
Tiedemann, and Preslav Nakov. 2015. Overview
of the DSL shared task 2015. In Proc. of the Joint
Workshop on Language Technology for Closely Re-
lated Languages, Varieties and Dialects. pages 1–9.
</p>
<p>Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .
</p>
<p>Arkaitz Zubiaga, Inaki San Vicente, Pablo Gamallo,
José Ramom Pichel, Inaki Alegria, Nora Aranberri,
Aitzol Ezeiza, and Vı́ctor Fresno. 2016. TweetLID:
a benchmark for tweet language identification. Lan-
guage Resources and Evaluation 50(4):729–766.
</p>
<p>57</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 58–63
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2010
</p>
<p>Evaluating Compound Splitters Extrinsically with Textual Entailment
</p>
<p>Glorianna Jagfeld Patrick Ziering
Institute for Natural Language Processing
</p>
<p>University of Stuttgart
{jagfelga,zierinpk}
</p>
<p>@ims.uni-stuttgart.de
</p>
<p>Lonneke van der Plas
Institute of Linguistics
</p>
<p>University of Malta, Malta
Lonneke.vanderPlas
</p>
<p>@um.edu.mt
</p>
<p>Abstract
</p>
<p>Traditionally, compound splitters are eval-
uated intrinsically on gold-standard data
or extrinsically on the task of statistical
machine translation. We explore a novel
way for the extrinsic evaluation of com-
pound splitters, namely recognizing tex-
tual entailment. Compound splitting has
great potential for this novel task that is
both transparent and well-defined. More-
over, we show that it addresses certain as-
pects that are either ignored in intrinsic
evaluations or compensated for by task-
internal mechanisms in statistical machine
translation. We show significant improve-
ments using different compound splitting
methods on a German textual entailment
dataset.
</p>
<p>1 Introduction
</p>
<p>Closed compounding, i.e., the formation of a one-
word unit composing several lexemes, is a com-
mon linguistic phenomenon in several languages
such as German, Dutch, Greek, and Finnish. The
goal of compound splitting is to obtain the con-
stituents of a compound to increase its semantic
transparency. For example, for the German com-
pound Apfelsaft ‘apple1 juice2’ the desired output
of a compound splitter is Apfel1 Saft2.
</p>
<p>Intrinsic evaluation of compound splitting mea-
sures the correctness of the determined split point
(Riedl and Biemann, 2016) and the resulting lem-
mas by means of precision, recall, F1-score and
accuracy (e.g., Koehn and Knight (2003)). In ex-
trinsic evaluation setups, compound splitting is
applied to the input data of an external natural lan-
guage processing (NLP) task that benefits from
split compounds. As closed compounding intro-
duces semantic opaqueness and vastly increases
the vocabulary size of a language, many NLP tasks
</p>
<p>benefit from compound splitters. Still, previous
work that evaluates compound splitting with ex-
trinsic evaluation methods mostly focuses on sta-
tistical machine translation (SMT) (e.g., Nießen
and Ney (2000), Koehn and Knight (2003)).
Some other external tasks such as information re-
trieval (Kraaij and Pohlmann, 1998) or speech
recognition (Larson et al., 2000) have been shown
to benefit from prior compound splitting, yet these
works have not compared the extrinsic perfor-
mance of different compound splitting methods.
</p>
<p>Interestingly, the performance found in in-
trinsic evaluations does not automatically propa-
gate to performance in downstream evaluations as
shown in (Fritzinger and Fraser, 2010) for SMT,
where oversplit compounds are simply learned as
phrases (Dyer, 2009; Weller et al., 2014). Over-
splitting is an example of a feature that might
not be measured in intrinsic evaluations, because
some available gold standards contain positive ex-
amples only (Ziering and van der Plas, 2016). It
is highly relevant to increase the number of extrin-
sic tasks for the evaluation of compound splitting
to be able to evaluate features that intrinsic evalu-
ations and known extrinsic evaluations ignore.
</p>
<p>In this paper we investigate the suitability of
Recognizing Textual Entailment (RTE) for the
task of compound splitting, inspired by the fact
that previous work in RTE underlined the potential
benefits of compound splitting for this task (Zeller,
2016). Textual Entailment (TE) is a directional
relationship between an entailing text fragment
T and an entailed hypothesis, H, saying that the
meaning of T entails (or implies) the meaning of
H. This relation holds if ‘typically, a human, read-
ing T, would infer that H is most likely true’ (Da-
gan et al., 2006). The following is an example of
an entailing T-H pair:
</p>
<p>T: Yoko Ono unveiled a bronze statue of her
late husband, John Lennon.
H: Yoko Ono is John Lennon’s widow.
</p>
<p>58</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2010">https://doi.org/10.18653/v1/P17-2010</a></div>
</div>
<div class="page"><p />
<p>We opted for exploring the use of RTE as an ex-
trinsic evaluation for compound splitting for three
main reasons: first, in contrast to SMT systems,
most RTE systems are less complex. In fact, we
deliberately chose an RTE system that reaches
good performance with a method that is transpar-
ent, i.e., a method that allows for exploring the ef-
fect of compounding.1 It is not our goal to reach
state-of-the-art performance for the RTE task. We
aim to find a suitable alternative extrinsic eval-
uation for compound splitting. Second, human
agreement on the binary RTE decisions is very
high, e.g., on the dataset used in our experiments,
an average agreement rate of 87.8% with a κ level
of 0.75 was reported (Giampiccolo et al., 2007).
Third, the potential benefits for RTE are large.
According to Zeller (2016, p. 182) the number
of T/H pairs in their phenomenon-specific RTE
dataset would rise by about 16 percentage points
by compound splitting. In the dataset we use in
our experiments, about three-quarters of the T-H
pairs contain at least one closed compound.
</p>
<p>2 Relevance of Compound Splitting for
RTE
</p>
<p>The approach to RTE taken in this paper fol-
lows the Lexical Overlap Hypothesis (LOH),
which states that the higher the number of lex-
ical matches between T and H, the more likely
the T-H pair is entailing rather than non-entailing
(Zeller, 2016). In other words, H is more likely
to be entailed by T if most of its lexical content
also occurs in T. While this hypothesis is a simpli-
fication of the TE problem, it has been shown to
perform reasonably well for some datasets (Noh
et al., 2015). We argue that the brittleness of the
chosen LOH-based RTE system may actually be a
strength in terms of evaluation, since it will penal-
ize oversplitting more severely than, e.g., an RNN-
based RTE system or a phrase-based MT method
that can recover from systematic oversplitting by
chunking the splits.
</p>
<p>Under the LOH, the problem caused by the
opacity of closed compounds becomes evident. As
shown in the example below, missing informa-
tion on the constituents of closed compounds hin-
ders the matching of words from T in H1. Con-
versely, compound splitting also helps to detect
</p>
<p>1We did not opt for neural RTE systems (Bowman et al.,
2015), albeit state-of-the-art, in this first study because of the
opacity of the models and the inclusion of phrase-level infor-
mation, which will make interpretation of the effect harder.
</p>
<p>non-entailing T-H pairs. By compound splitting,
we increase the number of uncovered tokens in
H2, which makes a non-entailment decision more
likely2.
</p>
<p>T: Kinder lieben Fruchtsäfte1 aus Äpfeln2 ‘Chil-
dren love fruit juices1 made of apples2’
</p>
<p>H1: Peters Sohn liebt Apfel3saft4 ‘Peter’s son
loves apple3 juice4’
</p>
<p>H2: Peters Sohn liebt Apfel5kuchen6stücke7
‘Peter’s son loves pieces7 of apple5 pie6’
</p>
<p>3 Materials and Methods
</p>
<p>In this section we explain the splitters and the RTE
framework used in our experiments.
</p>
<p>3.1 Inspected Compound Splitters
Our proposed extrinsic evaluation approach for
compound splitting is language-independent as
we do not use any language-specific parameters.
However, in the present work we test it on the
most prominent closed-compounding language,
German (Ziering and van der Plas, 2014). We in-
spect the impact of three different types of auto-
matic compound splitting3 methods that follow a
generate-and-rank principle, where the candidate
splits are ranked according to the geometric mean
of the constituents’ frequencies in a given training
corpus (Koehn and Knight, 2003).
</p>
<p>FF2010 The compound splitter by Fritzinger
and Fraser (2010) relies on the output of the
German morphological analyzer SMOR (Schmid
et al., 2004) to generate several plausible com-
pound splits (e.g., due to word sense ambiguity).
</p>
<p>WH2012 As an alternative method, we use
the statistical approach presented in Weller and
Heid (2012) for German compound splitting. In-
stead of using the knowledge-rich SMOR, it in-
cludes an extensive list of hand-crafted transfor-
mation rules that allows to map constituents to
corpus lemmas (e.g., by truncating linking mor-
phemes) to generate all possible splits with up to
four constituents per compound. Moreover, mis-
leading lemmas are removed from the training cor-
pus using hand-crafted filters.
</p>
<p>2Note that we need to apply lemmatization prior to deter-
mining the lexical matches between T and H.
</p>
<p>3The compound splitters are designed to split com-
pounds with any content word as head, i.e., noun compounds
(Hunde|hütte ‘doghouse’), verb compounds (eis|laufen ‘to
ice-skate’) and adjective compounds (hunde|müde ‘dog-
tired’) and disregard constructions with a functional modifier
(as in the particle verb auf|stehen ‘to stand up’).
</p>
<p>59</p>
<p />
</div>
<div class="page"><p />
<p>System Acc Entailment Non-entailmentP R F1 P R F1
INIT 64.13 62.50 74.57 68.00 66.67 53.20 59.18
</p>
<p>manual splitting ? 67.88 65.08 80.20 71.85 72.64 54.99 62.59
ZvdP2016 66.63 64.55 77.02 70.23 69.87 55.75 62.02
FF2010 ? 67.38 65.48 76.53 70.58 70.19 57.80 63.39
WH2012 66.00 63.73 77.75 70.04 69.77 53.71 60.69
</p>
<p>Table 1: Results on RTE performance without (INIT) and with prior compound splitting. ?: significant
difference of the performance in comparison to INIT
</p>
<p>ZvdP2016 Finally, the method using least
language-specific knowledge was proposed by
Ziering and van der Plas (2016). Instead of us-
ing a morphological analyzer or manually com-
piling a hand-crafted list of rules, they recursively
generate all possible binary splits by learning con-
stituent transformations from regular inflection de-
rived from a monolingual lemmatized corpus, e.g.,
the s-suffix in the case of a genitive marker is often
used as linking morpheme. The recursion stops if
a non-splitting (atomic) analysis is ranked highest.
</p>
<p>Additionally, to provide an upper bound, we
manually split development and test data.
</p>
<p>3.2 RTE Framework
We conduct our RTE experiments using the open-
source Excitement Open Platform (EOP) (Padó
et al., 2015; Magnini et al., 2014), which provides
comprehensive implementations of algorithms and
lexical resources for textual inference. We use
the alignment-based algorithm P1EDA (Noh et al.,
2015) in all our experiments as it has been shown
to be simple and transparent while yielding rela-
tively good results. P1EDA is based on the LOH
for RTE explained in Section 2. The algorithm
works in three steps: First, it extracts all possible
alignments between sequences of identical lem-
mas in T and H. Then, it extracts various fea-
tures4 from the alignments. Finally, these fea-
tures are given as input to a multinomial logis-
tic regression classifier which is trained on anno-
tated data. For the sake of simplicity, for now
we only use one basic aligner which aligns (se-
quences of) words in T and H that consist of iden-
tical lemmas. We will investigate the impact of
prior compound splitting given additional lexical
resources (such as a derivational morphology lex-
</p>
<p>4We use a similar feature set as Noh et al. (2015), namely
the ratio of aligned vs. unaligned words in H with respect to
all words, content words, and named entities.
</p>
<p>icon (Zeller et al., 2013)) in future work. We use
TreeTagger (Schmid, 1995) as integrated in EOP
to provide tokenization, lemmatization and Part-
of-Speech tagging as linguistic preprocessing.
</p>
<p>We train and evaluate all models on the Ger-
man translation of the RTE-3 dataset (Dagan et al.,
2006; Magnini et al., 2014). The training and test
dataset contain 800 T-H pairs each. In both sets,
entailing and non-entailing T-H pairs are equally
distributed (chance baseline of 50% accuracy).
</p>
<p>We apply a compound splitter on the RTE train-
ing and test dataset before we input the data to the
EOP pipeline. We replace all compounds by their
constituents, separated by white-space. Thus, they
are subsequently treated as individual words by
EOP and the lexical aligner can benefit from the
increased transparency of the compounds.
</p>
<p>4 Results and Discussion
</p>
<p>Table 1 shows accuracy, precision, recall and F1-
score for the entailment and non-entailment class
on the RTE-3 dataset. As reflected in the results,
reducing the opacity of compounds via the appli-
cation of a compound splitter improves the subse-
quent RTE performance. This holds for all com-
pound splitters that we used in our experiments.
It is also noticeable that the different compound
splitters yield different results in the downstream
task, with FF2010 being the most beneficial and
significantly5 outperforming the initial RTE setup
without prior compound splitting (INIT) by up to
four percentage points in accuracy and F1-score.
</p>
<p>As expected, manual splitting performs best
overall. The performance difference with FF2010
is however not statistically significant. This is not
surprising because FF2010 reaches an accuracy
of around 90% in intrinsic evaluations (Ziering
and van der Plas, 2016) and the small underperfor-
</p>
<p>5McNemar test (McNemar, 1947), p &lt; 0.05
</p>
<p>60</p>
<p />
</div>
<div class="page"><p />
<p>mance is leveled out by the small size of the test
set. Moreover, manual inspections revealed that
FF2010 has a higher recall than manual splitting
in the non-entailment class due to its undersplit-
ting which results in less lexical overlap between
T and H, pointing to the non-entailment class.
</p>
<p>When we compare these results from the ex-
trinsic evaluation with intrinsic evaluation results
(in terms of splitting accuracy) reported in Zier-
ing and van der Plas (2016), we see the same per-
formance ordering with respect to the three com-
pound splitters, while the current extrinsic evalua-
tion on RTE differentiates between the best system
(FF2010) and the two others in that only the for-
mer reached statistically significant improvements
over the INIT baseline.
</p>
<p>To analyze the possible causes of difference
in performance between the systems and to see
the benefits of using RTE for compound splitting
evaluation we performed a manual error analysis.
First, we examined all entailment classifications
that were correct using FF2010 and incorrect when
using the INIT baseline. Using FF2010, the classi-
fier was able to correctly classify an additional 36
entailing and 25 non-entailing T-H pairs. As ex-
pected, most of the hypotheses in these pairs con-
tained correctly split compounds where the RTE
system could benefit from the increased trans-
parency. Conversely, we also examined the 28 T-H
pairs that the classifier missed to identify as entail-
ing while they were correct in INIT. Most of the
examples were cases in which there was almost no
lexical overlap between T and H even with com-
pound splitting.
</p>
<p>Furthermore, we compared the correct entail-
ment classifications of FF2010 with the other two
splitters. For ZvdP2016, most errors can be at-
tributed to oversplitting. Precisely, 25 out of its 37
(67.5%) misclassifications compared to FF2010
can be attributed to this problem. For exam-
ple, ZvdP2016 oversplit the name Landowska into
Line Dow Ska6 that appeared in both T and H in
an non-entailing pair, which artificially increased
the coverage ratio of words in H and therefore
pointed to the incorrect entailment classification.
For WH2012, oversplitting is also a major con-
tributor of RTE errors, however it appeares not as
predominant as for ZvdP2016. 10 out of its 29
(34.5%) misclassifications compared to FF2010
</p>
<p>6Misleading knowledge about verbal inflection automati-
cally derived from a lemmatized corpus is responsible for the
oversplitting by ZvdP2016.
</p>
<p>can be attributed to oversplitting, while 4 (13.8%)
missclassifications are due to undersplitting. For
example, in an entailing T-H pair WH2012 cor-
rectly split Amazonas-Regenwald ‘Amazon rain-
forest’ in H into Amazonas Regen Wald, however it
oversplit Amazonas in T into Amazon As ‘Amazon
ace’ and thus, Amazonas in H remained unaligned.
To the contrary, FF2010 did not split Amazonas in
T, which lead to a higher token coverage ratio in H.
Again, in the H Die EU senkt die Fangquoten ‘The
EU lowers the fishing quota’ of another entailing
T-H pair, WH2010 correctly split Fang1quoten2
‘fishing quota’ in H into fangen1 Quote2 but failed
to split EU-Quote in T, failing to cover both EU
and Quote in H.
</p>
<p>Our closer inspections also showed that com-
pound splitting does not always suffice to reveal
a lexical match between T and H as shown in the
following example:
</p>
<p>T: Ben fährt1 einen Mercedes2 ‘Ben drives1 a
Mercedes2’
</p>
<p>H: Ben ist Auto3fahrer4 ‘Ben is a car3 driver4’
Given a correct splitting of Autofahrer to
</p>
<p>Auto Fahrer, a derivational morphology resource
(Zeller et al., 2013) would be required to discover
the relationship between fahren and Fahrer and a
synonym database to find that Mercedes is a hy-
ponym of Auto. This does not weaken the claim
that RTE is useful for evaluating compound split-
ters. It just shows that deeper, semantic compound
analysis could improve RTE further.
</p>
<p>Besides, the error analysis shed some light on
the treatment of compound heads and modifiers. It
seems advisable to weight the compound head and
modifiers differently when computing the ratio of
aligned tokens in H. As illustrated by the follow-
ing example, coverage of the head should be more
important for the entailment decision than of the
modifiers. Given a correct split of Kinder1buch2
into Kind1 Buch2, H1 and H2 have the same token
coverage ratio while only H1 is entailed by T.
</p>
<p>T: Yuki kauft ein Kinder1buch2 ‘Yuki buys a
children’s1 book2’
</p>
<p>H1: Yuki kauft ein Buch ‘Yuki buys a book’
H2: Yuki ist ein Kind ‘Yuki is a child’
</p>
<p>It should be noted that the transparency gain us-
ing compound splitting is limited to closed com-
pounds that are compositional with respect to at
least one constituent. Splitting compounds in
</p>
<p>61</p>
<p />
</div>
<div class="page"><p />
<p>H that are fairly non-compositional with respect
to all constituents (e.g., Maulwurf ‘mole’ (lit.
‘mouth throw’)) is counterproductive. However,
since most compounds (in particular ad-hoc pro-
ductions) are compositional, this is only a side is-
sue. In fact, we did not observe any cases of non-
compositional compounds in the course of our er-
ror analysis.
</p>
<p>In summary, compound splitting is a complex
task that comprises many subtasks. The multiple
evaluation methods available, both intrinsic and
extrinsic, vary in their suitability to evaluate them.
One of these subtasks concerns the ability of com-
pound splitters to determine whether to split or
not, which is an integral part of compound anal-
ysis. While aspects such as oversplitting were not
consistently evaluated in previous intrinsic evalu-
ations, or compensated for by task-internal mech-
anisms in SMT, RTE proved more strict in this re-
spect. Moreover, the transparency of the models
made it possible to better estimate the impact of
splitting. Despite the small size of the dataset, we
were able to show significant differences, partly
due to the clear definition of this binary classifica-
tion task.
</p>
<p>On a side note, to the best of our knowledge,
the result we obtained using the FF2010 com-
pound splitter is the best result on the German
RTE-3 dataset that has been reported using EOP.
Notably, we obtain an accuracy which is almost
three percentage points higher than the results of
Noh et al. (2015), although they include further
(language-specific) linguistic knowledge.
</p>
<p>5 Conclusion
</p>
<p>Inspired by the potential benefits of compound
splitting from the RTE literature and supported by
the transparency of the models and the clear defi-
nition of this binary classification task, we set out
to explore whether RTE is a suitable method to ex-
trinsically evaluate the performance of compound
splitting. We compared several compound split-
ters on a German textual entailment dataset and
found that compound splitting is helpful for RTE
across the board. More importantly, we found that
certain aspects of compound splitters, neglected in
previous evaluations, such as oversplitting, had a
large impact on this task and nicely differentiated
the systems tested. We conclude that RTE repre-
sents a suitable alternative to SMT for the extrinsic
evaluation of compound splitters.
</p>
<p>In future work, we would like to investigate the
interaction between additional lexical resources
(such as GermaNet (Hamp and Feldweg, 1997;
Henrich and Hinrichs, 2010) or DErivBase (Zeller
et al., 2013)) and compound splitting, and the im-
pact on the RTE performance.
</p>
<p>Acknowledgments
</p>
<p>We thank Sebastian Padó and Britta Zeller for their
inspiring ideas about using compound splitting for
improving RTE, and for their support in using
the RTE framework. We are also grateful to the
anonymous reviewers for their helpful feedback.
This research was funded by the German Research
Foundation (Collaborative Research Centre 732,
Project D11).
</p>
<p>References
Samuel R Bowman, Gabor Angeli, Christopher Potts,
</p>
<p>and Christopher D Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of the First
International Conference on Machine Learning
Challenges: Evaluating Predictive Uncertainty Vi-
sual Object Classification, and Recognizing Tex-
tual Entailment. Springer-Verlag, Berlin, Heidel-
berg, MLCW’05.
</p>
<p>Chris Dyer. 2009. Using a Maximum Entropy Model
to Build Segmentation Lattices for MT. In Proceed-
ings of NAACL-HLT 2009. NAACL ’09.
</p>
<p>Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the ACL 2010
Joint 5th Workshop on Statistical Machine Transla-
tion and Metrics MATR.
</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nizing Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing. Stroudsburg, PA, USA,
RTE ’07.
</p>
<p>Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In Proceedings
of ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications.
</p>
<p>Verena Henrich and Erhard Hinrichs. 2010. GernEdiT
- The GermaNet Editing Tool. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Bente
</p>
<p>62</p>
<p />
</div>
<div class="page"><p />
<p>Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, edi-
tors, Proceedings of the Seventh International Con-
ference on Language Resources and Evaluation
(LREC’10). European Language Resources Associ-
ation (ELRA), Valletta, Malta.
</p>
<p>Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL.
</p>
<p>Wessel Kraaij and Renée Pohlmann. 1998. Comparing
the Effect of Syntactic vs. Statistical Phrase Index
Strategies for Dutch. In Proceedings ECDL’98.
</p>
<p>Martha Larson, Daniel Willett, Joachim Köhler, and
Gerhard Rigoll. 2000. Compound splitting and lexi-
cal unit recombination for improved performance of
a speech recognition system for German parliamen-
tary speeches. In Sixth International Conference
on Spoken Language Processing, ICSLP / INTER-
SPEECH.
</p>
<p>Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin
Eichler, Günter Neumann, Tae-Gil Noh, Sebastian
Pado, Asher Stern, and Omer Levy. 2014. The Ex-
citement Open Platform for Textual Inferences. In
Proceedings of the ACL 2014 System Demonstra-
tions.
</p>
<p>Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika 12(2).
</p>
<p>Sonja Nießen and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
COLING 2000.
</p>
<p>Tae-Gil Noh, Sebastian Padó, Vered Shwartz, Ido Da-
gan, Vivi Nastase, Kathrin Eichler, Lili Kotlerman,
and Meni Adler. 2015. Multi-Level Alignments As
An Extensible Representation Basis for Textual En-
tailment Algorithms. In Proceedings of the Fourth
Joint Conference on Lexical and Computational Se-
mantics, *SEM 2015. Denver, Colorado, USA.
</p>
<p>Sebastian Padó, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2015. Design and Realization
of a Modular Architecture for Textual Entailment.
Natural Language Engineering 21(2).
</p>
<p>Martin Riedl and Chris Biemann. 2016. Unsupervised
Compound Splitting With Distributional Semantics
Rivals Supervised Methods. In NAACL-HTL 2016.
</p>
<p>Helmut Schmid. 1995. Improvements In Part-of-
Speech Tagging With an Application To German. In
In Proceedings of the ACL SIGDAT-Workshop.
</p>
<p>Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In LREC 2004.
</p>
<p>Marion Weller, Fabienne Cap, Stefan Müller, Sabine
Schulte im Walde, and Alexander Fraser. 2014. Dis-
tinguishing Degrees of Compositionality in Com-
pound Splitting for Statistical Machine Translation.
In ComAComA 2014.
</p>
<p>Marion Weller and Ulrich Heid. 2012. Analyzing and
Aligning German compound nouns. In LREC 2012.
</p>
<p>Britta Dorothee Zeller. 2016. Induction, Semantic Val-
idation and Evaluation of a Derivational Morphol-
ogy Lexicon for German. Ph.D. thesis, Heidelberg,
Germany.
</p>
<p>Britta Dorothee Zeller, Jan Snajder, and Sebastian
Padó. 2013. DErivBase: Inducing and Evaluating a
Derivational Morphology Resource for German. In
ACL (1). The Association for Computer Linguistics.
</p>
<p>Patrick Ziering and Lonneke van der Plas. 2014.
What good are ’Nominalkomposita’ for ’noun com-
pounds’: Multilingual Extraction and Structure
Analysis of Nominal Compositions using Linguis-
tic Restrictors. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers.
</p>
<p>Patrick Ziering and Lonneke van der Plas. 2016.
Towards Unsupervised and Language-independent
Compound Splitting using Inflectional Morpholog-
ical Transformations. In Proceedings of NAACL-
HLT 2016.
</p>
<p>63</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 64–71
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2011
</p>
<p>An Analysis of Action Recognition Datasets for
Language and Vision Tasks
</p>
<p>Spandana Gella and Frank Keller
Institute for Language, Cognition and Computation
</p>
<p>School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
</p>
<p>S.Gella@sms.ed.ac.uk, keller@inf.ed.ac.uk
</p>
<p>Abstract
</p>
<p>A large amount of recent research has
focused on tasks that combine language
and vision, resulting in a proliferation of
datasets and methods. One such task
is action recognition, whose applications
include image annotation, scene under-
standing and image retrieval. In this
survey, we categorize the existing ap-
proaches based on how they conceptualize
this problem and provide a detailed review
of existing datasets, highlighting their di-
versity as well as advantages and disad-
vantages. We focus on recently devel-
oped datasets which link visual informa-
tion with linguistic resources and provide
a fine-grained syntactic and semantic anal-
ysis of actions in images.
</p>
<p>1 Introduction
</p>
<p>Action recognition is the task of identifying the
action being depicted in a video or still image.
The task is useful for a range of applications such
as generating descriptions, image/video retrieval,
surveillance, and human–computer interaction. It
has been widely studied in computer vision, of-
ten on videos (Nagel, 1994; Forsyth et al., 2005),
where motion and temporal information provide
cues for recognizing actions (Taylor et al., 2010).
However, many actions are recognizable from still
images, see the examples in Figure 1. Due to the
absence of motion cues and temporal features (Ik-
izler et al., 2008) action recognition from stills is
more challenging. Most of the existing work can
be categorized into four tasks: (a) action classi-
fication (AC); (b) determining human–object in-
teraction (HOI); (c) visual verb sense disambigua-
tion (VSD); and (d) visual semantic role labeling
(VSRL). In Figure 2 we illustrate each of these
</p>
<p>riding horse running playing guitar jumping
</p>
<p>Figure 1: Examples of actions in still images
</p>
<p>tasks and show how they are related to each other.
Until recently, action recognition was studied as
</p>
<p>action classification on small-scale datasets with a
limited number of predefined actions labels (Iki-
zler et al., 2008; Gupta et al., 2009; Yao and Fei-
Fei, 2010; Everingham et al., 2010; Yao et al.,
2011). Often the labels in action classification
tasks are verb phrases or a combination of verb
and object such as playing baseball, riding horse.
These datasets have helped in building models and
understanding which aspects of an image are im-
portant for classifying actions, but most methods
are not scalable to larger numbers of actions (Ra-
manathan et al., 2015). Action classification mod-
els are trained on images annotated with mutually
exclusive labels, i.e., the assumption is that only
a single label is relevant for a given image. This
ignores the fact that actions such as holding bicy-
cle and riding bicycle can co-occur in the same
image. To address these issues and also to under-
stand the range of possible interactions between
humans and objects, the human–object interaction
(HOI) detection task has been proposed, in which
all possible interactions between a human and a
given object have to be identified (Le et al., 2014;
Chao et al., 2015; Lu et al., 2016).
</p>
<p>However, both action classification and HOI de-
tection do not consider the ambiguity that arises
when verbs are used as labels, e.g., the verb play
has multiple meanings in different contexts. On
the other hand, action labels consisting of verb-
object pairs can miss important generalizations:
</p>
<p>64</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2011">https://doi.org/10.18653/v1/P17-2011</a></div>
</div>
<div class="page"><p />
<p>Action 
Recognition
</p>
<p>      Action     
classification (AC)
</p>
<p>Visual verb sense 
disambiguation (VSD)
</p>
<p>Visual 
relationship 
detection
</p>
<p>Human-object  
interaction (HOI)
</p>
<p>Visual semantic 
role labeling (VSRL)
</p>
<p>Verb Sense 
or verb frames
</p>
<p>ride
jump
hold
walk
</p>
<p>object : bicycle
riding bicycle
</p>
<p>riding horse
</p>
<p>verb: riding
agent: girl
vehicle: horse
place: park
</p>
<p>     ride
ride-1: sit and travel 
on back of an animal
ride-2:  sit on and 
control a vehicle
ride-3: be carried in 
a vehicle
</p>
<p>Figure 2: Categorization of action recognition
tasks in images
</p>
<p>riding horse and riding elephant both instanti-
ate the same verb semantics, i.e., riding animal.
Thirdly, existing action labels miss generaliza-
tions across verbs, e.g., the fact that fixing bike
and repairing bike are semantically equivalent, in
spite of the use of different verbs. These observa-
tions have led authors to argue that actions should
be analyzed at the level of verb senses. Gella
et al. (2016) propose the new task of visual verb
sense disambiguation (VSD), in which a verb–
image pair is annotated with a verb sense taken
from an existing lexical database (OntoNotes in
this case). While VSD handles distinction be-
tween different verb senses, it does not identify or
localize the objects that participate in the action
denoted by the verb. Recent work (Gupta and Ma-
lik, 2015; Yatskar et al., 2016) has filled this gap
by proposing the task of visual semantic role la-
beling (VSRL), in which images are labeled with
verb frames, and the objects that fill the semantic
roles of the frame are identified in the image.
</p>
<p>In this paper, we provide a unified view of ac-
tion recognition tasks, pointing out their strengths
and weaknesses. We survey existing literature and
provide insights into existing datasets and models
for action recognition tasks.
</p>
<p>2 Datasets for Action Recognition
</p>
<p>We give an overview of commonly used datasets
for action recognition tasks in Table 1 and group
them according to subtask. We observe that the
number of verbs covered in these datasets is often
smaller than the number of action labels reported
(see Table 1, columns #V and #L) and in many
cases the action label involves object reference. A
few of the first action recognition datasets such
as the Ikizler and Willow datasets (Ikizler et al.,
</p>
<p>2008; Delaitre et al., 2010) had action labels such
as throwing and running; they were taken from
the sports domain and exhibited diversity in cam-
era view point, background and resolution. Then
datasets were created to capture variation in hu-
man poses in the sports domain for actions such
as tennis serve and cricket bowling; typically fea-
tures based on poses and body parts were used to
build models (Gupta et al., 2009). Further datasets
were created based on the intuition that object
information helps in modeling action recognition
(Li and Fei-Fei, 2007; Ikizler-Cinbis and Sclaroff,
2010), which resulted in the use of action labels
such as riding horse or riding bike (Everingham
et al., 2010; Yao et al., 2011). Not only were most
of these datasets domain specific, but the labels
were also manually selected and mutually exclu-
sive, i.e., two actions cannot co-occur in the same
image. Also, most of these datasets do not localize
objects or identify their semantic roles.
</p>
<p>2.1 Identifying Visual Verbs and Verb Senses
</p>
<p>The limitations with early datasets (small scale,
domain specificity, and the use of ad-hoc labels
that combine verb and object) have been recently
addressed in a number of broad-coverage datasets
that offer linguistically motivated labels. Of-
ten these datasets use existing linguistic resources
such as VerbNet (Schuler, 2005), OntoNotes
(Hovy et al., 2006) and FrameNet (Baker et al.,
1998) to classify verbs and their senses. This al-
lows for a more general, semantically motivated
treatment of verbs and verb phrases, and also takes
into account that not all verbs are depictable. For
example, abstract verbs such as presuming and ac-
quiring are not depictable at all, while other verbs
have both depictable and non-depictable senses:
play is non-depictable in playing with emotions,
but depictable in playing instrument and play-
ing sport. The process of identifying depictable
verbs or verb senses is used by Ronchi and Perona
(2015), Gella et al. (2016) and Yatskar et al. (2016)
to identify visual verbs, visual verb senses, and the
semantic roles of the participating objects respec-
tively. In all the cases the process of identifying
visual verbs or senses is carried out by human an-
notators via crowd-sourcing platforms. Visualness
labels for 935 OntoNotes verb senses correspond-
ing to 154 verbs is provided by Gella et al. (2016),
while Yatskar et al. (2016) provides visualness la-
bels for 9683 FrameNet verbs.
</p>
<p>65</p>
<p />
</div>
<div class="page"><p />
<p>Dataset Task #L #V Obj Imgs Sen Des Cln ML Resource Example Labels
Ikizler (Ikizler et al., 2008) AC 6 6 0 467 N N Y N − running, walking
Sports Dataset (Gupta et al., 2009) AC 6 6 4 300 N N Y N − tennis serve, cricket bowling
Willow (Delaitre et al., 2010) AC 7 6 5 986 N N Y Y − riding bike, photographing
PPMI (Yao and Fei-Fei, 2010) AC 24 2 12 4.8k N N Y N − play guitar, hold violin
Stanford 40 Actions (Yao et al., 2011) AC 40 33 31 9.5k N N Y N − cut vegetables, ride horse
PASCAL 2012 (Everingham et al., 2015) AC 11 9 6 4.5k N N Y Y − riding bike, riding horse
89 Actions (Le et al., 2013) AC 89 36 19 2k N N Y N − ride bike, fix bike
MPII Human Pose (Andriluka et al., 2014) AC 410 − 66 40.5k N N Y N − riding car, hair styling
TUHOI (Le et al., 2014) HOI 2974 − 189 10.8k N N Y Y − sit on chair, play with dog
COCO-a (Ronchi and Perona, 2015) HOI − 140 80 10k N Y Y Y VerbNet walk bike, hold bike
Google Images (Ramanathan et al., 2015) AC 2880 − − 102k N N N N − riding horse, riding camel
HICO (Chao et al., 2015) HOI 600 111 80 47k Y N Y Y WordNet ride#v#1 bike; hold#v#2 bike
VCOCO-SRL (Gupta and Malik, 2015) VSRL − 26 48 10k N Y Y Y − verb: hit; instr: bat; obj: ball
imSitu (Yatskar et al., 2016) VSRL − 504 11k 126k Y N Y N FrameNet
</p>
<p>WordNet
verb: ride; agent: girl#n#2
vehicle: bike#n#1;
place: road#n#2
</p>
<p>VerSe (Gella et al., 2016) VSD 163 90 − 3.5k Y Y Y N OntoNotes ride.v.01, play.v.02
Visual Genome (Krishna et al., 2016) VRD 42.3k − 33.8k 108k N N Y Y − man playing frisbee
</p>
<p>Table 1: Comparison of various existing action recognition datasets. #L denotes number of action labels
in the dataset; #V denotes number of verbs covered in the dataset; Obj indicates number of objects
annotated; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image
descriptions are included; Cln indicates whether dataset is manually verified; ML indicates the possibility
of multiple labels per image; Resource indicates linguistic resource used to label actions.
</p>
<p>2.2 Datasets Beyond Action Classification
Over the last few years tasks that combine lan-
guage and vision such as image description and
visual question answering have gained much at-
tention. This has led to the creation of new, large
datasets such as MSCOCO (Chen et al., 2015) and
the VQA dataset (Antol et al., 2015). Although
these datasets are not created for action recogni-
tion, a number of attempts have been made to use
the verbs present in image descriptions to annotate
actions. The COCO-a, VerSe and VCOCO-SRL
datasets all use the MSCOCO image descriptions
to annotate fine-grained aspects of interaction and
semantic roles.
</p>
<p>HICO: The HICO dataset has 47.8k images an-
notated with 600 categories of human-object in-
teractions with 111 verbs applying to 80 object
categories of MSCOCO. It is annotated to include
diverse interactions for objects and has an aver-
age of 6.5 distinct interactions per object category.
Unlike other HOI datasets such as TUHOI which
label interactions as verbs and ignore senses, the
HOI categories of HICO are based on WordNet
(Miller, 1995) verb senses. The HICO dataset also
has multiple annotations per object and it incorpo-
rates the information that certain interactions such
as riding a bike and holding a bike often co-occur.
However, it fails to include annotations to distin-
guish between multiple senses of a verb.
</p>
<p>Visual Genome: The dataset created by Krishna
et al. (2016) has dense annotations of objects, at-
</p>
<p>tributes, and relationships between objects. The
Visual Genome dataset contains 105k images with
40k unique relationships between objects. Unlike
other HOI datasets such as HICO, visual genome
relationships also include prepositions, compara-
tive and prepositional phrases such as near and
taller than, making the visual relationship task
more generic than action recognition. Krishna
et al. (2016) combine all the annotations of ob-
jects, relationships, and attributes into directed
graphs known as scene graphs.
</p>
<p>COCO-a: Ronchi and Perona (2015) present
Visual VerbNet (VVN), a list of 140 common vi-
sual verbs manually mined from English VerbNet
(Schuler, 2005). The coverage of visual verbs
in this dataset is not complete, as many visual
verbs such as dive, perform and shoot are not in-
cluded. This also highlights a bias in this dataset
as the authors relied on occurrence in MSCOCO
as a verification step to consider a verb as vi-
sual. They annotated 10k images containing hu-
man subjects with one of the 140 visual verbs, for
80 MSCOCO objects. This dataset has better cov-
erage of human-object interactions than the HICO
dataset despite of missing many visual verbs.
</p>
<p>VerSe: Gella et al. (2016) created a dataset of
3.5k images sampled from the MSCOCO and
TUHOI datasets and annotated it with 90 verbs
and their OntoNotes senses to distinguish differ-
ent verb senses using visual context. This is the
first dataset that aims to annotate all visual senses
</p>
<p>66</p>
<p />
</div>
<div class="page"><p />
<p>of a verb. However, the total number of images
annotated and number of images for some senses
is relatively small, which makes it difficult to use
this dataset to train models. The authors further
divided their 90 verbs into motion and non-motion
verbs according to Levin (1993) verb classes and
analyzed visual ambiguity in the task of visual
sense disambiguation.
</p>
<p>VCOCO-SRL: Gupta and Malik (2015) anno-
tated a dataset of 16k person instances in 10k im-
ages with 26 verbs and associated objects in the
scene with the semantic roles for each action. The
main aim of the dataset is to build models for vi-
sual semantic role labeling in images. This task
involves identifying the actions depicted in an im-
age, along with the people and objects that in-
stantiate the semantic roles of the actions. In the
VCOCO-SRL dataset, each person instance is an-
notated with a mean of 2.8 actions simultaneously.
</p>
<p>imSitu: Yatskar et al. (2016) annotated a large
dataset of 125k images with 504 verbs, 1.7k se-
mantic roles and 11k objects. They used FrameNet
verbs, frames and associated objects or scenes
with roles to develop the dataset. They annotate
every image with a single verb and the semantic
roles of the objects present in the image. VCOCO-
SRL the is dataset most similar to imSitu, however
VCOCO-SRL includes localization information of
agents and all objects and provides multiple action
annotations per image. On the other hand, imSitu
is the dataset that covers highest number of verbs,
while also omitting many commonly studied poly-
semous verbs such as play.
</p>
<p>2.3 Diversity in Datasets
With the exception of a few datasets such as
COCO-a, VerSe, imSitu all action recognition
datasets have manually picked labels or focus
on covering actions in specific domains such as
sports. Alternatively, many datasets only cover
actions relevant to specific object categories such
as musical instruments, animals and vehicles. In
the real world, people interact with many more
objects and perform actions relevant to a wide
range of domains such as personal care, house-
hold activities, or socializing. This limits the di-
versity and coverage of existing action recogni-
tion datasets. Recently proposed datasets partly
handle this issue by using generic linguistic re-
sources to extend the vocabulary of verbs in ac-
tion labels. The diversity issue has also been high-
</p>
<p>lighted and addressed in recent video action recog-
nition datasets (Caba Heilbron et al., 2015; Sig-
urdsson et al., 2016), which include generic house-
hold activities. An analysis of various image de-
scription and question answering datasets by Fer-
raro et al. (2015) shows the bias in the distribution
of word categories. Image description datasets
have a higher distribution of nouns compared to
other word categories, indicating that the descrip-
tions are object specific, limiting their usefulness
for action-based tasks.
</p>
<p>3 Relevant Language and Vision Tasks
</p>
<p>Template based description generation systems
for both videos and images rely on identifying
subject–verb–object triples and use language mod-
eling to generate or rank descriptions (Yang et al.,
2011; Thomason et al., 2014; Bernardi et al.,
2016). Understanding actions also plays an impor-
tant role in question answering, especially when
the question is pertaining to an action depicted in
the image. There are some specifically curated
question answering datasets which target human
activities or relationships between a pair of objects
(Yu et al., 2015). Mallya and Lazebnik (2016)
have shown that systems trained on action recog-
nition datasets could be used to improve the ac-
curacy of visual question answering systems that
handle questions related to human activity and
human–object relationships. Action recognition
datasets could be used to learn actions that are vi-
sually similar such as interacting with panda and
feeding a panda or tickling a baby and calming
a baby, which cannot be learned from text alone
(Ramanathan et al., 2015). Visual semantic role
labeling is a crucial step for grounding actions in
the physical world (Yang et al., 2016).
</p>
<p>4 Action Recognition Models
</p>
<p>Most of the models proposed for action classifi-
cation and human–object interaction tasks rely on
identifying higher-level visual cues present in the
image, including human bodies or body parts (Ik-
izler et al., 2008; Gupta et al., 2009; Yao et al.,
2011; Andriluka et al., 2014), objects (Gupta et al.,
2009), and scenes (Li and Fei-Fei, 2007). Higher-
level visual cues are obtained through low-level
features extracted from the image such as Scale In-
variant Feature Transforms (SIFT), Histogram of
Oriented Gradients (HOG), and Spatial Envelopes
(Gist) features (Lowe, 1999; Dalal and Triggs,
</p>
<p>67</p>
<p />
</div>
<div class="page"><p />
<p>2005). These are useful in identifying key points,
detecting humans, and scene or background infor-
mation in images, respectively. In addition to iden-
tifying humans and objects, the relative position
or angle between a human and an object is useful
in learning human–object interactions (Le et al.,
2014). Most of the existing approaches rely on
learning supervised classifiers over low-level fea-
tures to predict action labels.
</p>
<p>More recent approaches are based on end-to-
end convolutional neural network architectures
which learn visual cues such as objects and im-
age features for action recognition (Chao et al.,
2015; Zhou et al., 2016; Mallya and Lazebnik,
2016). While most of the action classification
models rely solely on visual information, mod-
els proposed for human–object interaction or vi-
sual relationship detection sometimes combine hu-
man and object identification (using visual fea-
tures) with linguistic knowledge (Le et al., 2014;
Krishna et al., 2016; Lu et al., 2016). Other work
on identifying actions, especially methods that fo-
cus on relationships that are infrequent or unseen,
utilize word vectors learned on large text corpora
as an additional source of information (Lu et al.,
2016). Similarly, Gella et al. (2016) show that em-
beddings generated from textual data associated
with images (object labels, image descriptions) is
useful for visual verb sense disambiguation, and is
complementary to visual information.
</p>
<p>5 Discussion
</p>
<p>Linguistic resources such as WordNet, OntoNotes,
and FrameNet play a key role in textual sense
disambiguation and semantic role labeling. The
visual action disambiguation and visual semantic
role labeling tasks are extensions of their textual
counterparts, where context is provided as an im-
age instead of as text. Linguistic resources there-
fore have to play a key role if we are to make
rapid progress in these language and vision tasks.
However, as we have shown in this paper, only a
few of the existing datasets for action recognition
and related tasks are based on linguistic resources
(Chao et al., 2015; Gella et al., 2016; Yatskar et al.,
2016). This is despite the fact that the WordNet
noun hierarchy (for example) has played an impor-
tant role in recent progress in object recognition,
by virtue of underlying the ImageNet database, the
de-facto standard for this task (Russakovsky et al.,
2015). The success of ImageNet for objects has
</p>
<p>in turn helped NLP tasks such as bilingual lexi-
con induction (Vulić et al., 2016). In our view,
language and vision datasets that are based on the
WordNet, OntoNotes, or FrameNet verb sense in-
ventories can play a similar role for tasks such as
action recognition or visual semantic role labeling,
and ultimately be useful also for more distantly re-
lated tasks such as language grounding.
</p>
<p>Another argument for linking language and vi-
sion datasets with linguistic resources is that this
enables us to deploy the datasets in a multilingual
setting. For example a polysemous verb such as
ride in English has multiple translations in Ger-
man and Spanish, depending on the context and
the objects involved. Riding a horse is trans-
lated as reiten in German and cabalgar in Span-
ish, whereas riding a bicycle is translated as fahren
in German and pedalear in Spanish. In contrast,
some polysemous verb (e.g., English play) are
always translated as the same verb, independent
of sense (spielen in German). Such sense map-
pings are discoverable from multilingual lexical
resources (e.g., BabelNet, Navigli and Ponzetto
2010), which makes it possible to construct lan-
guage and vision models that are applicable to
multiple languages. This opportunity is lost if lan-
guage and vision dataset are constructed in isola-
tion, instead of using existing linguistic resources.
</p>
<p>6 Conclusions
</p>
<p>In this paper, we have shown the evolution of
action recognition datasets and tasks from sim-
ple ad-hoc labels to the fine-grained annotation
of verb semantics. It is encouraging to see the
recent increase in datasets that deal with sense
ambiguity and annotate semantic roles, while us-
ing standard linguistic resources. One major re-
maining issue with existing datasets is their lim-
ited coverage, and the skewed distribution of verbs
or verb senses. Another challenge is the incon-
sistency in annotation schemes and task defini-
tions across datasets. For example Chao et al.
(2015) used WordNet senses as interaction labels,
while Gella et al. (2016) used the more coarse-
grained OntoNotes senses. Yatskar et al. (2016)
used FrameNet frames for semantic role annota-
tion, while Gupta and Malik (2015) used manually
curated roles. If we are to develop robust, domain
independent models, then we need to standardize
annotation schemes and use the same linguistic re-
sources across datasets.
</p>
<p>68</p>
<p />
</div>
<div class="page"><p />
<p>References
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler,
</p>
<p>and Bernt Schiele. 2014. 2d human pose estimation:
New benchmark and state of the art analysis. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition. pages 3686–3693.
</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: visual question an-
swering. In 2015 IEEE International Conference on
Computer Vision, ICCV 2015, Santiago, Chile, De-
cember 7-13, 2015. pages 2425–2433.
</p>
<p>Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 1. Association for Computational Linguis-
tics, pages 86–90.
</p>
<p>Raffaella Bernardi, Ruket Cakici, Desmond Elliott,
Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis,
Frank Keller, Adrian Muscat, and Barbara Plank.
2016. Automatic description generation from im-
ages: A survey of models, datasets, and evaluation
measures. Journal of Artifical Intelligence Research
55:409–442.
</p>
<p>Fabian Caba Heilbron, Victor Escorcia, Bernard
Ghanem, and Juan Carlos Niebles. 2015. Activ-
itynet: A large-scale video benchmark for human
activity understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition. pages 961–970.
</p>
<p>Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang,
and Jia Deng. 2015. HICO: A benchmark for recog-
nizing human-object interactions in images. In 2015
IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015.
pages 1017–1025.
</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C. Lawrence Zitnick. 2015. Microsoft COCO cap-
tions: Data collection and evaluation server. CoRR
abs/1504.00325.
</p>
<p>Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In Com-
puter Vision and Pattern Recognition, 2005. CVPR
2005. IEEE Computer Society Conference on. IEEE,
volume 1, pages 886–893.
</p>
<p>Vincent Delaitre, Ivan Laptev, and Josef Sivic. 2010.
Recognizing human actions in still images: a study
of bag-of-features and part-based representations.
In BMVC 2010-21st British Machine Vision Confer-
ence.
</p>
<p>Mark Everingham, S. M. Ali Eslami, Luc Van Gool,
Christopher K. I. Williams, John M. Winn, and An-
drew Zisserman. 2015. The Pascal visual object
</p>
<p>classes challenge: A retrospective. International
Journal of Computer Vision 111(1):98–136.
</p>
<p>Mark Everingham, Luc J. Van Gool, Christopher K. I.
Williams, John M. Winn, and Andrew Zisserman.
2010. The Pascal visual object classes (VOC) chal-
lenge. International Journal of Computer Vision
88(2):303–338.
</p>
<p>Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao (Ken-
neth) Huang, Lucy Vanderwende, Jacob Devlin,
Michel Galley, and Margaret Mitchell. 2015. A sur-
vey of current datasets for vision and language re-
search. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015. pages 207–213.
</p>
<p>David A. Forsyth, Okan Arikan, Leslie Ikemoto,
James F. O’Brien, and Deva Ramanan. 2005. Com-
putational studies of human motion: Part 1, tracking
and motion synthesis. Foundations and Trends in
Computer Graphics and Vision 1(2/3).
</p>
<p>Spandana Gella, Mirella Lapata, and Frank Keller.
2016. Unsupervised visual sense disambiguation
for verbs using multimodal embeddings. In Pro-
ceedings of the 2016 Conference on North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016. pages 182–192.
</p>
<p>Abhinav Gupta, Aniruddha Kembhavi, and Larry S.
Davis. 2009. Observing human-object interactions:
Using spatial and functional compatibility for recog-
nition. IEEE Transactions on Pattern Analysis and
Machine Intelligence 31(10):1775–1789.
</p>
<p>Saurabh Gupta and Jitendra Malik. 2015. Visual se-
mantic role labeling. CoRR abs/1505.04474.
</p>
<p>Eduard H. Hovy, Mitchell P. Marcus, Martha Palmer,
Lance A. Ramshaw, and Ralph M. Weischedel.
2006. Ontonotes: The 90% solution. In Hu-
man Language Technology Conference of the North
American Chapter of the Association of Compu-
tational Linguistics, Proceedings, June 4-9, 2006,
New York, New York, USA. pages 57–60.
</p>
<p>Nazli Ikizler, Ramazan Gokberk Cinbis, Selen Pehli-
van, and Pinar Duygulu. 2008. Recognizing actions
from still images. In 19th International Conference
on Pattern Recognition (ICPR 2008), December 8-
11, 2008, Tampa, Florida, USA. pages 1–4.
</p>
<p>Nazli Ikizler-Cinbis and Stan Sclaroff. 2010. Object,
scene and actions: Combining multiple features for
human action recognition. In European conference
on computer vision. Springer, pages 494–507.
</p>
<p>Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2016. Visual genome: Connecting language and vi-
sion using crowdsourced dense image annotations.
arXiv preprint arXiv:1602.07332 .
</p>
<p>69</p>
<p />
</div>
<div class="page"><p />
<p>Dieu Thu Le, Raffaella Bernardi, and Jasper Uijlings.
2013. Exploiting language models to recognize un-
seen actions. In Proceedings of the 3rd ACM con-
ference on International conference on multimedia
retrieval. ACM, pages 231–238.
</p>
<p>Dieu-Thu Le, Jasper Uijlings, and Raffaella Bernardi.
2014. Proceedings of the Third Workshop on Vi-
sion and Language, Dublin City University and the
Association for Computational Linguistics, chapter
TUHOI: Trento Universal Human Object Interaction
Dataset, pages 17–24.
</p>
<p>Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. University of
Chicago Press.
</p>
<p>Li-Jia Li and Li Fei-Fei. 2007. What, where and who?
classifying events by scene and object recognition.
In Computer Vision, 2007. ICCV 2007. IEEE 11th
International Conference on. IEEE, pages 1–8.
</p>
<p>David G Lowe. 1999. Object recognition from local
scale-invariant features. In Computer vision, 1999.
The proceedings of the seventh IEEE international
conference on. Ieee, volume 2, pages 1150–1157.
</p>
<p>Cewu Lu, Ranjay Krishna, Michael Bernstein, and
Li Fei-Fei. 2016. Visual relationship detection with
language priors. In European Conference on Com-
puter Vision. Springer, pages 852–869.
</p>
<p>Arun Mallya and Svetlana Lazebnik. 2016. Learn-
ing models for actions and person-object interac-
tions with transfer to question answering. In Eu-
ropean Conference on Computer Vision. Springer,
pages 414–428.
</p>
<p>George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41.
</p>
<p>Hans-Hellmut Nagel. 1994. A vision of ”vision and
language” comprises action: An example from road
traffic. Artif. Intell. Rev. 8(2-3):189–214.
</p>
<p>Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In ACL 2010, Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, July 11-16, 2010, Uppsala, Swe-
den. pages 216–225.
</p>
<p>Vignesh Ramanathan, Congcong Li, Jia Deng, Wei
Han, Zhen Li, Kunlong Gu, Yang Song, Samy
Bengio, Chuck Rossenberg, and Li Fei-Fei. 2015.
Learning semantic relationships for better action re-
trieval in images. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion. pages 1100–1109.
</p>
<p>Matteo Ruggero Ronchi and Pietro Perona. 2015. De-
scribing common human visual actions in images.
In Proceedings of the British Machine Vision Con-
ference (BMVC 2015). BMVA Press, pages 52.1–
52.12.
</p>
<p>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael S. Bernstein,
Alexander C. Berg, and Fei-Fei Li. 2015. Imagenet
large scale visual recognition challenge. Interna-
tional Journal of Computer Vision 115(3):211–252.
</p>
<p>Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
</p>
<p>Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. 2016.
Hollywood in homes: Crowdsourcing data collec-
tion for activity understanding. In European Confer-
ence on Computer Vision. Springer, pages 510–526.
</p>
<p>Graham W Taylor, Rob Fergus, Yann LeCun, and
Christoph Bregler. 2010. Convolutional learning of
spatio-temporal features. In European conference
on computer vision. Springer, pages 140–153.
</p>
<p>Jesse Thomason, Subhashini Venugopalan, Sergio
Guadarrama, Kate Saenko, and Raymond J.
Mooney. 2014. Integrating language and vision to
generate natural language descriptions of videos in
the wild. In COLING 2014, 25th International Con-
ference on Computational Linguistics,Proceedings
of the Conference: Technical Papers, August 23-29,
2014, Dublin, Ireland. pages 1218–1227.
</p>
<p>Ivan Vulić, Douwe Kiela, Stephen Clark, and Marie-
Francine Moens. 2016. Multi-modal representations
for improved bilingual lexicon learning. In Pro-
ceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics. ACL, pages
188–194.
</p>
<p>Shaohua Yang, Qiaozi Gao, Changsong Liu, Caiming
Xiong, Song-Chun Zhu, and Joyce Y. Chai. 2016.
Grounded semantic role labeling. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016. pages 149–159.
</p>
<p>Yezhou Yang, Ching Lik Teo, Hal Daumé III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, pages 444–454.
</p>
<p>Bangpeng Yao and Li Fei-Fei. 2010. Grouplet: A
structured image representation for recognizing hu-
man and object interactions. In Computer Vision
and Pattern Recognition (CVPR), 2010 IEEE Con-
ference on. IEEE, pages 9–16.
</p>
<p>Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai
Lin, Leonidas Guibas, and Li Fei-Fei. 2011. Human
action recognition by learning bases of action at-
tributes and parts. In Computer Vision (ICCV), 2011
IEEE International Conference on. IEEE, pages
1331–1338.
</p>
<p>70</p>
<p />
</div>
<div class="page"><p />
<p>Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi.
2016. Situation recognition: Visual semantic role
labeling for image understanding. In 2016 IEEE
Conference on Computer Vision and Pattern Recog-
nition, CVPR 2016, Las Vegas, NV, USA, June 26-
July 1, 2016.
</p>
<p>Licheng Yu, Eunbyung Park, Alexander C. Berg, and
Tamara L. Berg. 2015. Visual madlibs: Fill in the
blank description generation and question answer-
ing. In 2015 IEEE International Conference on
Computer Vision, ICCV 2015, Santiago, Chile, De-
cember 7-13, 2015. pages 2461–2469.
</p>
<p>Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude
Oliva, and Antonio Torralba. 2016. Learning deep
features for discriminative localization. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition. pages 2921–2929.
</p>
<p>71</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72–78
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2012
</p>
<p>Learning to Parse and Translate Improves Neural Machine Translation
</p>
<p>Akiko Eriguchi†, Yoshimasa Tsuruoka†, and Kyunghyun Cho‡
†The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, tsuruoka}@logos.t.u-tokyo.ac.jp
</p>
<p>‡New York University, New York, NY 10012, USA
kyunghyun.cho@nyu.edu
</p>
<p>Abstract
</p>
<p>There has been relatively little attention
to incorporating linguistic prior to neu-
ral machine translation. Much of the
previous work was further constrained to
considering linguistic prior on the source
side. In this paper, we propose a hybrid
model, called NMT+RNNG, that learns
to parse and translate by combining the
recurrent neural network grammar into
the attention-based neural machine trans-
lation. Our approach encourages the neu-
ral machine translation model to incorpo-
rate linguistic prior during training, and
lets it translate on its own afterward. Ex-
tensive experiments with four language
pairs show the effectiveness of the pro-
posed NMT+RNNG.
</p>
<p>1 Introduction
</p>
<p>Neural Machine Translation (NMT) has enjoyed
impressive success without relying on much, if
any, prior linguistic knowledge. Some of the most
recent studies have for instance demonstrated that
NMT systems work comparably to other systems
even when the source and target sentences are
given simply as flat sequences of characters (Lee
et al., 2016; Chung et al., 2016) or statistically, not
linguistically, motivated subword units (Sennrich
et al., 2016; Wu et al., 2016). Shi et al. (2016)
recently made an observation that the encoder of
NMT captures syntactic properties of a source sen-
tence automatically, indirectly suggesting that ex-
plicit linguistic prior may not be necessary.
</p>
<p>On the other hand, there have only been a
couple of recent studies showing the potential
benefit of explicitly encoding the linguistic prior
into NMT. Sennrich and Haddow (2016) for in-
stance proposed to augment each source word with
its corresponding part-of-speech tag, lemmatized
</p>
<p>form and dependency label. Eriguchi et al. (2016)
instead replaced the sequential encoder with a
tree-based encoder which computes the represen-
tation of the source sentence following its parse
tree. Stahlberg et al. (2016) let the lattice from a
hierarchical phrase-based system guide the decod-
ing process of neural machine translation, which
results in two separate models rather than a single
end-to-end one. Despite the promising improve-
ments, these explicit approaches are limited in that
the trained translation model strictly requires the
availability of external tools during inference time.
More recently, researchers have proposed meth-
ods to incorporate target-side syntax into NMT
models. Alvarez-Melis and Jaakkola (2017) have
proposed a doubly-recurrent neural network that
can generate a tree-structured sentence, but its ef-
fectiveness in a full scale NMT task is yet to be
shown. Aharoni and Goldberg (2017) introduced
a method to serialize a parsed tree and to train the
serialized parsed sentences.
</p>
<p>We propose to implicitly incorporate linguis-
tic prior based on the idea of multi-task learn-
ing (Caruana, 1998; Collobert et al., 2011). More
specifically, we design a hybrid decoder for NMT,
called NMT+RNNG1, that combines a usual con-
ditional language model and a recently pro-
posed recurrent neural network grammars (RN-
NGs, Dyer et al., 2016). This is done by plugging
in the conventional language model decoder in the
place of the buffer in RNNG, while sharing a sub-
set of parameters, such as word vectors, between
the language model and RNNG. We train this hy-
brid model to maximize both the log-probability of
a target sentence and the log-probability of a parse
action sequence. We use an external parser (An-
dor et al., 2016) to generate target parse actions,
but unlike the previous explicit approaches, we do
not need it during test time.
</p>
<p>1Our code is available at https://github.com/
tempra28/nmtrnng.
</p>
<p>72</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2012">https://doi.org/10.18653/v1/P17-2012</a></div>
</div>
<div class="page"><p />
<p>We evaluate the proposed NMT+RNNG on four
language pairs ({JP, Cs, De, Ru}-En). We observe
significant improvements in terms of BLEU scores
on three out of four language pairs and RIBES
scores on all the language pairs.
</p>
<p>2 Neural Machine Translation
</p>
<p>Neural machine translation is a recently proposed
framework for building a machine translation sys-
tem based purely on neural networks. It is of-
ten built as an attention-based encoder-decoder
network (Cho et al., 2015) with two recurrent
networks—encoder and decoder—and an atten-
tion model. The encoder, which is often imple-
mented as a bidirectional recurrent network with
long short-term memory units (LSTM, Hochre-
iter and Schmidhuber, 1997) or gated recurrent
units (GRU, Cho et al., 2014), first reads a source
sentence represented as a sequence of words x =
(x1, x2, . . . , xN ). The encoder returns a sequence
of hidden states h = (h1, h2, . . . , hN ). Each hid-
den state hi is a concatenation of those from the
forward and backward recurrent network: hi =[−→
h i;
←−
h i
</p>
<p>]
, where
</p>
<p>−→
h i =
</p>
<p>−→
f enc(
</p>
<p>−→
h i−1, Vx(xi)),
</p>
<p>←−
h i =
</p>
<p>←−
f enc(
</p>
<p>←−
h i+1, Vx(xi)).
</p>
<p>Vx(xi) refers to the word vector of the i-th source
word.
</p>
<p>The decoder is implemented as a conditional re-
current language model which models the target
sentence, or translation, as
</p>
<p>log p(y|x) =
∑
</p>
<p>j
</p>
<p>log p(yj |y&lt;j ,x),
</p>
<p>where y = (y1, . . . , yM ). Each of the conditional
probabilities in the r.h.s is computed by
</p>
<p>p(yj = y|y&lt;j ,x) = softmax(W&gt;y s̃j), (1)
s̃j = tanh(Wc[sj ; cj ]), (2)
</p>
<p>sj = fdec(sj−1, [Vy(yj−1); s̃j−1]), (3)
</p>
<p>where fdec is a recurrent activation function, such
as LSTM or GRU, and Wy is the output word vec-
tor of the word y.
cj is a time-dependent context vector that is
</p>
<p>computed by the attention model using the se-
quence h of hidden states from the encoder. The
attention model first compares the current hidden
</p>
<p>state sj against each of the hidden states and as-
signs a scalar score: βi,j = exp(h&gt;i Wdsj) (Lu-
ong et al., 2015). These scores are then normal-
ized across the hidden states to sum to 1, that is
αi,j =
</p>
<p>βi,j∑
i βi,j
</p>
<p>. The time-dependent context vector
is then a weighted-sum of the hidden states with
these attention weights: cj =
</p>
<p>∑
i αi,jhi.
</p>
<p>3 Recurrent Neural Network Grammars
</p>
<p>A recurrent neural network grammar (RNNG,
Dyer et al., 2016) is a probabilistic syntax-based
language model. Unlike a usual recurrent lan-
guage model (see, e.g., Mikolov et al., 2010), an
RNNG simultaneously models both tokens and
their tree-based composition. This is done by
having a (output) buffer, stack and action his-
tory, each of which is implemented as a stack
LSTM (sLSTM, Dyer et al., 2015). At each time
step, the action sLSTM predicts the next action
based on the (current) hidden states of the buffer,
stack and action sLSTM. That is,
</p>
<p>p(at = a|a&lt;t) ∝ eW
&gt;
a faction(h
</p>
<p>buffer
t ,h
</p>
<p>stack
t ,h
</p>
<p>action
t ), (4)
</p>
<p>where Wa is the vector of the action a. If the se-
lected action is shift, the word at the beginning of
the buffer is moved to the stack. When the re-
duce action is selected, the top-two words in the
stack are reduced to build a partial tree. Addi-
tionally, the action may be one of many possible
non-terminal symbols, in which case the predicted
non-terminal symbol is pushed to the stack.
</p>
<p>The hidden states of the buffer, stack and action
sLSTM are correspondingly updated by
</p>
<p>hbuffert = StackLSTM(h
buffer
top , Vy(yt−1)), (5)
</p>
<p>hstackt = StackLSTM(h
stack
top , rt),
</p>
<p>hactiont = StackLSTM(h
action
top , Va(at−1)),
</p>
<p>where Vy and Va are functions returning the target
word and action vectors. The input vector rt of the
stack sLSTM is computed recursively by
</p>
<p>rt = tanh(Wr[r
d; rp;Va(at)]),
</p>
<p>where rd and rp are the corresponding vectors
of the parent and dependent phrases, respec-
tively (Dyer et al., 2015). This process is iter-
ated until a complete parse tree is built. Note that
the original paper of RNNG (Dyer et al., 2016)
uses constituency trees, but we employ depen-
dency trees in this paper. Both types of trees are
</p>
<p>73</p>
<p />
</div>
<div class="page"><p />
<p>represented as a sequence of the three types of ac-
tions in a transition-based parsing model.
</p>
<p>When the complete sentence is provided, the
buffer simply summarizes the shifted words.
When the RNNG is used as a generator, the buffer
further generates the next word when the selected
action is shift. The latter can be done by replacing
the buffer with a recurrent language model, which
is the idea on which our proposal is based.
</p>
<p>4 Learning to Parse and Translate
</p>
<p>4.1 NMT+RNNG
</p>
<p>Our main proposal in this paper is to hybridize the
decoder of the neural machine translation and the
RNNG. We continue from the earlier observation
that we can replace the buffer of RNNG to a recur-
rent language model that simultaneously summa-
rizes the shifted words as well as generates future
words. We replace the RNNG’s buffer with the
neural translation model’s decoder in two steps.
</p>
<p>Construction First, we replace the hidden state
of the buffer hbuffer (in Eq. (5)) with the hidden
state of the decoder of the attention-based neural
machine translation from Eq. (3). As is clear from
those two equations, both the buffer sLSTM and
the translation decoder take as input the previous
hidden state (hbuffertop and sj−1, respectively) and
the previously decoded word (or the previously
shifted word in the case of the RNNG’s buffer),
and returns its summary state. The only difference
is that the translation decoder additionally consid-
ers the state s̃j−1. Once the buffer of the RNNG
is replaced with the NMT decoder in our proposed
model, the NMT decoder is also under control of
the actions provided by the RNNG.2 Second, we
let the next word prediction of the translation de-
coder as a generator of RNNG. In other words,
the generator of RNNG will output a word, when
asked by the shift action, according to the condi-
tional distribution defined by the translation de-
coder in Eq. (1). Once the buffer sLSTM is re-
placed with the neural translation decoder, the ac-
tion sLSTM naturally takes as input the translation
decoder’s hidden state when computing the action
conditional distribution in Eq. (4). We call this hy-
brid model NMT+RNNG.
</p>
<p>2The j-th hidden state in Eq. (3) is calculated only when
the action (shift) is predicted by the RNNG. This is why our
proposed model can handle the sequences of words and ac-
tions which have different lengths.
</p>
<p>Learning and Inference After this integration,
our hybrid NMT+RNNG models the conditional
distribution over all possible pairs of transla-
tion and its parse given a source sentence, i.e.,
p(y,a|x). Assuming the availability of parse
annotation in the target-side of a parallel cor-
pus, we train the whole model jointly to maxi-
mize E(x,y,a)∼data [log p(y,a|x)]. In doing so, we
notice that there are two separate paths through
which the neural translation decoder receives er-
ror signal. First, the decoder is updated in or-
der to maximize the conditional probability of the
correct next word, which has already existed in
the original neural machine translation. Second,
the decoder is updated also to maximize the con-
ditional probability of the correct parsing action,
which is a novel learning signal introduced by the
proposed hybridization. Furthermore, the second
learning signal affects the encoder as well, encour-
aging the whole neural translation model to be
aware of the syntactic structure of the target lan-
guage. Later in the experiments, we show that this
additional learning signal is useful for translation,
even though we discard the RNNG (the stack and
action sLSTMs) in the inference time.
</p>
<p>4.2 Knowledge Distillation for Parsing
</p>
<p>A major challenge in training the proposed hybrid
model is that there is not a parallel corpus aug-
mented with gold-standard target-side parse, and
vice versa. In other words, we must either parse
the target-side sentences of an existing parallel
corpus or translate sentences with existing gold-
standard parses. As the target task of the proposed
model is translation, we start with a parallel cor-
pus and annotate the target-side sentences. It is
however costly to manually annotate any corpus
of reasonable size (Table 6 in Alonso et al., 2016).
</p>
<p>We instead resort to noisy, but automated an-
notation using an existing parser. This approach
of automated annotation can be considered along
the line of recently proposed techniques of knowl-
edge distillation (Hinton et al., 2015) and distant
supervision (Mintz et al., 2009). In knowledge dis-
tillation, a teacher network is trained purely on a
training set with ground-truth annotations, and the
annotations predicted by this teacher are used to
train a student network, which is similar to our ap-
proach where the external parser could be thought
of as a teacher and the proposed hybrid network’s
RNNG as a student. On the other hand, what we
</p>
<p>74</p>
<p />
</div>
<div class="page"><p />
<p>Train. Dev. Test Voc. (src, tgt, act)
Cs-En 134,453 2,656 2,999 (33,867, 27,347, 82)
De-En 166,313 2,169 2,999 (33,820, 30,684, 80)
Ru-En 131,492 2,818 2,998 (32,442, 27,979, 82)
Jp-En 100,000 1,790 1,812 (23,509, 28,591, 80)
</p>
<p>Table 1: Statistics of parallel corpora.
</p>
<p>propose here is a special case of distant supervi-
sion in that the external parser provides noisy an-
notations to otherwise an unlabeled training set.
</p>
<p>Specifically, we use SyntaxNet, released by An-
dor et al. (2016), on a target sentence.3 We convert
a parse tree into a sequence of one of three tran-
sition actions (SHIFT, REDUCE-L, REDUCE-R).
We label each REDUCE action with a correspond-
ing dependency label and treat it as a more fine-
grained action.
</p>
<p>5 Experiments
</p>
<p>5.1 Language Pairs and Corpora
</p>
<p>We compare the proposed NMT+RNNG against
the baseline model on four different language
pairs–Jp-En, Cs-En, De-En and Ru-En. The ba-
sic statistics of the training data are presented in
Table 1. We mapped all the low-frequency words
to the unique symbol “UNK” and inserted a spe-
cial symbol “EOS” at the end of both source and
target sentences.
</p>
<p>Ja We use the ASPEC corpus (“train1.txt”) from
the WAT’16 Jp-En translation task. We tokenize
each Japanese sentence with KyTea (Neubig et al.,
2011) and preprocess according to the recommen-
dations from WAT’16 (WAT, 2016). We use the
first 100K sentence pairs of length shorter than 50
for training. The vocabulary is constructed with
all the unique tokens that appear at least twice in
the training corpus. We use “dev.txt” and “test.txt”
provided by WAT’16 respectively as development
and test sets.
</p>
<p>Cs, De and Ru We use News Commentary v8.
We removed noisy metacharacters and used the to-
kenizer from Moses (Koehn et al., 2007) to build a
vocabulary of each language using unique tokens
that appear at least 6, 6 and 5 times respectively for
Cs, Ru and De. The target-side (English) vocab-
ulary was constructed with all the unique tokens
</p>
<p>3When the target sentence is parsed as data preprocessing,
we use all the vocabularies in a corpus and do not cut off
any words. We use the plain SyntaxNet and do not train it
furthermore.
</p>
<p>appearing more than three times in each corpus.
We also excluded the sentence pairs which include
empty lines in either a source sentence or a target
sentence. We only use sentence pairs of length 50
or less for training. We use “newstest2015” and
“newstest2016” as development and test sets re-
spectively.
</p>
<p>5.2 Models, Learning and Inference
</p>
<p>In all our experiments, each recurrent network has
a single layer of LSTM units of 256 dimensions,
and the word vectors and the action vectors are
of 256 and 128 dimensions, respectively. To re-
duce computational overhead, we use BlackOut (Ji
et al., 2015) with 2000 negative samples and α =
0.4. When employing BlackOut, we shared the
negative samples of each target word in a sen-
tence in training time (Hashimoto and Tsuruoka,
2017), which is similar to the previous work (Zoph
et al., 2016). For the proposed NMT+RNNG, we
share the target word vectors between the decoder
(buffer) and the stack sLSTM.
</p>
<p>Each weight is initialized from the uniform dis-
tribution [−0.1, 0.1]. The bias vectors and the
weights of the softmax and BlackOut are initial-
ized to be zero. The forget gate biases of LSTMs
and Stack-LSTMs are initialized to 1 as recom-
mended in Józefowicz et al. (2015). We use
stochastic gradient descent with minibatches of
128 examples. The learning rate starts from 1.0,
and is halved each time the perplexity on the de-
velopment set increases. We clip the norm of the
gradient (Pascanu et al., 2012) with the thresh-
old set to 3.0 (2.0 for the baseline models on Ru-
En and Cs-En to avoid NaN and Inf). When the
perplexity of development data increased in train-
ing time, we halved the learning rate of stochastic
gradient descent and reloaded the previous model.
The RNNG’s stack computes the vector of a de-
pendency parse tree which consists of the gener-
ated target words by the buffer. Since the complete
parse tree has a “ROOT” node, the special token of
the end of a sentence (“EOS”) is considered as the
ROOT. We use beam search in the inference time,
with the beam width selected based on the devel-
opment set performance.
</p>
<p>It took about 15 minutes per epoch and about 20
minutes respectively for the baseline and the pro-
posed model to train a full JP-EN parallel corpus
in our implementation.4
</p>
<p>4We run all the experiments on multi-core CPUs (10
</p>
<p>75</p>
<p />
</div>
<div class="page"><p />
<p>De-En Ru-En Cs-En Jp-En
BLEU
</p>
<p>NMT 16.61 12.03 11.22 17.88
NMT+RNNG 16.41 12.46† 12.06† 18.84†
</p>
<p>RIBES
NMT 73.75 69.56 69.59 71.27
NMT+RNNG 75.03† 71.04† 70.39† 72.25†
</p>
<p>Table 2: BLEU and RIBES scores by the baseline
and proposed models on the test set. We use the
bootstrap resampling method from Koehn (2004)
to compute the statistical significance. We use † to
mark those significant cases with p &lt; 0.005.
</p>
<p>Jp-En (Dev) BLEU
NMT+RNNG 18.60
w/o Buffer 18.02
w/o Action 17.94
w/o Stack 17.58
NMT 17.75
</p>
<p>Table 3: Effect of each component in RNNG.
</p>
<p>5.3 Results and Analysis
In Table 2, we report the translation qualities of
the tested models on all the four language pairs.
We report both BLEU (Papineni et al., 2002) and
RIBES (Isozaki et al., 2010). Except for De-
En, measured in BLEU, we observe the statis-
tically significant improvement by the proposed
NMT+RNNG over the baseline model. It is worth-
while to note that these significant improvements
have been achieved without any additional param-
eters nor computational overhead in the inference
time.
</p>
<p>Ablation Since each component in RNNG may
be omitted, we ablate each component in the pro-
posed NMT+RNNG to verify their necessity.5 As
shown in Table 3, we see that the best performance
could only be achieved when all the three compo-
nents were present. Removing the stack had the
most adverse effect, which was found to be the
case for parsing as well by Kuncoro et al. (2017).
</p>
<p>Generated Sentences with Parsed Actions
The decoder part of our proposed model consists
of two components: the NMT decoder to gener-
</p>
<p>threads on Intel(R) Xeon(R) CPU E5-2680 v2 @2.80GHz)
5 Since the buffer is the decoder, it is not possible to com-
</p>
<p>pletely remove it. Instead we simply remove the dependency
of the action distribution on it.
</p>
<p>Figure 1: An example of translation and its depen-
dency relations obtained by our proposed model.
</p>
<p>ate a translated sentence and the RNNG decoder
to predict its parsing actions. The proposed model
can therefore output a dependency structure along
with a translated sentence. Figure 1 shows an
example of JP-EN translation in the development
dataset and its dependency parse tree obtained by
the proposed model. The special symbol (“EOS”)
is treated as the root node (“ROOT”) of the parsed
tree. The translated sentence was generated by
using beam search, which is the same setting of
NMT+RNNG shown in Table 3. The parsing ac-
tions were obtained by greedy search. The re-
sulting dependency structure is mostly correct but
contains a few errors; for example, dependency re-
lation between “The” and “ transition” should not
be “pobj”.
</p>
<p>6 Conclusion
</p>
<p>We propose a hybrid model, to which we refer
as NMT+RNNG, that combines the decoder of an
attention-based neural translation model with the
RNNG. This model learns to parse and translate si-
multaneously, and training it encourages both the
encoder and decoder to better incorporate linguis-
tic priors. Our experiments confirmed its effec-
tiveness on four language pairs ({JP, Cs, De, Ru}-
En). The RNNG can in principle be trained with-
out ground-truth parses, and this would eliminate
the need of external parsers completely. We leave
the investigation into this possibility for future re-
search.
</p>
<p>Acknowledgments
</p>
<p>We thank Yuchen Qiao and Kenjiro Taura for their
help to speed up the implementations of training
and also Kazuma Hashimoto for his valuable com-
ments and discussions. This work was supported
by JST CREST Grant Number JPMJCR1513 and
JSPS KAKENHI Grant Number 15J12597 and
</p>
<p>76</p>
<p />
</div>
<div class="page"><p />
<p>16H01715. KC thanks support by eBay, Face-
book, Google and NVIDIA.
</p>
<p>References
Roee Aharoni and Yoav Goldberg. 2017. Towards
</p>
<p>string-to-tree neural machine translation. In Pro-
ceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics. to appear.
</p>
<p>Héctor Martı́nez Alonso, Djamé Seddah, and Benoı̂t
Sagot. 2016. From noisy questions to minecraft
texts: Annotation challenges in extreme syntax sce-
nario. In Proceedings of the 2nd Workshop on Noisy
User-generated Text (WNUT). pages 13–23.
</p>
<p>David Alvarez-Melis and Tommi S. Jaakkola. 2017.
Tree-structured decoding with doubly-recurrent
neural networks. In Proceedings of International
Conference on Learning Representations 2017.
</p>
<p>Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics. pages 2442–
2452.
</p>
<p>Rich Caruana. 1998. Multitask learning. In Learning
to learn, Springer, pages 95–133.
</p>
<p>Kyunghyun Cho, Aaron Courville, and Yoshua Ben-
gio. 2015. Describing multimedia content using
attention-based encoder-decoder networks. IEEE
Transactions on Multimedia 17(11):1875–1886.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing. pages 1724–1734.
</p>
<p>Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
plicit segmentation for neural machine translation.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, pages 1693–
1703.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12:2493–2537.
</p>
<p>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and A. Noah Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
</p>
<p>Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing. pages 334–343.
</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and A. Noah Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. pages 199–209.
</p>
<p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics. pages 823–833.
</p>
<p>Kazuma Hashimoto and Yoshimasa Tsuruoka.
2017. Neural Machine Translation with Source-
Side Latent Graph Parsing. arXiv preprint
arXiv:1702.02265 .
</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780.
</p>
<p>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic eval-
uation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing. pages 944–952.
</p>
<p>Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish,
Michael J. Anderson, and Pradeep Dubey. 2015.
Blackout: Speeding up recurrent neural network lan-
guage models with very large vocabularies. Pro-
ceedings of International Conference on Learning
Representations 2015 .
</p>
<p>Rafal Józefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of the
32nd International Conference on Machine Learn-
ing. pages 2342–2350.
</p>
<p>Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing. pages 388–395.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions. pages 177–180.
</p>
<p>77</p>
<p />
</div>
<div class="page"><p />
<p>Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What do recurrent neural network
grammars learn about syntax? In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers. pages 1249–1258.
</p>
<p>Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2016. Fully character-level neural machine trans-
lation without explicit segmentation. arXiv preprint
arXiv:1610.03017 .
</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing. pages 1412–1421.
</p>
<p>Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2010). International Speech Communica-
tion Association, pages 1045–1048.
</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. pages
1003–1011.
</p>
<p>Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. pages 529–533.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics. pages 311–318.
</p>
<p>Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2012. Understanding the exploding gra-
dient problem. arXiv preprint arXiv:1211.5063
abs/1211.5063.
</p>
<p>Rico Sennrich and Barry Haddow. 2016. Linguistic
input features improve neural machine translation.
In Proceedings of the First Conference on Machine
Translation. pages 83–91.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics. pages 1715–1725.
</p>
<p>Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. pages 1526–
1534.
</p>
<p>Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill
Byrne. 2016. Syntactically guided neural machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics. pages 299–305.
</p>
<p>WAT. 2016. http://lotus.kuee.
kyoto-u.ac.jp/WAT/baseline/
dataPreparationJE.html.
</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the
gap between human and machine translation. arXiv
preprint arXiv:1609.08144 .
</p>
<p>Barret Zoph, Ashish Vaswani, Jonathan May, and
Kevin Knight. 2016. Simple, Fast Noise-Contrastive
Estimation for Large RNN Vocabularies. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
1217–1222.
</p>
<p>78</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 79–84
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2013
</p>
<p>On the Distribution of Lexical Features at Multiple Levels of Analysis
</p>
<p>Fatemeh Almodaresi† Lyle Ungar§ Vivek Kulkarni† Mohsen Zakeri†
Salvatore Giorgi§ H. Andrew Schwartz†
</p>
<p>†Stony Brook University §University of Pennsylvania
{falmodaresit,has}@cs.stonybrook.edu
</p>
<p>Abstract
</p>
<p>Natural language processing has increas-
ingly moved from modeling documents
and words toward studying the people be-
hind the language. This move to working
with data at the user or community level
has presented the field with different char-
acteristics of linguistic data. In this paper,
we empirically characterize various lexi-
cal distributions at different levels of anal-
ysis, showing that, while most features are
decidedly sparse and non-normal at the
message-level (as with traditional NLP),
they follow the central limit theorem to
become much more Log-normal or even
Normal at the user- and county-levels. Fi-
nally, we demonstrate that modeling lexi-
cal features for the correct level of analysis
leads to marked improvements in common
social scientific prediction tasks.
</p>
<p>1 Introduction
</p>
<p>NLP for studying people has grown rapidly as
more than one-third of the human population use
social media actively.1 While traditional NLP
tasks (e.g. POS tagging, parsing, sentiment anal-
ysis) mostly work at the word, sentence, or doc-
ument level, the increased focus on social scien-
tific applications has shifted attention to new lev-
els of analysis (e.g. user-level and community-
level) (Koppel et al., 2009; Sarawgi et al., 2011;
Schwartz et al., 2013a; Coppersmith et al., 2014;
Flekova et al., 2016).
</p>
<p>Figure 1 shows the distribution of two uni-
grams, ‘the’ and ‘love’ at three levels of analy-
sis. While both words have zero counts in most
messages, ‘the’ starts to look Normal across
</p>
<p>1Social Insights; Global social media research summary
2017
</p>
<p>users, and both words are approximately Normal
at the county level. Methods performing optimally
at the document level may suffer at the user or
community level due to this shift in the distribu-
tion of lexical features.2
</p>
<p>In this paper, we ask a fundamental statistical
question: How does the shift in unit-of-analysis
from document-level to user-or-community level
shift lexical distributions in social media?3 The
central limit theorem suggests that count data is
better approximated by a Normal distribution as
one increases the number of events, or as one ag-
gregates more features (e.g. combining words us-
ing LDA topics or hand-built word sets). However,
we do not know how far towards a Normal these
new levels of analysis bring us.
</p>
<p>Related work. The question we ask harks
back to work from pioneers in corpus-based
computational linguistics, including Shannon
(1948) who suggested that probabilistic distribu-
tions of ngrams could be used to solve a range
of communications problems, and Mosteller and
Wallace (1963) who found that a negative bino-
mial distribution seemed to model unigram usage
by authors of the Federalist Papers. Numerous
works have since continued the tradition of ex-
amining the distribution of lexical features. For
example, McCallum et al. (1998) compares the
results of probabilistic models based on multi-
variate Bernoulli with those based on multinomial
distributions for document classification. Jansche
</p>
<p>2While the distribution of word frequencies (i.e. a Zipfian
distribution) is often discussed in NLP, it is important to note
that we are focused on the distribution of single features (e.g.
words) over documents, users, or communities.
</p>
<p>3While other sources of corpora can also be aggregated
to the user- or community-level (e.g. newswire, books), we
believe the question of distributions is particularly important
in social media because it often contains very short posts and
a growing body of work in NLP for social science focuses on
social media.
</p>
<p>79</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2013">https://doi.org/10.18653/v1/P17-2013</a></div>
</div>
<div class="page"><p />
<p>Figure 1: Histograms for unigrams “the” (a very frequent feature) and “love” (less frequent) at different levels of analysis:
message, user, and community (from left to right). The bars at zero are cut-off at the message and user levels to increase
readability of the remaining distribution.
</p>
<p>(2003) extended this line of work, observing lex-
ical count data often display an extra probability
mass concentrated at zero and suggesting Zero-
Inflated negative binomial distributions can cap-
ture this phenomenon better and are easier to im-
plement than alternatives such as overdispersed bi-
nomial models. While these works are numerous,
none, to the best of our knowledge, have focused
on distributions across social media or at multiple
levels of analysis.
</p>
<p>Contributions. Our study is perhaps unconven-
tional in modern computational linguistics due to
the elementary nature of our contributions, focus-
ing on understanding the empirical distributions
of lexical features in Twitter. First, we use zero-
inflated kernel density estimated plots to show
how distributions of different language features
(words, LDA topics, and hand-curated word sets)
vary with level of analysis (message, user, and
county). Second, we quantify which distributions
best describe the different feature types and anal-
ysis levels of social media. Finally, we show
the utility of such information, finding that us-
ing the appropriate model for each feature type
improves Naive Bayes classification results across
three common social scientific tasks: sarcasm de-
tection at the message-level, gender identification
at the user-level, and political ideology classifica-
tion at the community-level.
</p>
<p>2 Methods
</p>
<p>Examining data at three different levels of analy-
sis and across three different lexical feature types
(unigrams, data-driven topics, and manual lexica),
we seek to (1) visually characterize distributions,
(2) empirically test which distributions best fit the
data, and (3) evaluate classification models utiliz-
ing multiple distributions at each level. Unigrams
underlie all data where as each level of analysis
</p>
<p>and feature type represent a different degree of ag-
gregation and covariance structure.
</p>
<p>Data preparation. We start with a set of about
two million Twitter posts and supplemental infor-
mation about the users: their ID, county, and gen-
der. The data was based on that of Volkova et al.
(2013), who provide tweet ids and gender, and
mapped to counties using the method of Schwartz
et al. (2013a). We limit our data to users who
have used at least 1000 words and counties that
have at least 30 users and a total word count of
5000. Applying these constraints, the final set of
data consists of 1,639,750 tweets (representing the
message-level) from 5,226 users in 420 different
counties (representing the community-level).
</p>
<p>We consider three lexical features that are
commonly used in NLP for social science: 1-
grams (the top 10,000 most common unigrams
found with happierFunTokenizing social media
tokenizer), 2000 LDA topics downloaded from
Schwartz et al. (2013b)), and lexica (64 categories
from the linguistic inquiry and word count dictio-
nary (Pennebaker et al., 2007)). Note that the fea-
tures progress from most sparse (1grams) to least
sparse (lexica).
</p>
<p>Distributions. Figure 2 shows the empirical dis-
tributions of different lexical features at differ-
ent levels of analysis. 500 features were sampled
from the top 20,000 unigrams 4, 2000 social me-
dia LDA topics (Schwartz et al., 2013a), and all
64 categories from the LIWC lexica (Pennebaker
et al., 2007). To encode the variables continu-
ously we used relative frequencies for unigrams
and lexica (count of word or category divided by
count of all words), and probability of topics, cal-
culated from the posterior probabilities from the
LDA models. Each line in the kernel density plot
</p>
<p>4In social media analyses, the top 20,000 features are of-
ten used (Schwartz and Ungar, 2015)
</p>
<p>80</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: Kernel Density Estimate (KDE) plots showing the distribution of 500 random features at different levels of analysis.
Each row represents a specific level of analysis (county, user, message) and each column represents a specific type of feature
(Lexicon, Topic, Unigram). The bar on the left of each plot represents the percentage of observations that are zero for each
feature where the shading represents the percent of features reaching the given threshold. As the bar gets darker it means more
features out of 500 are zero in that percentage of individuals. The right portion of each plot is based on standardized relative
frequencies of the variables (mean centered and divided by the standard deviation).
</p>
<p>is semi-transparent such that an aggregate trend
across multiple features will emerge darkest. As
we move along a row ranging specific features
(unigrams) to generic features (lexicon), the em-
pirical distribution gradually changes from resem-
bling a “power law” (or binomial distribution with
low number of trials and probability of success) to
something more “Normal”. Similar shifts are also
observed as we move across levels of modeling.
</p>
<p>We investigate whether the best-fitting distribu-
tions vary across the three levels of analysis and
three types of lexical features. We consider the
following candidate distributions to see how well
they fit each of these empirical distributions:
</p>
<p>• Continuous Distributions: (a) Power-law,
(b), Log-normal and (c) Normal
</p>
<p>• Discrete Distributions: (a) Bernoulli, (b)
Multinomial, (c) Poisson, and (d) Zero In-
flated Poisson
</p>
<p>Since most of the distributions outlined above are
standard distributions, we only briefly describe the
zero-inflated variants which handle excess zero
counts. Zero-inflated models explicitly model the
idea that a distribution does not fully capture the
mass at 0 in real world data. They assume that the
data is generated from two components. The first
</p>
<p>component is governed by a Bernoulli distribution
that generates excess zeros, while the second com-
ponent generates counts, some of which also could
be zero (Jansche, 2003).
</p>
<p>3 Evaluation
</p>
<p>We evaluate the distributions we considered by
first characterizing the goodness of fit at different
levels of analyses and then by their predictive per-
formance on social media prediction tasks, both of
which we describe below.
</p>
<p>3.1 Goodness of fit
Following the central limit theorem, we seek to de-
termine across the range levels of analysis and fea-
ture types, whether the distribution can be approx-
imated by a Normal. Focusing just on the non-zero
portions of data encoded as relative frequencies,
we quantify the fit of each candidate distribution
to the data.
</p>
<p>We estimate the parameters for each distribu-
tion using MLE on a training data set (i.e. 80%
of data). Then, we evaluate their likelihoods of a
held-out test dataset, given the estimated param-
eters. Since we are trying to approximate the dis-
crete distribution with a continuous model, all data
were converted to relative frequencies. Finally, the
distribution under which the test data is most likely
</p>
<p>81</p>
<p />
</div>
<div class="page"><p />
<p>Dist Message User County
1gram Topic Lex. 1gram Topic Lex. 1gram Topic Lex.
</p>
<p>Power Law 71 10 0 4 0 0 7 0 0
Log-Normal 25 89 100 96 97 64 92 86 44
Normal 4 1 0 0 3 36 1 14 56
</p>
<p>Table 1: Percentage of best-fitted distributions in each level of message, user, and county for different types of features such
as “Lexicons”, “Topics”, and “1grams”. Note that the best-fitting distribution for each feature type is a function of the level of
analysis.
</p>
<p>is chosen as the ’best fit’ distribution. We repeat
this 100 times and pick the most likely distribution
over all these 100 independent runs.
</p>
<p>Results. Table 1 shows the percentage of fea-
tures in each level that were best fit from an un-
derlying distribution of Normal, Log-Normal, or
Power Law. We see empirically that there is a
trend toward Normal approximation moving from
message to county level, as well as 1grams to lex-
ica. In fact, a majority of lexica at the county-level
were best approximated by a Normal distribution.
</p>
<p>3.2 Predictive Power
In the previous section, we showed that the dis-
tribution of lexical features depends on the scale
of analysis considered (for example, the message
level or the user level). Here, we demonstrate
that predictive models which use these lexical fea-
tures as co-variates can leverage this information
to boost predictive performance. We consider
three predictive tasks using a generative predictive
model. The primary purpose of this evaluation is
not to characterize the best distribution at a level or
task, but to demonstrate that the choice of distribu-
tion assumed when modeling features significantly
affects the predictive performance.
</p>
<p>Predictive Tasks : We consider the following
common predictive tasks and also outline details
of the datasets considered:
</p>
<p>1. Sarcasm Detection (Message level): This
task consists of determining whether tweets
contain a sarcastic expression (Bamman and
Smith, 2015). The dataset consists of 16,833
messages with an average of 12 words per
message.
</p>
<p>2. Gender Identification (User level): This
task involves determining the gender of the
author utilizing a previously described Twit-
ter dataset (Volkova et al., 2013). This dataset
consists of 5,044 users each of which have
</p>
<p>at least a 1,000 tokens as is standard in user-
level analyses (Schwartz et al., 2013b).
</p>
<p>3. Ideology Classification (Community level):
We utilized county voting records from 2012
along with a dataset of tweets mapped to
counties. This data consists of 2,175 counties
with atleast 10,000 unigrams as is common in
community level analyses (Eichstaedt et al.,
2015).
</p>
<p>We consider a Naive Bayes classifier (a gener-
ative model) which enables one to directly incor-
porate the inferred feature distribution at a partic-
ular level of analysis, the results of which we dis-
cuss in Table 2. Variable encoding for the clas-
sifiers varied from binary encoding of present or
not (Bernoulli), to counts (Poisson, Zero-inflated
Poisson), multivariate counts (Multinomial), and
continuous relative frequencies (Normal). All dis-
tributions have closed form MLE solutions ex-
cept for Zero-Inflated Poisson, in which case we
used LBFGS optimization to fit both of its param-
eters (Head and Zerner, 1985).
</p>
<p>Results. We report macro F1-score for each of
the underlying distributions in Table 2. For each
of the tasks, we used 80% of the data for train-
ing and evaluate on the held-out 20%. We observe
a similar pattern as that observed in the goodness
of fit setting, with a shift in the best performing
distribution from Bernoulli (which simply models
if a feature exists or not) toward something more
Gaussian (Poisson or Normal) as we move along
from message-level to county-level analysis and
from unigrams to lexica. Specifically note that at
higher levels of analysis (at user and county levels)
as the distribution of features becomes closer to
Normal, modeling features as Bernoulli is clearly
sub-optimal where as at the message level model-
ing unigrams as a Bernoulli is superior. These ob-
servations underscore the main insight that the dis-
tribution family used to model features can be con-
</p>
<p>82</p>
<p />
</div>
<div class="page"><p />
<p>Feature Distribution Message (Sarcasm) User (Gender) County (Political Ideology)
1gram Topic Lex. 1gram Topic Lex. 1gram Topic Lex.
</p>
<p>most frequent class .33 .33 .33 .31 .31 .31 .42 .42 .42
Bernoulli .71 .62 .61 .68 .52 .48 .66 .42 .42
Multinomial .70 .63 .63 .66 .54 .64 .60 .74 .71
Poisson .70 .59 .64 .51 .47 .49 .73 .60 .73
ZeroInflated-Poisson .34 .64 .63 .50 .47 .49 .75 .74 .73
Normal .57 .47 .54 .51 .59 .65 .56 .78 .70
</p>
<p>Table 2: F1-Score of Naive Bayes classifiers using various distributions and levels of analysis across tasks of sarcasm detec-
tion, gender identification, and political ideology classification. Observe that predictive power is once again a function of the
distribution family used to model feature distribution and depends on level of analysis.
</p>
<p>sidered a function of level of analysis and feature-
type considered and has a significant bearing on
predictive performance.
</p>
<p>4 Conclusion
</p>
<p>While computational linguistics has a long his-
tory of studying the distributions of lexical fea-
tures, social media and social scientific studies
have brought about a need to understand how these
change at multiple levels of analyses. Here, we
explored empirical distributions of different types
of linguistic features (unigrams, topics, lexica) in
three different levels of analysis in Twitter data
(message, user, and community). To show which
distribution can better describe features of differ-
ent levels, we approached the problem in three dif-
ferent ways: (1) visualization of empirical distri-
butions, (2) goodness-of-fit comparisons, and (3)
for predictive tasks.
</p>
<p>We showed that the best-fit distribution depends
on feature-type (i.e. unigram versus lexica) and
the level of analysis (i.e. message-, user-, or
community-level). Following the central limit the-
orem, all user-level features were predominantly
Log-normal, while a power law best fit unigrams
at the message level and a Normal distribution
best approximated lexica at the community level.
Finally, we demonstrated that predictive perfor-
mance can also vary considerably by the level
of analysis and feature-type, following a similar
trend from Bernoulli distributions at the message-
level to Poisson or Normal at the community-level.
Our results underscore the significance of the level
of analysis for the ever-growing focus in NLP on
social scientific problems which seek to not only
better model words and documents but also the
people and communities generating them.
</p>
<p>Acknowledgements
</p>
<p>This work was supported in part by the Templeton
Religion Trust, Grant TRT-0048.
</p>
<p>References
David Bamman and Noah A Smith. 2015. Contextu-
</p>
<p>alized sarcasm detection on twitter. In Proceedings
to the International Conference on Web-blogs and
Social Media. pages 574–577.
</p>
<p>Glen Coppersmith, Mark Dredze, and Craig Harman.
2014. Quantifying mental health signals in twitter.
In Proceedings of the ACL workshop on Computa-
tional Linguistics and Clinical Psychology.
</p>
<p>Johannes C Eichstaedt, H Andrew Schwartz, Mar-
garet L Kern, Gregory Park, Darwin R Labarthe,
Raina M Merchant, Sneha Jha, Megha Agrawal,
Lukasz A Dziurzynski, Maarten Sap, et al. 2015.
Psychological language on twitter predicts county-
level heart disease mortality. Psychological Science
1:11.
</p>
<p>Lucie Flekova, Jordan Carpenter, Salvatore Giorgi,
Lyle Ungar, and Daniel Preoctiuc-Pietro. 2016. An-
alyzing Biases in Human Perception of User Age
and Gender from Text. In Proceedings of the 54th
annual meeting of the Association for Computa-
tional Linguistics. ACL.
</p>
<p>John D Head and Michael C Zerner. 1985. A broyden-
fletchergoldfarbshanno optimization procedure for
molecular geometries. Chemical physics letters
122(3):264–270.
</p>
<p>Martin Jansche. 2003. Parametric models of linguistic
count data. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1. Association for Computational Linguis-
tics, pages 288–295.
</p>
<p>Moshe Koppel, Jonathan Schler, and Shlomo Arga-
mon. 2009. Computational methods in authorship
attribution. Journal of the American Society for in-
formation Science and Technology 60(1):9–26.
</p>
<p>83</p>
<p />
</div>
<div class="page"><p />
<p>Andrew McCallum, Kamal Nigam, et al. 1998. A com-
parison of event models for naive bayes text classi-
fication. In AAAI-98 workshop on learning for text
categorization. Citeseer, volume 752, pages 41–48.
</p>
<p>Frederick Mosteller and David L Wallace. 1963. In-
ference in an authorship problem: A comparative
study of discrimination methods applied to the au-
thorship of the disputed federalist papers. Journal of
the American Statistical Association 58(302):275–
309.
</p>
<p>James W Pennebaker, Roger J Booth, and Martha E
Francis. 2007. Liwc2007: Linguistic inquiry and
word count. Austin, Texas: LIWC.net .
</p>
<p>Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.
2011. Gender attribution: tracing stylometric evi-
dence beyond topic and genre. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics, pages 78–86.
</p>
<p>H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Richard E Lucas,
Megha Agrawal, Gregory J Park, Shrinidhi K Lak-
shmikanth, Sneha Jha, Martin EP Seligman, et al.
2013a. Characterizing geographic variation in well-
being using tweets. In Proceedings of the 7th Inter-
national AAAI Conference on Web and Social Me-
dia. ICWSM.
</p>
<p>H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin EP Seligman, et al.
2013b. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one 8(9):e73791.
</p>
<p>H Andrew Schwartz and Lyle H Ungar. 2015. Data-
driven content analysis of social media a systematic
overview of automated methods. The ANNALS of
the American Academy of Political and Social Sci-
ence 659(1):78–94.
</p>
<p>Claude E Shannon. 1948. A mathematical theory
of communication, bell system technical journal
27: 379-423 and 623–656. Mathematical Reviews
(MathSciNet): MR10, 133e .
</p>
<p>Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic lan-
guage variations to improve multilingual sentiment
analysis in social media. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing. EMNLP, pages 1815–1827.
</p>
<p>84</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 85–91
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2014
</p>
<p>Exploring Neural Text Simplification Models
</p>
<p>Sergiu Nisioi1,3,∗ Sanja Štajner2,∗ Simone Paolo Ponzetto2 Liviu P. Dinu1
</p>
<p>1Human Language Technologies Research Center, University of Bucharest, Romania
2Data and Web Science Group, University of Mannheim, Germany
</p>
<p>3Oracle Corporation, Romania
{sergiu.nisioi,ldinu}@fmi.unibuc.ro
</p>
<p>{sanja,simone}@informatik.uni-mannheim.de
</p>
<p>Abstract
</p>
<p>We present the first attempt at using se-
quence to sequence neural networks to
model text simplification (TS). Unlike the
previously proposed automated TS sys-
tems, our neural text simplification (NTS)
systems are able to simultaneously per-
form lexical simplification and content re-
duction. An extensive human evaluation
of the output has shown that NTS systems
achieve almost perfect grammaticality and
meaning preservation of output sentences
and higher level of simplification than the
state-of-the-art automated TS systems.
</p>
<p>1 Introduction
</p>
<p>Neural sequence to sequence models have been
successfully used in many applications (Graves,
2012), from speech and signal processing to text
processing or dialogue systems (Serban et al.,
2015). Neural machine translation (Cho et al.,
2014; Bahdanau et al., 2014) is a particular type
of sequence to sequence model that recently at-
tracted a lot of attention from industry (Wu et al.,
2016) and academia, especially due to the capa-
bility to obtain state-of-the-art results for various
translation tasks (Bojar et al., 2016). Unlike classi-
cal statistical machine translation (SMT) systems
(Koehn, 2010), neural networks are being trained
end-to-end, without the need to have external de-
coders, language models or phrase tables. The ar-
chitectures are relatively simpler and more flexi-
ble, making possible the use of character models
(Luong and Manning, 2016) or even training mul-
tilingual systems in one go (Firat et al., 2016).
</p>
<p>Automated text simplification (ATS) systems
are meant to transform original texts into differ-
</p>
<p>∗∗Both authors have contributed equally to this work
</p>
<p>ent (simpler) variants which would be understood
by wider audiences and more successfully pro-
cessed by various NLP tools. In the last sev-
eral years, great attention has been given to ad-
dressing ATS as a monolingual machine transla-
tion problem translating from ‘original’ to ‘sim-
ple’ sentences. So far, attempts were made at stan-
dard phrase-based SMT (PBSMT) models (Spe-
cia, 2010; Štajner et al., 2015), PBSMT mod-
els with added phrasal deletion rules (Coster and
Kauchak, 2011) or reranking of the n-best out-
puts according to their dissimilarity to the output
(Wubben et al., 2012), tree-based translation mod-
els (Zhu et al., 2010; Paetzold and Specia, 2013),
and syntax-based MT with specially designed tun-
ing function (Xu et al., 2016). Recently, lexi-
cal simplification (LS) was addressed by unsuper-
vised approaches leveraging word-embeddings,
with reported good success (Glavaš and Štajner,
2015; Paetzold and Specia, 2016).
</p>
<p>To the best of our knowledge, our work is the
first to address the applicability of neural sequence
to sequence models for ATS. We make use of
the recent advances in neural machine translation
(NMT) and adapt the existing architectures for our
specific task. We also perform an extensive human
evaluation to directly compare our systems with
the current state-of-the-art (supervised) MT-based
and unsupervised lexical simplification systems.
</p>
<p>2 Neural Text Simplification (NTS)
</p>
<p>We use the OpenNMT framework (Klein et al.,
2017) to train and build our architecture with
two LSTM layers (Hochreiter and Schmidhuber,
1997), hidden states of size 500 and 500 hidden
units, and a 0.3 dropout probability (Srivastava
et al., 2014). The vocabulary size is set to 50,000
and we train the model for 15 epochs with plain
SGD optimizer, and after epoch 8 we halve the
</p>
<p>85</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2014">https://doi.org/10.18653/v1/P17-2014</a></div>
</div>
<div class="page"><p />
<p>learning rate. At the end of each epoch we save the
current state of the model and predict the perplex-
ity values of the models on the development set.
We employ early-stopping and select the model re-
sulted from the epoch with the best perplexity to
avoid over-fitting. The parameters are initialized
over uniform distribution with support [-0.1, 0.1].
Additionally, for the decoder we employ global at-
tention in combination with input feeding as de-
scribed by Luong et al. (2015). The architecture1
</p>
<p>is depicted in Figure 1, with the input feeding ap-
proach represented only for the last hidden state of
the decoder.
</p>
<p>Attention Layer
</p>
<p>ct
</p>
<p>Encoder Decoder
</p>
<p>ht
</p>
<p>h̃t
</p>
<p>yt
</p>
<p>at
</p>
<p>.......
</p>
<p>h̄s
</p>
<p>Figure 1: Architecture of the neural simplification
model with global attention and input feeding.
</p>
<p>For the attention layer, we compute a context
vector ct by using the information provided from
the hidden states of the source sentence and by
computing a weighted average with the alignment
weights at. The new hidden state is obtained us-
ing a concatenation of the previous hidden state
and the context vector:
</p>
<p>h̃t = tanhW [ct;ht]
</p>
<p>The global alignment weights at are being com-
puted with a softmax function over the general
scoring method for attention:
</p>
<p>at(s) =
exphTt Wash̄s∑
s′ exph
</p>
<p>T
t Was′ h̄s′
</p>
<p>Input feeding is a process that sends the pre-
vious hidden state obtained using the alignment
</p>
<p>1The architecture configurations, data, and the pre-
trained models are released in https://github.com/
senisioi/NeuralTextSimplification
</p>
<p>method, to the input at the next step, presumably
making the model keep track of anterior align-
ment decisions. Luong et al. (2015) showed this
approach can increase the evaluation scores for
neural machine translation, while in our case, for
monolingual data, we believe it can be helpful to
create better alignments. Our approach does not
involve the use of character-based models (Sen-
nrich et al., 2015; Luong and Manning, 2016) to
handle out of vocabulary words and entities. In-
stead, we make use of alignment probabilities be-
tween the predictions and the original sentences to
retrieve the original words.
</p>
<p>2.1 Word2vec Embeddings
Furthermore, we are interested to explore whether
large scale pre-trained embeddings can improve
text simplification models. Kauchak (2013) indi-
cates that combining normal data with simplified
data can increase the performance of ATS systems.
Therefore, we construct a secondary model (NTS-
w2v) using a combination of pre-trained word2vec
from Google News corpus (Mikolov et al., 2013a)
of size 300 and locally trained embeddings of
size 200. To ensure good representations of low-
frequency words, we use word2vec (Řehůřek and
Sojka, 2010; Mikolov et al., 2013b) to train skip-
gram with hierarchical softmax and we set a win-
dow of 10 words.
</p>
<p>Following Garten et al. (2015) who showed that
simple concatenation can improve the word rep-
resentations, we construct two different sets of
embeddings for the encoder and for the decoder.
The former are constructed using the word2vec
trained on the original English texts combined
with Google News and the later (decoder) embed-
dings are built from word2vec trained on the sim-
plified version of the training data combined with
Google News. To merge the local and global em-
beddings, we concatenate the representations for
each word in the vocabulary, thus obtaining a new
representation of size 500. If a word is missing in
the global embeddings, we replace it with a sam-
ple from a Gaussian distribution with mean 0 and
standard deviation of 0.9. The remaining param-
eters are left unchanged from the previous model
description.
</p>
<p>2.2 Prediction Ranking
To ensure the best predictions and the best simpli-
fied sentences at each step, we use beam search
to sample multiple outputs from the two systems
</p>
<p>86</p>
<p />
</div>
<div class="page"><p />
<p>described previously (NTS and NTS-w2v). Beam
search works by generating the first k hypotheses
at each step ordered by the log-likelihood of the
target sentence given the input sentence. By de-
fault, we use a beam size of 5 and take the first hy-
pothesis, but we also observe that higher beam size
and lower-ranked hypotheses can generate good
simplification results. Therefore, we generate the
first two candidate hypotheses for each beam size
from 5 to 12. We then attempt to find the best
beam size and hypothesis based on two metrics:
the traditional MT-evaluation metric, BLEU (Pa-
pineni et al., 2002; Bird et al., 2009) with NIST
smoothing (Bird et al., 2009), and SARI (Xu et al.,
2016), a recent text-simplification metric.
</p>
<p>2.3 Dataset
</p>
<p>To train our models, we use the publicly avail-
able dataset provided by Hwang et al. (2015)
based on manual and automatic alignments be-
tween standard English Wikipedia and Simple En-
glish Wikipedia (EW–SEW). We discard the un-
categorized matches, and use only good matches
and partial matches which were above the 0.45
threshold (Hwang et al., 2015), totaling to 280K
aligned sentences (around 150K full matches and
130K partial matches). It is one of the largest
freely available resources for text simplification,
and unlike the previously used EW–SEW cor-
pus2 (Kauchak, 2013), which only contains full
matches (167K pairs), the newer dataset also con-
tains partial matches. Therefore, it is not only
larger, but it also allows for learning sentence
shortening (dropping irrelevant parts) transforma-
tions (see Table 3, Appendix A).
</p>
<p>original simplified
locations 158,394 127,349
persons 161,808 127,742
</p>
<p>organizations 130,679 101,239
misc 95,168 71,138
</p>
<p>vocabulary 187,137 144,132
tokens 7,400,499 5,634,834
</p>
<p>Table 1: The number of tokens and entities in the
corpus.
</p>
<p>We use the Stanford NER system (Finkel et al.,
2005) to get an approximate number of locations,
persons, organizations and miscellaneous entities
</p>
<p>2http://www.cs.pomona.edu/˜dkauchak/
simplification/
</p>
<p>in the corpus. A brief analysis of the vocabulary is
rendered in Table 1.
</p>
<p>The dataset we use contains an abundant
amount of named entities and consequently a large
amount of low frequency words, but the majority
of entities are not part of the model’s 50,000 words
vocabulary due to their small frequency. These
words are replaced with ’UNK’ symbols during
training. At prediction time, we replace the un-
known words with the highest probability score
from the attention layer. We believe it is impor-
tant to ensure that the models learn good word
representations, either during the model training
or through word2vec, in order to accurately create
alignments between source and target sentences.
</p>
<p>Given that in TS there is not only one best
simplification, and that the quality of simplifi-
cations in Simple English Wikipedia has been
disputed before (Amancio and Specia, 2014; Xu
et al., 2015), for tuning and testing we use the
dataset previously released by Xu et al. (2016),
which contains 2000 sentences for tuning and 359
for testing, each with eight simplification variants
obtained by eight Amazon Mechanical Turkers.3
</p>
<p>The tune subset is also used as reference corpus
in combination with BLEU and SARI to select
the best beam size and hypothesis for prediction
reranking.
</p>
<p>3 Evaluation
</p>
<p>For the first 70 original sentences of the Xu et al.’s
(2016) test set4 we perform three types of human
evaluation to assess the output of our best sys-
tems and three ATS systems of different architec-
tures: (1) the PBSMT system with reranking of
n-best outputs (Wubben et al., 2012), which rep-
resent the best PBSMT approach to ATS, trained
and tuned over the same datasets as our systems;
(2) the state-of-the-art SBMT system (Xu et al.,
2016) with modified tuning function (using SARI)
and using PPDB paraphrase database (Ganitke-
vitch et al., 2013);5 and (3) one of the state-of-the-
art unsupervised lexical simplification (LS) sys-
tems that leverages word-embeddings (Glavaš and
</p>
<p>3None of the 359 test sentences was present in the datasets
we used for training and tuning.
</p>
<p>4https://github.com/cocoxu/
simplification/
</p>
<p>5For the first two systems, we use publicly
available output at: https://github.com/
cocoxu/simplification/tree/master/data/
systemoutputs
</p>
<p>87</p>
<p />
</div>
<div class="page"><p />
<p>Štajner, 2015).6
</p>
<p>We evaluate the output of all systems using
three types of human evaluation.
</p>
<p>Correctness and Number of Changes. First,
we count the total number of changes made by
each system (Total), counting the change of a
whole phrase (e.g. “become defunct” → “was
dissolved”) as one change. Those changes that
preserve the original meaning and grammatical-
ity of the sentence (assessed by two native English
speakers) and, at the same time, make the sentence
easier to understand (assessed by two non-native
fluent English speakers) are marked as Correct. In
the case of content reduction, we instructed the
annotators to count the deletion of each array of
consecutive words as one change and consider the
meaning unchanged if the main information of the
sentence was retained and unchanged. The sen-
tences for which the two annotators did not agree
were given to a third annotator to obtain the ma-
jority vote.
</p>
<p>Grammaticality and Meaning Preservation.
Second, three native English speakers rate the
grammaticality (G) and meaning preservation (M)
of each (whole) sentence with at least one change
on a 1–5 Likert scale (1 – very bad; 5 – very good).
The obtained inter-annotator agreement (quadratic
Cohens kappa) was 0.78 for G and 0.63 for M.
</p>
<p>Simplicity of sentences. Third, the three non-
native fluent English speakers were shown origi-
nal (reference) sentences and target (output) sen-
tences, one pair at the time, and asked whether the
target sentence is: +2 – much simpler; +1 – some-
what simpler; 0 – equally difficult; -1 – somewhat
more difficult; -2 – much more difficult, than the
reference sentence. The obtained inter-annotator
agreement (quadratic Cohens kappa) was 0.66.
</p>
<p>While the correctness of changes takes into ac-
count the influence of each individual change on
grammaticality, meaning and simplicity of a sen-
tence, the Scores (G and M) and Rank (S) take into
account the mutual influence of all changes within
a sentence.
</p>
<p>4 Results and Discussion
</p>
<p>The results of the human evaluation (Table 2) re-
vealed that all NTS models achieve higher per-
centage of correct changes and more simplified
output than any of the state-of-the-art ATS systems
</p>
<p>6For the LightLS system (Glavaš and Štajner, 2015) we
use the output of the original system provided by the authors.
</p>
<p>with different architectures (PBSMT-R, SBMT,
and LightLS). We also notice that the best models
according to BLEU are obtained with hypothesis
1 and the maximum beam size for both models,
while the SARI re-ranker prefers hypothesis 2 and
beam size 5 for the first NTS and the maximum
beam size for the custom word embeddings model.
</p>
<p>The NTS with custom word2vec embeddings
ranked with the text simplification specific met-
ric (SARI) obtained the highest total number of
changes among the neural systems, one of the
highest percentage of correct changes, the second
highest simplicity score, and solid grammaticality
and meaning preservation scores. An example of
the output of different systems is presented in Ta-
ble 4 (Appendix A).
</p>
<p>The use of different metrics for ranking the NTS
predictions optimizes the output towards different
evaluation objectives: SARI leads to the highest
number of total changes, BLEU to the highest per-
centage of correct changes, and the default beam
scores to the best grammaticality (G) and meaning
preservation (M). In addition, custom composed
global and local word embeddings in combination
with SARI metric improve the default translation
system, given the joint scores for each evaluation
criterion.
</p>
<p>Here is important to note that for ATS sys-
tems, the precision of the system (correctness of
changes, grammaticality, meaning preservation,
and simplicity of the output) is more important
than the recall (the total number of changes made).
The low recall would just leave the sentences sim-
ilar to their originals thus not improving much the
understanding or reading speed of the target users,
or not improving much the NLP systems in which
they are used as a pre-processing step. A low pre-
cision, on the other hand, would make texts even
more difficult to read and understand, and would
worsen the performances of the NLP systems in
which ATS is used as a pre-processing step.
</p>
<p>5 Conclusions
</p>
<p>We presented a first attempt at modelling sentence
simplification with a neural sequence to sequence
model. Our extensive human evaluation showed
that our NTS systems, if the output is ranked with
the right metric, can significantly7 outperform
the best phrase-based and syntax-based MT ap-
proaches, and unsupervised lexical ATS approach,
</p>
<p>7Wilcoxon’s signed rank test, p &lt; 0.001.
</p>
<p>88</p>
<p />
</div>
<div class="page"><p />
<p>Approach
Changes Scores Rank
</p>
<p>SARI BLEU
Total Correct G M S
</p>
<p>NTS default (beam 5, hypothesis 1) 36 72.2% 4.92 4.31 +0.46 30.65 84.51
NTS SARI (beam 5, hypothesis 2) 72 51.6% 4.19 3.62 +0.38 37.25 80.69
NTS BLEU (beam 12, hypothesis 1) 44 73.7% 4.77 4.15 +0.92 30.77 84.70
NTS-w2v default (beam 5, hypothesis 1) 31 54.8% 4.79 4.17 +0.21 31.11 87.50
NTS-w2v SARI (beam 12, hypothesis 2) 110 68.1% 4.53 3.83 +0.63 36.10 79.38
NTS-w2v BLEU (beam 12, hypothesis 1) 61 76.9% 4.67 4.00 +0.40 30.67 85.03
PBSMT-R (Wubben et al., 2012) 171 41.0% 3.10 2.71 −0.55 34.07 67.79
SBMT (SARI+PPDB) (Xu et al., 2016) 143 34.3% 4.28 3.57 +0.03 38.59 73.62
LightLS (Unsupervised) (Glavaš and Štajner, 2015) 132 26.6% 4.47 2.67 −0.01 34.96 83.54
</p>
<p>Table 2: Human evaluation results (the highest scores by each evaluation criterion are shown in bold).
</p>
<p>by grammaticality, meaning preservation and sim-
plicity of the output sentences, the percentage of
correct transformations, while at the same time
achieving more than 1.5 changes per sentence, on
average. Furthermore, we discovered that NTS
systems are capable of correctly performing sig-
nificant content reduction, thus being the only TS
models proposed so far which can jointly perform
lexical simplification and content reduction.
</p>
<p>Acknowledgments
</p>
<p>This work has been partially supported by a grant
of the Romanian National Authority for Scientific
Research and Innovation, CNCS/CCCDI UEFIS-
CDI, project number PN-III-P2-2.1-53BG/2016,
within PNCDI III, and by the SFB 884 on the Po-
litical Economy of Reforms at the University of
Mannheim (project C4), funded by the German
Research Foundation (DFG).
</p>
<p>References
Marcelo Adriano Amancio and Lucia Specia. 2014. An
</p>
<p>Analysis of Crowdsourced Text Simplifications . In
Proceedings of the 3rd Workshop on Predicting and
Improving Text Readability for Target Reader Popu-
lations (PITR). pages 123–130.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473. http://arxiv.org/abs/1409.0473.
</p>
<p>Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
ing text with the natural language toolkit. ” O’Reilly
Media, Inc.”.
</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
</p>
<p>Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation. Association for
Computational Linguistics, pages 131–198.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing. pages 1724–1734.
</p>
<p>William Coster and David Kauchak. 2011. Simple En-
glish Wikipedia: a new text simplification task. In
Proceedings of ACL&amp;HLT . pages 665–669.
</p>
<p>Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd annual meet-
ing on association for computational linguistics. As-
sociation for Computational Linguistics, pages 363–
370.
</p>
<p>Orhan Firat, KyungHyun Cho, and Yoshua Ben-
gio. 2016. Multi-way, multilingual neural ma-
chine translation with a shared attention mechanism.
CoRR abs/1601.01073.
</p>
<p>Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL-HLT . pages
758–764.
</p>
<p>Justin Garten, Kenji Sagae, Volkan Ustun, and Morteza
Dehghani. 2015. Combining distributed vector rep-
resentations for words. In Proceedings of NAACL-
HLT . pages 95–101.
</p>
<p>Goran Glavaš and Sanja Štajner. 2015. Simplifying
Lexical Simplification: Do We Need Simplified Cor-
pora? In Proceedings of the ACL&amp;IJCNLP 2015
(Volume 2: Short Papers). pages 63–68.
</p>
<p>89</p>
<p />
</div>
<div class="page"><p />
<p>Alex Graves. 2012. Supervised sequence labelling with
recurrent neural networks, volume 385. Springer.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.
</p>
<p>William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,
and Wei Wu. 2015. Aligning Sentences from Stan-
dard Wikipedia to Simple Wikipedia. In Proceed-
ings of NAACL&amp;HLT . pages 211–217.
</p>
<p>David Kauchak. 2013. Improving text simplification
language modeling using unsimplified text data. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). ACL, pages 1537–1546.
</p>
<p>G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints .
</p>
<p>Philipp Koehn. 2010. Statistical machine translation.
</p>
<p>Minh-Thang Luong and Christopher D Manning. 2016.
Achieving open vocabulary neural machine trans-
lation with hybrid word-character models. arXiv
preprint arXiv:1604.00788 .
</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP. The Asso-
ciation for Computational Linguistics, pages 1412–
1421.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at International Conference on Learning Represen-
tations.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.
</p>
<p>Gustavo Henrique Paetzold and Lucia Specia. 2013.
Text simplification as tree transduction. In Proceed-
ings of the 9th Brazilian Symposium in Information
and Human Language Technology. pages 116–125.
</p>
<p>Gustavo Henrique Paetzold and Lucia Specia. 2016.
Unsupervised lexical simplification for non-native
speakers. In Proceedings of the 30th AAAI.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.
</p>
<p>Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. ELRA, Valletta,
Malta, pages 45–50.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909 .
</p>
<p>Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2015. Build-
ing end-to-end dialogue systems using generative hi-
erarchical neural network models. arXiv preprint
arXiv:1507.04808 .
</p>
<p>Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of the 9th inter-
national conference on Computational Processing
of the Portuguese Language (PROPOR). Springer
Berlin Heidelberg, volume 6001 of Lecture Notes in
Computer Science, pages 30–39.
</p>
<p>Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.
</p>
<p>Sanja Štajner, Hannah Bechara, and Horacio Saggion.
2015. A Deeper Exploration of the Standard PB-
SMT Approach to Text Simplification and its Eval-
uation. In Proceedings of ACL&amp;IJCNLP (Volume 2:
Short Papers). pages 823–828.
</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144 .
</p>
<p>Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL): Long Papers - Volume
1. Association for Computational Linguistics, pages
1015–1024.
</p>
<p>Wei Xu, Chris Callison-Burch, and Courtney Napoles.
2015. Problems in Current Text Simplification Re-
search: New Data Can Help. Transactions of the
Association for Computational Linguistics (TACL)
3:283–297.
</p>
<p>Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze
Chen, and Chris Callison-Burch. 2016. Optimizing
statistical machine translation for text simplification.
Transactions of the Association for Computational
Linguistics 4:401–415.
</p>
<p>Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010). pages 1353–1361.
</p>
<p>90</p>
<p />
</div>
<div class="page"><p />
<p>A Appendix - Data Sample and System Output
</p>
<p>Match Transformation Sentence pair
Full syntactic simplification; reorder-
</p>
<p>ing of sentence constituents
“During the 13th century, gingerbread was brought to Swe-
den by German immigrants.” and “German immigrants
brought it to Sweden during the 13th century.”
</p>
<p>Full lexical paraphrasing “During the 13th century, gingerbread was brought to Swe-
den by German immigrants.” and “German immigrants
brought it to Sweden during the 13th century.”
</p>
<p>Partial strong paraphrasing “Gingerbread foods vary, ranging from a soft, moist loaf
cake to something close to a ginger biscuit.” and “Ginger-
bread is a word which describes different sweet food prod-
ucts from soft cakes to a ginger biscuit.”
</p>
<p>Partial adding explanations “Humidity is the amount of water vapor in the air.” and
“Humidity (adjective: humid) refers to water vapor in the
air, but not to liquid droplets in fog, clouds, or rain.”
</p>
<p>Partial sentence compression; dropping
irrelevant information
</p>
<p>“Falaj irrigation is an ancient system dating back thousands
of years and is used widely in Oman, the UAE, China, Iran
and other countries.” and “The ancient falaj system of irri-
gation is still in use in some areas.”
</p>
<p>Table 3: Examples of full and partial matches from the EW–SEW dataset (Hwang et al., 2015).
</p>
<p>System Output
NTS-w2v default Perry Saturn (with terri) defeated Eddie Guerrero (with
</p>
<p>chyna) to win the WWF European Championship (8:10);
Saturn pinned Guerrero after a diving elbow drop.
</p>
<p>NTS-w2v SARI Perry Saturn pinned Guerrero to win the WWF Euro-
pean Championship.
</p>
<p>NTS-w2v BLEU Perry Saturn pinned Guerrero after a diving drop drop.
NTS default He (with terri) defeated Eddie Guerrero (with chyna) to
</p>
<p>win the WWF European Championship (8:10); Saturn
pinned Guerrero after a diving elbow drop.
</p>
<p>NTS BLEU/SARI He defeated Eddie Guerrero (with Chyna) to win the WWF
European Championship (8:10); Saturn pinned Guerrero
after a diving elbow drop.
</p>
<p>LightLS (Glavaš and Štajner, 2015) Perry Saturn (with terri) defeated Eddie Guerrero (with
chyna) to win the WWF European Championship (8:10);
Saturn pinned Guerrero after a swimming shoulder fall.
</p>
<p>SBMT (Xu et al., 2016) Perry Saturn (with terri) beat Eddie Guerrero (with chyna)
to win the WWF European League (8:10); Saturn pinned
Guerrero after a diving elbow drop.
</p>
<p>PBSMT-R (Wubben et al., 2012) Perry Saturn with terri and Eddie Guerrero , chyna , to win
the European Championship then-wwf 8:10); he pinned
Guerrero after a diving elbow drop.
</p>
<p>Original Perry Saturn (with terri) defeated Eddie Guerrero (with
chyna) to win the WWF European Championship (8:10);
Saturn pinned Guerrero after a diving elbow drop.
</p>
<p>Table 4: Output examples, differences to the original sentence are shown in bold.
</p>
<p>91</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 92–96
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2015
</p>
<p>On the Challenges of Translating NLP Research into Commercial
Products
</p>
<p>Daniel Dahlmeier
SAP Innovation Center Singapore
d.dahlmeier@sap.com
</p>
<p>Abstract
</p>
<p>This paper highlights challenges in in-
dustrial research related to translating re-
search in natural language processing into
commercial products. While the interest
in natural language processing from indus-
try is significant, the transfer of research to
commercial products is non-trivial and its
challenges are often unknown to or under-
estimated by many researchers. I discuss
current obstacles and provide suggestions
for increasing the chances for translating
research to commercial success based on
my experience in industrial research.
</p>
<p>1 Introduction
</p>
<p>Natural language processing (NLP) has made sig-
nificant progress over the last two decades, in par-
ticular due to the success of data-driven machine
learning methods. Recently, deep learning has led
to another wave of remarkable improvements in
NLP and other areas of machine learning and ar-
tificial intelligence (AI). Not surprisingly, many
industry players are investing heavily in machine
learning and AI to create new products and ser-
vices (MIT Technology Review, 2016).
</p>
<p>However, translating research into a success-
ful product has its own challenges. Tradition-
ally, technology transfer is often assumed to hap-
pen in a linear transition from pure research to
applied research to commercialization (Stokes,
1997). The model assumes that the discoveries
from researchers will naturally be picked up by
engineers and industry players who will use it to
build new products. In reality, the transfer from
research to commercial products is considerably
more complex and far from guaranteed. In fact,
many research projects fail to successfully trans-
fer their discoveries to commercial products.
</p>
<p>In this position paper, I highlight some of the
reasons why it is so difficult to translate NLP re-
search into successful products. This paper does
not contain any new algorithms, experiments, or
results. Instead, it seeks to share my experience
working at the intersection of academic research
and industry with the aim to stimulate a discus-
sion how technology transfer of NLP research can
be improved. I want to emphasize upfront that
the paper is not arguing that all NLP researchers
should focus their efforts on building commercial
products nor does every new product require a re-
search breakthrough to be successful. The paper’s
aim is rather to discuss how we can improve use-
inspired basic research that satisfies both the de-
sire for fundamental understanding and consider-
ations of use, sometimes referred to as Pasteur’s
quadrant (Stokes, 1997).
</p>
<p>The contributions of this paper are twofold.
First, I highlight common obstacles in the path
of transferring research into commercial prod-
ucts. Second, I offer suggestions for increasing
the chances of success based on my experience at
SAP, the world’s largest enterprise software com-
pany.
</p>
<p>2 Challenges to Innovation
</p>
<p>This section highlights challenges in NLP research
that make it difficult to translate the results into
impactful innovation.
</p>
<p>2.1 Lack of Value Focus
The first step to creating a successful product is
understanding your customers. That is why many
methodologies for creating new products or busi-
ness models start with a user persona and how to
create value for the user (Ries, 2011; Osterwalder
et al., 2014). Similarly, to conduct research with
practical impact, it is worthwhile to consider what
potential applications the research could enable
</p>
<p>92</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2015">https://doi.org/10.18653/v1/P17-2015</a></div>
</div>
<div class="page"><p />
<p>and what the value proposition for a potential user
might be. The value proposition is closely linked
to the user persona and the tasks that she tries to
solve in her daily life (Christensen and Raynor,
2013). Thus, choosing the right research task is
important when aiming for impactful research. It
is instructive that NLP tasks which solve practical
problems, like machine translation or sentiment
analysis, have seen significant adoption in com-
mercial applications. But many applications that
are requested by industry are still beyond the capa-
bilities of current NLP research, for example chat-
bots that can respond to arbitrary user questions.
</p>
<p>It is also important for researchers to understand
that the priorities in industry are different from
priorities in academic research. In academic re-
search, the priorities are to create contributions to
the body of knowledge in the field, e.g., defining a
new task, a novel, elegant model, or a new state-
of-the-art benchmark result. In industry the prior-
ities are creating innovative products that delight
users and create new revenue streams. To have
the best of both worlds, researchers should occa-
sionally take a step back and consider what value
proposition their work has for people outside the
NLP community.
</p>
<p>2.2 Lack of Reproducibility
Reproducible research is one of the pillars of the
scientific method and thus important to good re-
search work in general. But the ability to repro-
duce a model is also a prerequisite to incorporat-
ing it into a product. As NLP models often depend
on a complex set of parameters and pre-processing
steps which cannot always be explained in all de-
tail in a paper, it is often hard to reproduce other’s
results. The author himself has his own experience
trying to (unsuccessfully) reproduce published re-
sults. As problems to reproduce research are sel-
dom reported (but see (Bikel, 2004) for an excep-
tion), it is also hard for researchers to find informa-
tion on how to improve their implementation when
they struggle to re-produce published results.
</p>
<p>2.3 Lack of (Domain) Data
Data is the fuel that powers machine learning and
most of NLP research. While the “big data” rev-
olution has given us access to large quantities of
text data from some domains, for many industry
problems there is no or very limited data avail-
able to conduct research on. For example, in
my group we have been working on text classi-
</p>
<p>fication for customer service tickets. While there
are many datasets available for text classification,
these are primarily from newswire or online re-
views. For customer service, there is no public
dataset to compare to. Due to the confidential na-
ture of the data and data privacy concerns, compa-
nies who have such data cannot easily release it for
research purposes. Some companies host shared
tasks or data science competitions in which they
make data available, for example on Kaggle1, but
access to data remains one of the biggest obsta-
cles for researcher who want to work on industry
problems.
</p>
<p>Even when there is data available from public
sources, e.g., from the web, using the data for
commercial purposes can be tricky from a legal
standpoint. Crawling data from web (or using cor-
pora created by others in this manner) might be ac-
ceptable for research purposes, but when building
a commercial product the exact license, copyright,
etc. of every data source needs to be checked. The
same holds for publicly available NLP models de-
rived from such data.
</p>
<p>For everyone who believes that working in in-
dustry solves all data problem, I note here that
working with real data sets has its own challenges.
Real data sets are often small, noisy, scrambled, or
otherwise incomplete, making it hard to achieve
good results. To effectively use the data, re-
searchers also have to understand the data schema
and the business process behind the data. This
can be challenging without and in-depth domain
knowledge.
</p>
<p>2.4 Overemphasis on Test Scores
</p>
<p>The empirical evaluation of statistical methods on
common benchmarks has without a doubt revolu-
tionized NLP (Johnson, 2009). However, some-
times the score on the test set is taken as the
only factor that determines the success of a piece
of research. For practical applications, the test
score on a benchmark dataset is only one criteria
among many when it comes to choosing an algo-
rithm for practical use. Other factors include the
time and costs required to implement the method,
the computational resources required, speed and
performance, the ease of integration, support for
multi-lingual input, the ability to adapt and cus-
tomize the method, the ability to incorporate prior
knowledge, and the ability to interpret and explain
</p>
<p>1https://www.kaggle.com
</p>
<p>93</p>
<p />
</div>
<div class="page"><p />
<p>the model. For example, in our text categoriza-
tion work, we encountered the requirement to ac-
commodate changes in the the output classes, i.e.,
adding, merging, splitting, and removing classes,
without re-training the model from scratch. These
factors are currently underrepresented in NLP re-
search.
</p>
<p>2.5 Difficulty of Adoption
</p>
<p>No matter how good an NLP model is, it cannot
have practical impact if it is never implemented.
But in any application, the NLP model will only
be one component in a larger software system.
How easily the NLP component can work together
with the remaining components is important for
the ease of adoption of the method into productive
applications. Unlike rule-based methods, statisti-
cal NLP models often require expensive collection
and labeling of data, data pre-processing, model
(re-)training, parameter tuning, and monitoring of
the model to avoid model staleness. This makes it
harder to adopt statistical models in practical ap-
plications (Chiticariu et al., 2013).
</p>
<p>2.6 Timelines
</p>
<p>The time horizon within which stakeholders
expect results is generally shorter in industry
projects. While research grants typically run for
three to five years, industry research is under pres-
sure to deliver tangible outcomes in less than two
years. For projects with actual customers and
proof of concepts, timelines are usually not longer
than a few months. This results in the following
chicken and egg problem: it is difficult to produce
groundbreaking research within a short time frame
but long investments into research are hard to jus-
tify if the value the research will ultimately pro-
duce is not clear. That is why academic research
is generally better equipped to focus on funda-
mental research questions. Fundamental research
does not exclude practical usage but incremental
research that fine-tunes every aspect of the imple-
mentation of an NLP model is often better done in
industry labs.
</p>
<p>3 Bridging the Gap
</p>
<p>In this section, I offer some suggestions about how
the disconnect between NLP research and com-
mercial products can be reduced.
</p>
<p>3.1 A “Recipe” for Qualifying a Research
Problem
</p>
<p>The following approach describes the criteria that
we typically apply in our team when we evaluate
new machine learning use cases, including NLP
use cases.
</p>
<p>First, we make sure we understand the “job to
be done”: what is the business problem, who is
the potential user and what problem are we try-
ing to solve? Once we have understood the task,
a first question to ask is whether the task actually
requires NLP. Is the data volume so high that au-
tomation is needed? Would it be easier or cheaper
to solve the task manually? Can the task be solved
via simple rules? Typically, tasks with high data
volume and complex or ambiguous rules are good
candidates for NLP.
</p>
<p>To ensure that the use cases we work on have
practical relevance, we include stakeholders from
the lines of business and industry units in the com-
pany in any new project right from the beginning
and gather feedback from actual customers.
</p>
<p>Once we believe that NLP is required, we try to
formulate the problem as a machine learning task.
The simple template given X, predict Y together
with the question what are the inputs and what
are the outputs? helps significantly to get from
a vague idea to a concrete task formulation. At
this stage, we can often already map the problem
to a standard NLP task, e.g., text classification, se-
quence tagging, or sequence-to-sequence learning.
</p>
<p>Next, we establish whether data is available. If
real data is not available easily, can we work with
publicly available proxy data? For example for
learning to classify customer service tickets, we
can start with text classification on public datasets.
If it is unlikely that data will be available in the
foreseeable future, we do not proceed with a use
case.
</p>
<p>Next, we make a best guess whether the prob-
lem can be solved with the current state of the art
in NLP. Is there an intuitive regularity in the data
which we believe a statistical model could pick
up? Can we represent the input via meaningful
features? Do we have a way to measure the suc-
cess of the method with a well-defined metric?
</p>
<p>Finally, we determine the right approach to ex-
ecute the use case. If it is a hard problem which
needs at least a few more years of research be-
fore it becomes useful, we would most likely de-
cide on a research project. We fund external
</p>
<p>94</p>
<p />
</div>
<div class="page"><p />
<p>research projects at top universities around the
world, where we provide the research problem and
the data and let others try to crack the tough prob-
lems. We also sponsor Ph.D. students who are
working at SAP during their studies.
</p>
<p>If we think that the use case has a strong busi-
ness case and the technology is mature enough,
we will move it to building a proof of concept,
and ultimately a commercial product. While this
“recipe” for qualifying an NLP use case is simple
and common sense, we have found it helpful in
prioritizing use cases.
</p>
<p>Researchers in academia might not have access
to a business unit to provide feedback on research
ideas but many funding bodies are trying to en-
courage increased collaborations between industry
and academia. The European Union, for example,
has specifically funded an initiative, LT-innovate2
</p>
<p>to encourage commercial exploitation of NLP re-
search.
</p>
<p>3.2 Engineering Approach to NLP
</p>
<p>I believe that a more rigorous application of (soft-
ware) engineering principles and tools can greatly
increase the odds of having practical impact with
NLP research.
</p>
<p>To address the problem of reproducibility, I sug-
gest the following. First, the community should
be more stringent about reproducibility. In some
research communities, for example databases, the
criteria for reproducible research are a lot stricter.
If the results are not reproducible, the results are
generally not considered valid. However, the large
number of parameters and implementation details
in NLP systems makes it hard to exactly repro-
duce published results based on the paper alone.
Therefore, we should encourage the dissemination
of results through software tools that make code
reproducible. To reproduce the results in a pa-
per, we essentially need the code, the data, and
the parameters of the experimental run that pro-
duced the results of the experiment. Fortunately,
the open source community has created great tools
that make this possible. First, social code reposi-
tory platforms, such as GitHub3, make it easy to
share code and data. In fact, the ease of shar-
ing and contributing to code has arguably acceler-
ated the progress in machine learning significantly.
Second, interactive computational environments,
</p>
<p>2http://www.lt-innovate.org
3https://github.com/
</p>
<p>such as Jupyter notebooks4, that tie together data,
code, and documentation, allow for reproducible
results that can easily be shared and published. Fi-
nally, software containers, such as Docker5, allow
light-weight virtualization that pulls in all soft-
ware dependencies and allow the same code to run
in a reliable and reproducible manner. If a Jupyter
notebook or Dockerfile is published with the pa-
per, it should be easier for other researchers to re-
produce results and integrate them into larger sys-
tems. Projects like CodaLab6 try to build online
platforms for reproducible research with similar
goals.
</p>
<p>On the problem of data availability, there is al-
ready a considerable amount of work in the area
of building NLP models in low-resource environ-
ments (see for example (Duong et al., 2014; Gar-
rette and Baldridge, 2013; Wang et al., 2015))
which deals with limited data availability. Tech-
niques like domain adaptation, semi-supervised
learning and transfer learning (Pan and Yang,
2010) are extremely relevant to address the prob-
lem of data availability for industry applications.
Finally, recent work on learning models from pri-
vate data (Papernot et al., 2016) and federated
learning across many devices (McMahan et al.,
2016) appear to be promising directions for prac-
tical NLP engineering research.
</p>
<p>3.3 Industry Papers
</p>
<p>I believe that there is an opportunity to increase
the exchange between industry and the research
community by establishing an industry paper sub-
mission format, potentially with its own industry
track at NLP conferences. Such a track could
offer a venue to discuss practical challenges in
building large-scale NLP systems and deploying
NLP models in production settings, such as scal-
ability, trade-offs between accuracy and compu-
tational costs, robustness, data quality, etc. This
would help to counter-balance the overemphasis
on test scores in pure research papers and aid the
adoption of research in industry applications. In-
dustry tracks are common in other communities
and have strong participation from industry play-
ers there.
</p>
<p>4http://jupyter.org/
5https://www.docker.com/
6http://codalab.org/
</p>
<p>95</p>
<p />
</div>
<div class="page"><p />
<p>4 Related Work
</p>
<p>Wagstaff (2012) argues for making machine learn-
ing research more relevant. He laments a hyper-
focus on UCI benchmark datasets and abstract
metrics. Spector et al. (2012) present Google’s
hybrid approach to research, which tries to avoid
separation between research and engineering. Re-
cently, several groups at Google have published
papers on practical challenges in deploying ma-
chine learning in production (Sculley et al., 2014;
McMahan et al., 2013; Breck et al., 2016). Belz
(2009) discusses the practical applications of NLP
research. Mani (2011) gives suggestions for im-
proving the review process. None of the works
provides a detailed discussion on the difficulties in
bringing NLP research to commercial products –
the main contribution of this paper.
</p>
<p>5 Conclusion
</p>
<p>I have highlighted difficulties that exist for re-
searchers who try to bring NLP research into com-
mercially products and offered suggestions for im-
proving the odds of commercial success. I hope
that my experience can stimulate creative thought
and a healthy discussion in the NLP community.
</p>
<p>References
Anja Belz. 2009. That’s nice... what can you do with
</p>
<p>it? Computational Linguistics 35(1).
</p>
<p>Daniel Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics 30(4).
</p>
<p>Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib,
and D. Sculley. 2016. Whats your ML test score? A
rubric for ML production systems. In Proceedings
of Reliable Machine Learning in the Wild - NIPS
2016 Workshop (2016).
</p>
<p>Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.
2013. Rule-based information extraction is dead!
long live rule-based information extraction systems!
In Proceedings of EMNLP.
</p>
<p>Clayton M. Christensen and Michael E. Raynor. 2013.
The Innovator’s Solution: Creating and Sustaining
Successful Srowth. Harvard Business Review Press.
</p>
<p>Long Duong, Trevor Cohn, Karin Verspoor, Steven
Bird, and Paul Cook. 2014. What can we get from
1000 tokens? A case study of multilingual POS tag-
ging for resource-poor languages. In Proceedings of
EMNLP.
</p>
<p>Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of HLT-NAACL.
</p>
<p>Mark Johnson. 2009. How the statistical revolution
changes (computational) linguistics. In Proceedings
of the EACL 2009 Workshop on the Interaction be-
tween Linguistics and Computational Linguistics.
</p>
<p>Inderjeet Mani. 2011. Improving our reviewing pro-
cesses. Computational Linguistics 37(1).
</p>
<p>H. Brendan McMahan, Gary Holt, D. Sculley, Michael
Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd
Phillips, Eugene Davydov, Daniel Golovin, Sharat
Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar
Hrafnkelsson, Tom Boulos, and Jeremy Kubica.
2013. Ad click prediction: a view from the trenches.
In Proceedings of ACM SIGKDD.
</p>
<p>H. Brendan McMahan, Eider Moore, Daniel Ramage,
Seth Hampson, and Blaise Agera y Arcas. 2016.
Communication-efficient learning of deep networks
from decentralized data. In Proceedings of AIS-
TATS.
</p>
<p>MIT Technology Review. 2016. AI Takes Off.
https://www.technologyreview.com/
business-report/ai-takes-off/. On-
line; accessed 22 April 2017.
</p>
<p>Alexander Osterwalder, Yves Pigneur, Gregory
Bernarda, and Alan Smith. 2014. Value proposition
design: How to create products and services
customers want. John Wiley &amp; Sons.
</p>
<p>Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on knowledge
and data engineering 22(10).
</p>
<p>Nicolas Papernot, Martı́n Abadi, Úlfar Erlingsson,
Ian Goodfellow, and Kunal Talwar. 2016. Semi-
supervised knowledge transfer for deep learning
from private training data.
</p>
<p>Eric Ries. 2011. The lean startup: How today’s en-
trepreneurs use continuous innovation to create rad-
ically successful businesses. Crown Business.
</p>
<p>D. Sculley, Gary Holt, Daniel Golovin, Eugene Davy-
dov, Todd Phillips, Dietmar Ebner, Vinay Chaud-
hary, and Michael Young. 2014. Machine learning:
The high interest credit card of technical debt. In
Proceedings of SE4ML: Software Engineering for
Machine Learning (NIPS 2014 Workshop).
</p>
<p>Alfred Spector, Peter Norvig, and Slav Petrov. 2012.
Google’s hybrid approach to research. Communica-
tions of the ACM 55(7).
</p>
<p>Donald E. Stokes. 1997. Pasteur’s quadrant: Basic
Science and Technological Innovation. Brookings
Institution Press.
</p>
<p>Kiri Wagstaff. 2012. Machine learning that matters. In
Proceedings of ICML.
</p>
<p>Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of ACL.
</p>
<p>96</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2016
</p>
<p>Sentence Alignment Methods for Improving Text Simplification Systems
</p>
<p>Sanja Štajner1, Marc Franco-Salvador2,3,
Simone Paolo Ponzetto1, Paolo Rosso3, Heiner Stuckenschmidt1
</p>
<p>1 DWS Research Group, University of Mannheim, Germany
2 Symanto Research, Nuremberg, Germany
</p>
<p>3 PRHLT Research Center, Universitat Politècnica de València, Spain
{sanja,simone,heiner}@informatik.uni-mannheim.de
marc.franco@symanto.net, prosso@prhlt.upv.es
</p>
<p>Abstract
</p>
<p>We provide several methods for sentence-
alignment of texts with different complex-
ity levels. Using the best of them, we
sentence-align the Newsela corpora, thus
providing large training materials for au-
tomatic text simplification (ATS) systems.
We show that using this dataset, even the
standard phrase-based statistical machine
translation models for ATS can outper-
form the state-of-the-art ATS systems.
</p>
<p>1 Introduction
</p>
<p>Automated text simplification (ATS) tries to auto-
matically transform (syntactically, lexically and/or
semantically) complex sentences into their simpler
variants without significantly altering the original
meaning. It has attracted much attention recently
as it could make texts more accessible to wider
audiences (Aluı́sio and Gasperin, 2010; Saggion
et al., 2015), and used as a pre-processing step,
improve performances of various NLP tasks and
systems (Vickrey and Koller, 2008; Evans, 2011;
Štajner and Popović, 2016).
</p>
<p>However, the state-of-the-art ATS systems still
do not reach satisfying performances and require
some human post-editing (Štajner and Popović,
2016). While the best supervised approaches gen-
erally lead to grammatical output with preserved
original meaning, they are overcautious, making
almost no changes to the input sentences (Spe-
cia, 2010; Štajner et al., 2015), probably due to
the limited size or bad quality of parallel TS
corpora used for training. The largest existing
sentence-aligned TS dataset for English is the
English Wikipedia – Simple English Wikipedia
</p>
<p>(EW–SEW) dataset, which contains 160-280,000
sentence pairs, depending on whether we want
to model only traditional sentence rewritings or
also to model content reduction and stronger para-
phrasing (Hwang et al., 2015). For Spanish, the
largest existing parallel TS corpus contains only
1,000 sentence pairs thus impeding the use of
fully supervised approaches. The best unsuper-
vised lexical simplification (LS) systems for En-
glish which leverage word-embeddings (Glavaš
and Štajner, 2015; Paetzold and Specia, 2016)
seem to perform more lexical substitutions but at
the cost of having less grammatical output and
more often changed meaning. However, there
have been no direct comparisons of supervised and
unsupervised state-of-the-art approaches so far.
</p>
<p>The Newsela corpora1 offers over 2,000 original
news articles in English and around 250 in Span-
ish, manually simplified to 3–4 different complex-
ity levels following strict guidelines (Xu et al.,
2015). Although it was suggested that it has bet-
ter quality than the EW–SEW corpus (Xu et al.,
2015), Newsela has not yet been used for train-
ing end-to-end ATS systems, due to the lack of its
sentence (and paragraph) alignments. Such align-
ments, between various text complexity levels,
would offer large training datasets for modelling
different levels of simplification, i.e. ‘mild’ sim-
plifications (using the alignments from the neigh-
bouring levels) and ‘heavy’ simplifications (using
the alignments of level pairs: 0–3, 0–4, 1–4).
</p>
<p>Contributions. We: (1) provide several meth-
ods for paragraph- and sentence alignment of
parallel texts, and for assessing similarity level
between pairs of text snippets, as freely avail-
</p>
<p>1Freely available: https://newsela.com/data/
</p>
<p>97</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2016">https://doi.org/10.18653/v1/P17-2016</a></div>
</div>
<div class="page"><p />
<p>able software;2 (2) compare the performances of
lexically- and semantically-based alignment meth-
ods across various text complexity levels; (3) test
the hypothesis that the original order of infor-
mation is preserved during manual simplification
(Bott and Saggion, 2011) by offering customized
MST-LIS alignment strategy (Section 3.1); and
(4) show that the new sentence-alignments lead to
the state-of-the-art ATS systems even in a basic
phrase-based statistical machine translation (PB-
SMT) approach to text simplifications.
</p>
<p>2 Related Work
</p>
<p>The current state-of-the-art systems for automatic
sentence-alignment of original and manually sim-
plified texts are the GSWN method (Hwang et al.,
2015) used for sentence-alignment of original
and simple English Wikipedia, and the HMM-
based method (Bott and Saggion, 2011) used for
sentence-alignment of the Spanish Simplext cor-
pus (Saggion et al., 2015).
</p>
<p>The HMM-based method can be applied to any
language as it does not require any language-
specific resources. It is based on two hypotheses:
(H1) that the original order of information is pre-
served, and (H2) that every ‘simple’ sentence has
at least one corresponding ‘original’ sentence (it
can have more than one in the case of ‘n-1’ or ‘n-
m’ alignments).
</p>
<p>As Simple Wikipedia does not represent direct
simplification of the ‘original’ Wikipedia articles
(‘simple’ articles were written independently of
the ‘original’ ones), GSWN method does not as-
sume H1 or H2. The main limitations of this
method are that it only allows for ‘1-1’ sentence
alignments – which is very restricting for TS as it
does not allow for sentence splitting (‘1-n’), and
summarisation and compression (‘n-1’ and ‘n-m’)
alignments – and it is language-dependent as it re-
quires English Wiktionary.
</p>
<p>Unlike the GSWN method, all the methods we
apply are language-independent, resource-light
and allow for ‘1-n’, ‘n-1’, and ‘n-m’ alignments.
Similar to the HMM-method, our methods assume
the hypothesis H2. We provide them in both vari-
ants, using the hypothesis H1 and without it (Sec-
tion 3.1).
</p>
<p>2https://github.com/neosyon/
SimpTextAlign
</p>
<p>3 Approach
</p>
<p>Having a set of ‘simple’ text snippets S and a set
of ‘complex’ text snippets C, we offer two strate-
gies (Section 3.1) to obtain the alignments (si, cj),
where si ∈ S, cj ∈ C. Each alignment strategy,
in turn, can use one of the three methods (Sec-
tion 3.2) to calculate similarity scores between text
snippets (either paragraphs or sentences).
</p>
<p>3.1 Alignment strategies
</p>
<p>Most Similar Text (MST): Given one of the sim-
ilarity methods (Section 3.2), MST compares sim-
ilarity scores of all possible pairs (si, cj), and
aligns each si ∈ S with the closest one in C.
MST with Longest Increasing Sequence (MST-
LIS): MST-LIS uses the hypothesis H1. It first
uses the MST strategy, and then postprocess the
output by extracting – from all obtained align-
ments – only those alignments li ∈ L, which con-
tain the longest increasing sequence of offsets jk
in C. In order to allow for ‘1–n’ alignments (i.e.
sentence splitting), we allow for repeated offsets
of C (‘complex’ text snippets) in L. The ‘simple’
text snippets not contained in L are included in the
set U of unaligned snippets. Finally, we align each
um ∈ U by restricting the search space in C to
those offsets of ‘complex’ text snippets that corre-
spond to the previous and the next aligned ‘simple’
snippets. For instance, if L = {(s1, c4), (s3, c7)}
and U = {s2}, then the search space for the align-
ments of s2 is reduced to {c4...c7}. We denote
this strategy with an ‘*’ in the results (Table 2),
e.g. C3G*.
</p>
<p>3.2 Similarity Methods
</p>
<p>C3G: We employ the Character N -Gram
(CNG) (Mcnamee and Mayfield, 2004) similarity
model (for n = 3) with log TF-IDF weight-
ing (Salton and McGill, 1986) and compare
vectors using the cosine similarity.
WAVG: We use the continuous skip-gram
model (Mikolov et al., 2013b) of the TensorFlow
toolkit3 to process the whole English Wikipedia
and generate continuous representations of its
words.4 For each text snippet, we average its
word vectors to obtain a single representation of
its content as this setting has shown good results
</p>
<p>3https://www.tensorflow.org/
4We use 300-dimensional vectors, context windows of
</p>
<p>size 10, and 20 negative words for each sample, in all our
continuous word-based models.
</p>
<p>98</p>
<p />
</div>
<div class="page"><p />
<p>Match Transformation Original Simple
Full syntactic simplification;
</p>
<p>reordering of sentence
constituents
</p>
<p>During the 13th century, gingerbread was
brought to Sweden by German immi-
grants.
</p>
<p>German immigrants brought it to Sweden
during the 13th century.
</p>
<p>Full lexical paraphrasing During the 13th century, gingerbread was
brought to Sweden by German immi-
grants.
</p>
<p>German immigrants brought it to Sweden
during the 13th century.
</p>
<p>Partial strong paraphrasing Gingerbread foods vary, ranging from a
soft, moist loaf cake to something close
to a ginger biscuit.
</p>
<p>Gingerbread is a word which describes
different sweet food products from soft
cakes to a ginger biscuit.
</p>
<p>Partial adding explanations Humidity is the amount of water vapor in
the air.
</p>
<p>Humidity (adjective: humid) refers to
water vapor in the air, but not to liquid
droplets in fog, clouds, or rain.
</p>
<p>Partial sentence compression Falaj irrigation is an ancient system dat-
ing back thousands of years and is used
widely in Oman, the UAE, China, Iran
and other countries.
</p>
<p>The ancient falaj system of irrigation is
still in use in some areas.
</p>
<p>Table 1: Examples of full and partial matches from the EW–SEW dataset (Hwang et al., 2015).
</p>
<p>in other NLP tasks (e.g. for selecting out-of-the-
list words (Mikolov et al., 2013a)). Finally, the
similarity between text snippets is estimated using
the cosine similarity.
CWASA: We employ the Continuous Word
Alignment-based Similarity Analysis (CWASA)
model (Franco-Salvador et al., 2016), which finds
the optimal word alignment by computing cosine
similarity between continuous representations of
all words (instead of averaging word vectors as in
the case of WAVG). It was originally proposed for
plagiarism detection with excellent results, espe-
cially for longer text snippets.
</p>
<p>4 Manual Evaluation
</p>
<p>To compare the performances of different align-
ment methods, we randomly selected 10 original
texts (Level 0) and their corresponding simpler
versions at Levels 1, 3 and 4. Instead of creating a
‘gold standard’ and then automatically evaluating
the performances, we asked two annotators to rate
each pair of automatically aligned paragraphs and
sentences – by each of the possible six alignment
methods and the HMM-based method (Bott and
Saggion, 2011) – for three pairs of text complexity
levels (0–1, 0–4, and 3–4) on a 0–2 scale, where: 0
– no semantic overlap in the content; 1 – partial se-
mantic overlap (partial matches); 2 – same seman-
tic content (good matches). This resulted in a total
of 1526 paragraph- and 1086 sentence-alignments
for the 0–1 pairs, and 1218 paragraph- and 1266
sentence-alignments for the 0–4 and 3–4 pairs. In
the context of TS, both good- and partial matches
</p>
<p>are important. While full semantic overlap models
full paraphrases (‘1-1’ alignments), partial over-
lap models sentence splitting (“1-n” alignments),
deleting irrelevant sentence parts, adding explana-
tions, or summarizing (‘n-m’ alignments). Sev-
eral examples of full and partial matches from the
EW–SEW dataset (Hwang et al., 2015) are given
in Table 1.
</p>
<p>We expect that the automatic-alignment task is
the easiest between the 0–1 text complexity lev-
els, and much more difficult between the 0-4 levels
(Level 4 is obtained after four stages of simplifi-
cation and thus contains stronger paraphrases and
less lexical overlap with Level 0 than Level 1 has).
We also explore whether the task is equally diffi-
cult whenever we align two neighbouring levels,
or the difficulty of the task depends on the level
complexity (0–1 vs. 3–4). The obtained inter-
annotator agreement, weighted Cohen’s κ (on 400
double-annotated instances) was between 0.71 and
0.74 depending on the task and levels.
</p>
<p>The results of the manual analysis (Table 2)
showed that: (1) all applied methods significantly
(p &lt; 0.001) outperformed the HMM method on
both paragraph- and sentence-alignment tasks;5
</p>
<p>(2) the methods which do not assume hypothe-
sis H1 (C3G, CWASA, and WAVG) led to (not
significantly) higher percentage of correct align-
ments than their counterparts which do assume
</p>
<p>5Although some of our methods share the same percent-
age of good+partial matches with the HMM method on the
paragraph-alignment 0–1 task, there is still significant differ-
ence in the obtained scores (in some cases, our methods led to
good matches whereas the HMM only led to partial matches).
</p>
<p>99</p>
<p />
</div>
<div class="page"><p />
<p>Method
Sentence Paragraph
</p>
<p>0–1 0–4 3–4 0–1 0–4 3–4
C3G 98.3 56.1 81.1 98.6 86.8 95.2
C3G* 96.7 54.7 78.8 95.4 77.0 92.3
CWASA 98.3 45.3 79.7 98.2 83.3 94.1
CWASA* 96.1 42.1 76.4 95.4 74.1 90.5
WAVG 97.8 56.1 79.7 96.8 75.9 91.7
WAVG* 96.1 50.0 79.7 96.8 69.5 89.3
C3G-2s 98.5 57.8 83.5 / / /
HMM 86.2 25.2 65.6 96.8 41.2 65.5
</p>
<p>Table 2: Percentage of good+partial sentence- and
paragraph-alignments on the English Newsela cor-
pus. All results are significantly better (p &lt; 0.001,
Wilcoxon’s signed rank test) than those obtained
by the HMM method (Bott and Saggion, 2011).
The best scores are in bold.
</p>
<p>H1 (C3G*, CWASA*, WAVG*); (3) the differ-
ence in the performances of the lexical approach
(C3G) and semantic approaches (CWASA and
WAVG) was significant only in the 0–4 sentence-
alignment task, where CWASA performed sig-
nificantly worse (p &lt; 0.001) than the other
two methods, and in the 0–4 paragraph-alignment
task, where WAVG performed significantly worse
than C3G; (4) the 2-step C3G alignment-method
(C3G-2s), which first aligns paragraphs using the
best paragraph-alignment method (C3G) and then
within each paragraph align sentences with the
best sentence-alignment method (C3G), led to
more good+partial alignments than the ‘direct’
sentence-alignment C3G method.
</p>
<p>5 Extrinsic Evaluation
</p>
<p>Finally, we test our new English Newsela (C3G-
2s) sentence-alignments (both for the neighbour-
ing levels – neighb. and for all levels – all) and
Newsela sentence-alignments for neighboring lev-
els obtained with HMM-method6 (Bott and Sag-
gion, 2011) in the ATS task using standard PB-
SMT models7 in the Moses toolkit (Koehn et al.,
2007). We vary the training dataset and the cor-
pus used to build language models (LMs), while
keeping always the same 2,000 sentence pairs for
tuning (Xu et al., 2016) and the first 70 sentence
</p>
<p>6Given that the performance of the HMM-method was
poor for non-neighboring levels (Table 2).
</p>
<p>7GIZA++ implementation of the IBM word alignment
model 4 (Och and Ney, 2003), refinement and phrase-
extraction heuristics (Koehn et al., 2003), the minimum error
rate training (Och, 2003) for tuning, and 5-gram LMs with
Kneser-Ney smoothing trained with SRILM (Stolcke, 2002).
</p>
<p>pairs of their test set8 for our human evaluation.
Using that particular test set allow us to compare
our (PBSMT) systems with the output of the state-
of-the-art syntax-based MT (SBMT) system for
TS (Xu et al., 2016) which is not freely available.
We compare: (1) the performance of the standard
PBSMT model which uses only the already avail-
able EW–SEW dataset (Hwang et al., 2015) with
the performances of the same PBSMT models but
this time using the combination of the EW–SEW
dataset and our newly-created Newsela datasets;
(2) the latter PBSMT models (which use both
EW–SEW and new Newsela datasets) against the
state-of-the-art supervised ATS system (Xu et al.,
2016), and one of the recently proposed unsuper-
vised lexical simplification systems, the LightLS
system (Glavaš and Štajner, 2015).9
</p>
<p>We perform three types of human evaluation on
the outputs of all systems. First, we count the total
number of changes made by each system (Total),
counting the change of a whole phrase (e.g. “be-
come defunct”→ “was dissolved”) as one change.
We mark as Correct those changes that preserve
the original meaning and grammaticality of the
sentence (assessed by two native English speak-
ers) and, at the same time, make the sentence
easier to understand (assessed by two non-native
fluent English speakers).10 Second, three native
English speakers rate the grammaticality (G) and
meaning preservation (M) of each sentence with
at least one change on a 1–5 Likert scale (1 –
very bad; 5 – very good). Third, the three non-
native fluent English speakers were shown original
(reference) sentences and target (output) sentences
(one pair at the time) and asked whether the target
sentence is: +2 – much simpler; +1 – somewhat
simpler; 0 – equally difficult; -1 – somewhat more
difficult; -2 – much more difficult, than the refer-
ence sentence. While the correctness of changes
takes into account the influence of each individual
change on grammaticality, meaning and simplic-
ity of a sentence, the Scores (G and M) and Rank
(S) take into account the mutual influence of all
changes within a sentence.
</p>
<p>Adding our sentence-aligned Newsela corpus
</p>
<p>8Both freely available from: https://github.com/
cocoxu/simplification/
</p>
<p>9We use the output of the original SBMT (Xu et al., 2016)
and LightLS (Glavaš and Štajner, 2015) systems, obtained
from the authors.
</p>
<p>10Those cases in which the two annotators did not agree are
additionally evaluated by a third annotator to obtain majority.
</p>
<p>100</p>
<p />
</div>
<div class="page"><p />
<p>Approach
Training LM Changes Scores Rank
</p>
<p>Dataset Size (sent) Corpus Size (sent) Total Correct G M S
PBSMT Wiki(Good+Partial) 284,499 Wiki 391,572 76 27 (35.5%) 4.09 3.31 0.26
PBSMT Newsela(neighb. C3G-2s)+Wiki 593,947 Newsela+Wiki 766,446 81 38 (46.9%) 4.40 3.84 0.30
PBSMT Newsela(all C3G-2s)+Wiki 764,572 Newsela+Wiki 766,446 87 42 (48.3%) 4.25 3.73 0.30
PBSMT Newsela(neighb.-HMM)+Wiki 584,106 Newsela+Wiki 766,446 75 33 (44.0%) 4.20 3.65 0.20
</p>
<p>s.o.t.a.
supervised SBMT (PPDB+SARI) (Xu et al., 2016) 143 49 (34.3%) 4.28 3.57 0.03
unsupervised (LightLS) (Glavaš and Štajner, 2015) 132 35 (26.6%) 4.47 2.67 -0.01
</p>
<p>Table 3: Extrinsic evaluation (PBSMT-based automatic text simplification systems vs. state of the art).
</p>
<p>System Output
Original He advocates applying a user-centered design process in product develop-
</p>
<p>ment cycles and also works towards popularizing interaction design as a
mainstream discipline.
</p>
<p>PBSMT (Newsela neighb. C3G-2s + Wiki) He advocates a user-centered design process in product development cy-
cles and also works for popularizing interaction design as a mainstream
discipline.
</p>
<p>PBSMT (Newsela all C3G-2s + Wiki) He supports a user-centered design process in product development cycles
and also works for popularizing interaction design as a mainstream disci-
pline.
</p>
<p>PBSMT (Newsela HMM neighb. + Wiki) He advocates a user-centered design process in product development cy-
cles and also works towards popularizing interaction design as a main-
stream discipline.
</p>
<p>PBSMT (Wiki) He advocates applying a user-centered design process in product develop-
ment cycles and also works towards popularizing interaction design as a
mainstream discipline.
</p>
<p>SBMT (Xu et al., 2016) He advocates using a user-centered design process in product development
cycles and also works for popularizing trade design as a whole field.
</p>
<p>LightLS He argues allowing a user-centered design process in product development
cycles and also works towards popularizing interaction design as a main-
stream discipline.
</p>
<p>Table 4: Outputs of different ATS systems (the correct changes/simplifications are shown in bold and the
incorrect ones in italics).
</p>
<p>(either neighb. C3G-2l or all C3G-2l) to the cur-
rently best sentence-aligned Wiki corpus (Hwang
et al., 2015) in a standard PBSMT setup signifi-
cantly11 improves grammaticality (G) and mean-
ing preservation (M), and increases the percent-
age of correct changes (Table 3). It also signif-
icantly outperforms the state-of-the-art ATS sys-
tems by simplicity rankings (S), meaning preser-
vation (M), and number of correct changes (Cor-
rect), while achieving almost equally good gram-
maticality (G).
</p>
<p>The level of simplification applied in the train-
ing dataset (Newsela neighb. C3G-2s vs. Newsela
all C3G-2s) significantly influences G and M
scores.
</p>
<p>The use of the HMM-method for aligning
Newsela (instead of ours) lead to significantly
worse simplifications by all five criteria.
</p>
<p>11Wilcoxon’s signed rank test, p &lt; 0.001.
</p>
<p>An example of the outputs of different ATS sys-
tems is presented in Table 4.
</p>
<p>6 Conclusions
</p>
<p>We provided several methods for paragraph-
and sentence-alignment from parallel TS corpora,
made the software publicly available, and showed
that the use of the new sentence-aligned (freely
available) Newsela dataset leads to state-of-the-art
ATS systems even in a basic PBSMT setup. We
also showed that lexically-based C3G method is
superior to semantically-based methods (CWASA
and WAVG) in aligning paraphraphs and sentences
with ‘heavy’ simplifications (0–4 alignments), and
that 2-step sentence alignment (aligning first para-
graphs and then sentences within the paragraphs)
lead to more correct alignments than the ‘direct’
sentence alignment.
</p>
<p>101</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgments
</p>
<p>This work has been partially supported by the
SFB 884 on the Political Economy of Reforms at
the University of Mannheim (project C4), funded
by the German Research Foundation (DFG), and
also by the SomEMBED TIN2015-71147-C2-1-P
MINECO research project.
</p>
<p>References
Sandra Maria Aluı́sio and Caroline Gasperin. 2010.
</p>
<p>Fostering Digital Inclusion and Accessibility: The
PorSimples project for Simplification of Portuguese
Texts. In Proceedings of YIWCALA Workshop at
NAACL-HLT 2010. pages 46–53.
</p>
<p>Stefan Bott and Horacio Saggion. 2011. An Unsu-
pervised Alignment Algorithm for Text Simplifica-
tion Corpus Construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation.
ACL, pages 20–26.
</p>
<p>Richard J. Evans. 2011. Comparing methods for
the syntactic simplification of sentences in informa-
tion extraction. Literary and Linguistic Computing
26(4):371–388.
</p>
<p>Marc Franco-Salvador, Parth Gupta, Paolo Rosso,
and Rafael E. Banchs. 2016. Cross-language
plagiarism detection over continuous-space- and
knowledge graph-based representations of language.
Knowledge-Based Systems 111:87 – 99.
</p>
<p>Goran Glavaš and Sanja Štajner. 2015. Simplifying
Lexical Simplification: Do We Need Simplified Cor-
pora? In Proceedings of the ACL&amp;IJCNLP 2015
(Volume 2: Short Papers). pages 63–68.
</p>
<p>William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,
and Wei Wu. 2015. Aligning Sentences from Stan-
dard Wikipedia to Simple Wikipedia. In Proceed-
ings of NAACL&amp;HLT . pages 211–217.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL. pages 177–180.
</p>
<p>Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the NAACL&amp;HLT, Vol. 1. pages 48–54.
</p>
<p>Paul Mcnamee and James Mayfield. 2004. Character
n-gram tokenization for European language text re-
trieval. Information Retrieval 7(1):73–97.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
</p>
<p>at International Conference on Learning Represen-
tations.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26. pages 3111–3119.
</p>
<p>Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of the
ACL. pages 160–167.
</p>
<p>Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics 29(1):19–51.
</p>
<p>Gustavo Henrique Paetzold and Lucia Specia. 2016.
Unsupervised lexical simplification for non-native
speakers. In Proceedings of the 30th AAAI.
</p>
<p>Horacio Saggion, Sanja Štajner, Stefan Bott, Simon
Mille, Luz Rello, and Biljana Drndarevic. 2015.
Making It Simplext: Implementation and Evaluation
of a Text Simplification System for Spanish. ACM
Transactions on Accessible Computing 6(4):14:1–
14:36.
</p>
<p>Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc.
</p>
<p>Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of the 9th PRO-
POR. Springer Berlin Heidelberg, volume 6001 of
Lecture Notes in Computer Science, pages 30–39.
</p>
<p>Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP). pages 901–904.
</p>
<p>David Vickrey and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL&amp;HLT . volume 344–352.
</p>
<p>Sanja Štajner, Hannah Bechara, and Horacio Saggion.
2015. A Deeper Exploration of the Standard PB-
SMT Approach to Text Simplification and its Eval-
uation. In Proceedings of ACL&amp;IJCNLP (Volume 2:
Short Papers). pages 823–828.
</p>
<p>Sanja Štajner and Maja Popović. 2016. Can Text Sim-
plification Help Machine Translation? Baltic Jour-
nal of Modern Computing 4(2):230–242.
</p>
<p>Wei Xu, Chris Callison-Burch, and Courtney Napoles.
2015. Problems in Current Text Simplification Re-
search: New Data Can Help. Transactions of the
Association for Computational Linguistics (TACL)
3:283–297.
</p>
<p>Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze
Chen, and Chris Callison-Burch. 2016. Optimizing
statistical machine translation for text simplification.
Transactions of the Association for Computational
Linguistics 4:401–415.
</p>
<p>102</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 103–109
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2017
</p>
<p>Understanding Task Design Trade-offs in
Crowdsourced Paraphrase Collection
</p>
<p>Youxuan Jiang, Jonathan K. Kummerfeld and Walter S. Lasecki
Computer Science &amp; Engineering
</p>
<p>University of Michigan, Ann Arbor
{lyjiang,jkummerf,wlasecki}@umich.edu
</p>
<p>Abstract
</p>
<p>Linguistically diverse datasets are criti-
cal for training and evaluating robust ma-
chine learning systems, but data collection
is a costly process that often requires ex-
perts. Crowdsourcing the process of para-
phrase generation is an effective means
of expanding natural language datasets,
but there has been limited analysis of the
trade-offs that arise when designing tasks.
In this paper, we present the first system-
atic study of the key factors in crowdsourc-
ing paraphrase collection. We consider
variations in instructions, incentives, data
domains, and workflows. We manually an-
alyzed paraphrases for correctness, gram-
maticality, and linguistic diversity. Our
observations provide new insight into the
trade-offs between accuracy and diversity
in crowd responses that arise as a result of
task design, providing guidance for future
paraphrase generation procedures.
</p>
<p>1 Introduction
</p>
<p>Paraphrases are useful for a range of tasks, includ-
ing machine translation evaluation (Kauchak and
Barzilay, 2006), semantic parsing (Wang et al.,
2015), and question answering (Fader et al., 2013).
Crowdsourcing has been widely used as a scal-
able and cost-effective means of generating para-
phrases (Negri et al., 2012; Wang et al., 2012;
Tschirsich and Hintz, 2013), but there has been
limited analysis of the factors influencing diversity
and correctness of the paraphrases workers write.
</p>
<p>In this paper, we perform a systematic investiga-
tion of design decisions for crowdsourcing para-
phrases, including the first exploration of worker
incentives for paraphrasing. For worker incen-
tives, we either provide a bonus payment when
a paraphrase is novel (encouraging diversity) or
</p>
<p>when it matches a paraphrase from another worker
(encouraging agreement/correctness). We also
varied the type of example paraphrases shown
to workers, the number of paraphrases requested
from each worker per sentence, the subject do-
main of the data, whether to show answers to ques-
tions, and whether the prompt sentence is the same
for multiple workers or varies, with alternative
prompts drawn from the output of other workers.
</p>
<p>Effective paraphrasing has two desired proper-
ties: correctness and diversity. To measure cor-
rectness, we hand-labeled all paraphrases with
semantic equivalence and grammaticality scores.
For diversity, we measure the fraction of para-
phrases that are distinct, as well as Paraphrase
In N-gram Changes (PINC), a measure of n-gram
variation. We have released all 2,600 paraphrases
along with accuracy annotations. Our analysis
shows that the most important factor is how work-
ers are primed for a task, with the choice of ex-
amples and the prompt sentence affecting diversity
and correctness significantly.
</p>
<p>2 Related Work
</p>
<p>Previous work on crowdsourced paraphrase gener-
ation fits into two categories: work on modifying
the creation process or workflow, and studying the
effect of prompting or priming on crowd worker
output. Beyond crowdsourced generation, other
work has explored using experts or automated sys-
tems to generate paraphrases.
</p>
<p>2.1 Workflows for Crowd-Paraphrasing
</p>
<p>The most common approach to crowdsourcing
paraphrase generation is to provide a sentence as
a prompt and request a single paraphrase from a
worker. One frequent addition is to ask a differ-
ent set of workers to evaluate whether a generated
paraphrase is correct (Buzek et al., 2010; Burrows
et al., 2013). Negri et al. (2012) also explored an
alternate workflow in which each worker writes
</p>
<p>103</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2017">https://doi.org/10.18653/v1/P17-2017</a></div>
</div>
<div class="page"><p />
<p>two paraphrases, which are then given to other
workers as the prompt sentence, forming a binary
tree of paraphrases. They found that paraphrases
deeper in the tree were more diverse, but under-
standing how correctness and grammaticality vary
across such a tree still remains an open ques-
tion. Near real-time crowdsourcing (Bigham et al.,
2010) allowed Lasecki et al. (2013a) to elicit vari-
ations on entire conversations by providing a set-
ting and goal to pairs of crowd workers. Continu-
ous real-time crowdsourcing (Lasecki et al., 2011)
allows Chorus Lasecki et al. (2013b) users to hold
conversations with groups of crowd workers as if
the crowd was a single individual, allowing for the
collection of example conversations in more real-
istic settings. The only prior work regarding in-
centives we are aware of is by Chklovski (2005),
who collected paraphrases in a game where the
goal was to match an existing paraphrase, with ex-
tra points awarded for doing so with fewer hints.
The disadvantage of this approach was that 29% of
the collected paraphrases were duplicates. In our
experiments, duplication ranged from 1% to 13%
in each condition.
</p>
<p>2.2 The Effects of Priming
When crowd workers perform a task, they are
primed (influenced) by the examples, instructions,
and context that they see. This priming can re-
sult in systematic variations in the resulting para-
phrases. Mitchell et al. (2014) showed that pro-
viding context, in the form of previous utterances
from a dialogue, only provides benefits once four
or more are included. Kumaran et al. (2014)
provided drawings as prompts, obtaining diverse
paraphrases, but without exact semantic equiva-
lence. When each sentence expresses a small set
of slot-filler predicates, Wang et al. (2012) found
that providing the list of predicates led to slightly
faster paraphrasing than giving either a complete
sentence or a short sentence for each predicate. We
further expand on this work by exploring how the
type of examples shown affects paraphrasing.
</p>
<p>2.3 Expert and Automated Generation
Finally, there are two general lines of research
on paraphrasing not focused on using crowds.
The first of these is the automatic collection of
paraphrases from parallel data sources, such as
translations of the same text or captions for the
same image (Ganitkevitch et al., 2013; Chen and
Dolan, 2011; Bouamor et al., 2012; Pavlick et al.,
</p>
<p>Paraphrase/Reword Sentences
</p>
<p>For each sentence below, please write 2 new sentence
that express the same meaning in different ways (para-
phrase/reword).
</p>
<p>For example: ’Which 400 level courses don’t have labs?’
could be rewritten as:
</p>
<p>• Of all the 400 level courses, which ones do not in-
clude labs?
</p>
<p>• What are the 400 level courses without lab sessions?
</p>
<p>BONUS: You will receive 5 cents bonus for each sentence
you write that matches one written by another worker on
the task.
</p>
<p>Figure 1: Baseline task instructions.
</p>
<p>2015). These resources are extremely large, but
usually (1) do not provide the strong semantic
equivalence we are interested in, and (2) focus on
phrases rather than complete sentences. The sec-
ond line of work explores the creation of lattices
that compactly encode hundreds of thousands of
paraphrases (Dreyer and Marcu, 2012; Bojar et al.,
2013). Unfortunately, these lattices are typically
expensive to produce, taking experts one to three
hours per sentence.
</p>
<p>3 Experimental Design
</p>
<p>We conducted a series of experiments to investi-
gate factors in crowdsourced paraphrase creation.
To do so in a controlled manner, we studied a sin-
gle variation per condition.
</p>
<p>3.1 Definition of Valid Paraphrases
</p>
<p>This project was motivated by the need for
strongly equivalent paraphrases in semantic pars-
ing datasets. We consider two sentences para-
phrases if they would have equivalent interpreta-
tions when represented as a structured query, i.e.,
”a pair of units of text deemed to be interchange-
able” (Dras, 1999). For example:
</p>
<p>Prompt: Which upper-level classes are four credits?
Are there any four credit upper-level classes?
</p>
<p>We considered the above two questions as para-
phrases since they are both requests for a list
of classes, explicit and implicit, respectively, al-
though the second one is a polar question and the
first one is not. However:
</p>
<p>Prompt:Which is easier out of EECS 378 and EECS 280?
Is EECS 378 easier than EECS 280?
</p>
<p>We did not consider the above two questions as
paraphrases since the first one is requesting one of
</p>
<p>104</p>
<p />
</div>
<div class="page"><p />
<p>two class options and the second one is requesting
a yes or no answer.
</p>
<p>3.2 Baseline
</p>
<p>We used Amazon Mechanical Turk, presenting
workers with the instructions and examples in Fig-
ure 1. Workers were shown prompt sentences
one at a time, and asked to provide two para-
phrases for each. To avoid confusion or training
effects between different conditions, we only al-
lowed workers to participate once across all con-
ditions. The initial instructions shown to work-
ers were the same across all conditions (variations
were only seen after a worker accepted the task).
</p>
<p>Workers were paid 5 cents per paraphrase they
wrote plus, once all workers were done, a 5
cent bonus for paraphrases that matched another
worker’s paraphrase in the same condition. While
we do not actually want duplicate paraphrases,
this incentive may encourage workers to more
closely follow the instructions, producing gram-
matical and correct sentences. We chose this pay-
ment rate to give around minimum wage, estimat-
ing time based on prior work.
</p>
<p>3.3 Conditions
</p>
<p>Examples We provided workers with an ex-
ample prompt sentence and two paraphrases, as
shown in Figure 1. We showed either: no ex-
amples (No Examples), two examples with lexi-
cal changes only (Lexical Examples), one exam-
ple with lexical changes and one with syntactic
changes (Mixed Examples), or two examples that
each contained both lexical and syntactic changes
(Baseline). The variations between these condi-
tions may prime workers differently, leading them
to generate different paraphrases.
</p>
<p>Incentive The 5 cent bonus payment per para-
phrase was either not included (No Bonus),
awarded for each sentence that was a duplicate at
the end of the task (Baseline), or awarded for each
sentence that did not match any other worker’s
paraphrase (Novelty Bonus). Bonuses that depend
on other workers’ actions may encourage either
creativity or conformity. We did not vary the base
level of payment because prior work has found
that workers work quality is not increased by in-
creased financial incentives due to an anchoring
effect relative to the base rate we define (Mason
and Watts, 2010).
</p>
<p>Workflow We considered three variations to
workflow. First, for each sentence, we either asked
workers to provide two paraphrases (Baseline), or
one (One Paraphrase). Asking for multiple para-
phrases reduces duplication (since workers will
not repeat themselves), but may result in lower di-
versity. Second, since our baseline prompt sen-
tences are questions, we ran a condition with an-
swers shown to workers (Answers). Third, we
started all conditions with the same set of prompt
sentences, but once workers had produced para-
phrases, we had the option to either prompt future
workers with the original prompt, or to use para-
phrase from another worker. Treating sentences as
points and the act of paraphrasing as creating an
edge, the space can be characterized as a graph.
We prompted workers with either the original sen-
tences only (Baseline), or formed a chain struc-
tured graph by randomly choosing a sentence that
was (1) not a duplicate, and (2) furthest from the
original sentence (Chain). These changes could
impact paraphrasing because the prompt sentence
is a form of priming.
</p>
<p>Data domains We ran with five data sources:
questions about university courses (Baseline),
messages from dialogues between two stu-
dents in a simulated academic advising ses-
sion (ADVISING), questions about US geogra-
phy (GEOQUERY Tang and Mooney, 2001), text
from the Wall Street Journal section of the Penn
Treebank (WSJ Marcus et al., 1993), and discus-
sions on the Ubuntu IRC channel (UBUNTU). We
randomly selected 20 sentences as prompts from
each data source with the lengths representative of
the sentence length distribution in that source.
</p>
<p>3.4 Metrics
</p>
<p>Semantic Equivalence For a paraphrase to be
valid, its meaning must match the original sen-
tence. To assess this match, two of the authors—
one native speaker and one non-native but fluent
speaker—rated every sentence independently, then
discussed every case of disagreement to determine
a consensus judgement. Prior to the consensus-
finding step, the inter-annotator agreement kappa
scores were .50 for correctness (moderate agree-
ment), and .36 for grammaticality (fair agree-
ment) (Altman, 1990). For the results in Table 1,
we used a χ2 test to measure significance, since
this is a binary classification process.
</p>
<p>105</p>
<p />
</div>
<div class="page"><p />
<p>Grammaticality We also judged whether the
sentences were grammatical, again with two an-
notators rating every sentence and resolving dis-
agreements. Again, since this was a binary classi-
fication, we used a χ2 test for significance.
</p>
<p>Time The time it takes to write paraphrases is
important for estimating time-to-completion, and
ensuring workers receive fair payment. We mea-
sured the time between when a worker submitted
one pair of paraphrases and the next. The first
paraphrase was excluded since it would skew the
data by including the time spent reading the in-
structions and understanding the task. We report
the median time to avoid skewing due to outliers,
e.g. a value of five minutes when a worker prob-
ably took a break. We apply Mood’s Median test
for statistical significance.
</p>
<p>Diversity We use two metrics for diversity, mea-
sured over correct sentences only. First, a simple
measurement of exact duplication: the number of
distinct paraphrases divided by the total number
of paraphrases, as a percentage (Distinct). Sec-
ond, a measure of n-gram diversity (PINC Chen
and Dolan, 2011)1. In both cases, a higher score
means greater diversity. For PINC, we used a t-
test for statistical significance, and for Distinct we
used a permutation test.
</p>
<p>4 Results
</p>
<p>We collected 2600 paraphrases: 10 paraphrases
per sentence, for 20 sentences, for each of the
13 conditions. The cost, including initial testing,
was $196.30, of which $20.30 was for bonus pay-
ments. Table 1 shows the results for all metrics.
</p>
<p>4.1 Discussion: Task Variation
</p>
<p>Qualitatively, we observed a wide variety of lexi-
cal and syntactic changes, as shown by these ex-
ample prompts and paraphrases (one low PINC
and one high PINC in each case):
</p>
<p>Prompt: How long has EECS 280 been offered for?
How long has EECS 280 been offered?
EECS 280 has been in the course listings how many years?
</p>
<p>Prompt: Can I take 280 on Mondays and Wednesdays?
On Mondays and Wednesdays, can I take 280?
Is 280 available as a Monday/Wednesday class?
</p>
<p>There was relatively little variation in grammat-
icality or time across the conditions. The times
</p>
<p>1 We also considered BLEU (Papineni et al., 2002), which
measures n-gram overlap and is used as a proxy for correct-
ness in MT. As expected, it strongly correlated with PINC.
</p>
<p>Accuracy (%) Time Diversity
Condition Corr Gram (s) Distinct PINC
</p>
<p>Baseline 74 97 36 99 68
Lexical Examples 90† 98 27 93 55†
Mixed Examples 89† 96 36 87† 58†
No Examples 84 96 30 95 63
Novelty Bonus 72 96 30 99 69
No Bonus 78 94 28 99 66
One Paraphrase 82 89 38 96 65
Chain 68 94 25 98 74
Answers 80 94 29 96 65
ADVISING 78 94 31 97 70
GEOQUERY 77 85† 25† 94 63
WSJ 68 90 61† 94† 38†
UBUNTU 56† 92 44 97 67
</p>
<p>Table 1: Variation across conditions for a range of
metrics (defined in § 3.4). Bold indicates a statisti-
cally significant difference compared to the base-
line at the 0.05 level, and a † indicates significance
at the 0.01 level, both after applying the Holm-
Bonferroni method across each row (Holm, 1979).
</p>
<p>we observed are consistent with prior work: e.g.
Wang et al. (2015) report ∼28 sec/paraphrase.
</p>
<p>Priming had a major impact, with the shift to
lexical examples leading to a significant improve-
ment in correctness, but much lower diversity. The
surprising increase in correctness when providing
no examples has a p-value of 0.07 and probably
reflects random variation in the pool of workers.
Meanwhile, changing the incentives by providing
either a bonus for novelty, or no bonus at all, did
not substantially impact any of the metrics.
</p>
<p>Changing the number of paraphrases written by
each worker did not significantly impact diversity
(we worried that collecting more than one may
lead to a decrease). We further confirmed this
by calculating PINC between the two paraphrases
provided by each user, which produced scores
similar to comparing with the prompt. How-
ever, the One Paraphrase condition did have lower
grammaticality, emphasizing the value of evaluat-
ing and filtering out workers who write ungram-
matical paraphrases.
</p>
<p>Changing the source of the prompt sentence to
create a chain of paraphrases led to a significant
increase in diversity. This fits our intuition that
the prompt is a form of priming. However, cor-
rectness decreases along the chain, suggesting the
need to check paraphrases against the original sen-
tence during the overall process, possibly using
other workers as described in § 2.1. Meanwhile,
showing the answer to the question being para-
</p>
<p>106</p>
<p />
</div>
<div class="page"><p />
<p>phrased did not significantly affect correctness or
diversity, and in 2.5% of cases workers incorrectly
used the answer as part of their paraphrase.
</p>
<p>We also analyzed the distribution of incorrect
or ungrammatical paraphrases by worker. 7%
of workers accounted for 25% of incorrect para-
phrases, while the best 30% of workers made no
mistakes at all. Similarly, 8% of workers wrote
50% of the ungrammatical paraphrases, while
70% of workers wrote only grammatical para-
phrases. Many crowdsourcing tasks address these
issues by showing workers some gold standard in-
stances, to evaluate workers’ performance during
annotation. Unfortunately, in paraphrasing there
is no single correct answer, though other workers
could be used to check outputs.
</p>
<p>Finally, we checked the distribution of incorrect
paraphrases per prompt sentence. Two prompts
accounted for 22% of incorrect paraphrases:
</p>
<p>Prompt:Which is easier out of EECS 378 and EECS 280?
Is EECS 378 easier than EECS 280?
</p>
<p>Prompt: Is Professor Stout the only person who teaches
Algorithms?
Are there professors other than Stout who teach Algo-
rithms?
</p>
<p>These paraphrases are not semantically equiva-
lent to the original question, but they would elicit
equivalent information, which explains why work-
ers provided them. Providing negative examples
may help guide workers to avoid such mistakes.
</p>
<p>4.2 Discussion: Domains
</p>
<p>The bottom section of Table 1 shows measure-
ments using the baseline setup, but with variations
in the source domain of data. The only signifi-
cant change in correctness is on UBUNTU, which
is probably due to the extensive use of jargon in
the dataset, for example:
</p>
<p>Prompt: ok, what does journalctl show
That journalistic show is about what?
</p>
<p>For grammaticality, GEOQUERY is particularly
low; common mistakes included confusion be-
tween singular/plural and has/have. WSJ is the do-
main with the greatest variations. It has consider-
ably longer sentences on average, which explains
the greater time taken. This could also explain the
lower distinctness and PINC score, because work-
ers would often retain large parts of the sentence,
sometimes re-arranged, but otherwise unchanged.
</p>
<p>5 Conclusion
</p>
<p>While previous work has used crowdsourcing to
generate paraphrases, we perform the first sys-
tematic study of factors influencing the process.
We find that the most substantial variations are
caused by priming effects: using simpler exam-
ples leads to lower diversity, but more frequent se-
mantic equivalence. Meanwhile, prompting work-
ers with paraphrases collected from other work-
ers (rather than re-using the original prompt) in-
creases diversity. Our findings provide clear guid-
ance for future paraphrase generation, supporting
the creation of larger, more diverse future datasets.
</p>
<p>6 Acknowledgements
</p>
<p>We would like to thank the members of the
UMich/IBM Sapphire project, as well as all of our
study participants and the anonymous reviewers
for their helpful suggestions on this work.
</p>
<p>This material is based in part upon work sup-
ported by IBM under contract 4915012629 . Any
opinions, findings, conclusions or recommenda-
tions expressed above are those of the authors and
do not necessarily reflect the views of IBM.
</p>
<p>References
Douglas G Altman. 1990. Practical statis-
</p>
<p>tics for medical research. CRC press.
https://www.crcpress.com/Practical-Statistics-for-
Medical-Research/Altman/p/book/9780412276309.
</p>
<p>Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg
Little, Andrew Miller, Robert C Miller, Robin
Miller, Aubrey Tatarowicz, Brandyn White, Samual
White, et al. 2010. Vizwiz: nearly real-time
answers to visual questions. In Proc. of the
23nd annual ACM symposium on User interface
software and technology. ACM, pages 333–342.
http://dl.acm.org/citation.cfm?id=1866029.1866080.
</p>
<p>Ondřej Bojar, Matouš Macháček, Aleš Tamchyna,
and Daniel Zeman. 2013. Scratching the sur-
face of possible translations, Springer Berlin
Heidelberg, Berlin, Heidelberg, pages 465–474.
http://dx.doi.org/10.1007/978-3-642-40585-3 59.
</p>
<p>Houda Bouamor, Aurlien Max, Gabriel Illouz, and
Anne Vilnat. 2012. A contrastive review of para-
phrase acquisition techniques. In Proc. of the Eight
International Conference on Language Resources
and Evaluation (LREC’12). http://www.lrec-
conf.org/proceedings/lrec2012/pdf/555Paper.pdf .
</p>
<p>107</p>
<p />
</div>
<div class="page"><p />
<p>Steven Burrows, Martin Potthast, and Benno Stein.
2013. Paraphrase acquisition via crowdsourcing
and machine learning. ACM Transactions on In-
telligent Systems and Technology 4(3):43:1–43:21.
http://doi.acm.org/10.1145/2483669.2483676.
</p>
<p>Olivia Buzek, Philip Resnik, and Ben Bederson.
2010. Error driven paraphrase annotation us-
ing mechanical turk. In Proc. of the NAACL
HLT 2010 Workshop on Creating Speech and
Language Data with Amazon’s Mechanical Turk.
http://www.aclweb.org/anthology/W10-0735.
</p>
<p>David Chen and William Dolan. 2011. Collect-
ing highly parallel data for paraphrase eval-
uation. In Proc. of the 49th Annual Meet-
ing of the Association for Computational
Linguistics: Human Language Technologies.
http://www.aclweb.org/anthology/P11-1020.
</p>
<p>Timothy Chklovski. 2005. Collecting paraphrase cor-
pora from volunteer contributors. In Proc. of the
3rd International Conference on Knowledge Cap-
ture. http://doi.acm.org/10.1145/1088622.1088644.
</p>
<p>Mark Dras. 1999. Tree adjoining grammar and
the reluctant paraphrasing of text. Ph.D. the-
sis, Macquarie University NSW 2109 Australia.
http://web.science.mq.edu.au/ madras/papers/thesis.pdf.
</p>
<p>Markus Dreyer and Daniel Marcu. 2012. HyTER:
Meaning-equivalent semantics for translation eval-
uation. In Proc. of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
http://www.aclweb.org/anthology/N12-1017.
</p>
<p>Anthony Fader, Luke Zettlemoyer, and Oren Et-
zioni. 2013. Paraphrase-driven learning for
open question answering. In Proc. of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).
http://www.aclweb.org/anthology/P/P13/P13-
1158.pdf.
</p>
<p>Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proc. of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. http://aclweb.org/anthology/N/N13/N13-
1092.pdf.
</p>
<p>Sture Holm. 1979. A simple sequentially re-
jective multiple test procedure. Scandi-
navian Journal of Statistics 6(2):65–70.
http://www.jstor.org/stable/4615733.
</p>
<p>David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proc.
of the Human Language Technology Con-
ference of the NAACL, Main Conference.
http://www.aclweb.org/anthology/N/N06/N06-
1058.pdf.
</p>
<p>A Kumaran, Melissa Densmore, and Shaishav
Kumar. 2014. Online gaming for crowd-
sourcing phrase-equivalents. In Proc. of COL-
ING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers.
http://www.aclweb.org/anthology/C14-1117.
</p>
<p>Walter S. Lasecki, Ece Kamar, and Dan Bohus. 2013a.
Conversations in the crowd: Collecting data for
task-oriented dialog learning. In Scaling Speech,
Language Understanding and Dialogue through
Crowdsourcing Workshop at the First AAAI Con-
ference on Human Computation and Crowdsourc-
ing. http://www.aaai.org/ocs/index.php/HCOMP/
HCOMP13/paper/view/7637.
</p>
<p>Walter S Lasecki, Kyle I Murray, Samuel White,
Robert C Miller, and Jeffrey P Bigham. 2011. Real-
time crowd control of existing interfaces. In Proc.
of the 24th annual ACM symposium on User inter-
face software and technology. ACM, pages 23–32.
http://dl.acm.org/citation.cfm?id=2047200.
</p>
<p>Walter S Lasecki, Rachel Wesley, Jeffrey Nichols,
Anand Kulkarni, James F Allen, and Jeffrey P
Bigham. 2013b. Chorus: a crowd-powered con-
versational assistant. In Proc. of the 26th an-
nual ACM symposium on User interface soft-
ware and technology. ACM, pages 151–162.
http://dl.acm.org/citation.cfm?id=2502057.
</p>
<p>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large an-
notated corpus of English: The Penn Tree-
bank. Computational Linguistics 19(2):313–330.
http://aclweb.org/anthology/J93-2004.
</p>
<p>Winter Mason and Duncan J Watts. 2010. Financial
incentives and the performance of crowds. ACM
SigKDD Explorations Newsletter 11(2):100–108.
http://dl.acm.org/citation.cfm?id=1600175.
</p>
<p>Margaret Mitchell, Dan Bohus, and Ece Ka-
mar. 2014. Crowdsourcing language genera-
tion templates for dialogue systems. In Proc.
of the INLG and SIGDIAL 2014 Joint Session.
http://www.aclweb.org/anthology/W14-5003.
</p>
<p>Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli.
2012. Chinese whispers: Cooperative para-
phrase acquisition. In Proc. of the Eighth In-
ternational Conference on Language Resources
and Evaluation (LREC’12). http://www.lrec-
conf.org/proceedings/lrec2012/pdf/772 Paper.pdf.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-Jing Zhu. 2002. BLEU: a method
for automatic evaluation of machine transla-
tion. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics.
http://www.aclweb.org/anthology/P/P02/P02-
1040.pdf.
</p>
<p>108</p>
<p />
</div>
<div class="page"><p />
<p>Ellie Pavlick, Pushpendre Rastogi, Juri Ganitke-
vich, and Chris Callison-Burch Ben Van Durme.
2015. PPDB 2.0: Better paraphrase rank-
ing, fine-grained entailment relations, word em-
beddings, and style classification. In Proc.
of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2015).
http://aclweb.org/anthology/P/P15/P15-2070.pdf.
</p>
<p>Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proc. of
the 12th European Conference on Machine Learn-
ing. https://link.springer.com/chapter/10.1007/3-
540-44795-440.
</p>
<p>Martin Tschirsich and Gerold Hintz. 2013. Lever-
aging crowdsourcing for paraphrase recogni-
tion. In Proc. of the 7th Linguistic Annotation
Workshop and Interoperability with Discourse.
http://www.aclweb.org/anthology/W13-2325.
</p>
<p>W. Y. Wang, D. Bohus, E. Kamar, and E. Horvitz. 2012.
Crowdsourcing the acquisition of natural language
corpora: Methods and observations. In 2012 IEEE
Spoken Language Technology Workshop (SLT).
http://ieeexplore.ieee.org/document/6424200/.
</p>
<p>Yushi Wang, Jonathan Berant, and Percy Liang.
2015. Building a semantic parser overnight. In
Proc. of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the
7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers).
http://www.aclweb.org/anthology/P15-1129.
</p>
<p>109</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 110–117
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2018
</p>
<p>Arc-swift: A Novel Transition System for Dependency Parsing
</p>
<p>Peng Qi Christopher D. Manning
Computer Science Department
</p>
<p>Stanford University
{pengqi, manning}@cs.stanford.edu
</p>
<p>Abstract
</p>
<p>Transition-based dependency parsers of-
ten need sequences of local shift and re-
duce operations to produce certain attach-
ments. Correct individual decisions hence
require global information about the sen-
tence context and mistakes cause error
propagation. This paper proposes a novel
transition system, arc-swift, that enables
direct attachments between tokens farther
apart with a single transition. This al-
lows the parser to leverage lexical infor-
mation more directly in transition deci-
sions. Hence, arc-swift can achieve sig-
nificantly better performance with a very
small beam size. Our parsers reduce error
by 3.7–7.6% relative to those using exist-
ing transition systems on the Penn Tree-
bank dependency parsing task and English
Universal Dependencies.
</p>
<p>1 Introduction
</p>
<p>Dependency parsing is a longstanding natural lan-
guage processing task, with its outputs crucial to
various downstream tasks including relation ex-
traction (Schmitz et al., 2012; Angeli et al., 2015),
language modeling (Gubbins and Vlachos, 2013),
and natural logic inference (Bowman et al., 2016).
</p>
<p>Attractive for their linear time complexity and
amenability to conventional classification meth-
ods, transition-based dependency parsers have
sparked much research interest recently. A
transition-based parser makes sequential predic-
tions of transitions between states under the re-
strictions of a transition system (Nivre, 2003).
Transition-based parsers have been shown to excel
at parsing shorter-range dependency structures, as
well as languages where non-projective parses are
less pervasive (McDonald and Nivre, 2007).
</p>
<p> 
</p>
<p> &lt;root&gt;      I      ate       fish    with     chopsticks      . 
</p>
<p>root 
nsubj dobj case 
</p>
<p>stack buffer 
arc-eager 
</p>
<p>arc-swift 
</p>
<p>Sh LA Sh RA Sh LA Re RA Re RA 
</p>
<p>Sh LA[1] Sh RA[1] Sh LA[1] RA[2] RA[2] 
</p>
<p>Figure 1: An example of the state of a transition-
based dependency parser, and the transition se-
quences used by arc-eager and arc-swift to induce
the correct parse. The state shown is generated by
the first six transitions of both systems.
</p>
<p>However, the transition systems employed in
state-of-the-art dependency parsers usually define
very local transitions. At each step, only one or
two words are affected, with very local attach-
ments made. As a result, distant attachments re-
quire long and not immediately obvious transi-
tion sequences (e.g., ate→chopsticks in Figure 1,
which requires two transitions). This is further ag-
gravated by the usually local lexical information
leveraged to make transition predictions (Chen
and Manning, 2014; Andor et al., 2016).
</p>
<p>In this paper, we introduce a novel transition
system, arc-swift, which defines non-local transi-
tions that directly induce attachments of distance
up to n (n = the number of tokens in the sentence).
Such an approach is connected to graph-based
dependency parsing, in that it leverages pairwise
scores between tokens in making parsing deci-
sions (McDonald et al., 2005).
</p>
<p>We make two main contributions in this paper.
Firstly, we introduce a novel transition system for
dependency parsing, which alleviates the difficulty
of distant attachments in previous systems by al-
lowing direct attachments anywhere in the stack.
Secondly, we compare parsers by the number of
mistakes they make in common linguistic con-
</p>
<p>110</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2018">https://doi.org/10.18653/v1/P17-2018</a></div>
</div>
<div class="page"><p />
<p>arc-standard arc-hybrid
Shift (σ, i|β,A)⇒ (σ|i, β, A)
LArc (σ|i|j, β,A)⇒ (σ|j, β,A ∪ {(j → i)})
RArc (σ|i|j, β,A)⇒ (σ|i, β, A ∪ {(i→ j)})
</p>
<p>Shift (σ, i|β,A)⇒ (σ|i, β,A)
LArc (σ|i, j|β,A)⇒ (σ, j|β,A ∪ {(j → i)})
RArc (σ|i|j, β,A)⇒ (σ|i, β, A ∪ {(i→ j)})
</p>
<p>arc-eager arc-swift
</p>
<p>Shift (σ, i|β,A)⇒ (σ|i, β, A)
LArc (σ|i, j|β,A)⇒ (σ, j|β,A ∪ {(j → i)})
RArc (σ|i, j|β,A)⇒ (σ|i|j, β,A ∪ {(i→ j)})
Reduce (σ|i, β, A)⇒ (σ, β,A)
</p>
<p>Shift (σ, i|β,A)⇒ (σ|i, β, A)
LArc[k] (σ|ik| . . . |i1, j|β,A)
</p>
<p>⇒ (σ, j|β,A ∪ {(j → ik)})
RArc[k] (σ|ik| . . . |i1, j|β,A)
</p>
<p>⇒ (σ|ik|j, β,A ∪ {(ik → j)})
</p>
<p>Figure 2: Transitions defined by different transition systems.
</p>
<p>structions. We show that arc-swift parsers reduce
errors in attaching prepositional phrases and con-
junctions compared to parsers using existing tran-
sition systems.
</p>
<p>2 Transition-based Dependency Parsing
</p>
<p>Transition-based dependency parsing is performed
by predicting transitions between states (see Fig-
ure 1 for an example). Parser states are usu-
ally written as (σ|i, j|β,A), where σ|i denotes
the stack with token i on the top, j|β denotes the
buffer with token j at its leftmost, and A the set of
dependency arcs. Given a state, the goal of a de-
pendency parser is to predict a transition to a new
state that would lead to the correct parse. A tran-
sition system defines a set of transitions that are
sound and complete for parsers, that is, every tran-
sition sequence would derive a well-formed parse
tree, and every possible parse tree can also be de-
rived from some transition sequence.1
</p>
<p>Arc-standard (Nivre, 2004) is one of the first
transition systems proposed for dependency pars-
ing. It defines three transitions: shift, left arc
(LArc), and right arc (RArc) (see Figure 2 for defi-
nitions, same for the following transition systems),
where all arc-inducing transitions operate on the
stack. This system builds the parse bottom-up, i.e.,
a constituent is only attached to its head after it has
received all of its dependents. A potential draw-
back is that during parsing, it is difficult to predict
if a constituent has consumed all of its right de-
pendents. Arc-eager (Nivre, 2003) remedies this
drawback by defining arc-inducing transitions that
operate between the stack and the buffer. As a re-
sult, a constituent no longer needs to be complete
</p>
<p>1We only focus on projective parses for the scope of this
paper.
</p>
<p>before it can be attached to its head to the left,
as a right arc doesn’t prevent the attached depen-
dent from taking further dependents of its own.2
</p>
<p>Kuhlmann et al. (2011) propose a hybrid system
derived from a tabular parsing scheme, which they
have shown both arc-standard and arc-eager can
be derived from. Arc-hybrid combines LArc from
arc-eager and RArc from arc-standard to build
dependencies bottom-up.
</p>
<p>3 Non-local Transitions with arc-swift
</p>
<p>The traditional transition systems discussed in
Section 2 only allow very local transitions affect-
ing one or two words, which makes long-distance
dependencies difficult to predict. To illustrate the
limitation of local transitions, consider parsing the
following sentences:
</p>
<p>I ate fish with ketchup.
I ate fish with chopsticks.
</p>
<p>The two sentences have almost identical struc-
tures, with the notable difference that the prepo-
sitional phrase is complementing the direct object
in the first case, and the main verb in the second.
</p>
<p>For arc-standard and arc-hybrid, the parser
would have to decide between Shift and RArc
when the parser state is as shown in Figure 3a,
where ? stands for either “ketchup” or “chop-
sticks”.3 Similarly, an arc-eager parser would deal
with the state shown in Figure 3b. Making the cor-
rect transition requires information about context
words “ate” and “fish”, as well as “?”.
</p>
<p>2A side-effect of arc-eager is that there is sometimes spu-
rious ambiguity between Shift and Reduce transitions. For
the example in Figure 1, the first Reduce can be inserted be-
fore the third Shift without changing the correctness of the
resulting parse, i.e., both are feasible at that time.
</p>
<p>3For this example, we assume that the sentence is being
parsed into Universal Dependencies.
</p>
<p>111</p>
<p />
</div>
<div class="page"><p />
<p> 
</p>
<p> &lt;root&gt;      I      ate       fish    with   *     . 
stack buffer 
</p>
<p> &lt;root&gt;      I      ate       fish    with   *     . 
stack buffer 
</p>
<p>a 
 
 
 
 
 
b 
</p>
<p>Figure 3: Parser states that present difficult tran-
sition decisions in traditional systems. In these
states, parsers would need to incorporate context
about “ate”, “fish”, and “?” to make the correct
local transition.
</p>
<p>Parsers employing traditional transition systems
would usually incorporate more features about the
context in the transition decision, or employ beam
search during parsing (Chen and Manning, 2014;
Andor et al., 2016).
</p>
<p>In contrast, inspired by graph-based parsers, we
propose arc-swift, which defines non-local tran-
sitions as shown in Figure 2. This allows direct
comparison of different attachment points, and
provides a direct solution to parsing the two exam-
ple sentences. When the arc-swift parser encoun-
ters a state identical to Figure 3b, it could directly
compare transitions RArc[1] and RArc[2] instead
of evaluating between local transitions. This re-
sults in a direct attachment much like that in a
graph-based parser, informed by lexical informa-
tion about affinity of the pairs of words.
</p>
<p>Arc-swift also bears much resemblance to arc-
eager. In fact, an LArc[k] transition can be viewed
as k− 1 Reduce operations followed by one LArc
in arc-eager, and similarly for RArc[k]. Reduce is
no longer needed in arc-swift as it becomes part of
LArc[k] and RArc[k], removing the ambiguity in
derived transitions in arc-eager. arc-swift is also
equivalent to arc-eager in terms of soundness and
completeness.4 A caveat is that the worst-case
time complexity of arc-swift is O(n2) instead of
O(n), which existing transition-based parsers en-
joy. However, in practice the runtime is nearly
</p>
<p>4This is easy to show because in arc-eager, all Reduce
transitions can be viewed as preparing for a later LArc or
RArc transition. We also note that similar to arc-eager
transitions, arc-swift transitions must also satisfy certain
pre-conditions. Specifically, an RArc[k] transition requires
that the top k − 1 elements in the stack are already at-
tached; LArc[k] additionally requires that the k-th element
is unattached, resulting in no more than one feasible LArc
candidate for any parser state.
</p>
<p>linear, thanks to the usually small number of re-
ducible tokens in the stack.
</p>
<p>4 Experiments
</p>
<p>4.1 Data and Model
</p>
<p>We use the Wall Street Journal portion of Penn
Treebank with standard parsing splits (PTB-
SD), along with Universal Dependencies v1.3
(Nivre et al., 2016) (EN-UD). PTB-SD is con-
verted to Stanford Dependencies (De Marneffe
and Manning, 2008) with CoreNLP 3.3.0 (Man-
ning et al., 2014) following previous work. We
report labelled and unlabelled attachment scores
(LAS/UAS), removing punctuation from all eval-
uations.
</p>
<p>Our model is very similar to that of (Kiper-
wasser and Goldberg, 2016), where features are
extracted from tokens with bidirectional LSTMs,
and concatenated for classification. For the three
traditional transition systems, features of the top
3 tokens on the stack and the leftmost token in
the buffer are concatenated as classifier input. For
arc-swift, features of the head and dependent to-
kens for each arc-inducing transition are concate-
nated to compute scores for classification, and fea-
tures of the leftmost buffer token is used for Shift.
For other details we defer to Appendix A. The full
specification of the model can also be found in
our released code online at https://github.
com/qipeng/arc-swift.
</p>
<p>4.2 Results
</p>
<p>We use static oracles for all transition systems, and
for arc-eager we implement oracles that always
Shift/Reduce when ambiguity is present (arc-
eager-S/R). We evaluate our parsers with greedy
parsing (i.e., beam size 1). The results are shown
in Table 1.5 Note that K&amp;G 2016 is trained with a
dynamic oracle (Goldberg and Nivre, 2012), An-
dor 2016 with a CRF-like loss, and both Andor
2016 and Weiss 2015 employed beam search (with
sizes 32 and 8, respectively).
</p>
<p>For each pair of the systems we implemented,
we studied the statistical significance of their dif-
ference by performing a paired test with 10,000
bootstrap samples on PTB-SD. The resulting p-
values are analyzed with a 10-group Bonferroni-
Holm test, with results shown in Table 2. We note
</p>
<p>5In the interest of space, we abbreviate all transition sys-
tems (TS) as follows in tables: asw for arc-swift, asd for arc-
standard, aeS/R for arc-eager-S/R, and ah for arc-hybrid.
</p>
<p>112</p>
<p />
</div>
<div class="page"><p />
<p>Model TS
PTB-SD EN-UD
</p>
<p>UAS LAS UAS LAS
</p>
<p>This work asd 94.0 91.7 85.6 81.5
This work aeS 94.0 91.8 85.4 81.4
This work aeR 93.8 91.7 85.2 81.2
This work ah 93.9 91.7 85.4 81.3
This work asw 94.3 92.2 86.1 82.2
</p>
<p>Andor 2016 asd 94.6 92.8 84.8* 80.4*
K&amp;G 2016 ah 93.9 91.9
Weiss 2015 asd 94.0 92.1
C&amp;M 2014 asd 91.8 89.6
</p>
<p>Table 1: Performance of parsers using different
transition systems on the Penn Treebank dataset.
*: Obtained from their published results online.6
</p>
<p>aeS asd ah aeR
</p>
<p>asw ***/*** ***/*** ***/*** ***/***
aeS -/- -/- */-
asd -/- */-
ah -/-
</p>
<p>Table 2: Significance test for transition systems.
Each grid shows adjusted test result for UAS and
LAS, respectively, showing whether the system on
the row is significantly better than that on the col-
umn. “***” stands for p &lt; 0.001, “**” p &lt; 0.01,
“*” p &lt; 0.05, and “-” p ≥ 0.05.
</p>
<p>that with almost the same implementation, arc-
swift parsers significantly outperform those using
traditional transition systems. We also analyzed
the performance of parsers on attachments of dif-
ferent distances. As shown in Figure 4, arc-swift
is equally accurate as existing systems for short
dependencies, but is more robust for longer ones.
</p>
<p>While arc-swift introduces direct long-distance
transitions, it also shortens the overall sequence
necessary to induce the same parse. A parser could
potentially benefit from both factors: direct attach-
ments could make an easier classification task, and
shorter sequences limit the effect of error propa-
gation. However, since the two effects are corre-
lated in a transition system, precise attribution of
the gain is out of the scope of this paper.
</p>
<p>Computational efficiency. We study the com-
putational efficiency of the arc-swift parser by
</p>
<p>6https://github.com/tensorflow/models/
blob/master/syntaxnet/g3doc/universal.md
</p>
<p>1 2 3-5 6-10 11-15 16-20 &gt;20
0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>Dependency Length
</p>
<p>A
tta
</p>
<p>ch
m
</p>
<p>en
te
</p>
<p>rr
or
</p>
<p>(%
) asd
</p>
<p>aeS
aeR
ah
</p>
<p>asw
</p>
<p>Figure 4: Parser attachment errors on PTB-SD
binned by the length of the gold dependency.
</p>
<p>comparing it to an arc-eager parser. On the PTB-
SD development set, the average transition se-
quence length per sentence of arc-swift is 77.5%
of that of arc-eager. At each step of parsing,
arc-swift needs to evaluate only about 1.24 times
the number of transition candidates as arc-eager,
which results in very similar runtime. In contrast,
beam search with beam size 2 for arc-eager re-
quires evaluating 4 times the number of transition
candidates compared to greedy parsing, which re-
sults in a UAS 0.14% worse and LAS 0.22% worse
for arc-eager compared to greedily decoded arc-
swift.
</p>
<p>4.3 Linguistic Analysis
We automatically extracted all labelled attachment
errors by error type (incorrect attachment or re-
lation), and categorized a few top parser errors
by hand into linguistic constructions. Results on
PTB-SD are shown in Table 3.7 We note that
the arc-swift parser improves accuracy on prepo-
sitional phrase (PP) and conjunction attachments,
while it remains comparable to other parsers on
other common errors. Analysis on EN-UD shows
a similar trend. As shown in the table, there are
still many parser errors unaccounted for in our
analysis. We leave this to future work.
</p>
<p>7We notice that for some examples the parsers predicted
a ccomp (complement clause) attachment to verbs “says” and
“said”, where the CoreNLP output simply labelled the rela-
tion as dep (unspecified). For other examples the relation be-
tween the prepositions in “out of” is labelled as prep (preposi-
tion) instead of pcomp (prepositional complement). We sus-
pect this is due to the converter’s inability to handle certain
corner cases, but further study is warranted.
</p>
<p>113</p>
<p />
</div>
<div class="page"><p />
<p>asw aeS asd
</p>
<p>PP attachment 545 569 571
Noun/Adjective confusion 221 230 221
Conjunction attachment 156 170 164
Adverbial attachment 123 122 143
</p>
<p>Total Errors 3884 4100 4106
</p>
<p>Table 3: Common parser errors on PTB-SD. The
top 4 common errors are categorized and shown
in this table. Errors not shown are in a long-tail
distribution and warrant analyses in future work.
</p>
<p>5 Related Work
</p>
<p>Previous work has also explored augmenting
transition systems to facilitate longer-range at-
tachments. Attardi (2006) extended the arc-
standard system for non-projective parsing, with
arc-inducing transitions that are very similar to
those in arc-swift. A notable difference is that
their transitions retain tokens between the head
and dependent. Fernández-González and Gómez-
Rodrı́guez (2012) augmented the arc-eager sys-
tem with transitions that operate on the buffer,
which shorten the transition sequence by reduc-
ing the number of Shift transitions needed. How-
ever, limited by the sparse feature-based classi-
fiers used, both of these parsers just mentioned
only allow direct attachments of distance up to 3
and 2, respectively. More recently, Sartorio et al.
(2013) extended arc-standard with transitions that
directly attach to left and right “spines” of the top
two nodes in the stack. While this work shares
very similar motivations as arc-swift, it requires
additional data structures to keep track of the left
and right spines of nodes. This transition system
also introduces spurious ambiguity where multiple
transition sequences could lead to the same cor-
rect parse, which necessitates easy-first training to
achieve a more noticeable improvement over arc-
standard. In contrast, arc-swift can be easily im-
plemented given the parser state alone, and does
not give rise to spurious ambiguity.
</p>
<p>For a comprehensive study of transition sys-
tems for dependency parsing, we refer the reader
to (Bohnet et al., 2016), which proposed a gener-
alized framework that could derive all of the tra-
ditional transition systems we described by con-
figuring the size of the active token set and the
maximum arc length, among other control param-
eters. However, this framework does not cover
</p>
<p>arc-swift in its original form, as the authors limit
each of their transitions to reduce at most one to-
ken from the active token set (the buffer). On the
other hand, the framework presented in (Gómez-
Rodrı́guez and Nivre, 2013) does not explicitly
make this constraint, and therefore generalizes to
arc-swift. However, we note that arc-swift still
falls out of the scope of existing discussions in that
work, by introducing multiple Reduces in a single
transition.
</p>
<p>6 Conclusion
</p>
<p>In this paper, we introduced arc-swift, a novel tran-
sition system for dependency parsing. We also
performed linguistic analyses on parser outputs
and showed arc-swift parsers reduce errors in con-
junction and adverbial attachments compared to
parsers using traditional transition systems.
</p>
<p>Acknowledgments
</p>
<p>We thank Timothy Dozat, Arun Chaganty, Danqi
Chen, and the anonymous reviewers for helpful
discussions. Stanford University gratefully ac-
knowledges the support of the Defense Advanced
Research Projects Agency (DARPA) Deep Ex-
ploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL)
contract No. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
</p>
<p>References
Daniel Andor, Chris Alberti, David Weiss, Aliak-
</p>
<p>sei Severyn, Alessandro Presta, Kuzman Ganchev,
Slav Petrov, and Michael Collins. 2016. Glob-
ally normalized transition-based neural networks.
In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics.
https://www.aclweb.org/anthology/P16-1231.
</p>
<p>Gabor Angeli, Melvin Johnson Premkumar, and
Christopher D Manning. 2015. Leveraging linguis-
tic structure for open domain information extrac-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics (ACL
2015). http://www.aclweb.org/anthology/P15-1034.
</p>
<p>Giuseppe Attardi. 2006. Experiments with a mul-
tilanguage non-projective dependency parser. In
Proceedings of the Tenth Conference on Com-
putational Natural Language Learning. Associa-
tion for Computational Linguistics, pages 166–170.
http://www.aclweb.org/anthology/W06-2922.
</p>
<p>114</p>
<p />
</div>
<div class="page"><p />
<p>Bernd Bohnet, Ryan McDonald, Emily Pitler, and
Ji Ma. 2016. Generalized transition-based depen-
dency parsing via control parameters. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics. volume 1, pages
150–160. https://www.aclweb.org/anthology/P16-
1015.
</p>
<p>Samuel R Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceed-
ings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2016).
http://www.aclweb.org/anthology/P16-1139.
</p>
<p>Danqi Chen and Christopher D Manning. 2014.
A fast and accurate dependency parser using
neural networks. In EMNLP. pages 740–750.
http://www.aclweb.org/anthology/D14-1082.
</p>
<p>Marie-Catherine De Marneffe and Christopher D
Manning. 2008. The Stanford typed dependen-
cies representation. In COLING 2008: Pro-
ceedings of the Workshop on Cross-framework
and Cross-domain Parser Evaluation. pages 1–8.
http://www.aclweb.org/anthology/W08-1301.
</p>
<p>Daniel Fernández-González and Carlos Gómez-
Rodrı́guez. 2012. Improving transition-based
dependency parsing with buffer transitions. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning. pages
308–319. http://www.aclweb.org/anthology/D12-
1029.
</p>
<p>Yoav Goldberg and Joakim Nivre. 2012. A
dynamic oracle for arc-eager dependency
parsing. In COLING. pages 959–976.
http://www.aclweb.org/anthology/C12-1059.
</p>
<p>Carlos Gómez-Rodrı́guez and Joakim Nivre. 2013.
Divisible transition systems and multiplanar de-
pendency parsing. Computational Linguistics
39(4):799–845.
</p>
<p>Joseph Gubbins and Andreas Vlachos. 2013. De-
pendency language models for sentence comple-
tion. In EMNLP. volume 13, pages 1405–1410.
http://www.aclweb.org/anthology/D13-1143.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
</p>
<p>Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics
(TACL) https://aclweb.org/anthology/Q16-1023.
</p>
<p>Marco Kuhlmann, Carlos Gómez-Rodrı́guez, and Gior-
gio Satta. 2011. Dynamic programming algo-
rithms for transition-based dependency parsers.
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies. pages 673–682.
http://www.aclweb.org/anthology/P11-1068.
</p>
<p>Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard,
and David McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit.
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.
</p>
<p>Ryan McDonald, Fernando Pereira, Kiril Ribarov,
and Jan Hajič. 2005. Non-projective depen-
dency parsing using spanning tree algorithms. In
Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in
Natural Language Processing. pages 523–530.
http://www.aclweb.org/anthology/H05-1066.
</p>
<p>Ryan T McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In EMNLP-CoNLL. pages 122–131.
http://www.aclweb.org/anthology/D07-1013.
</p>
<p>Joakim Nivre. 2003. An efficient algorithm
for projective dependency parsing. In Pro-
ceedings of the 8th International Workshop
on Parsing Technologies. pages 149–160.
http://stp.lingfil.uu.se/ nivre/docs/iwpt03.pdf.
</p>
<p>Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the
Workshop on Incremental Parsing: Bringing En-
gineering and Cognition Together. pages 50–57.
https://www.aclweb.org/anthology/W04-0308.
</p>
<p>Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependen-
cies v1: A multilingual treebank collection. In Pro-
ceedings of the 10th International Conference on
Language Resources and Evaluation (LREC 2016).
pages 1659–1666.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543. http://www.aclweb.org/anthology/D14-1162.
</p>
<p>Francesco Sartorio, Giorgio Satta, and Joakim Nivre.
2013. A transition-based dependency parser us-
ing a dynamic parsing strategy. In Associa-
tion for Computational Linguistics. pages 135–144.
http://www.aclweb.org/anthology/P13-1014.
</p>
<p>Michael Schmitz, Robert Bart, Stephen Soderland,
Oren Etzioni, et al. 2012. Open language learn-
ing for information extraction. In Proceedings
</p>
<p>115</p>
<p />
</div>
<div class="page"><p />
<p>of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning. pages 523–534.
https://www.aclweb.org/anthology/D12-1048.
</p>
<p>David Weiss, Chris Alberti, Michael Collins, and
Slav Petrov. 2015. Structured training for neu-
ral network transition-based parsing. In Proceed-
ings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2015).
http://www.aclweb.org/anthology/P15-1032.
</p>
<p>A Model and Training Details
</p>
<p>Our model setup is similar to that of (Kiperwasser
and Goldberg, 2016) (See Figure 5). We employ
two blocks of bidirectional long short-term mem-
ory (BiLSTM) networks (Hochreiter and Schmid-
huber, 1997) that share very similar structures, one
for part-of-speech (POS) tagging, the other for
parsing. Both BiLSTMs have 400 hidden units
in each direction, and the output of both are con-
catenated and fed into a dense layer of rectified
linear units (ReLU) before 32-dimensional rep-
resentations are derived as classification features.
As the input to the tagger BiLSTM, we represent
words with 100-dimensional word embeddings,
initialized with GloVe vectors (Pennington et al.,
2014).8 The output distribution of the tagger clas-
sifier is used to compute a weighted sum of 32-
dimensional POS embeddings, which is then con-
catenated with the output of the tagger BiLSTM
(800-dimensional per token) as the input to the
parser BiLSTM. For the parser BiLSTM, we use
two separate sets of dense layers to derive a “head”
and a “dependent” representation for each token.
These representations are later merged according
to the parser state to make transition predictions.
</p>
<p>For traditional transition systems, we follow
(Kiperwasser and Goldberg, 2016) by featurizing
the top 3 tokens on the stack and the leftmost token
in the buffer. To derive features for each token, we
take its head representation vhead and dependent
representation vdep, and perform the following bi-
affine combination
</p>
<p>vfeat,i = [f(vhead, vdep)]i
</p>
<p>= ReLU
(
v&gt;headWivdep + b
</p>
<p>&gt;
i vhead
</p>
<p>+ c&gt;i vdep + di
) (1)
</p>
<p>where Wi ∈ R32×32, bi, ci ∈ R32, and di is
a scalar for i = 1, . . . , 32. The resulting 32-
dimensional features are concatenated as the input
</p>
<p>8We also kept the vectors of the top 400k words trained
on Wikipedia and English Gigaword for a broader coverage
of unseen words.
</p>
<p> 
</p>
<p> N/A            PN         VBD          NN           . 
</p>
<p> &lt;root&gt;        I            ate             fish            . 
</p>
<p>Tagger BiLSTM (2-layer, 400d+400d) 
</p>
<p>Parser BiLSTM (2-layer, 400d+400d) 
</p>
<p>MLP POS 
classifier 
</p>
<p>32d POS 
Embeddings 
</p>
<p>100d ReLU 
</p>
<p>head and dependent representations 
</p>
<p>Figure 5: Illustration of the model.
</p>
<p>to a fixed-dimensional softmax classifier for tran-
sition decisions.
</p>
<p>For arc-swift, we featurize for each arc-
inducing transition with the same composition
function in Equation (1) with vhead of the head to-
ken and vdep of the dependent token of the arc to
be induced. For Shift, we simply combine vhead
and vdep of the leftmost token in the buffer with
the biaffine combination, and obtain its score by
computing the inner-product of the feature and a
vector. At each step, the scores of all feasible tran-
sitions are normalized to a probability distribution
by a softmax function.
</p>
<p>In all of our experiments, the parsers are trained
to maximize the log likelihood of the desired
transition sequence, along with the tagger being
trained to maximize the log likelihood of the cor-
rect POS tag for each token.
</p>
<p>To train the parsers, we use the ADAM opti-
mizer (Kingma and Ba, 2014), with β2 = 0.9,
an initial learning rate of 0.001, and minibatches
of size 32 sentences. Parsers are trained for 10
passes through the dataset on PTB-SD. We also
find that annealing the learning rate by a factor of
0.5 for every pass after the 5th helped improve per-
formance. For EN-UD, we train for 30 passes, and
anneal the learning rate for every 3 passes after the
15th due to the smaller size of the dataset. For
all of the biaffine combination layers and dense
layers, we dropout their units with a small prob-
ability of 5%. Also during training time, we ran-
domly replace 10% of the input words by an arti-
ficial 〈UNK〉 token, which is then used to replace
</p>
<p>116</p>
<p />
</div>
<div class="page"><p />
<p>all unseen words in the development and test sets.
Finally, we repeat each experiment with 3 inde-
pendent random initializations, and use the aver-
age result for reporting and statistical significance
tests.
</p>
<p>The code for the full specification of our models
and aforementioned training details are avail-
able at https://github.com/qipeng/
arc-swift.
</p>
<p>117</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118–124
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2019
</p>
<p>A Generative Parser with a Discriminative Recognition Algorithm
</p>
<p>Jianpeng Cheng Adam Lopez and Mirella Lapata
School of Informatics, University of Edinburgh
</p>
<p>jianpeng.cheng@ed.ac.uk, {alopez,mlap}@inf.ed.ac.uk
</p>
<p>Abstract
</p>
<p>Generative models defining joint distribu-
tions over parse trees and sentences are
useful for parsing and language modeling,
but impose restrictions on the scope of fea-
tures and are often outperformed by dis-
criminative models. We propose a frame-
work for parsing and language modeling
which marries a generative model with
a discriminative recognition model in an
encoder-decoder setting. We provide in-
terpretations of the framework based on
expectation maximization and variational
inference, and show that it enables parsing
and language modeling within a single im-
plementation. On the English Penn Treen-
bank, our framework obtains competi-
tive performance on constituency parsing
while matching the state-of-the-art single-
model language modeling score.1
</p>
<p>1 Introduction
</p>
<p>Generative models defining joint distributions
over parse trees and sentences are good theoretical
models for interpreting natural language data, and
appealing tools for tasks such as parsing, grammar
induction and language modeling (Collins, 1999;
Henderson, 2003; Titov and Henderson, 2007;
Petrov and Klein, 2007; Dyer et al., 2016). How-
ever, they often impose strong independence as-
sumptions which restrict the use of arbitrary fea-
tures for effective disambiguation. Moreover, gen-
erative parsers are typically trained by maximiz-
ing the joint probability of the parse tree and the
sentence—an objective that only indirectly relates
to the goal of parsing. At test time, these mod-
els require a relatively expensive recognition algo-
</p>
<p>1Our code is available at https://github.com/
cheng6076/virnng.git.
</p>
<p>rithm (Collins, 1999; Titov and Henderson, 2007)
to recover the parse tree, but the parsing per-
formance consistently lags behind their discrim-
inative competitors (Nivre et al., 2007; Huang,
2008; Goldberg and Elhadad, 2010), which are di-
rectly trained to maximize the conditional proba-
bility of the parse tree given the sentence, where
linear-time decoding algorithms exist (e.g., for
transition-based parsers).
</p>
<p>In this work, we propose a parsing and lan-
guage modeling framework that marries a gener-
ative model with a discriminative recognition al-
gorithm in order to have the best of both worlds.
The idea of combining these two types of mod-
els is not new. For example, Collins and Koo
(2005) propose to use a generative model to gen-
erate candidate constituency trees and a discrimi-
native model to rank them. Sangati et al. (2009)
follow the opposite direction and employ a gener-
ative model to re-rank the dependency trees pro-
duced by a discriminative parser. However, pre-
vious work combines the two types of models in a
goal-oriented, pipeline fashion, which lacks model
interpretations and focuses solely on parsing.
</p>
<p>In comparison, our framework unifies genera-
tive and discriminative parsers with a single objec-
tive, which connects to expectation maximization
and variational inference in grammar induction
settings. In a nutshell, we treat parse trees as latent
factors generating natural language sentences and
parsing as a posterior inference task. We showcase
the framework using Recurrent Neural Network
Grammars (RNNGs; Dyer et al. 2016), a recently
proposed probabilistic model of phrase-structure
trees based on neural transition systems. Differ-
ent from this work which introduces separately
trained discriminative and generative models, we
integrate the two in an auto-encoder which fits our
training objective. We show how the framework
enables grammar induction, parsing and language
</p>
<p>118</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2019">https://doi.org/10.18653/v1/P17-2019</a></div>
</div>
<div class="page"><p />
<p>modeling within a single implementation. On the
English Penn Treebank, we achieve competitive
performance on constituency parsing and state-of-
the-art single-model language modeling score.
</p>
<p>2 Preliminaries
</p>
<p>In this section we briefly describe Recurrent Neu-
ral Network Grammars (RNNGs; Dyer et al.
2016), a top-down transition-based algorithm for
parsing and generation. There are two versions of
RNNG, one discriminative, the other generative.
We follow the original paper in presenting the dis-
criminative variant first.
</p>
<p>The discriminative RNNG follows a shift-
reduce parser that converts a sequence of words
into a parse tree. As in standard shift-reduce
parsers, the RNNG uses a buffer to store unpro-
cessed terminal symbols and a stack to store par-
tially completed syntactic constituents. At each
timestep, one of the following three operations2 is
performed:
</p>
<p>• NT(X) introduces an open non-terminal X
onto the top of the stack, represented as an
open parenthesis followed by X, e.g., (NP.
</p>
<p>• SHIFT fetches the terminal in the front of the
buffer and pushes it onto the top of the stack.
</p>
<p>• REDUCE completes a subtree by repeatedly
popping the stack until an open non-terminal
is encountered. The non-terminal is popped
as well, after which a composite term repre-
senting the entire subtree is pushed back onto
the top of the stack, e.g., (NP the cat).
</p>
<p>The above transition system can be adapted
with minor modifications to an algorithm that gen-
erates trees and sentences. In generator tran-
sitions, there is no input buffer of unprocessed
words but there is an output buffer for storing
words that have been generated. To reflect the
change, the previous SHIFT operation is modified
into a GEN operation defined as follows:
</p>
<p>• GEN generates a terminal symbol and add it
to the stack and the output buffer.
</p>
<p>2To be precise, the total number of operations under our
description is |X|+2 since the NT operation varies with the
non-terminal choice X.
</p>
<p>3 Methodology
</p>
<p>Our framework unifies generative and discrimi-
native parsers within a single training objective.
For illustration, we adopt the two RNNG variants
introduced above with our customized features.
Our starting point is the generative model (§ 3.1),
which allows us to make explicit claims about the
generative process of natural language sentences.
Since this model alone lacks a bottom-up recog-
nition mechanism, we introduce a discriminative
recognition model (§ 3.2) and connect it with the
generative model in an encoder-decoder setting.
To offer a clear interpretation of the training objec-
tive (§ 3.3), we first consider the parse tree as la-
tent and the sentence as observed. We then discuss
extensions that account for labeled parse trees. Fi-
nally, we present various inference techniques for
parsing and language modeling within the frame-
work (§ 3.4).
</p>
<p>3.1 Decoder (Generative Model)
The decoder is a generative RNNG that models
the joint probability p(x, y) of a latent parse tree y
and an observed sentence x. Since the parse tree
is defined by a sequence of transition actions a,
we write p(x, y) as p(x, a).3 The joint distribu-
tion p(x, a) is factorized into a sequence of transi-
tion probabilities and terminal probabilities (when
actions are GEN), which are parametrized by a
transitional state embedding u:
</p>
<p>p(x, a) = p(a)p(x|a)
</p>
<p>=
</p>
<p>|a|∏
</p>
<p>t=1
</p>
<p>p(at|ut)p(xt|ut)I(at=GEN) (1)
</p>
<p>where I is an indicator function and ut represents
the state embedding at time step t. Specifically,
the conditional probability of the next action is:
</p>
<p>p(at|ut) =
|a|∏
</p>
<p>t=1
</p>
<p>exp(atu
T
t + ba)∑
</p>
<p>a′∈A exp(a
′uTt + ba′)
</p>
<p>(2)
</p>
<p>where at represents the action embedding at time
step t, A the action space and ba the bias. Simi-
larly, the next word probability (when GEN is in-
voked) is computed as:
</p>
<p>p(wt|ut) =
|a|∏
</p>
<p>t=1
</p>
<p>exp(wtu
T
t + bw)∑
</p>
<p>w′∈W exp(w
′uTt + bw′)
</p>
<p>(3)
</p>
<p>3We assume that the action probability does not take the
actual terminal choice into account.
</p>
<p>119</p>
<p />
</div>
<div class="page"><p />
<p>whereW denotes all words in the vocabulary.
To satisfy the independence assumptions im-
</p>
<p>posed by the generative model, ut uses only a
restricted set of features defined over the output
buffer and the stack — we consider p(a) as a con-
text insensitive prior distribution. Specifically, we
use the following features: 1) the stack embed-
ding dt which encodes the stack of the decoder
and is obtained with a stack-LSTM (Dyer et al.,
2015, 2016); 2) the output buffer embedding ot;
we use a standard LSTM to compose the output
buffer and ot is represented as the most recent state
of the LSTM; and 3) the parent non-terminal em-
bedding nt which is accessible in the generative
model because the RNNG employs a depth-first
generation order. Finally, ut is computed as:
</p>
<p>ut = W2 tanh(W1[dt,ot,nt] + bd) (4)
</p>
<p>where Ws are weight parameters and bd the bias.
</p>
<p>3.2 Encoder (Recognition Model)
The encoder is a discriminative RNNG that com-
putes the conditional probability q(a|x) of the
transition action sequence a given an observed
sentence x. This conditional probability is factor-
ized over time steps as:
</p>
<p>q(a|x) =
|a|∏
</p>
<p>t=1
</p>
<p>q(at|vt) (5)
</p>
<p>where vt is the transitional state embedding of the
encoder at time step t.
</p>
<p>The next action is predicted similarly to Equa-
tion (2), but conditioned on vt. Thanks to the dis-
criminative property, vt has access to any contex-
tual features defined over the entire sentence and
the stack — q(a|x) acts as a context sensitive pos-
terior approximation. Our features4 are: 1) the
stack embedding et obtained with a stack-LSTM
that encodes the stack of the encoder; 2) the in-
put buffer embedding it; we use a bidirectional
LSTM to compose the input buffer and repre-
sent each word as a concatenation of forward and
backward LSTM states; it is the representation
of the word on top of the buffer; 3) to incorpo-
rate more global features and a more sophisticated
look-ahead mechanism for the buffer, we also use
an adaptive buffer embedding īt; the latter is com-
puted by having the stack embedding et attend to
</p>
<p>4Compared to Dyer et al. (2016), the new features we in-
troduce are 3) and 4), which we found empirically useful.
</p>
<p>all remaining embeddings on the buffer with the
attention function in Vinyals et al. (2015); and
4) the parent non-terminal embedding nt. Finally,
vt is computed as follows:
</p>
<p>vt = W4 tanh(W3[et, it, īt,nt] + be) (6)
</p>
<p>where Ws are weight parameters and be the bias.
</p>
<p>3.3 Training
</p>
<p>Consider an auto-encoder whose encoder infers
the latent parse tree and the decoder generates the
observed sentence from the parse tree.5 The max-
imum likelihood estimate of the decoder parame-
ters is determined by the log marginal likelihood
of the sentence:
</p>
<p>log p(x) = log
∑
</p>
<p>a
</p>
<p>p(x, a) (7)
</p>
<p>We follow expectation-maximization and varia-
tional inference techniques to construct an ev-
idence lower bound of the above quantity (by
Jensen’s Inequality), denoted as follows:
</p>
<p>log p(x) ≥ Eq(a|x) log
p(x, a)
</p>
<p>q(a|x) = Lx (8)
</p>
<p>where p(x, a) = p(x|a)p(a) comes from the de-
coder or the generative model, and q(a|x) comes
from the encoder or the recognition model. The
objective function6 in Equation (8), denoted by
Lx, is unsupervised and suited to a grammar in-
duction task. This objective can be optimized with
the methods shown in Miao and Blunsom (2016).
</p>
<p>Next, consider the case when the parse tree
is observed. We can directly maximize the log
likelihood of the parse tree for the encoder out-
put log q(a|x) and the decoder output log p(a):
</p>
<p>La = log q(a|x) + log p(a) (9)
</p>
<p>This supervised objective leverages extra informa-
tion of labeled parse trees to regularize the distri-
bution q(a|x) and p(a), and the final objective is:
</p>
<p>L = Lx + La (10)
where Lx and La can be balanced with the task
focus (e.g, language modeling or parsing).
</p>
<p>5Here, GEN and SHIFT refer to the same action with dif-
ferent definitions for encoding and decoding.
</p>
<p>6See § 4 and Appendix A for comparison between this
objective and the importance sampler of Dyer et al. (2016).
</p>
<p>120</p>
<p />
</div>
<div class="page"><p />
<p>Learned word embedding dimensions 40
Pretrained word embedding dimensions 50
POS tag embedding dimensions 20
Encoder LSTM dimensions 128
Decoder LSTM dimensions 256
LSTM layer 2
Encoder dropout 0.2
Decoder dropout 0.3
</p>
<p>Table 1: Hyperparameters.
</p>
<p>3.4 Inference
</p>
<p>We consider two inference tasks, namely parsing
and language modeling.
</p>
<p>Parsing In parsing, we are interested in the
parse tree that maximizes the posterior p(a|x) (or
the joint p(a, x)). However, the decoder alone
does not have a bottom-up recognition mecha-
nism for computing the posterior. Thanks to the
encoder, we can compute an approximated pos-
terior q(a|x) in linear time and select the parse
tree that maximizes this approximation. An al-
ternative is to generate candidate trees by sam-
pling from q(a|x), re-rank them with respect to
the joint p(x, a) (which is proportional to the true
posterior), and select the sample that maximizes
the true posterior.
</p>
<p>Language Modeling In language modeling, our
goal is to compute the marginal probability
p(x) =
</p>
<p>∑
a p(x, a), which is typically intractable.
</p>
<p>To approximate this quantity, we can use Equa-
tion (8) to compute a lower bound of the log like-
lihood log p(x) and then exponentiate it to get a
pessimistic approximation of p(x).7
</p>
<p>Another way of computing p(x) (without lower
bounding) would be to use the variational approxi-
mation q(a|x) as the proposal distribution as in the
importance sampler of Dyer et al. (2016). How-
ever, this is beyond the scope of this work and we
leave detailed discussions to Appendix A.
</p>
<p>4 Related Work
</p>
<p>Our framework is related to a class of variational
autoencoders (Kingma and Welling, 2014), which
use neural networks for posterior approximation
in variational inference. This technique has been
previously used for topic modeling (Miao et al.,
</p>
<p>7As a reminder, the language modeling objective is
exp(NLL/T ), where NLL denotes the total negative log
likelihood of the test data and T the token counts.
</p>
<p>Discrimina-
tive parsers
</p>
<p>Socher et al. (2013) 90.4
Zhu et al. (2013) 90.4
Dyer et al. (2016) 91.7
Cross and Huang (2016) 89.9
Vinyals et al. (2015) 92.8
</p>
<p>Generative
parsers
</p>
<p>Petrov and Klein (2007) 90.1
Shindo et al. (2012) 92.4
Dyer et al. (2016) 93.3
</p>
<p>This work
argmaxa q(a|x) 89.3
argmaxa p(a, x) 90.1
</p>
<p>Table 2: Parsing results (F1) on the PTB test set.
</p>
<p>2016) and sentence compression (Miao and Blun-
som, 2016). Another interpretation of the pro-
posed framework is from the perspective of guided
policy search in reinforcement learning (Bachman
and Precup, 2015), where a generative parser is
trained to imitate the trace of a discriminative
parser. Further connections can be drawn with
the importance-sampling based inference of Dyer
et al. (2016). There, a generative RNNG and a
discriminative RNNG are trained separately; dur-
ing language modeling, the output of the discrim-
inative model serves as the proposal distribution
of an importance sampler p(x) = Eq(a|x)
</p>
<p>p(x,a)
q(a|x) .
</p>
<p>Compared to their work, we unify the generative
and discriminative RNNGs in a single framework,
and adopt a joint training objective.
</p>
<p>5 Experiments
</p>
<p>We performed experiments on the English Penn
Treebank dataset; we used sections 2–21 for train-
ing, 24 for validation, and 23 for testing. Follow-
ing Dyer et al. (2015), we represent each word
in three ways: as a learned vector, a pretrained
vector, and a POS tag vector. The encoder word
embedding is the concatenation of all three vec-
tors while the decoder uses only the first two since
we do not consider POS tags in generation. Ta-
ble 1 presents details on the hyper-parameters we
used. To find the MAP parse tree argmaxa p(a, x)
(where p(a, x) is used rank the output of q(a|x))
and to compute the language modeling perplex-
ity with the evidence lower bound (where a ∼
q(a|x)), we collect 100 samples from q(a|x),
same as Dyer et al. (2016).
</p>
<p>Experimental results for constituency parsing
and language modeling are shown in Tables 2
and 3, respectively. As can be seen, the single
framework we propose obtains competitive pars-
</p>
<p>121</p>
<p />
</div>
<div class="page"><p />
<p>KN-5 255.2
LSTM 113.4
Dyer et al. (2016) 102.4
This work: a ∼ q(a|x) 99.8
</p>
<p>Table 3: Language modeling results (perplexity).
</p>
<p>ing performance. Comparing the two inference
methods for parsing, ranking approximated MAP
trees from q(a|x) with respect to p(a, x) yields a
small improvement, as in Dyer et al. (2016). It is
worth noting that our parsing performance lags be-
hind Dyer et al. (2016). We believe this is due to
implementation disparities, such as the modeling
of the reduce operation. While Dyer et al. (2016)
use an LSTM as the syntactic composition func-
tion of each subtree, we adopt a rather simple com-
position function based on embedding averaging
for memory concern.
</p>
<p>On language modeling, our framework achieves
lower perplexity compared to Dyer et al. (2016)
and baseline models. This gain possibly comes
from the joint optimization of both the genera-
tive and discriminative components towards a lan-
guage modeling objective. However, we acknowl-
edge a subtle difference between Dyer et al. (2016)
and our approach compared to baseline language
models: while the latter incrementally estimate
the next word probability, our approach (and Dyer
et al. 2016) directly assigns probability to the en-
tire sentence. Overall, the advantage of our frame-
work compared to Dyer et al. (2016) is that it
opens an avenue to unsupervised training.
</p>
<p>6 Conclusions
</p>
<p>We proposed a framework that integrates a gen-
erative parser with a discriminative recognition
model and showed how it can be instantiated with
RNNGs. We demonstrated that a unified frame-
work, which relates to expectation maximization
and variational inference, enables effective pars-
ing and language modeling algorithms. Evalua-
tion on the English Penn Treebank, revealed that
our framework obtains competitive performance
on constituency parsing and state-of-the-art results
on single-model language modeling. In the fu-
ture, we would like to perform grammar induc-
tion based on Equation (8), with gradient descent
and posterior regularization techniques (Ganchev
et al., 2010).
</p>
<p>Acknowledgments We thank three anonymous
reviewers and members of the ILCC for valu-
able feedback, and Muhua Zhu and James Cross
for help with data preparation. The support of
the European Research Council under award num-
ber 681760 “Translating Multiple Modalities into
Text” is gratefully acknowledged.
</p>
<p>References
Philip Bachman and Doina Precup. 2015. Data gener-
</p>
<p>ation as sequential decision making. In Advances in
Neural Information Processing Systems, MIT Press,
pages 3249–3257.
</p>
<p>Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Pennsylvania, Philadel-
phia.
</p>
<p>Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics 31(1):25–70.
</p>
<p>James Cross and Liang Huang. 2016. Incremental
parsing with minimal features using bi-directional
lstm. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers). Berlin, Germany, pages 32–
37.
</p>
<p>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers). Beijing, China, pages 334–343.
</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. San Diego, California, pages
199–209.
</p>
<p>Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar,
et al. 2010. Posterior regularization for structured
latent variable models. Journal of Machine Learn-
ing Research 11(Jul):2001–2049.
</p>
<p>Zoubin Ghahramani and Matthew J. Beal. 2000. Vari-
ational inference for Bayesian mixtures of factor
analysers. In Advances in Neural Information Pro-
cessing Systems, MIT Press, pages 449–455.
</p>
<p>Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. Los Angeles, California, pages 742–750.
</p>
<p>122</p>
<p />
</div>
<div class="page"><p />
<p>James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics. Edmon-
ton, Canada, pages 24–31.
</p>
<p>Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT . Columbus, Ohio, pages 586–594.
</p>
<p>Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in Neural Information Processing Systems, MIT
Press, pages 3581–3589.
</p>
<p>Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational Bayes. In Proceedings of the
International Conference on Learning Representa-
tions. Banff, Canada.
</p>
<p>Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing. Austin, Texas, pages 319–328.
</p>
<p>Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In Pro-
ceedings of The 33rd International Conference on
Machine Learning. New York, New York, USA,
pages 1727–1736.
</p>
<p>Andriy Mnih and Karol Gregor. 2014. Neural vari-
ational inference and learning in belief networks.
In Proceedings of the 31st International Conference
on Machine Learning. Beijing, China, pages 1791–
1799.
</p>
<p>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser: A
language-independent system for data-driven de-
pendency parsing. Natural Language Engineering
13(2):95–135.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Doha, Qatar, pages 1532–
1543.
</p>
<p>Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence. Rochester, New York, pages 404–411.
</p>
<p>Reuven Y Rubinstein and Dirk P Kroese. 2008. Simu-
lation and the Monte Carlo method. John Wiley &amp;
Sons.
</p>
<p>Federico Sangati, Willem Zuidema, and Rens Bod.
2009. A generative re-ranking model for depen-
dency parsing. In Proceedings of the 11th In-
ternational Conference on Parsing Technologies
(IWPT’09). Paris, France, pages 238–241.
</p>
<p>Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1. Jeju Island, Korea, pages 440–
448.
</p>
<p>Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Sofia,
Bulgaria, pages 455–465.
</p>
<p>Ivan Titov and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics. Prague,
Czech Republic, pages 632–639.
</p>
<p>Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, MIT Press, pages
2773–2781.
</p>
<p>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). Sofia,
Bulgaria, pages 434–443.
</p>
<p>A Comparison to Importance Sampling
(Dyer et al., 2016)
</p>
<p>In this appendix we highlight the connections be-
tween importance sampling and variational infer-
ence, thereby comparing our method with Dyer
et al. (2016).
</p>
<p>Consider a simple directed graphical model
with discrete latent variables a (e.g., a is the tran-
sition action sequence) and observed variables x
(e.g., x is the sentence). The model evidence, or
the marginal likelihood p(x) =
</p>
<p>∑
a p(x, a) is of-
</p>
<p>ten intractable to compute. Importance sampling
transforms the above quantity into an expectation
over a distribution q(a), which is known and easy
to sample from:
</p>
<p>p(x) =
∑
</p>
<p>a
</p>
<p>p(x, a)
q(a)
</p>
<p>q(a)
= Eq(a)w(x, a) (11)
</p>
<p>where q(a) is the proposal distribution
and w(x, a) = p(x,a)q(a) the importance weight.
</p>
<p>123</p>
<p />
</div>
<div class="page"><p />
<p>The proposal distribution can potentially depend
on the observations x, i.e., q(a) , q(a|x).
</p>
<p>A challenge with importance sampling lies in
choosing a proposal distribution which leads to
low variance. As shown in Rubinstein and Kroese
(2008), the optimal choice of the proposal distri-
bution is in fact the true posterior p(a|x), in which
case the importance weight p(a,x)p(a|x) = p(x) is con-
stant with respect to a. In Dyer et al. (2016), the
proposal distribution depends on x, i.e., q(a) ,
q(a|x), and is computed with a separately-trained,
discriminative model. This proposal choice is
close to optimal, since in a fully supervised setting
a is also observed and the discriminative model
can be trained to approximate the true posterior
well. We hypothesize that the performance of their
importance sampler is dependent on this specific
proposal distribution. Besides, their training strat-
egy does not generalize to an unsupervised setting.
</p>
<p>In comparison, variational inference approach
approximates the log marginal likelihood log p(x)
with the evidence lower bound. It is a natural
choice when one aims to optimize Equation (11)
directly:
</p>
<p>log p(x) = log
∑
</p>
<p>a
</p>
<p>p(x, a)
q(a)
</p>
<p>q(a)
</p>
<p>≥ Eq(a) log
p(x, a)
</p>
<p>q(a)
</p>
<p>(12)
</p>
<p>where q(a) is the variational approximation of
the true posterior. Again, the variational approx-
imation can potentially depend on the observa-
tion x (i.e., q(a) , q(a|x)) and can be com-
puted with a discriminative model. Equation (12)
is a well-defined, unsupervised training objec-
tive which allows us to jointly optimize genera-
tive (i.e., p(x, a)) and discriminative (i.e., q(a|x))
models. To further support the observed vari-
able a, we augment this objective with supervised
terms shown in Equation (10), following Kingma
et al. (2014) and Miao and Blunsom (2016).
</p>
<p>Equation (12) can be also used to approximate
the marginal likelihood p(x) (e.g., in language
modeling) with its lower bound. An alternative
choice without lower bounding is to use the vari-
ational approximation q(a|x) as the proposal dis-
tribution in importance sampling (Equation (11)).
Ghahramani and Beal (2000) show that this pro-
posal distribution leads to improved results of im-
portance samplers. However, a potential drawback
of importance sampling-based approach is that it
</p>
<p>is prone to numerical underflow. In practice, we
observed similar language modeling performance
for both methods.
</p>
<p>124</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 125–131
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2020
</p>
<p>Hybrid Neural Network Alignment and Lexicon Model in Direct HMM
for Statistical Machine Translation
</p>
<p>Weiyue Wang, Tamer Alkhouli, Derui Zhu, Hermann Ney
Human Language Technology and Pattern Recognition, Computer Science Department
</p>
<p>RWTH Aachen University, 52056 Aachen, Germany
&lt;surname&gt;@i6.informatik.rwth-aachen.de
</p>
<p>Abstract
</p>
<p>Recently, the neural machine translation
systems showed their promising perfor-
mance and surpassed the phrase-based
systems for most translation tasks. Re-
treating into conventional concepts ma-
chine translation while utilizing effective
neural models is vital for comprehend-
ing the leap accomplished by neural ma-
chine translation over phrase-based meth-
ods. This work proposes a direct hid-
den Markov model (HMM) with neu-
ral network-based lexicon and alignment
models, which are trained jointly using the
Baum-Welch algorithm. The direct HMM
is applied to rerank the n-best list created
by a state-of-the-art phrase-based transla-
tion system and it provides improvements
by up to 1.0% BLEU scores on two differ-
ent translation tasks.
</p>
<p>1 Introduction
</p>
<p>The hidden Markov model (HMM) was first in-
troduced to statistical machine translation for ad-
dressing the word alignment problem (Vogel et al.,
1996). Then the HMM-based approach was
widely used along with the IBM models (Brown
et al., 1993) for aligning the source and target
words. In the conventional approach, the Bayes’
theorem is used and the HMM is applied to the
inverse translation model
</p>
<p>Pr(eI1|fJ1 ) = Pr(eI1) · Pr(fJ1 |eI1)
=
∑
</p>
<p>aJ1
</p>
<p>Pr(fJ1 , a
J
1 |eI1) (1)
</p>
<p>In this case, as a part of a noisy channel model, the
marginalisation becomes intractable for every e.
</p>
<p>This work proposes a novel concept focusing on
direct HMM for Pr(eI1|fJ1 ), in which the align-
ment direction is from target to source positions.
This specific property allows us to introduce de-
pendencies into the translation model that take
the full source sentence into account. This as-
pect will be important for the future decoder to
be developed. The lexicon and alignment prob-
abilities in the HMM are modeled using feed-
forward neural networks (FFNN) and they are
trained jointly. The trained HMM is then ap-
plied for reranking the n-best lists created by a
state-of-the-art open source phrase-based transla-
tion system. The experiments are conducted on
the IWSLT 2016 German→English and BOLT
Chinese→English translation tasks. The FFNN-
based hybrid HMM provides improvements by up
to 1.0% BLEU scores.
</p>
<p>2 Related Work
</p>
<p>In order to discuss related work, we will consider
the following two key concepts that are essential
for the work to be presented:
</p>
<p>• Neural lexicon and alignment models
The idea of using neural networks for lexicon
modeling is not new (Schwenk, 2012; Sun-
dermeyer et al., 2014; Devlin et al., 2014).
Apart from differences in the neural network
architecture, the important difference to this
work is that those approaches did not include
the concepts of HMM models and end-to-end
training. In addition to neural lexicon model-
ing, (Alkhouli et al., 2016) also applied a neu-
ral network for alignment modeling like this
work, but their training procedure was based
on the maximum approximation and on pre-
defined GIZA++ (Och and Ney, 2003) align-
ments.
</p>
<p>125</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2020">https://doi.org/10.18653/v1/P17-2020</a></div>
</div>
<div class="page"><p />
<p>There were other studies that focused on
feature-rich alignment models (Blunsom and
Cohn, 2006; Berg-Kirkpatrick et al., 2010;
Dyer et al., 2011), but those studies did not
use a neural network to automatically learn
features (as we do in this work). (Yang et al.,
2013) used neural network-based lexicon and
alignment models inside the HMM alignment
model, but they model alignments using a
simple distortion model that has no depen-
dence on lexical context. Their goal was to
improve the alignment quality in the context
of a phrase-based translation system. How-
ever, apart from (Dyer et al., 2011), no results
on translation were reported.
</p>
<p>The idea of using neural networks is the
basis of the state-of-the-art attention-based
approach to machine translation (Bahdanau
et al., 2015; Luong et al., 2015). However,
that approach is not based on the principle of
an explicit and separate lexicon model.
</p>
<p>• End-to-end training
The HMM in combination with the neural
translation model lends itself to what is usu-
ally called end-to-end training. The training
criterion is the logarithm of the target sen-
tence posterior probability. This criterion re-
sults in a specific training algorithm that can
be interpreted as a combination of forward-
backward algorithm (as in EM style training
of HHMs) and backpropagation. To the best
of our knowledge, this end-to-end training
has not been considered before for machine
translation. In the context of signal process-
ing and recognition, the connectionist tem-
poral classification (CTC) approach (Graves
et al., 2006) leads to a similar training proce-
dure. (Tran et al., 2016) studied neural net-
works for unsupervised training for a part-of-
speech tagging task. In their approach, the
training criterion for this problem results in
a combination of EM framework and back-
propagation, which has a certain similarity to
the training algorithm for translation as pre-
sented in this work.
</p>
<p>3 Definition of neural network-based
HMM
</p>
<p>Similar to hidden alignments aj = j → i between
the source string fJ1 = f1...fj ...fJ and the target
</p>
<p>string eI1 = e1...ei...eI in the conventional HMM,
we define the alignments in direct HMM as bi =
i→ j. Then the model can be defined as:
</p>
<p>Pr(eI1|fJ1 ) =
∑
</p>
<p>bI1
</p>
<p>Pr(eI1, b
I
1|fJ1 ) (2)
</p>
<p>Pr(eI1, b
I
1|fJ1 )
</p>
<p>=
I∏
</p>
<p>i=1
</p>
<p>p(ei, bi|bi−11 , ei−11 , fJ1 )
</p>
<p>=
I∏
</p>
<p>i=1
</p>
<p>p(ei|bi1, ei−11 , fJ1 )︸ ︷︷ ︸
lexicon model
</p>
<p>· p(bi|bi−11 , ei−11 , fJ1 )︸ ︷︷ ︸
alignment model
</p>
<p>(3)
</p>
<p>Our feed-forward alignment model has the
same architecture (Figure 1) as the one proposed
in (Alkhouli et al., 2016). Thus the alignment
probability can be modeled by:
</p>
<p>p(bi|bi−11 , ei−11 , fJ1 ) = p(∆i|f
bi−1+γm
bi−1−γm , e
</p>
<p>i−1
i−n)
</p>
<p>(4)
where γm = m−12 and m indicates the window
size. ∆i = bi − bi−1 denotes the jump from the
predecessor position to the current position. Thus,
the jump over the source is estimated based on a
m-words source context window and n predeces-
sor target words.
</p>
<p>fbi−1−2 fbi−1−1 fbi−1 fbi−1+1 fbi−1+2 ei−3 ei−2 ei−1
</p>
<p>p(∆i|f bi−1+2bi−1−2 , e
i−1
i−3)
</p>
<p>Figure 1: A feed-forward alignment neural net-
work with 3 target history words, 5-gram source
window, a projection layer, 2 non-linear hidden
layers and a small output layer to predict jumps.
</p>
<p>For the lexicon model, we assume a similar de-
pendence as in the alignment model with a shift,
namely on the source words within a window cen-
tred on the aligned source word and n predecessor
target words. To overcome the high costs of the
softmax function for large vocabularies, we adopt
the class-factored output layer consisting of a class
layer and a word layer (Goodman, 2001; Morin
</p>
<p>126</p>
<p />
</div>
<div class="page"><p />
<p>and Bengio, 2005). The model in this case is de-
fined as
</p>
<p>p(ei|bi1, ei−11 , fJ1 )
= p(ei|f bi+γmbi−γm , e
</p>
<p>i−1
i−n)
</p>
<p>= p(ei|c(ei), f bi+γmbi−γm , e
i−1
i−n) · p(c(ei)|f
</p>
<p>bi+γm
bi−γm , e
</p>
<p>i−1
i−n)
</p>
<p>(5)
where c denotes a word mapping that assigns each
target word to a single class, where the number
of classes is chosen to be much smaller than the
vocabulary size. The lexicon model architecture is
shown in Figure 2.
</p>
<p>fbi−2 fbi−1 fbi fbi+1 fbi+2 ei−3 ei−2 ei−1
</p>
<p>p(c(ei)|f bi+γmbi−γm , e
i−1
i−n) p(ei|c(ei), f bi+γmbi−γm , e
</p>
<p>i−1
i−n)
</p>
<p>Figure 2: A feed-forward lexicon neural network
with the same structure as the alignment model,
except a class-factored output layer.
</p>
<p>4 Training
</p>
<p>The training data of the direct HMM are the source
and target sequences, without any alignment in-
formation. In the training of direct HMM in-
cluding neural network-based models, the weights
have to be updated along with the posterior prob-
abilities calculated by the Baum-Welch algorithm.
Similar to the training procedure used in (Berg-
Kirkpatrick et al., 2010), we apply the EM algo-
rithm and define the auxiliary function as
</p>
<p>Q(θ; θ̂)
</p>
<p>=
∑
</p>
<p>bI1
</p>
<p>p(bI1|fJ1 , eI1, θ) log p(eI1, bI1|fJ1 , θ̂)
</p>
<p>=
∑
</p>
<p>bI1
</p>
<p>p(bI1|fJ1 , eI1, θ)
I∑
</p>
<p>i=1
</p>
<p>[log p(ei|fbi+γmbi−γm , e
i−1
i−n, α̂)
</p>
<p>+ log p(∆i|fbi−1+γmbi−1−γm , e
i−1
i−n, β̂)]
</p>
<p>=
∑
</p>
<p>i
</p>
<p>∑
</p>
<p>j
</p>
<p>pi(j|eI1, fJ1 , θ) log p(ei|f j+γmj−γm , e
i−1
i−n, α̂)
</p>
<p>+
∑
</p>
<p>i
</p>
<p>∑
</p>
<p>j′
</p>
<p>∑
</p>
<p>j
</p>
<p>pi(j
′, j|eI1, fJ1 , θ) log p(∆i|f j
</p>
<p>′+γm
j′−γm , e
</p>
<p>i−1
i−n, β̂)
</p>
<p>(6)
</p>
<p>where θ̂ = {α̂, β̂}, j′ = bi−1 and
</p>
<p>pi(j|eI1, fJ1 , θ) =
∑
</p>
<p>bI1:bi=j
</p>
<p>p(bI1|eI1, fJ1 , θ) (7)
</p>
<p>Then the parameters can be separated for lexi-
con model and alignment model:
</p>
<p>Q(θ; θ̂) = Qlex(θ; α̂) +Qalign(θ; β̂) (8)
</p>
<p>where
</p>
<p>∂Qlex(θ, α̂)
</p>
<p>∂α̂
=
∑
</p>
<p>i
</p>
<p>∑
</p>
<p>j
</p>
<p>forward-backward algorithm︷ ︸︸ ︷
pi(j|eI1, fJ1 , θ)
</p>
<p>· ∂
∂α̂
</p>
<p>log p(ei|f j+γmj−γm , e
i−1
i−n, α̂)
</p>
<p>︸ ︷︷ ︸
backpropagation
</p>
<p>(9)
</p>
<p>∂Qalign(θ, β̂)
</p>
<p>∂β̂
=
∑
</p>
<p>i
</p>
<p>∑
</p>
<p>j′
</p>
<p>∑
</p>
<p>j
</p>
<p>forward-backward algorithm︷ ︸︸ ︷
pi(j
</p>
<p>′, j|eI1, fJ1 , θ)
</p>
<p>· ∂
∂β̂
</p>
<p>log p(∆i|f j
′+γm
j′−γm , e
</p>
<p>i−1
i−n, β̂)
</p>
<p>︸ ︷︷ ︸
backpropagation
</p>
<p>(10)
From Equations (9) and (10) we can observe
</p>
<p>that the marginalisation of hidden alignments
(
∑
</p>
<p>j pi(j|eI1, fJ1 , θ)) is the only difference com-
pared to the derivative of neural network train-
ing based on word-aligned data. In this approach
we iterate over all source positions and the word
alignment toolkit such as GIZA++ is not required.
Furthermore, the word-aligned data generated e.g.
by GIZA++ might contain unaligned and multiply
aligned words, which make the data difficult to use
for training neural networks. Thus the heuristic-
based approaches (Sundermeyer et al., 2014; De-
vlin et al., 2014) have to be used in order to guar-
antee the one-on-one alignments, which may neg-
atively influence the quality of the alignments. By
contrast, the neural network-based HMM is not
constrained by these heuristics. In addition, even
though the training process of the direct HMM
takes more time than the neural network training
on the word-aligned data, we should note that gen-
erating the word-aligned data using GIZA++ is
also a time-consuming process.
</p>
<p>In general, our training procedure can be sum-
marized as follows:
</p>
<p>127</p>
<p />
</div>
<div class="page"><p />
<p>1. One iteration IBM-1 model training to cre-
ate lexicon table for initializing the forward-
backward table.
</p>
<p>2. In the first epoch, for each sentence
pair calculate and save the entire table
of posterior probabilities pi(b|eI1, fJ1 ) (also
pi(b
</p>
<p>′, b|eI1, fJ1 ) for alignment model) using
forward-backward algorithm based on the re-
sults of IBM-1 model.
</p>
<p>3. Training neural network lexicon and align-
ment models based on the posterior probabil-
ities.
</p>
<p>4. From the second epoch onwards:
</p>
<p>(a) For each sentence pair, calculating the
posterior probabilities based on the lex-
icon and alignment probabilities esti-
mated by neural network models.
</p>
<p>(b) Updating weights of neural networks
based on the posterior probabilities.
</p>
<p>(c) Repeating step 4 until the perplexity
converges.
</p>
<p>In this work the IBM-1 initialization is required.
We tried to train neural network models from
scratch, but the perplexity converges towards a bad
local minimum and gets stuck in it. We also at-
tempted other heuristics for initialization, such as
assigning probability 0.9 to diagonal alignments
and spreading the left 0.1 evenly among other
source positions. The resulted perplexity is much
higher compared to initializing using IBM-1.
</p>
<p>5 Experimental Results
</p>
<p>The experiments are conducted on the
IWSLT 2016 German→English and BOLT
Chinese→English translation tasks, which con-
sist of 20M and 4M parallel sentence pairs
respectively. The feed-forward neural network
alignment and lexicon models are jointly trained
on the subset of about 200K sentence pairs. As
an initial research of this topic, our new model
is only applied for reranking n-best lists created
by a phrase-based decoder. The maximum size
of the n-best lists is 500. The translation quality
is evaluated by case-insensitive BLEU (Papineni
et al., 2002) and TER (Snover et al., 2006) metrics
using MultEval (Clark et al., 2011). The scaling
factors are tuned with MERT (Och, 2003) with
BLEU as optimization criterion on the develop-
ment sets. For the translation experiments, the
</p>
<p>averaged scores are presented on the development
set from three optimization runs.
</p>
<p>Our direct HMM consists of a feed-forward
neural network lexicon model with following con-
figuration:
</p>
<p>• Five one-hot input vectors for source words
and three for target words
</p>
<p>• Projection layer size 100 for each word
• Two non-linear hidden layers with 1000 and
</p>
<p>500 nodes respectively
</p>
<p>• A class-factored output layer with 1000 sin-
gleton classes dedicated to the most frequent
words, and 1000 classes shared among the
rest of the words.
</p>
<p>and a feed-forward neural network alignment
model with the same configuration as the lexicon
model, except a small output layer with 201 nodes,
which reflects that the aligned position can jump
within the scope from −100 to 100 (Alkhouli
et al., 2016).
</p>
<p>We conducted experiments on the source and
target window size of both network models.
Larger source and target windows could not pro-
vide significant improvements on BLEU scores, at
least for rescoring experiments.
</p>
<p>The model is applied for reranking the n-best
lists created by the Jane toolkit (Vilar et al., 2010;
Wuebker et al., 2012) with a log-linear framework
containing phrasal and lexical smoothing models
for both directions, word and phrase penalties, a
distance-based reordering model, enhanced low
frequency features (Chen et al., 2011), a hierarchi-
cal reordering model (Galley and Manning, 2008),
a word class language mode (Wuebker et al., 2013)
and an n-gram language model. The word align-
ments used for the training of phrase-tables are
generated by GIZA++, which performs the align-
ment training sequentially for IBM-1, HMM and
IBM-4. More details about our phrase-based base-
line system can be found in (Peter et al., 2015).
</p>
<p>The experimental results are demonstrated in
Table 1. The rescoring experiments are conducted
by adding HMM probability as feature and tuned
with MERT. The applied attention-based neural
network is a neural machine translation system
similar to (Bahdanau et al., 2015). The decoder
and encoder word embeddings are of size 620,
the encoder uses a bidirectional layer with 1000
LSTMs (Hochreiter and Schmidhuber, 1997) to
encode the source side. A layer with 1000 LSTMs
</p>
<p>128</p>
<p />
</div>
<div class="page"><p />
<p>Table 1: Experimental results of rescoring using neural network-based direct HMM. The model with
sum denotes the system proposed in this work, while the model with Viterbi denotes the model with the
same neural network structure, which was trained based on the word-aligned data (alignments generated
by GIZA++) (Alkhouli et al., 2016). Improvements by systems marked by ∗ have a 95% statistical
significance from the NN-based direct HMM (Viterbi) system, whereas † denotes the 95% statistical
significant improvements with respect to the attention-based system in rescoring. 1 was used in reranking
the n-best lists, while 2 denotes the stand-alone attention-based decoder.
</p>
<p>IWSLT BOLT
TEDX.tst.2014 MSLT.dev2016 DEV12 P1R6
</p>
<p>BLEU[%] TER[%] BLEU[%] TER[%] BLEU[%] TER[%] BLEU[%] TER[%]
</p>
<p>Phrase-based translation system 25.9 55.7 39.7 39.8 17.9 68.3 17.1 67.4
+ NN-based direct HMM (Viterbi) 26.6 54.9 40.2 39.3 18.5 67.6 17.8 66.8
+ NN-based direct HMM (sum) 26.9 54.7 40.6∗ 38.9∗† 18.9∗ 67.3∗† 18.3∗ 66.4
+ attention-based system 1 26.8 55.0 40.4 39.3 18.9∗ 67.6 18.0 66.4∗
</p>
<p>Stand-alone attention-based system 2 27.0 54.8 40.4 39.2 19.6∗† 67.0∗† 18.5∗† 66.1∗
</p>
<p>is used by the decoder. The data is converted
into subword units using byte pair encoding with
20000 operations (Sennrich et al., 2016). During
training a batch size of 50 is used. More details
about our neural machine translation system can
be found in (Peter et al., 2016).
</p>
<p>With n-best rescoring, all neural network-based
systems achieve significant improvements over the
phrase-based system. The neural network-based
HMMs provide promising performance, even with
simple feed-forward neural networks. The direct
HMM trained by the EM procedure with marginal-
izing the hidden alignments outperformed the
same model trained on the word-aligned data.
For the rescoring tasks, it provides comparable
performance with the attention-based network.
The neural network-based HMM also helps the
phrase-based system achieve comparable results
with the stand-alone attention-based system on the
German→English task.
</p>
<p>6 Conclusion and Future Work
</p>
<p>This work aims to close the gap between the con-
ventional word alignment models and the novel
neural machine translation. The proposed di-
rect HMM consists of neural network-based align-
ment and lexicon models, both models are trained
jointly and without any alignment information.
With the simple feed-forward neural network
models, the HMM model already provides promis-
ing results and significantly improves the strong
phrase-based translation system.
</p>
<p>As future work, we are searching for alter-
natives to initialize the training instead of using
IBM-1. We will investigate recurrent model struc-
</p>
<p>tures, such as the LSTM representation for source
and target word embeddings (Luong et al., 2015).
In addition to the network structure, we will im-
plement a stand-alone decoder based on this novel
model. The first step would be to apply maxi-
mum approximation for the search problem as elu-
cidated in (Yu et al., 2017). Then we plan to in-
vestigate heuristics for marginalizing the hidden
alignment during search.
</p>
<p>Acknowledgments
</p>
<p>The work reported in this
paper results from two
projects, SEQCLAS and
QT21. SEQCLAS has
received funding from
</p>
<p>the European Research Council (ERC) under
the European Union’s Horizon 2020 research
and innovation programme under grant agree-
ment no 694537. QT21 has received funding
from the European Union’s Horizon 2020 research
and innovation programme under grant agreement
no 645452. The work reflects only the authors’
views and neither the European Commission nor
the European Research Council Executive Agency
are responsible for any use that may be made of the
information it contains.
</p>
<p>Tamer Alkhouli was partly funded by the 2016
Google PhD Fellowship for North America, Eu-
rope and the Middle East.
</p>
<p>The authors would like to thank Jan-Thorsten
Peter for providing the attention-based neural net-
work models.
</p>
<p>129</p>
<p />
</div>
<div class="page"><p />
<p>References
Tamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Pe-
</p>
<p>ter, Mohammed Hethnawi, Andreas Guta, and Her-
mann Ney. 2016. Alignment-Based Neural Machine
Translation. In Proceedings of the ACL 2016 First
Conference on Machine Translation (WMT 2016).
Berlin, Germany, pages 54–65.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations. San Diego, CA, USA.
</p>
<p>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
John DeNero, and Dan Klein. 2010. Painless Un-
supervised Learning with Features. In Proceedings
of the 2010 Annual Conference of the North Amer-
ican Chapter of the ACL. Los Angeles, CA, USA,
pages 582–590.
</p>
<p>Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the ACL. Sydney, Australia, pages 65–72.
</p>
<p>Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics
19(2):263–311.
</p>
<p>Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and Transform-
ing Feature Functions: New Ways to Smooth Phrase
Tables. In Proceedings of MT Summit XIII. Xiamen,
China, pages 269–275.
</p>
<p>Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better Hypothesis Testing for
Statistical Machine Translation: Controlling for Op-
timizer Instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics. Portland, OR, USA, pages 176–181.
</p>
<p>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics. Baltimore, MD, USA, pages
1370–1380.
</p>
<p>Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised Word Alignment with
Arbitrary Features. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics. Portland, OR, USA, pages 409–419.
</p>
<p>Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing. Honolulu, HI, USA, pages 848–856.
</p>
<p>Joshua Goodman. 2001. Classes for fast maximum en-
tropy training. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing. Salt Lake City, UT, USA, pages 561–
564.
</p>
<p>Alex Graves, Santiago Fernández, Faustino Gomez,
and Jürgen Schmidhuber. 2006. Connectionist Tem-
poral Classification: Labelling Unsegmented Se-
quence Data with Recurrent Neural Networks. In
Proceedings of the 23rd International Conference
on Machine Learning. Pittsburgh, PA, USA, pages
369–376.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation
9(8):1735–1780.
</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Conference on
Empirical Methods in Natural Language Process-
ing. Lisbon, Portugal, pages 1412–1421.
</p>
<p>Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on ar-
tificial intelligence and statistics. Barbados, pages
246–252.
</p>
<p>Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics. Sapporo, Japan, pages 160–
167.
</p>
<p>Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics 29:19–51.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics. Philadelphia, PA, USA,
pages 311–318.
</p>
<p>Jan-Thorsten Peter, Andreas Guta, Nick Rossenbach,
Miguel Graça, and Hermann Ney. 2016. The RWTH
Aachen Machine Translation System for IWSLT
2016. In International Workshop on Spoken Lan-
guage Translation. Seattle, WA, USA.
</p>
<p>Jan-Thorsten Peter, Farzad Toutounchi, Joern Wue-
bker, and Hermann Ney. 2015. The RWTH Aachen
German-English Machine Translation System for
WMT 2015. In EMNLP 2015 Tenth Workshop on
Statistical Machine Translation. Lisbon, Portugal,
pages 158–163.
</p>
<p>Holger Schwenk. 2012. Continuous Space Translation
Models for Phrase-Based Statistical Machine Trans-
lation. In Proceedings of the 24th International
Conference on Computational Linguistics. Mumbai,
India, pages 1071–1080.
</p>
<p>130</p>
<p />
</div>
<div class="page"><p />
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics. Berlin, Germany, pages 1715–1725.
</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the Conference of the As-
sociation for Machine Translation in the Americas.
Cambridge, MA, USA, pages 223–231.
</p>
<p>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation Modeling with
Bidirectional Recurrent Neural Networks. In Con-
ference on Empirical Methods in Natural Language
Processing. Doha, Qatar, pages 14–25.
</p>
<p>Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu,
and Kevin Knight. 2016. Unsupervised Neural Hid-
den Markov Models. In Proceedings of the Work-
shop on Structured Prediction for NLP. Austin, TX,
USA, pages 63–71.
</p>
<p>David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR.
Uppsala, Sweden, pages 262–270.
</p>
<p>Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based Word Alignment in Statistical
Translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2. Copen-
hagen, Denmark, COLING ’96, pages 836–841.
</p>
<p>Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
Source Phrase-based and Hierarchical Statistical
Machine Translation. In International Confer-
ence on Computational Linguistics. Mumbai, India,
pages 483–491.
</p>
<p>Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing. Seattle, WA, USA,
pages 1377–1381.
</p>
<p>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word Alignment Modeling with Context
Dependent Deep Neural Network. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics. Sofia, Bulgaria, pages 166–
175.
</p>
<p>Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-
stette, and Tomáš Kočiský. 2017. The Neural Noisy
Channel. In Proceedings of the 5th International
Conference on Learning Representations. Toulon,
France.
</p>
<p>131</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 132–140
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2021
</p>
<p>Towards String-to-Tree Neural Machine Translation
</p>
<p>Roee Aharoni &amp; Yoav Goldberg
Computer Science Department
</p>
<p>Bar-Ilan University
Ramat-Gan, Israel
</p>
<p>{roee.aharoni,yoav.goldberg}@gmail.com
</p>
<p>Abstract
</p>
<p>We present a simple method to incorporate
syntactic information about the target lan-
guage in a neural machine translation sys-
tem by translating into linearized, lexical-
ized constituency trees. Experiments on
the WMT16 German-English news trans-
lation task shown improved BLEU scores
when compared to a syntax-agnostic NMT
baseline trained on the same dataset.
An analysis of the translations from the
syntax-aware system shows that it per-
forms more reordering during translation
in comparison to the baseline. A small-
scale human evaluation also showed an ad-
vantage to the syntax-aware system.
</p>
<p>1 Introduction and Model
</p>
<p>Neural Machine Translation (NMT) (Kalchbren-
ner and Blunsom, 2013; Sutskever et al., 2014;
Bahdanau et al., 2014) has recently became the
state-of-the-art approach to machine translation
(Bojar et al., 2016), while being much simpler than
the previously dominant phrase-based statistical
machine translation (SMT) approaches (Koehn,
2010). NMT models usually do not make ex-
plicit use of syntactic information about the lan-
guages at hand. However, a large body of work
was dedicated to syntax-based SMT (Williams
et al., 2016). One prominent approach to syntax-
based SMT is string-to-tree (S2T) translation (Ya-
mada and Knight, 2001, 2002), in which a source-
language string is translated into a target-language
tree. S2T approaches to SMT help to ensure the
resulting translations have valid syntactic struc-
ture, while also mediating flexible reordering be-
tween the source and target languages. The main
formalism driving current S2T SMT systems is
GHKM rules (Galley et al., 2004, 2006), which are
</p>
<p>synchronous transduction grammar (STSG) frag-
ments, extracted from word-aligned sentence pairs
with syntactic trees on one side. The GHKM
translation rules allow flexible reordering on all
levels of the parse-tree.
We suggest that NMT can also benefit from the
incorporation of syntactic knowledge, and propose
a simple method of performing string-to-tree neu-
ral machine translation. Our method is inspired
by recent works in syntactic parsing, which model
trees as sequences (Vinyals et al., 2015; Choe and
Charniak, 2016). Namely, we translate a source
sentence into a linearized, lexicalized constituency
tree, as demonstrated in Figure 2. Figure 1 shows
a translation from our neural S2T model compared
to one from a vanilla NMT model for the same
source sentence, as well as the attention-induced
word alignments of the two models.
</p>
<p>Figure 1: Top - a lexicalized tree translation pre-
dicted by the bpe2tree model. Bottom - a trans-
lation for the same sentence from the bpe2bpe
model. The blue lines are drawn according to the
attention weights predicted by each model.
</p>
<p>Note that the linearized trees we predict are dif-
ferent in their structure from those in Vinyals et al.
(2015) as instead of having part of speech tags as
terminals, they contain the words of the translated
sentence. We intentionally omit the POS informa-
</p>
<p>132</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2021">https://doi.org/10.18653/v1/P17-2021</a></div>
</div>
<div class="page"><p />
<p>Jane hatte eine Katze . → (ROOT (S (NP Jane )NP (V P had (NP a cat )NP )V P . )S )ROOT
</p>
<p>Figure 2: An example of a translation from a string to a linearized, lexicalized constituency tree.
</p>
<p>tion as including it would result in significantly
longer sequences. The S2T model is trained on
parallel corpora in which the target sentences are
automatically parsed. Since this modeling keeps
the form of a sequence-to-sequence learning task,
we can employ the conventional attention-based
sequence to sequence paradigm (Bahdanau et al.,
2014) as-is, while enriching the output with syn-
tactic information.
Related Work Some recent works did propose
to incorporate syntactic or other linguistic knowl-
edge into NMT systems, although mainly on the
source side: Eriguchi et al. (2016a,b) replace
the encoder in an attention-based model with a
Tree-LSTM (Tai et al., 2015) over a constituency
parse tree; Bastings et al. (2017) encoded sen-
tences using graph-convolutional networks over
dependency trees; Sennrich and Haddow (2016)
proposed a factored NMT approach, where each
source word embedding is concatenated to em-
beddings of linguistic features of the word; Lu-
ong et al. (2015) incorporated syntactic knowl-
edge via multi-task sequence to sequence learning:
their system included a single encoder with multi-
ple decoders, one of which attempts to predict the
parse-tree of the source sentence; Stahlberg et al.
(2016) proposed a hybrid approach in which trans-
lations are scored by combining scores from an
NMT system with scores from a Hiero (Chiang,
2005, 2007) system. Shi et al. (2016) explored the
syntactic knowledge encoded by an NMT encoder,
showing the encoded vector can be used to pre-
dict syntactic information like constituency trees,
voice and tense with high accuracy.
</p>
<p>In parallel and highly related to our work,
Eriguchi et al. (2017) proposed to model the target
syntax in NMT in the form of dependency trees by
using an RNNG-based decoder (Dyer et al., 2016),
while Nadejde et al. (2017) incorporated target
syntax by predicting CCG tags serialized into the
target translation. Our work differs from those by
modeling syntax using constituency trees, as was
previously common in the “traditional” syntax-
based machine translation literature.
</p>
<p>2 Experiments &amp; Results
</p>
<p>Experimental Setup We first experiment in a
resource-rich setting by using the German-English
</p>
<p>portion of the WMT16 news translation task (Bo-
jar et al., 2016), with 4.5 million sentence pairs.
We then experiment in a low-resource scenario us-
ing the German, Russian and Czech to English
training data from the News Commentary v8 cor-
pus, following Eriguchi et al. (2017). In all cases
we parse the English sentences into constituency
trees using the BLLIP parser (Charniak and John-
son, 2005).1 To enable an open vocabulary trans-
lation we used sub-word units obtained via BPE
(Sennrich et al., 2016b) on both source and target.2
</p>
<p>In each experiment we train two models.
A baseline model (bpe2bpe), trained to trans-
late from the source language sentences to En-
glish sentences without any syntactic annotation,
and a string-to-linearized-tree model (bpe2tree),
trained to translate into English linearized con-
stituency trees as shown in Figure 2. Words
are segmented into sub-word units using the BPE
model we learn on the raw parallel data. We use
the NEMATUS (Sennrich et al., 2017)3 implemen-
tation of an attention-based NMT model.4 We
trained the models until there was no improvement
on the development set in 10 consecutive check-
points. Note that the only difference between the
baseline and the bpe2tree model is the syntactic in-
formation, as they have a nearly-identical amount
of model parameters (the only additional param-
eters to the syntax-aware system are the embed-
dings for the brackets of the trees).
</p>
<p>For all models we report results of the best
performing single model on the dev-set (new-
stest2013+newstest2014 in the resource rich set-
ting, newstest2015 in the rest, as measured by
BLEU) when translating newstest2015 and new-
stest2016, similarly to Sennrich et al. (2016a);
Eriguchi et al. (2017). To evaluate the string-to-
tree translations we derive the surface form by re-
moving the symbols that stand for non-terminals
in the tree, followed by merging the sub-words.
We also report the results of an ensemble of
the last 5 checkpoints saved during each model
training. We compute BLEU scores using the
</p>
<p>1https://github.com/BLLIP/bllip-parser
2https://github.com/rsennrich/
</p>
<p>subword-nmt
3https://github.com/rsennrich/nematus
4Further technical details of the setup and training are
</p>
<p>available in the supplementary material.
</p>
<p>133</p>
<p />
</div>
<div class="page"><p />
<p>mteval-v13a.pl script from the Moses toolkit
(Koehn et al., 2007).
</p>
<p>system newstest2015 newstest2016
bpe2bpe 27.33 31.19
bpe2tree 27.36 32.13
bpe2bpe ens. 28.62 32.38
bpe2tree ens. 28.7 33.24
</p>
<p>Table 1: BLEU results for the WMT16 experiment
</p>
<p>Results As shown in Table 1, for the resource-rich
setting, the single models (bpe2bpe, bpe2tree) per-
form similarly in terms of BLEU on newstest2015.
On newstest2016 we witness an advantage to the
bpe2tree model. A similar trend is found when
evaluating the model ensembles: while they im-
prove results for both models, we again see an ad-
vantage to the bpe2tree model on newstest2016.
Table 2 shows the results in the low-resource set-
ting, where the bpe2tree model is consistently bet-
ter than the bpe2bpe baseline. We find this in-
teresting as the syntax-aware system performs a
much harder task (predicting trees on top of the
translations, thus handling much longer output se-
quences) while having a nearly-identical amount
of model parameters. In order to better understand
where or how the syntactic information improves
translation quality, we perform a closer analysis of
the WMT16 experiment.
</p>
<p>3 Analysis
</p>
<p>The Resulting Trees Our model produced valid
trees for 5970 out of 6003 sentences in the devel-
opment set. While we did not perform an in-depth
error-analysis, the trees seem to follow the syntax
of English, and most choices seem reasonable.
</p>
<p>Quantifying Reordering English and German
differ in word order, requiring a significant amount
of reordering to generate a fluent translation. A
major benefit of S2T models in SMT is facilitat-
ing reordering. Does this also hold for our neural
S2T model? We compare the amount of reorder-
ing in the bpe2bpe and bpe2tree models using a
distortion score based on the alignments derived
from the attention weights of the corresponding
systems. We first convert the attention weights to
hard alignments by taking for each target word the
source word with highest attention weight. For an
n-word target sentence t and source sentence s let
a(i) be the position of the source word aligned to
the target word in position i. We define:
</p>
<p>system newstest2015 newstest2016
</p>
<p>D
E
</p>
<p>-E
N
</p>
<p>bpe2bpe 13.81 14.16
bpe2tree 14.55 16.13
bpe2bpe ens. 14.42 15.07
bpe2tree ens. 15.69 17.21
</p>
<p>R
U
</p>
<p>-E
N
</p>
<p>bpe2bpe 12.58 11.37
bpe2tree 12.92 11.94
bpe2bpe ens. 13.36 11.91
bpe2tree ens. 13.66 12.89
</p>
<p>C
S-
</p>
<p>E
N
</p>
<p>bpe2bpe 10.85 11.23
bpe2tree 11.54 11.65
bpe2bpe ens. 11.46 11.77
bpe2tree ens. 12.43 12.68
</p>
<p>Table 2: BLEU results for the low-resource exper-
iments (News Commentary v8)
</p>
<p>d(s, t) =
1
</p>
<p>n
</p>
<p>n∑
</p>
<p>i=2
</p>
<p>|a(i)− a(i− 1)|
</p>
<p>For example, for the translations in Figure 1, the
above score for the bpe2tree model is 2.73, while
the score for the bpe2bpe model is 1.27 as the
bpe2tree model did more reordering. Note that
for the bpe2tree model we compute the score only
on tokens which correspond to terminals (words
or sub-words) in the tree. We compute this score
for each source-target pair on newstest2015 for
each model. Figure 3 shows a histogram of the
binned score counts. The bpe2tree model has
more translations with distortion scores in bins 1-
onward and significantly less translations in the
least-reordering bin (0) when compared to the
bpe2bpe model, indicating that the syntactic in-
formation encouraged the model to perform more
reordering.5 Figure 4 tracks the distortion scores
throughout the learning process, plotting the av-
erage dev-set scores for the model checkpoints
saved every 30k updates. Interestingly, both mod-
els obey to the following trend: open with a rel-
atively high distortion score, followed by a steep
decrease, and from there ascend gradually. The
bpe2tree model usually has a higher distortion
score during training, as we would expect after our
previous findings from Figure 3.
Tying Reordering and Syntax The bpe2tree
model generates translations with their con-
stituency tree and their attention-derived align-
ments. We can use this information to extract
GHKM rules (Galley et al., 2004).6 We derive
</p>
<p>5We also note that in bins 4-6 the bpe2bpe model had
slightly more translations, but this was not consistent among
different runs, unlike the gaps in bins 0-3 which were consis-
tent and contain most of the translations.
</p>
<p>6github.com/joshua-decoder/galley-ghkm
</p>
<p>134</p>
<p />
</div>
<div class="page"><p />
<p>0 1 2 3 4 5 6 7
</p>
<p>0
</p>
<p>500
</p>
<p>1,000
</p>
<p>distortion score bin
</p>
<p>#
tr
</p>
<p>an
sl
</p>
<p>at
io
</p>
<p>ns
in
</p>
<p>bi
n
</p>
<p>bpe2tree
</p>
<p>bpe2bpe
</p>
<p>Figure 3: newstest2015 DE-EN
translations binned by distortion
amount
</p>
<p>24m 240m 456m
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p># examples seen
</p>
<p>av
er
</p>
<p>ag
e
</p>
<p>di
st
</p>
<p>or
tio
</p>
<p>n
sc
</p>
<p>or
e
</p>
<p>bpe2tree
</p>
<p>bpe2bpe
</p>
<p>Figure 4: Average distortion
score on the dev-set during differ-
ent training stages
</p>
<p>who which that whom whose
</p>
<p>0
</p>
<p>200
</p>
<p>400
</p>
<p>600
</p>
<p>pronouns
</p>
<p>bpe2tree
</p>
<p>bpe2bpe
</p>
<p>ref
</p>
<p>Figure 5: Amount of English rel-
ative pronouns in newstest2015
translations
</p>
<p>LHS Top-5 RHS, sorted according to count.
VP(x0:TER x1:NP) (244) x0 x1 (157) x1 x0 (80) x0 x1 ”,/.” (56) x1 x0 ”,/.” (17) x0 ”eine” x1
VP(x0:TER PP(x1:TER x2:NP)) (90) x1 x2 x0 (65) x0 x1 x2 (31) x1 x2 x0 ”,/.” (13) x0 x1 x2 ”,/.” (7) x1 ”der” x2 x0
VP(x0:TER x1:PP) (113) x1 x0 (82) x0 x1 (38) x1 x0 ”,/.” (18) x0 x1 ”,/.” (5) ”,/.” x0 x1
S(x0:NP VP(x1:TER x2:NP)) (69) x0 x1 x2 (51) x0 x2 x1 (35) x0 x1 x2 ”,/.” (20) x0 x2 x1 ”,/.” (6) ”die” x0 x1 x2
VP(x0:TER x1:NP x2:PP) (52) x0 x1 x2 (38) x1 x2 x0 (20) x1 x2 x0 ”,/.” (11) x0 x1 x2 ”,/.” (9) x2 x1 x0
VP(x0:TER x1:NP PP(x2:TER x3:NP)) (40) x0 x1 x2 x3 (32) x1 x2 x3 x0 (18) x1 x2 x3 x0 ”,/.” (8) x0 x1 x2 x3 ”,/.” (5) x2 x3 x1 x0
VP(x0:TER NP(x1:NP x2:PP)) (61) x0 x1 x2 (38) x1 x2 x0 (19) x0 x1 x2 ”,/.” (8) x0 ”eine” x1 x2 (8) x1 x2 x0 ”,/.”
NP(x0:NP PP(x1:TER x2:NP)) (728) x0 x1 x2 (110) ”die” x0 x1 x2 (107) x0 x1 x2 ”,/.” (56) x0 x1 ”der” x2 (54) ”der” x0 x1 x2
S(VP(x0:TER x1:NP)) (41) x1 x0 (26) x0 x1 (14) x1 x0 ”,/.” (7) ”die” x1 x0 (5) x0 x1 ”,/.”
VP(x0:TER x1:VP) (73) x0 x1 (38) x1 x0 (25) x0 x1 ”,/.” (15) x1 x0 ”,/.” (9) ”,/.” x0 x1
</p>
<p>Table 3: Top dev-set GHKM Rules with reordering. Numbers: rule counts. Bolded: reordering rules.
</p>
<p>src Dutzende türkischer Polizisten wegen ”Verschwörung” gegen die Regierung festgenommen
ref Tens of Turkish Policemen Arrested over ’Plotting’ against Gov’t
2tree dozens of Turkish police arrested for ”conspiracy” against the government.
2bpe dozens of turkish policemen on ”conspiracy” against the government arrested
src Die Menschen in London weinten, als ich unsere Geschichte erzhlte. Er ging einen Monat nicht zu Arbeit.
ref People in London were crying when I told our story. He ended up spending a month off work.
2tree the people of london wept as I told our story. he did not go to work a month.
2bpe the people of London, when I told our story. he went one month to work.
src Achenbach habe für 121 Millionen Euro Wertgegenstände für Albrecht angekauft.
ref Achenbach purchased valuables for Albrecht for 121 million euros.
2tree Achenbach has bought valuables for Albrecht for 121 million euros.
2bpe Achenbach have purchased value of 121 million Euros for Albrecht.
src Apollo investierte 2008 1 Milliarde $ in Norwegian Cruise. Könntest du mal mit dem ”ich liebe dich” aufhören?
ref Apollo made a $1 billion investment in Norwegian Cruise in 2008. Could you stop with the ”I love you”?
2tree Apollo invested EUR $1 billion in Norwegian Cruise in 2008. Could you stop saying ”I love you?
2bpe Apollo invested 2008 $1 billion in Norwegian Cruise. Can you say with the ”I love you” stop?
src Gerade in dieser schweren Phase hat er gezeigt, dass er für uns ein sehr wichtiger Spieler ist”, konstatierte Barisic.
ref Especially during these difficult times, he showed that he is a very important player for us”, Barisic stated.
2tree Especially at this difficult time he has shown that he is a very important player for us,” said Barisic.
2bpe It is precisely during this difficult period that he has shown us to be a very important player, ”Barisic said.
src Hopfen und Malz - auch in China eine beliebte Kombination. ”Ich weiß jetzt, dass ich das kann - prima!”
ref Hops and malt - a popular combination even in China. ”I now know that I can do it - brilliant!”
2tree Hops and malt - a popular combination in China. ”I now know that I can do that!
2bpe Hops and malt - even in China, a popular combination. I know now that I can that - prima!”
src Die Ukraine hatte gewarnt, Russland könnte auch die Gasversorgung für Europa unterbrechen.
ref Ukraine warned that Russia could also suspend the gas supply to Europe.
2tree Ukraine had warned that Russia could also stop the supply of gas to Europe.
2bpe Ukraine had been warned, and Russia could also cut gas supplies to Europe.
src Bis dahin gab es in Kollbach im Schulverband Petershausen-Kollbach drei Klassen und in Petershausen fünf.
ref Until then, the school district association of Petershausen-Kollbach had three classes in Kollbach and five in Petershausen.
2tree until then, in Kollbach there were three classes and five classes in Petershausen.
2bpe until then there were three classes and in Petershausen five at the school board in Petershausen-Kollbach.
</p>
<p>Table 4: Translation examples from newstest2015. The underlines correspond to the source word at-
tended by the first opening bracket (these are consistently the main verbs or structural markers) and
the target words this source word was most strongly aligned to. See the supplementary material for an
attention weight matrix example when predicting a tree (Figure 6) and additional output examples.
</p>
<p>135</p>
<p />
</div>
<div class="page"><p />
<p>hard alignments for that purpose by treating ev-
ery source/target token-pair with attention score
above 0.5 as an alignment. Extracting rules from
the dev-set predictions resulted in 233,657 rules,
where 22,914 of them (9.8%) included reorder-
ing, i.e. contained variables ordered differently in
the source and the target. We grouped the rules
by their LHS (corresponding to a target syntac-
tic structure), and sorted them by the total num-
ber of RHS (corresponding to a source sequential
structure) with reordering. Table 3 shows the top
10 extracted LHS, together with the top-5 RHS,
for each rule. The most common rule, VP(x0:TER
x1:NP) → x1 x0, found in 184 sentences in the
dev set (8.4%), is indicating that the sequence x1
x0 in German was reordered to form a verb phrase
in English, in which x0 is a terminal and x1 is a
noun phrase. The extracted GHKM rules reveal
very sensible German-English reordering patterns.
</p>
<p>Relative Constructions Browsing the produced
trees hints at a tendency of the syntax-aware model
to favor using relative-clause structures and sub-
ordination over other syntactic constructions (i.e.,
“several cameras that are all priced...” vs. “sev-
eral cameras, all priced...”). To quantify this, we
count the English relative pronouns (who, which,
that7, whom, whose) found in the newstest2015
translations of each model and in the reference
translations, as shown in Figure 5. The bpe2tree
model produces more relative constructions com-
pared to the bpe2bpe model, and both models pro-
duce more such constructions than found in the
reference.
</p>
<p>Main Verbs While not discussed until this
point, the generated opening and closing brack-
ets also have attention weights, providing another
opportunity to to peak into the model’s behavior.
Figure 6 in the supplementary material presents an
example of a complete attention matrix, including
the syntactic brackets. While making full sense of
the attention patterns of the syntactic elements re-
mains a challenge, one clear trend is that opening
the very first bracket of the sentence consistently
attends to the main verb or to structural mark-
ers (i.e. question marks, hyphens) in the source
sentence, suggesting a planning-ahead behavior of
the decoder. The underlines in Table 4 correspond
to the source word attended by the first opening
bracket, and the target word this source word was
</p>
<p>7”that” also functions as a determiner. We do not distin-
guish the two cases.
</p>
<p>most strongly aligned to. In general, we find the
alignments from the syntax-based system more
sensible (i.e. in Figure 1 – the bpe2bpe alignments
are off-by-1).
</p>
<p>Qualitative Analysis and Human Evaluations
The bpe2tree translations read better than their
bpe2bpe counterparts, both syntactically and se-
mantically, and we highlight some examples
which demonstrate this. Table 4 lists some rep-
resentative examples, highlighting improvements
that correspond to syntactic phenomena involving
reordering or global structure. We also performed
a small-scale human-evaluation using mechanical
turk on the first 500 sentences in the dev-set. Fur-
ther details are available in the supplementary ma-
terial. The results are summarized in the following
table:
</p>
<p>2bpe weakly better 100
2bpe strongly better 54
2tree weakly better 122
2tree strongly better 64
both good 26
both bad 3
disagree 131
</p>
<p>As can be seen, in 186 cases (37.2%) the human
evaluators preferred the bpe2tree translations, vs.
154 cases (30.8%) for bpe2bpe, with the rest of the
cases (30%) being neutral.
</p>
<p>4 Conclusions and Future Work
We present a simple string-to-tree neural transla-
tion model, and show it produces results which
are better than those of a neural string-to-string
model. While this work shows syntactic infor-
mation about the target side can be beneficial for
NMT, this paper only scratches the surface with
what can be done on the subject. First, better mod-
els can be proposed to alleviate the long sequence
problem in the linearized approach or allow a more
natural tree decoding scheme (Alvarez-Melis and
Jaakkola, 2017). Comparing our approach to other
syntax aware NMT models like Eriguchi et al.
(2017) and Nadejde et al. (2017) may also be of in-
terest. A Contrastive evaluation (Sennrich, 2016)
of a syntax-aware system vs. a syntax-agnostic
system may also shed light on the benefits of in-
corporating syntax into NMT.
</p>
<p>Acknowledgments
This work was supported by the Intel Collabora-
tive Research Institute for Computational Intelli-
gence (ICRI-CI), and The Israeli Science Founda-
tion (grant number 1555/15).
</p>
<p>136</p>
<p />
</div>
<div class="page"><p />
<p>References
David Alvarez-Melis and Tommi S. Jaakkola. 2017.
</p>
<p>Tree-structured decoding with doubly recurrent neu-
ral networks. International Conference on Learning
Representations (ICLR) .
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .
</p>
<p>Joost Bastings, Ivan Titov, Wilker Aziz, Diego
Marcheggiani, and Khalil Simaan. 2017. Graph
convolutional encoders for syntax-aware neural ma-
chine translation. arXiv preprint arXiv:1704.04675
.
</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation. Association for
Computational Linguistics, Berlin, Germany, pages
131–198.
</p>
<p>Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
Association for Computational Linguistics, pages
173–180.
</p>
<p>David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, pages 263–270.
</p>
<p>David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics 33(2):201–228.
</p>
<p>Do Kook Choe and Eugene Charniak. 2016. Pars-
ing as language modeling. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 2331–2336.
https://aclweb.org/anthology/D16-1257.
</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 199–209.
http://www.aclweb.org/anthology/N16-1024.
</p>
<p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016a. Character-based decoding in tree-
to-sequence attention-based neural machine transla-
</p>
<p>tion. In Proceedings of the 3rd Workshop on Asian
Translation (WAT2016). pages 175–183.
</p>
<p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshi-
masa Tsuruoka. 2016b. Tree-to-sequence atten-
tional neural machine translation. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 823–833.
http://www.aclweb.org/anthology/P16-1078.
</p>
<p>Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun
Cho. 2017. Learning to parse and translate im-
proves neural machine translation. arXiv preprint
arXiv:1702.03525 http://arxiv.org/abs/1702.03525.
</p>
<p>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 961–968.
</p>
<p>Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings. As-
sociation for Computational Linguistics, Boston,
Massachusetts, USA, pages 273–280.
</p>
<p>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Seattle, Washington, USA, pages
1700–1709. http://www.aclweb.org/anthology/D13-
1176.
</p>
<p>Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions. As-
sociation for Computational Linguistics, pages 177–
180.
</p>
<p>Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015. Multi-task
sequence to sequence learning. arXiv preprint
arXiv:1511.06114 .
</p>
<p>Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz
Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn,
and Alexandra Birch. 2017. Syntax-aware neu-
ral machine translation using CCG. arXiv preprint
arXiv:1702.01147 .
</p>
<p>137</p>
<p />
</div>
<div class="page"><p />
<p>Rico Sennrich. 2016. How grammatical is character-
level neural machine translation? assessing mt qual-
ity with contrastive translation pairs. arXiv preprint
arXiv:1612.04629 .
</p>
<p>Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel L”aubli, Antonio Vale-
rio Miceli Barone, Jozef Mokry, and Maria Nade-
jde. 2017. Nematus: a Toolkit for Neural Machine
Translation. In Proceedings of the Demonstrations
at the 15th Conference of the European Chapter of
the Association for Computational Linguistics. Va-
lencia, Spain.
</p>
<p>Rico Sennrich and Barry Haddow. 2016. Linguis-
tic input features improve neural machine trans-
lation. In Proceedings of the First Conference
on Machine Translation. Association for Computa-
tional Linguistics, Berlin, Germany, pages 83–91.
http://www.aclweb.org/anthology/W16-2209.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Edinburgh neural machine translation sys-
tems for wmt 16. In Proceedings of the First Confer-
ence on Machine Translation. Association for Com-
putational Linguistics, Berlin, Germany, pages 371–
376. http://www.aclweb.org/anthology/W16-2323.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016b. Neural machine translation of
rare words with subword units. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1715–1725.
http://www.aclweb.org/anthology/P16-1162.
</p>
<p>Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1526–1534. https://aclweb.org/anthology/D16-
1159.
</p>
<p>Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill
Byrne. 2016. Syntactically guided neural machine
translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 299–305.
http://anthology.aclweb.org/P16-2049.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. arXiv preprint arXiv:1409.3215 .
</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term mem-
ory networks. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
</p>
<p>Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1556–
1566. http://www.aclweb.org/anthology/P15-1150.
</p>
<p>Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems. pages 2773–2781.
</p>
<p>P. Williams, M. Gertz, and M. Post. 2016. Syntax-
Based Statistical Machine Translation. Morgan &amp;
Claypool publishing.
</p>
<p>Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceed-
ings of the 39th Annual Meeting on Association for
Computational Linguistics. Association for Compu-
tational Linguistics, pages 523–530.
</p>
<p>Kenji Yamada and Kevin Knight. 2002. A decoder
for syntax-based statistical mt. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics. Association for Computational
Linguistics, pages 303–310.
</p>
<p>Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .
</p>
<p>138</p>
<p />
</div>
<div class="page"><p />
<p>A Supplementary Material
</p>
<p>Data The English side of the corpus was tok-
enized (into Penn treebank format) and truecased
using the scripts provided in Moses (Koehn et al.,
2007). We ran the BPE process on a concatenation
of the source and target corpus, with 89500 BPE
operations in the WMT experiment and with 45k
operations in the other experiments. This resulted
in an input vocabulary of 84924 tokens and an out-
put vocabulary of 78499 tokens in the WMT16
experiment. The linearized constituency trees are
obtained by simply replacing the POS tags in the
parse trees with the corresponding word or sub-
words. The output vocabulary in the bpe2tree
models includes the target subwords and the tree
symbols which correspond to an opening or clos-
ing of a specific phrase type.
</p>
<p>Hyperparameters The word embedding size
was set to 500/256 and the encoder and decoder
sizes were set to 1024/256 (WMT16/other ex-
periments). For optimization we used Adadelta
(Zeiler, 2012) with minibatch size of 40. For de-
coding we used beam search with a beam size
of 12. We trained the bpe2tree WMT16 model
on sequences with a maximum length of 150 to-
kens (the average length for a linearized tree in the
training set was about 50 tokens). It was trained
for two weeks on a single Nvidia TitanX GPU.
The bpe2bpe WMT16 model was trained on se-
quences with a maximum length of 50 tokens, and
with minibatch size of 80. It was trained for one
week on a single Nvidia TitanX GPU. Only in the
low-resource experiments we applied dropout as
described in Sennrich et al. (2016a) for Romanian-
English.
</p>
<p>Human Evaluation We performed human-
evaluation on the Mechnical Turk platform. Each
sentence was evaluated using two annotators. For
each sentence, we presented the annotators with
the English reference sentence, followed by the
outputs of the two systems. The German source
was not shown, and the two system’s outputs were
shown in random order. The annotators were in-
structed to answer “Which of the two sentences, in
your view, is a better portrayal of the the reference
sentence.” They were then given 6 options: “sent
1 is better”, “sent 2 is better”, “sent 1 is a little bet-
ter”, “sent 2 is a little better”, “both sentences are
equally good”, “both sentences are equally bad”.
We then ignore differences between “better” and
</p>
<p>“a little better”. We count as “strongly better” the
cases where both annotators indicated the same
sentence as better, as “weakly better” the cases
were one annotator chose a sentence and the other
indicated they are both good/bad. Other cases are
treated as either “both good” / “both bad” or as
disagreements.
</p>
<p>Figure 6: The attention weights for the string-to-
tree translation in Figure 1
</p>
<p>Additional Output Examples from both mod-
els, in the format of Figure 1. Notice the improved
translation and alignment quality in the tree-based
translations, as well as the overall high structural
quality of the resulting trees. The few syntactic
mistakes in these examples are attachment errors
of SBAR and PP phrases, which will also chal-
lenge dedicated parsers.
</p>
<p>139</p>
<p />
</div>
<div class="page"><p />
<p>140</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 141–147
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2022
</p>
<p>Learning Lexico-Functional Patterns for First-Person Affect
</p>
<p>Lena Reed, Jiaqi Wu, Shereen Oraby
Pranav Anand and Marilyn Walker
University of California Santa Cruz
</p>
<p>{lireed,jwu64,soraby,panand,mawalker}@ucsc.edu
</p>
<p>Abstract
</p>
<p>Informal first-person narratives are a
unique resource for computational mod-
els of everyday events and people’s affec-
tive reactions to them. People blogging
about their day tend not to explicitly say
I am happy. Instead they describe situa-
tions from which other humans can read-
ily infer their affective reactions. However
current sentiment dictionaries are missing
much of the information needed to make
similar inferences. We build on recent
work that models affect in terms of lexical
predicate functions and affect on the pred-
icate’s arguments. We present a method to
learn proxies for these functions from first-
person narratives. We construct a novel
fine-grained test set, and show that the pat-
terns we learn improve our ability to pre-
dict first-person affective reactions to ev-
eryday events, from a Stanford sentiment
baseline of .67F to .75F.
</p>
<p>1 Introduction
</p>
<p>Across social media, thousands of posts daily
take the form of informal FIRST-PERSON NARRA-
TIVES. These narratives provide a rich resource
for computational modeling of how people feel
about the events they report on. Being able to re-
liably predict the affect a person may feel towards
events they encounter has a range of potential ap-
plications, including monitoring mood and men-
tal health (Isaacs et al., 2013) and getting conver-
sational assistants to respond appropriately (Bow-
den et al., 2017). Moreover, as these narratives are
told from the perspective of a protagonist, this re-
search could be used to understand other types of
protagonist-framed narratives, like those in fiction.
</p>
<p>We are interested in the opinions that a protag-
onist has, not the author per se. This is sometimes
referred to as internal sentiment or self reflective
sentiment. While in many situations that is over-
laid with the author’s opinions, in first-personal
narratives, because the author is the protagonist,
the two perspectives align. Here, we use the term
affect to reference this protagonist-centered notion
of opinion.
</p>
<p>A central obstacle to reliable affect prediction
is that that people tend not to explicitly flag their
affective state, by saying I am happy. Large-scale
sentiment dictionaries focus on compiling lexical
items that bear a consistent affect all on their own
(Wilson et al., 2005). But people tend to describe
situations, such as My friend bought me flowers,
or I got a parking ticket, from which other humans
can readily infer their implicit affective reactions.
</p>
<p>One approach to this problem aims to directly
learn units larger than a lexical item that reliably
bear some marker of polarity or emotion (Vu et al.,
2014; Li et al., 2014; Ding and Riloff, 2016; Goyal
et al., 2010; Russo et al., 2015; Kiritchenko et al.,
2014; Reckman et al., 2013).
</p>
<p>X Y Ehve Elck Example
</p>
<p>+ + + - I have a new kitten.
+ - - + I got a parking ticket.
- + - + My rival got a prize.
- - + - My rival got a reprimand.
</p>
<p>X have/lack Y
</p>
<p>Table 1: Functions for verbs of possession.
</p>
<p>Another approach aims to model the speaker’s
affect to an event compositionally, e.g. Anand
and Reschke (2010) (A&amp;R) proposed that the af-
fect a lexical predicate communicates should be
modeled as an n-ary function, taking as inputs the
affect that the speaker bears towards each partici-
</p>
<p>141</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2022">https://doi.org/10.18653/v1/P17-2022</a></div>
</div>
<div class="page"><p />
<p>pant. Table 1 contains A&amp;R’s functions for verbs
of possession: a state in which X has Y or X lacks
Y does not convey a clear affect unless we know
what the speaker thinks of both X and Y . If the
speaker has positive affect toward both X and Y
(Row 1), then we infer that her attitude toward the
event is positive, but if either is negative, then we
infer that the speaker is negative toward the event.
Similarly, Rashkin et al. (2016) represent the typ-
ical affect communicated by particular predicates
via connotation frames. Here we are finding the
internal sentiment of the speaker, or, as Rashkin et
al. refer to it, the ”mental state” of the speaker.
</p>
<p>Inspired by A&amp;R’s framework, our work learns
lexico-functional patterns (patterns involving lexi-
cal items or pairs of lexical items in specific gram-
matical relations that we show to capture functor-
argument relations in A&amp;R’s sense), about the ef-
fects of combining particular arguments with par-
ticular verbs (event types) from first-person narra-
tives. Our novel observation is that learning these
compositional functions is greatly simplified in the
case of first-person affect. People bear positive af-
fect to themselves, so sentences with first-person
elements, e.g. I/we/me, reduce the problem for
an approach like A&amp;R’s to learning the polarity
that results from composing the verb with only one
of its arguments, i.e. only Rows 1, 2 in Table 1
need to be learned for first person subjects. First-
person narratives are full of such sentences. See
Table 2. We show that the learned patterns are
often consonant with A&amp;R’s predictions, but are
richer, including e.g. many private state descrip-
tions (Wiebe et al., 2004; Wiebe, 1990).
</p>
<p>Positive Sentences
</p>
<p>We had a marvelous visit and drank coffee and ate home-
made chocolate chip cookies.
Now, I could swim both froggy and free style swimming!!
</p>
<p>Negative Sentences
But last week, he said that he doesn’t know if he has the
same feelings for me anymore.
I didn’t want to lose him.
</p>
<p>Table 2: Sentences from the training data
</p>
<p>In addition, we demonstrate that these lexico-
functional patterns improve the performance of
several off-the-shelf sentiment analyzers. We
show that Stanford sentiment (Socher et al., 2013)
has a best performance of 0.67 macro F on our test
set. We then supplement it with our learned pat-
terns and demonstrate significant improvements.
Our final ensemble achieves 0.75 F on the test set.
</p>
<p>We discuss related work in more detail in Sec. 5.
</p>
<p>2 Bootstrapping a First-Person
Sentiment Corpus
</p>
<p>We start with a set of first-person narratives (we-
blogs) drawn from the Spinn3r corpus, that cover a
wide range of topics (Burton et al., 2009; Gordon
and Swanson, 2009). To reduce noise, we restrict
the blogs to those from well-known blogging sites
(Ding and Riloff, 2016), and select 15,466 stories
whose length ranges from 225 to 375 words.
</p>
<p>Pattern Template Class Example Instantiations
</p>
<p>&lt;subj&gt; ActVP neg &lt;I&gt; cry at the thought of it and I’m cry-
ing now.
</p>
<p>&lt;subj&gt; ActInfVP pos &lt;I&gt; got to swim from the boat to a little
sandbar.
</p>
<p>&lt;subj&gt; AuxVP Dobj neg Yesterday &lt;it&gt; was my 1st molar, today
it’s my 2nd molar.
</p>
<p>ActVP &lt;dobj&gt; pos As I bake often, I have used &lt;several dif-
ferent kinds of recipes&gt;.
</p>
<p>PassInfVP &lt;dobj&gt; pos When we arrived at the Embarcadero, we
were surprised to find &lt; a music festival
taking place&gt;...
</p>
<p>Subj AuxVP &lt;dobj&gt; neg Our relationship was &lt;non existant&gt;
for over a year after that.
</p>
<p>NP Prep &lt;np&gt; neg he hurt me countless times but I still for-
gave him and i still tried to prove to him
that I did care for &lt;him&gt;.
</p>
<p>ActVP Prep &lt;np&gt; neg I didn’t think anything of it until I thought
about when he cheated on &lt;me&gt;.
</p>
<p>InfVP Prep &lt;np&gt; pos ...my friend from college who was so gen-
erous to offer his place to &lt;us&gt;...
</p>
<p>Table 3: AutoSlog-TS Templates and Example
Instantiations
</p>
<p>We hand-annotate a set of 477 positive and 440
negative stories, and use these to bootstrap a larger
set of 1,420 negative and 2,288 positive stories. To
bootstrap, we apply AutoSlog-TS, a weakly su-
pervised pattern learner that only requires train-
ing sets os stories labeled broadly as POSITIVE or
NEGATIVE (Riloff, 1996; Riloff and Wiebe, 2003).
AutoSlog uses a set of syntactic templates to de-
fine different types of linguistic expressions. The
left-hand side of Table 3 lists examples of Au-
toSlog patterns and the right-hand side illustrates a
specific lexical-syntactic pattern that corresponds
to each general pattern template, as instantiated in
first-person stories.1 When bootstrapping a larger
positive and negative story corpus, we use the
whole story, not just the first person sentences.
</p>
<p>The left-hand-side of Table 3 shows that the
learned patterns can involve syntactic arguments
</p>
<p>1The examples are shown as general expressions for read-
ability, but the actual patterns must match the syntactic con-
straints associated with the pattern template.
</p>
<p>142</p>
<p />
</div>
<div class="page"><p />
<p>of the verbal predicate, which means that these
patterns are proxies for one column of verbal func-
tion tables like those in Table 1. However, they
can also include verb-particle constructions, such
as cheated on, or verb-head-of-preposition con-
structions. In each case though, because these pat-
terns are localized to a verb and only one element,
they allow us to learn highly specific patterns that
could be incorporated into a dictionary such as +-
Effect (Choi and Wiebe, 2014). AutoSlog simul-
taneously harvests both (syntactically constrained)
MWE patterns and more compositionally regular
verb-argument groups at the same time.
</p>
<p>AutoSlog-TS computes statistics on the strength
of association of each pattern with each class, i.e.
P(POSITIVE | p) and P(NEGATIVE | p), along with
the pattern’s overall frequency. We define three
parameters for each class: θf , the frequency with
which a pattern occurs, θp, the probability with
which a pattern is associated with the given class
and θn, the number of patterns that must occur in
the text for it to be labeled. These parameters are
tuned on the dev set (Riloff, 1996; Oraby et al.,
2015; Riloff and Wiebe, 2003).
</p>
<p>To bootstrap a larger corpus, we want settings
that have lower recall but very high precision. We
select θp = 0.7, θf = 10 and θn = 3 for the
positive class and θp = 0.85, θf = 10 and θn = 4
for the negative class for bootstrapping.
</p>
<p>3 Experimental Setup
</p>
<p>Our experimental setup involves first creating a
corpus of training and test sentences, then apply-
ing AutoSlog-TS a second time to learn linguistic
patterns. We then set up methods for cascading
classifiers to explore whether ensemble classifiers
improve our results.
Training Set: From the bootstrapped set of sto-
ries, we create a corpus of sentences. A critical
simplifying assumption of our method is that a
multi-sentence story can be labelled as a whole
as positive or negative, and that each of its sen-
tences inherit this polarity. This means we can
learn the polarity of events in such narratives from
their (noisy) inherited polarity without labelling
individual sentences. Our training set consists of
46,255 positive and 25,069 negative sentences.
Test Set: We create the test set by selecting 4k ran-
dom first-person sentences. First-person sentences
either contain an explicit first person marker, i.e.
we and my or start with either a progressive verb or
pleonastic it. To collect gold labels, we designed a
qualifier and a HIT for Mechanical Turk, and put
</p>
<p>these out for annotation by 5 Turkers, who label
each instance as positive, negative, or neutral. To
ensure the high quality of the test set, we select
sentences that were labelled consistently positive
or negative by 4 or 5 Turkers. We collected 1,266
positive and 1,440 negative sentences.
</p>
<p>Dev Set: We created the dev set using the same
method as the test set, having Turkers annotate
2k random first-person sentences. We collected
498 positive and 754 negative sentences. The 4k
test and dev sentences available for download at
https://nlds.soe.ucsc.edu/first-person-sentiment.
</p>
<p>AutoSlog First-Person Sentence Classifier. In
order to learn new affect functions, we develop a
second sentence-level classifier using AutoSlog-
TS. We run AutoSlog over the training corpus, us-
ing the dev set to tune the parameters θf , θp and θn
(?), in order to maximize macro F-score. Our best
parameters on the dev set for positive is θf=18,
θp=0.85 and θn=1 and for negative is θf=1, θp=0.5
and θn=1. We specify that if the sentence is in both
classes we rename it as neutral. We will refer to
this classifier as the AutoSlog classifier.
</p>
<p>Baseline First-Person Sentence Classifiers. Our
goal is to see whether the knowledge we learn us-
ing AutoSlog-TS complements existing sentiment
classifiers. We thus experiment with a number
of baseline classifiers: the default SVM classi-
fier from Weka with unigram features (Hall et al.,
2005), a version of the NRC-Canada sentiment
classifier (Mohammad et al., 2013), provided to
us by Qadir and Riloff (2014), and the Stanford
Sentiment classifier (Socher et al., 2013).
</p>
<p>Retrained Stanford. The Stanford Sentiment
classifier is a based on Recursive Neural Net-
works, and trained on a compositional Sentiment
Treebank, which includes fine-grained sentiment
labels for 215,154 phrases from 11,855 sentences
from movie reviews. It can accurately predict
some compositional semantic effects and handle
negation. However since it was trained on movie
reviews, it is likely to be missing labelled data
for some common phrases in our blogs. Thus we
also retrained it (RETRAINED STANFORD) on high
precision phrases from AutoSlog extracted from
our training data of positive and negative blogs.
This provides 67,710 additional phrases, includ-
ing 58,972 positive phrases and 8,738 negative
phrases. The retrained model includes both the
labels from the original Sentiment Treebank and
the AutoSlog high precision phrases.
</p>
<p>143</p>
<p />
</div>
<div class="page"><p />
<p>4 Results and Analysis
</p>
<p>We present our experimental results and analyze
the results in terms of the lexico-functional lin-
guistic patterns we learn.
Baseline Classifiers. Rows 1-3 of Table 4 show
the results for the three baselines, in terms of F-
score for each class and the macro F. Stanford out-
performs both NRC and SVM, but misses many
cases of positive sentiment.
AutoSlog Classifier. Row 4 of Table 4 shows the
results for the AutoSlog classifier. Although Au-
toSlog itself does not perform highly, the patterns
that it learns represent a different type of knowl-
edge than what is contained in many sentiment
analysis tools. We therefore hypothesized that
a cascading classifier, which supplements one of
the baseline sentiment classifiers with the lexico-
functional patterns that AutoSlog learns might
yield higher performance.
Retrained Stanford. Row 5 of Table 4 shows the
results for RETRAINED STANFORD. The F-scores
for RETRAINED STANFORD are almost identical
to the standard Stanford classifier. This may be
because our data is a small percentage of the en-
tire number of phrases used in training Stanford.
Although RETRAINED STANFORD prioritizes our
phrases, it would not make sense to remove the
original training data.
Cascading Classifiers. We implement cascad-
ing classifiers to test our hypothesis. The cas-
cade classifier has primary and secondary classi-
fiers, and we invoke the secondary classifiers only
if the primary assigns a prediction of neutral to a
test instance, which reflects the lack of sentiment-
bearing lexical items. We also have a cascade clas-
sifier with a tertiary classifier, which is invoked in
the same fashion as the secondary classifier after
the primary and secondary classifiers have been
run. The cascading classifiers are named in the or-
der the classifier is employed, primary, secondary
or primary, secondary, tertiary. For our cascad-
ing classifiers, we combine our baseline classifiers
(NRC and Stanford), with our AutoSlog classifier.
We do not use SVM as a primary classifier since it
has no neutral label. The results for the cascading
experiments are in Rows 6-9 of Table 4.
</p>
<p>Cascading NRC and AutoSlog provides the best
performance, improving both the positive and neg-
ative classes, for a macro F of 0.71. This shows
that the learned implicit polarity information from
AutoSlog improves NRC’s performance.
</p>
<p>Since our best two-classifier cascade comes
</p>
<p>from combining NRC and AutoSlog, we also test
a cascade that adds Stanford or SVM. We achieve
our best macro F of 0.75 for the combination with
SVM.
</p>
<p>Classifier Pos Neg Macro
F1 F1 F
</p>
<p>1 SVM 0.66 0.60 0.64
2 NRC 0.58 0.69 0.64
3 Stanford 0.54 0.73 0.67
4 AutoSlog (ASlog) 0.11 0.68 0.53
5 Retrained Stanford 0.53 0.73 0.67
6 NRC, ASlog 0.60 0.78 0.71
7 Stanford, ASlog 0.55 0.76 0.70
8 NRC, ASlog, Stanford 0.64 0.79 0.74
9 NRC, ASlog, SVM 0.70 0.78 0.75
</p>
<p>Table 4: Test Set Results
</p>
<p>Analysis and Discussion. Here we discuss how
the patterns we learned from AutoSlog can sup-
plement the knowledge encoded in current senti-
ment classifiers, and in newly evolving sentiment
resources (Goyal et al., 2010; Choi and Wiebe,
2014; Balahur et al., 2012; Ruppenhofer and Bran-
des, 2015).
</p>
<p>POS PATTERNS Basic Entailment
HAVE FUN property
HAVE PARTY possession
HEADED FOR location
NEG PATTERNS Basic Entailment
HAVE CANCER property
LOST possession
NOT COME HOME location
NOT GOING KILL existence
</p>
<p>Table 5: Highly predictable AutoSlog extracted
case frames and functional description
</p>
<p>Tables 5 and 6 illustrate several learned lexico-
functional patterns for positive events used in the
AutoSlog classifier. The patterns shown in Table 5
are predicted by A&amp;R’s framework, some func-
tions of which can be seen in Table 1. For ex-
ample, we find a range of basic state descriptions
(have party, have cancer) whose basic entailment cat-
egory is either possessive or property state. Since
Ehave is positive for a first-person subject only
if the object is positive, and negative if the ob-
ject is negative, we predict that parties are good
to possess and that cancer is a bad property to
have. In this way, we can recruit the existing
function for have to induce new positive or neg-
ative things to “possess.” In line with A&amp;R’s
claims, many events are identified with their final
results: headed for results in being at a desired
location, while not coming home results in some-
thing failing to be at a desired location. We find it a
welcome result that our semi-supervised methods
</p>
<p>144</p>
<p />
</div>
<div class="page"><p />
<p>yield patterns that correspond to the A&amp;R classes,
thus validating our suspicion that first-person sen-
tences furnish a simplifying test ground for dis-
covering functional patterns in the wild.
</p>
<p>However, many patterns are not covered by
A&amp;R’s general classes, see Table 6. Looking first
at verbs, one major correlation is between positive
classes and public events and negative classes and
private states. Verbs extracted from the positive
class tend to be eventive and agentive describing
more dynamic activities and interactions, such as
played, swim, enjoyed, and danced. Even
many positive have uses are light verbs describing
an activity such as have lunch.
</p>
<p>Description POS PATTERNS
activities HAVE DINNER, HAVE WEDDING
success GOT SEE, WENT WELL
planning HEADED FOR, SET UP
free time HAVE TIME, TIME WITH
social bonding PICTURE OF, OLD FRIENDS
Description NEG PATTERNS
activities HAVE X-RAY, GET EXAM
knowledge REALIZE, NOT KNOW WHAT
unmet desire WANTS, NEED MONEY
social bonding NOT TRUST
</p>
<p>Table 6: Highly predictable AutoSlog case frames
outside A&amp;R’s functional system
</p>
<p>Verbs from the negative class are strikingly dif-
ferent. They are very often stative, where the
author is the experiencer (cognitive subject) of
that private state. While this state vs. event dis-
tinction is not one existing computational models
of sentiment or affect discuss explicitly, it repli-
cates a finding that consistently emerges in clin-
ical psychology, one that is explicitly argued for
in cognitive-behavioral accounts of the mood that
particular activities evoke (Lewinsohn et al., 1985;
MacPhillamy and Lewinsohn, 1982; Russo et al.,
2015). In addition, Table 6 reveals several novel
result state categories. The success, planning, and
unmet desire frames are all ultimately about goal-
fulfillment (or lack thereof). While the success
and unmet desire cases could be understood as
having or lacking something, the planning cases
indicate steps achieved toward a desired end-state.
Previous work on learning affect from eventuality
descriptions has largely focused on actions. Our
results indicate that private state descriptions are
another rich source of evidence.
</p>
<p>5 Related Work
</p>
<p>Previous work learns phrasal markers of im-
plicit polarity via bootstrapping from large-
scale text sources, e.g. Vu et al. (2014)
</p>
<p>learn emotion-specific event types by extracting
emotion,event pairs on Twitter. Li et al.
(2014) uses Twitter to bootstrap ‘major life events’
and typical replies to those events.
</p>
<p>Ding and Riloff (2016) extract subj-verb-obj
triples from blog posts. They then apply label
propagation to spread polarity from sentences to
events. However, the triples they learn do not fo-
cus on first-person experiencers. They also filter
private states out of the verbs used to learn their
triples, whereas we have found that verbs relating
to private states such as need, want and realize are
important indicators of first-person affect.
</p>
<p>Balahur et al. (2012) use the narratives pro-
duced by the ISEAR questionnaire (Scherer et al.,
1986) for first-person examples of particular emo-
tions and extract sequences of subject-verb-object
triples, which they annotate for basic emotions.
</p>
<p>Recent work has built on this idea, and devel-
oped methods to automatically expand Anand &amp;
Reschke’s verb classes to create completely new
lexical resources (Balahur et al., 2012; Choi and
Wiebe, 2014; Deng et al., 2013; Deng and Wiebe,
2014; Ruppenhofer and Brandes, 2015). Choi &amp;
Wiebe’s work comes closest to ours in trying to
induce (not annotate) lexical functions, but we at-
tempt to infer these from stories directly, whereas
they use a structured lexical resource.
</p>
<p>6 Conclusion
</p>
<p>We show that we can learn lexico-functional lin-
guistic patterns that reliably predict first-person af-
fect. We constructed a dataset of positive and neg-
ative first-person experiencer sentences and used
them to learn such patterns. We then showed that
the performance of current sentiment classifiers
can be enhanced by augmenting them with these
patterns. By adding our AutoSlog classifier’s re-
sults to existing classifiers we were able to im-
prove from a baseline 0.67 to 0.75 Macro F with a
cascading classifier of NRC, AutoSlog and SVM.
In addition, we analyze the linguistic functions
that indicate positivity and negativity for the first
person experiencer, and show that they are very
different. In first-person descriptions, positivity is
often signaled by active participations in events,
while negativity involves private states. In future
work, we plan to explore the integration of these
observations into sentiment resources such as the
+-Effect lexicon (Choi and Wiebe, 2014). We plan
to apply these high precision first-person lexical
patterns beyond blog data and with other person-
marking.
</p>
<p>145</p>
<p />
</div>
<div class="page"><p />
<p>References
Pranav Anand and Kevin Reschke. 2010. Verb classes
</p>
<p>as evaluativity functor classes. Proceedings of Verb
pages 98–103.
</p>
<p>Alexandra Balahur, Jesus M. Hermida, and Andres
Montoyo. 2012. Building and exploiting emotinet, a
knowledge base for emotion detection based on the
appraisal theory model. IEEE Trans. Affect. Com-
put. 3(1):88–101.
</p>
<p>Kevin Bowden, Shereen Oraby, Amita Misra, Jiaqu
Wu, Stephanie Lukin, and Marilyn Walker. 2017.
Data-driven dialogue systems for social agents. In
International Workshop on Spoken Dialogue Sys-
tems.
</p>
<p>Kevin Burton, Akshay Java, and Ian Soboroff. 2009.
The icwsm 2009 spinn3r dataset. In Proceedings of
the Annual Conference on Weblogs and Social Me-
dia (ICWSM).
</p>
<p>Yoonjung Choi and Janyce Wiebe. 2014. +/-
effectwordnet: Sense-level lexicon acquisition for
opinion inference. In EMNLP.
</p>
<p>Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer at-
titude annotation. In ACL. pages 120–125.
</p>
<p>Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In EACL.
pages 377–385.
</p>
<p>Haibo Ding and Ellen Riloff. 2016. Acquiring knowl-
edge of affective events from blogs using label prop-
agation. In AAAI.
</p>
<p>Andrew Gordon and Reid Swanson. 2009. Identify-
ing personal stories in millions of weblog entries.
In Third International Conference on Weblogs and
Social Media, Data Challenge Workshop, San Jose,
CA.
</p>
<p>Amit Goyal, Ellen Riloff, and Hal Daumé III. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing. pages 77–86.
</p>
<p>M. Hall, F. Eibe, G. Holms, B. Pfahringer, P. Reute-
mann, and I. Witten. 2005. The weka data mining
software: An update. SIGKDD Explorations 11(1).
</p>
<p>Ellen Isaacs, Artie Konrad, Alan Walendowski,
Thomas Lennig, Victoria Hollis, and Steve Whit-
taker. 2013. Echoes from the past: how technol-
ogy mediated reflection improves well-being. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. ACM, pages 1071–
1080.
</p>
<p>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. J. Artif. Int. Res. 50(1):723–762.
</p>
<p>Peter M Lewinsohn, Robin M Mermelstein, Carolyn
Alexander, and Douglas J MacPhillamy. 1985. The
unpleasant events schedule: A scale for the measure-
ment of aversive events. Journal of Clinical Psy-
chology 41(4):483–498.
</p>
<p>Jiwei Li, Xun Wang, and Eduard Hovy. 2014. What
a nasty day: Exploring mood-weather relationship
from twitter. In Proceedings of the 23rd ACM
International Conference on Conference on Infor-
mation and Knowledge Management. ACM, New
York, NY, USA, CIKM ’14, pages 1309–1318.
https://doi.org/10.1145/2661829.2662090.
</p>
<p>Douglas J MacPhillamy and Peter M Lewinsohn. 1982.
The pleasant events schedule: Studies on reliability,
validity, and scale intercorrelation. Journal of Con-
sulting and Clinical Psychology 50(3):363.
</p>
<p>Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the seventh international workshop on
Semantic Evaluation Exercises (SemEval-2013). At-
lanta, Georgia, USA.
</p>
<p>Shereen Oraby, Lena Reed, Ryan Compton, Ellen
Riloff, Marilyn Walker, and Steve Whittaker. 2015.
And thats a fact: Distinguishing factual and emo-
tional argumentation in online dialogue. In Proceed-
ings of the 2nd Workshop on Argumentation Mining.
pages 116–126.
</p>
<p>Ashequl Qadir and Ellen Riloff. 2014. Learning emo-
tion indicators from tweets: Hashtags, hashtag pat-
terns, and phrases. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Association for Computational
Linguistics. pages 1203–1209.
</p>
<p>Hannah Rashkin, Sameer Singh, and Yejin Choi. 2016.
Connotation frames: A data-driven investigation.
In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 311–321.
http://www.aclweb.org/anthology/P16-1030.
</p>
<p>Hilke Reckman, Cheyanne Baird, Jean Crawford,
Richard Crowell, Linnea Micciulla, Saratendu Sethi,
and Fruzsina Veress. 2013. teragram: Rule-based
detection of sentiment phrases using sas sentiment
analysis. In SemEval@NAACL-HLT .
</p>
<p>E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the 2003 conference on Empirical methods in natu-
ral language processing-Volume 10. Association for
Computational Linguistics, pages 105–112.
</p>
<p>Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of
the Thirteenth National Conference on Artificial In-
telligence - Volume 2. pages 1044–1049.
</p>
<p>146</p>
<p />
</div>
<div class="page"><p />
<p>Josef Ruppenhofer and Jasper Brandes. 2015. Extend-
ing effect annotation with lexical decomposition. In
6th Workshop on Computational Approaches to Sub-
jectivity, Sentiment and Social Media Analysis Wass
2015. page 67.
</p>
<p>Irene Russo, Tommaso Caselli, and Carlo Strapparava.
2015. Semeval-2015 task 9: Clipeval implicit polar-
ity of events. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval.
</p>
<p>Klaus R. Scherer, Harald G. Wallbott, and Angela B.
Summerfield. 1986. Experiencing emotion : a cross-
cultural study. Cambridge University Press.
</p>
<p>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) 2013.
</p>
<p>Hoa Trong Vu, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014. Acquir-
ing a dictionary of emotion-provoking events. EACL
2014 page 128.
</p>
<p>J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Mar-
tin. 2004. Learning subjective language. Computa-
tional linguistics 30(3):277–308.
</p>
<p>Janyce M Wiebe. 1990. Identifying subjective charac-
ters in narrative. In Proceedings of the 13th con-
ference on Computational linguistics-Volume 2. As-
sociation for Computational Linguistics, pages 401–
406.
</p>
<p>T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. Opinionfinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations. Association for Com-
putational Linguistics, pages 34–35.
</p>
<p>147</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2023
</p>
<p>Lifelong Learning CRF for Supervised Aspect Extraction
</p>
<p>Lei Shu, Hu Xu, Bing Liu
Department of Computer Science, University of Illinois at Chicago, USA
</p>
<p>{lshu3, hxu48, liub}@uic.edu
</p>
<p>Abstract
</p>
<p>This paper makes a focused contribution
to supervised aspect extraction. It shows
that if the system has performed aspect ex-
traction from many past domains and re-
tained their results as knowledge, Condi-
tional Random Fields (CRF) can leverage
this knowledge in a lifelong learning man-
ner to extract in a new domain markedly
better than the traditional CRF without us-
ing this prior knowledge. The key inno-
vation is that even after CRF training, the
model can still improve its extraction with
experiences in its applications.
</p>
<p>1 Introduction
</p>
<p>Aspect extraction is a key task of opinion min-
ing (Liu, 2012). It extracts opinion targets from
opinion text. For example, from the sentence “The
screen is great”, it aims to extract “screen”, which
is a product feature, also called an aspect.
</p>
<p>Aspect extraction is commonly done using a su-
pervised or an unsupervised approach. The unsu-
pervised approach includes methods such as fre-
quent pattern mining (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Zhu et al., 2009), syntactic
rules-based extraction (Zhuang et al., 2006; Wang
and Wang, 2008; Wu et al., 2009; Zhang et al.,
2010; Qiu et al., 2011; Poria et al., 2014), topic
modeling (Mei et al., 2007; Titov and McDonald,
2008; Li et al., 2010; Brody and Elhadad, 2010;
Wang et al., 2010; Moghaddam and Ester, 2011;
Mukherjee and Liu, 2012; Lin and He, 2009; Zhao
et al., 2010; Jo and Oh, 2011; Fang and Huang,
2012; Wang et al., 2016), word alignment (Liu
et al., 2013), label propagation (Zhou et al., 2013;
Shu et al., 2016), and others (Zhao et al., 2015).
</p>
<p>This paper focuses on the supervised approach
(Jakob and Gurevych, 2010; Choi and Cardie,
</p>
<p>2010; Mitchell et al., 2013) using Conditional
Random Fields (CRF) (Lafferty et al., 2001). It
shows that the results of CRF can be significantly
improved by leveraging some prior knowledge au-
tomatically mined from the extraction results of
previous domains, including domains without la-
beled data. The improvement is possible because
although every product (domain) is different, there
is a fair amount of aspects sharing across do-
mains (Chen and Liu, 2014). For example, every
review domain has the aspect price and reviews
of many products have the aspect battery life or
screen. Those shared aspects may not appear in
the training data but appear in unlabeled data and
the test data. We can exploit such sharing to help
CRF perform much better.
</p>
<p>Due to leveraging the knowledge gained from
the past to help the new domain extraction, we
are using the idea of lifelong machine learning
(LML) (Chen and Liu, 2016; Thrun, 1998; Sil-
ver et al., 2013), which is a continuous learning
paradigm that retains the knowledge learned in the
past and uses it to help future learning and prob-
lem solving with possible adaptations.
</p>
<p>The setting of the proposed approach L-CRF
(Lifelong CRF) is as follows: A CRF model M
has been trained with a labeled training review
dataset. At a particular point in time, M has ex-
tracted aspects from data in n previous domains
D1, . . . , Dn (which are unlabeled) and the ex-
tracted sets of aspects are A1, . . . , An. Now, the
system is faced with a new domain data Dn+1.
M can leverage some reliable prior knowledge in
A1, . . . , An to make a better extraction fromDn+1
than without leveraging this prior knowledge.
</p>
<p>The key innovation of L-CRF is that even after
supervised training, the model can still improve its
extraction in testing or its applications with expe-
riences. Note that L-CRF is different from semi-
supervised learning (Zhu, 2005) as the n previous
</p>
<p>148</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2023">https://doi.org/10.18653/v1/P17-2023</a></div>
</div>
<div class="page"><p />
<p>(unlabeled) domain data used in extraction are not
used or not available during model training.
</p>
<p>There are prior LML works for aspect extrac-
tion (Chen et al., 2014; Liu et al., 2016), but
they were all unsupervised methods. Supervised
LML methods exist (Chen et al., 2015; Ruvolo and
Eaton, 2013), but they are for classification rather
than for sequence learning or labeling like CRF.
A semi-supervised LML method is used in NELL
(Mitchell et al., 2015), but it is heuristic pattern-
based. It doesn’t use sequence learning and is not
for aspect extraction. LML is related to transfer
learning and multi-task learning (Pan and Yang,
2010), but they are also quite different (see (Chen
and Liu, 2016) for details).
</p>
<p>To the best of our knowledge, this is the first pa-
per that uses LML to help a supervised extraction
method to markedly improve its results.
</p>
<p>2 Conditional Random Fields
</p>
<p>CRF learns from an observation sequence x to es-
timate a label sequence y: p(y|x;θ), where θ is
a set of weights. Let l be the l-th position in the
sequence. The core parts of CRF are a set of
feature functions F = {fh(yl, yl−1,xl)}Hh=1 and
their corresponding weights θ = {θh}Hh=1.
Feature Functions: We use two types of feature
functions (FF). One is Label-Label (LL) FF:
</p>
<p>fLLij (yl, yl−1) = 1{yl = i}1{yl−1 = j},∀i, j ∈ Y, (1)
</p>
<p>where Y is the set of labels, and 1{·} an indicator
function. The other is Label-Word (LW) FF:
</p>
<p>fLWiv (yl,xl) = 1{yl = i}1{xl = v}, ∀i ∈ Y,∀v ∈ V, (2)
</p>
<p>where V is the vocabulary. This FF returns 1 when
the l-th word is v and the l-th label is v’s specific
label i; otherwise 0. xl is the current word, and
is represented as a multi-dimensional vector. Each
dimension in the vector is a feature of xl.
</p>
<p>Following the previous work in (Jakob and
Gurevych, 2010), we use the feature set {W, -1W,
+1W, P, -1P, +1P, G}, where W is the word and P
is its POS-tag, -1W is the previous word, -1P is its
POS-tag, +1W is the next word, +1P is its POS-
tag, and G is the generalized dependency feature.
</p>
<p>Under the Label-Word FF type, we have two
sub-types of FF: Label-dimension FF and Label-G
FF. Label-dimension FF is for the first 6 features,
and Label-G is for the G feature.
</p>
<p>The Label-dimension (Ld) FF is defined as
</p>
<p>fLd
ivd
</p>
<p>(yl,xl) = 1{yl = i}1{xdl = vd},∀i ∈ Y, ∀vd ∈ Vd, (3)
</p>
<p>where Vd is the set of observed values in feature
d ∈ {W,−1W,+1W,P,−1P,+1P} and we call
Vd feature d’s feature values. Eq. (3) is a FF that
returns 1 when xl’s feature d equals to the feature
value vd and the variable yl (lth label) equals to
the label value i; otherwise 0.
</p>
<p>We describe G and its feature function next,
which also holds the key to the proposed L-CRF.
</p>
<p>3 General Dependency Feature (G)
</p>
<p>Feature G uses generalized dependency relations.
What is interesting about this feature is that it en-
ables L-CRF to use past knowledge in its sequence
prediction at the test time in order to perform much
better. This will become clear shortly. This feature
takes a dependency pattern as its value, which is
generalized from dependency relations.
</p>
<p>The general dependency feature (G) of the vari-
able xl takes a set of feature values VG. Each fea-
ture value vG is a dependency pattern. The Label-
G (LG) FF is defined as:
</p>
<p>fLG
ivG
</p>
<p>(yl,xl) = 1{yl = i}1{xGl = vG},∀i ∈ Y, ∀vG ∈ VG. (4)
</p>
<p>Such a FF returns 1 when the dependency feature
of the variable xl equals to a dependency pattern
vG and the variable yl equals to the label value i.
</p>
<p>3.1 Dependency Relation
Dependency relations have been shown useful in
many sentiment analysis applications (Johansson
and Moschitti, 2010; Jakob and Gurevych, 2010).
A dependency relation 1 is a quintuple-tuple:
(type, gov, govpos, dep, deppos), where type is the
type of the dependency relation, gov is the gover-
nor word, govpos is the POS tag of the governor
word, dep is the dependent word, and deppos is
the POS tag of the dependent word. The l-th word
can either be the governor word or the dependent
word in a dependency relation.
</p>
<p>3.2 Dependency Pattern
We generalize dependency relations into depen-
dency patterns using the following steps:
</p>
<p>1. For each dependency relation, replace the
current word (governor word or dependent
word) and its POS tag with a wildcard since
we already have the word (W) and the POS
tag (P) features.
</p>
<p>1We obtain dependency relations using Stanford
CoreNLP: http://stanfordnlp.github.io/CoreNLP/.
</p>
<p>149</p>
<p />
</div>
<div class="page"><p />
<p>Index Word Dependency Relations
1 The {(det, battery, 2, NN , The, 1, DT) }
2 battery {(nsubj, great, 7, JJ , battery, 2, NN), (det, battery, 2, NN , The, 1, DT), (nmod, battery, 2, NN, camera, 5, NN) }
3 of {(case, camera, 5, NN, of, 3, IN) }
4 this {(det, camera, 5, NN, this, 4, DT) }
5 camera {(case, camera, 5, NN, of, 3, IN), (det, camera, 5, NN, this, 4, DT), (nmod, battery, 2, NN, camera, 5, NN) }
6 is {(cop, great, 7, JJ , is, 6, VBZ) }
7 great {(root, ROOT, 0, VBZ, great, 7, JJ), (nsubj, great, 7, JJ , battery, 2, NN), (cop, great, 7, JJ , is, 6, VBZ) }
</p>
<p>Table 1: Dependency relations parsed from “The battery of this camera is great”
</p>
<p>2. Replace the context word (the word other
than the l-th word) in each dependency re-
lation with a knowledge label to form a more
general dependency pattern. Let the set of as-
pects annotated in the training data be Kt. If
the context word in the dependency relation
appears in Kt, we replace it with a knowl-
edge label ‘A’ (aspect); otherwise ‘O’ (other).
</p>
<p>For example, we work on the sentence “The bat-
tery of this camera is great.” The dependency re-
lations are given in Table 1. Assume the current
word is “battery,” and “camera” is annotated as an
aspect. The original dependency relation between
“camera” and “battery” produced by a parser is
(nmod, battery, NN, camera, NN). Note that we do
not use the word positions in the relations in Table
1. Since the current word’s information (the word
itself and its POS-tag) in the dependency relation
is redundant, we replace it with a wild-card. The
relation becomes (nmod, *, camera, NN). Sec-
ondly, since “camera” is in Kt, we replace “cam-
era” with a general label ‘A’. The final dependency
pattern becomes (nmod,*, A, NN).
</p>
<p>We now explain why dependency patterns can
enable a CRF model to leverage the past knowl-
edge. The key is the knowledge label ‘A’ above,
which indicates a likely aspect. Recall that our
problem setting is that when we need to extract
from the new domain Dn+1 using a trained CRF
model M , we have already extracted from many
previous domains D1, . . . , Dn and retained their
extracted sets of aspects A1, . . . , An. Then, we
can mine reliable aspects from A1, . . . , An and
add them in Kt, which enables many knowl-
edge labels in the dependency patterns of the new
data An+1 due to sharing of aspects across do-
mains. This enriches the dependency pattern fea-
tures, which consequently allows more aspects to
be extracted from the new domain Dn+1.
</p>
<p>4 The Proposed L-CRF Algorithm
</p>
<p>We now present the L-CRF algorithm. As the de-
pendency patterns for the general dependency fea-
</p>
<p>Algorithm 1 Lifelong Extraction of L-CRF
1: Kp ← ∅
2: loop
3: F ← FeatureGeneration(Dn+1,K)
4: An+1 ← Apply-CRF-Model(M,F )
5: S ← S ∪ {An+1}
6: Kn+1 ← Frequent-Aspects-Mining(S, λ)
7: if Kp = Kn+1 then
8: break
9: else
</p>
<p>10: K ← Kt ∪Kn+1
11: Kp ← Kn+1
12: S ← S − {An+1}
13: end if
14: end loop
</p>
<p>ture do not use any actual words and they can also
use the prior knowledge, they are quite powerful
for cross-domain extraction (the test domain is not
used in training).
</p>
<p>Let K be a set of reliable aspects mined from
the aspects extracted in past domain datasets using
the CRF model M . Note that we assume that M
has already been trained using some labeled train-
ing data Dt. Initially, K is Kt (the set of all an-
notated aspects in the training data Dt). The more
domainsM has worked on, the more aspects it ex-
tracts, and the larger the set K gets. When faced
with a new domain Dn+1, K allows the general
dependency feature to generate more dependency
patterns related to aspects due to more knowledge
labels ‘A’ as we explained in the previous section.
Consequently, CRF has more informed features to
produce better extraction results.
</p>
<p>L-CRF works in two phases: training phase
and lifelong extraction phase. The training phase
trains a CRF model M using the training data Dt,
which is the same as normal CRF training, and
will not be discussed further. In the lifelong ex-
traction phase, M is used to extract aspects from
coming domains (M does not change and the do-
main data are unlabeled). All the results from the
domains are retained in past aspect store S. At
</p>
<p>150</p>
<p />
</div>
<div class="page"><p />
<p>Domain # Sent. # Asp. # non-asp. words
Computer 536 1173 7675
Camera 609 1640 9849
Router 509 1239 7264
Phone 497 980 7478
</p>
<p>Speaker 510 1299 7546
DVD Player 506 928 7552
Mp3 Player 505 1180 7607
Table 2: Annotation details of the datasets
</p>
<p>a particular time, it is assumed M has been ap-
plied to n past domains, and is now faced with
the n + 1 domain. L-CRF uses M and reliable
aspects (denoted Kn+1) mined from S and Kt
</p>
<p>(K = Kt ∪ Kn+1) to extract from Dn+1. Note
that aspects Kt from the training data are consid-
ered always reliable as they are manually labeled,
thus a subset ofK. We cannot use all extracted as-
pects from past domains as reliable aspects due to
many extraction errors. But those aspects that ap-
pear in multiple past domains are more likely to be
correct. ThusK contains those frequent aspects in
S. The lifelong extraction phase is in Algorithm 1.
</p>
<p>Lifelong Extraction Phase: Algorithm 1 per-
forms extraction on Dn+1 iteratively.
</p>
<p>1. It generates features (F ) on the data Dn+1
(line 3), and applies the CRF model M on F
to produce a set of aspects An+1 (line 4).
</p>
<p>2. An+1 is added to S, the past aspect store.
From S, we mine a set of frequent aspects
Kn+1. The frequency threshold is λ.
</p>
<p>3. If Kn+1 is the same as Kp from the previ-
ous iteration, the algorithm exits as no new
aspects can be found. We use an iterative
process because each extraction gives new re-
sults, which may increase the size of K, the
reliable past aspects or past knowledge. The
increased K may produce more dependency
patterns, which can enable more extractions.
</p>
<p>4. Else: some additional reliable aspects are
found. M may extract additional aspects in
the next iteration. Lines 10 and 11 update the
two sets for the next iteration.
</p>
<p>5 Experiments
</p>
<p>We now evaluate the proposed L-CRF method and
compare with baselines.
</p>
<p>5.1 Evaluation Datasets
</p>
<p>We use two types of data for our experiments. The
first type consists of seven (7) annotated bench-
mark review datasets from 7 domains (types of
products). Since they are annotated, they are used
in training and testing. The first 4 datasets are
from (Hu and Liu, 2004), which actually has 5
datasets from 4 domains. Since we are mainly
interested in results at the domain level, we did
not use one of the domain-repeated datasets. The
last 3 datasets of three domains (products) are
from (Liu et al., 2016). These datasets are used
to make up our CRF training data Dt and test data
Dn+1. The annotation details are given in Table 2.
</p>
<p>The second type has 50 unlabeled review
datasets from 50 domains or types of prod-
ucts (Chen and Liu, 2014). Each dataset has 1000
reviews. They are used as the past domain data,
i.e., D1, . . . , Dn (n = 50). Since they are not la-
beled, they cannot be used for training or testing.
</p>
<p>5.2 Baseline Methods
</p>
<p>We compare L-CRF with CRF. We will not com-
pare with unsupervised methods, which have been
shown improvable by lifelong learning (Chen
et al., 2014; Liu et al., 2016). The frequency
threshold λ in Algorithm 1 used in our experiment
to judge which extracted aspects are considered re-
liable is empirically set to 2.
</p>
<p>CRF: We use the linear chain CRF from 2. Note
that CRF uses all features including dependency
features as the proposed L-CRF but does not em-
ploy the 50 domains unlabeled data used for life-
long learning
</p>
<p>CRF+R: It treats the reliable aspect set K as
a dictionary. It adds those reliable aspects in K
that are not extracted by CRF but are in the test
data to the final results. We want to see whether
incorporating K into the CRF extraction through
dependency patterns in L-CRF is actually needed.
</p>
<p>We do not compare with domain adaptation or
transfer learning because domain adaption basi-
cally uses the source domain labeled data to help
learning in the target domain with few or no la-
beled data. Our 50 domains used in lifelong learn-
ing have no labels. So they cannot help in transfer
learning. Although in transfer learning, the target
domain usually has a large quantity of unlabeled
data, but the 50 domains are not used as the target
domains in our experiments.
</p>
<p>2https://github.com/huangzhengsjtu/pcrf/
</p>
<p>151</p>
<p />
</div>
<div class="page"><p />
<p>Cross-Domain
</p>
<p>Training Testing
CRF CRF+R L-CRF
</p>
<p>P R F1 P R F1 P R F1
−Computer Computer 86.6 51.4 64.5 23.2 90.4 37.0 82.2 62.7 71.1
−Camera Camera 84.3 48.3 61.4 21.8 86.8 34.9 81.9 60.6 69.6
−Router Router 86.3 48.3 61.9 24.8 92.6 39.2 82.8 60.8 70.1
−Phone Phone 72.5 50.6 59.6 20.8 81.2 33.1 70.1 59.5 64.4
−Speaker Speaker 87.3 60.6 71.6 22.4 91.2 35.9 84.5 71.5 77.4
−DVDplayer DVDplayer 72.7 63.2 67.6 16.4 90.7 27.7 69.7 71.5 70.6
−Mp3player Mp3player 87.5 49.4 63.2 20.6 91.9 33.7 84.1 60.7 70.5
</p>
<p>Average 82.5 53.1 64.3 21.4 89.3 34.5 79.3 63.9 70.5
In-Domain
</p>
<p>−Computer −Computer 84.0 71.4 77.2 23.2 93.9 37.3 81.6 75.8 78.6
−Camera −Camera 83.7 70.3 76.4 20.8 93.7 34.1 80.7 75.4 77.9
−Router −Router 85.3 71.8 78.0 22.8 93.9 36.8 82.6 76.2 79.3
−Phone −Phone 85.0 71.1 77.5 25.1 93.7 39.6 82.9 74.7 78.6
−Speaker −Speaker 83.8 70.3 76.5 20.1 94.3 33.2 80.1 75.8 77.9
−DVDplayer −DVDplayer 85.0 72.2 78.1 20.9 94.2 34.3 81.6 76.7 79.1
−Mp3player −Mp3player 83.2 72.6 77.5 20.4 94.5 33.5 79.8 77.7 78.7
</p>
<p>Average 84.3 71.4 77.3 21.9 94.0 35.5 81.3 76.0 78.6
Table 3: Aspect extraction results in precision, recall and F1 score: Cross-Domain and In-Domain (−X
means all except domain X)
</p>
<p>5.3 Experiment Setting
</p>
<p>To compare the systems using the same training
and test data, for each dataset we use 200 sen-
tences for training and 200 sentences for testing to
avoid bias towards any dataset or domain because
we will combine multiple domain datasets for
CRF training. We conducted both cross-domain
and in-domain tests. Our problem setting is cross-
domain. In-domain is used for completeness. In
both cases, we assume that extraction has been
done for the 50 domains.
</p>
<p>Cross-domain experiments: We combine 6
labeled domain datasets for training (1200 sen-
tences) and test on the 7th domain (not used in
training). This gives us 7 cross-domain results.
This set of tests is particularly interesting as it is
desirable to have the trained model used in cross-
domain situations to save manual labeling effort.
</p>
<p>In-domain experiments: We train and test on
the same 6 domains (1200 sentences for training
and 1200 sentences for testing). This also gives us
7 in-domain results.
</p>
<p>Evaluating Measures: We use the popular pre-
cision P , recallR, and F1-score.
</p>
<p>5.4 Results and Analysis
</p>
<p>All the experiment results are given in Table 3.
Cross-domain: Each −X in column 1 means
</p>
<p>that domain X is not used in training. X in col-
</p>
<p>umn 2 means that domain X is used in testing. We
can see that L-CRF is markedly better than CRF
and CRF+R in F1. CRF+R is very poor due to
poor precisions, which shows treating the reliable
aspects set K as a dictionary isn’t a good idea.
</p>
<p>In-domain: −X in training and test columns
means that the other 6 domains are used in both
training and testing (thus in-domain). We again
see that L-CRF is consistently better than CRF
and CRF+R in F1. The amount of gain is smaller.
This is expected because most aspects appeared in
training probably also appear in the test data as
they are reviews from the same 6 products.
</p>
<p>6 Conclusion
</p>
<p>This paper proposed a lifelong learning method
to enable CRF to leverage the knowledge gained
from extraction results of previous domains (unla-
beled) to improve its extraction. Experimental re-
sults showed the effectiveness of L-CRF. The cur-
rent approach does not change the CRF model it-
self. In our future work, we plan to modify CRF
so that it can consider previous extraction results
as well as the knowledge in previous CRF models.
</p>
<p>Acknowledgments
</p>
<p>This work was supported in part by grants from
National Science Foundation (NSF) under grant
no. IIS-1407927 and IIS-1650900.
</p>
<p>152</p>
<p />
</div>
<div class="page"><p />
<p>References
Samuel Brody and Noemie Elhadad. 2010. An unsu-
</p>
<p>pervised aspect-sentiment model for online reviews.
In NAACL ’10. pages 804–812.
</p>
<p>Zhiyuan Chen and Bing Liu. 2014. Topic modeling
using topics from many domains, lifelong learning
and big data. In ICML ’14. pages 703–711.
</p>
<p>Zhiyuan Chen and Bing Liu. 2016. Lifelong Machine
Learning. Morgan &amp; Claypool Publishers.
</p>
<p>Zhiyuan Chen, Nianzu Ma, and Bing Liu. 2015. Life-
long learning for sentiment classification. Volume 2:
Short Papers page 750.
</p>
<p>Zhiyuan Chen, Arjun Mukherjee, and Bing Liu. 2014.
Aspect extraction with automated prior knowledge
learning. In ACL ’14. pages 347–358.
</p>
<p>Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their
attributes. In ACL ’10. pages 269–274.
</p>
<p>Lei Fang and Minlie Huang. 2012. Fine granular aspect
analysis using latent structural models. In ACL ’12.
pages 333–337.
</p>
<p>Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD ’04. pages 168–
177.
</p>
<p>Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain set-
ting with conditional random fields. In EMNLP ’10.
pages 1035–1045.
</p>
<p>Yohan Jo and Alice H. Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In WSDM ’11. pages 815–824.
</p>
<p>Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning. pages 67–76.
</p>
<p>John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML ’01. pages 282–289.
</p>
<p>Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010.
Sentiment analysis with global topics and local de-
pendency. In AAAI ’10. pages 1371–1376.
</p>
<p>Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In CIKM
’09. pages 375–384.
</p>
<p>Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan &amp; Claypool Publishers.
</p>
<p>Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013.
Opinion target extraction using partially-supervised
word alignment model. In IJCAI ’13. pages 2134–
2140.
</p>
<p>Qian Liu, Bing Liu, Yuanlin Zhang, Doo Soon Kim,
and Zhiqiang Gao. 2016. Improving opinion aspect
extraction using semantic similarity and aspect asso-
ciations. In Thirtieth AAAI Conference on Artificial
Intelligence.
</p>
<p>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
WWW ’07. pages 171–180.
</p>
<p>Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,
and Benjamin Van Durme. 2013. Open domain tar-
geted sentiment. In ACL ’13. pages 1643–1654.
</p>
<p>T Mitchell, W Cohen, E Hruschka, P Talukdar, J Bet-
teridge, A Carlson, B Dalvi, M Gardner, B Kisiel,
J Krishnamurthy, N Lao, K Mazaitis, T Mohamed,
N Nakashole, E Platanios, A Ritter, M Samadi,
B Settles, R Wang, D Wijaya, A Gupta, X Chen,
A Saparov, M Greaves, and J Welling. 2015. Never-
ending learning. In AAAI’2015.
</p>
<p>Samaneh Moghaddam and Martin Ester. 2011. ILDA:
interdependent lda model for learning latent aspects
and their ratings from online product reviews. In
SIGIR ’11. pages 665–674.
</p>
<p>Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In ACL ’12.
volume 1, pages 339–348.
</p>
<p>Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on knowledge
and data engineering 22(10):1345–1359.
</p>
<p>Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT-EMNLP ’05. pages 339–346.
</p>
<p>Soujanya Poria, Erik Cambria, Lun-Wei Ku, Chen Gui,
and Alexander Gelbukh. 2014. A rule-based ap-
proach to aspect extraction from product reviews.
In Proceedings of the second workshop on natural
language processing for social media (SocialNLP).
pages 28–37.
</p>
<p>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics 37(1):9–27.
</p>
<p>Paul Ruvolo and Eric Eaton. 2013. Ella: An efficient
lifelong learning algorithm. ICML (1) 28:507–515.
</p>
<p>Lei Shu, Bing Liu, Hu Xu, and Annice Kim. 2016.
Lifelong-rl: Lifelong relaxation labeling for sepa-
rating entities and aspects in opinion targets. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
</p>
<p>Daniel L Silver, Qiang Yang, and Lianghao Li. 2013.
Lifelong machine learning systems: Beyond learn-
ing algorithms. In AAAI Spring Symposium: Life-
long Machine Learning. Citeseer, pages 49–55.
</p>
<p>153</p>
<p />
</div>
<div class="page"><p />
<p>Sebastian Thrun. 1998. Lifelong learning algorithms.
In Learning to learn, Springer, pages 181–209.
</p>
<p>Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In ACL ’08: HLT . pages 308–316.
</p>
<p>Bo Wang and Houfeng Wang. 2008. Bootstrapping
both product features and opinion words from chi-
nese customer reviews with cross-inducing. In IJC-
NLP ’08. pages 289–295.
</p>
<p>Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: A
rating regression approach. In KDD ’10. pages 783–
792.
</p>
<p>Shuai Wang, Zhiyuan Chen, and Bing Liu. 2016. Min-
ing aspect-specic opinion using a holistic lifelong
topic model. In WWW ’16.
</p>
<p>Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP ’09. pages 1533–1541.
</p>
<p>Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O’Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In COLING ’10:
Posters. pages 1462–1470.
</p>
<p>Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opinions
with a maxent-lda hybrid. In EMNLP ’10. pages 56–
65.
</p>
<p>Yanyan Zhao, Bing Qin, and Ting Liu. 2015. Creating
a fine-grained corpus for chinese sentiment analysis.
IEEE Intelligent Systems 30(1):36–43.
</p>
<p>Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2013.
Collective opinion target extraction in Chinese mi-
croblogs. In EMNLP ’13. pages 1840–1850.
</p>
<p>Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In CIKM ’09. pages 1799–
1802.
</p>
<p>Xiaojin Zhu. 2005. Semi-supervised learning literature
survey .
</p>
<p>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In CIKM ’06.
pages 43–50.
</p>
<p>154</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2024
</p>
<p>Exploiting Domain Knowledge via Grouped Weight Sharing
with Application to Text Categorization
</p>
<p>Ye Zhang1 Matthew Lease2 Byron C. Wallace3
</p>
<p>1Department of Computer Science, University of Texas at Austin
2School of Information, University of Texas at Austin
</p>
<p>3College of Computer &amp; Information Science, Northeastern University
yezhang@utexas.edu, ml@utexas.edu, byron@ccs.neu.edu
</p>
<p>Abstract
</p>
<p>A fundamental advantage of neural mod-
els for NLP is their ability to learn rep-
resentations from scratch. However, in
practice this often means ignoring existing
external linguistic resources, e.g., Word-
Net or domain specific ontologies such
as the Unified Medical Language System
(UMLS). We propose a general, novel
method for exploiting such resources via
weight sharing. Prior work on weight
sharing in neural networks has considered
it largely as a means of model compres-
sion. In contrast, we treat weight shar-
ing as a flexible mechanism for incorpo-
rating prior knowledge into neural models.
We show that this approach consistently
yields improved performance on classifi-
cation tasks compared to baseline strate-
gies that do not exploit weight sharing.
</p>
<p>1 Introduction
Neural models are powerful in part due to their
ability to learn good representations of raw tex-
tual inputs, mitigating the need for extensive
task-specific feature engineering (Collobert et al.,
2011). However, a downside of learning from
scratch is failing to capitalize on prior linguistic
or semantic knowledge, often encoded in existing
resources such as ontologies. Such prior knowl-
edge can be particularly valuable when estimating
highly flexible models. In this work, we address
how to exploit known relationships between words
when training neural models for NLP tasks.
</p>
<p>We propose exploiting the feature-hashing
trick, originally proposed as a means of neural net-
work compression (Chen et al., 2015). Here we
instead view the partial parameter sharing induced
by feature hashing as a flexible mechanism for ty-
ing together network node weights that we believe
</p>
<p>e11 e13e12
</p>
<p>good nice
</p>
<p>e21 e23e22
</p>
<p>g13g12g11
</p>
<p>g1:{good, nice, amazing}
</p>
<p>g23g22g21
</p>
<p>g2:{good, interesting}
</p>
<p>e31 e33e32
</p>
<p>interesting
</p>
<p>hi(j)
</p>
<p>Figure 1: An example of grouped partial weight
sharing. Here there are two groups. We stochas-
tically select embedding weights to be shared be-
tween words belonging to the same group(s).
</p>
<p>to be similar a priori. In effect, this acts as a reg-
ularizer that constrains the model to learn weights
that agree with the domain knowledge codified in
external resources like ontologies.
</p>
<p>More specifically, as external resources we use
Brown clusters (Brown et al., 1992), WordNet
(Miller, 1995) and the Unified Medical Language
System (UMLS) (Bodenreider, 2004). From these
we derive groups of words with similar meaning.
We then use feature hashing to share a subset of
weights between the embeddings of words that be-
long to the same semantic group(s). This forces
the model to respect prior domain knowledge, in-
sofar as words similar under a given ontology are
compelled to have similar embeddings.
</p>
<p>Our contribution is a novel, simple and flexible
method for injecting domain knowledge into neu-
ral models via stochastic weight sharing. Results
on seven diverse classification tasks (three senti-
ment and four biomedical) show that our method
consistently improves performance over (1) base-
lines that fail to capitalize on domain knowledge,
and (2) an approach that uses retrofitting (Faruqui
et al., 2014) as a preprocessing step to encode do-
main knowledge prior to training.
</p>
<p>155</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2024">https://doi.org/10.18653/v1/P17-2024</a></div>
</div>
<div class="page"><p />
<p>2 Grouped Weight Sharing
</p>
<p>We incorporate similarity relations codified in ex-
isting resources (here derived from Brown clus-
ters, SentiWordNet and the UMLS) as prior
knowledge in a Convolutional Neural Network
(CNN).1 To achieve this we construct a shared em-
bedding matrix such that words known a priori to
be similar are constrained to share some fraction
of embedding weights.
</p>
<p>Concretely, suppose we have N groups of
words derived from an external resource. Note that
one could derive such groups in several ways; e.g.,
using the synsets in SentiWordNet. We denote
groups by {g1, g2, ..., gN}. Each group is associ-
ated with an embedding ggi , which we initialize
by averaging the pre-trained embeddings of each
word in the group.
</p>
<p>To exploit both grouped and independent word
weights, we adopt a two-channel CNN model
(Zhang et al., 2016b). The embedding matrix
of the first channel is initialized with pre-trained
word vectors. We denote this input by Ep ∈ RV×d
(V is the vocabulary size and d the dimension of
the word embeddings). The second channel in-
put matrix is initialized with our proposed weight-
sharing embedding Es ∈ RV×d. Es is initialized
by drawing from both Ep and the external resource
following the process we describe below.
</p>
<p>Given an input text sequence of length l,
we construct sequence embedding representations
Wp ∈ Rl×d and Ws ∈ Rl×d using the cor-
responding embedding matrices. We then apply
independent sets of linear convolution filters on
these two matrices. Each filter will generate a fea-
ture map vector v ∈ Rl−h+1 (h is the filter height).
We perform 1-max pooling over each v, extract-
ing one scalar per feature map. Finally, we con-
catenate scalars from all of the feature maps (from
both channels) into a feature vector which is fed to
a softmax function to predict the label (Figure 2).
</p>
<p>We initialize Es as follows. Each row ei ∈ Rd
of Es is the embedding of word i. Words may
belong to one or more groups. A mapping func-
tion G(i) retrieves the groups that word i belongs
to, i.e., G(i) returns a subset of {g1, g2, ..., gN},
which we denote by {g(i)1 , g
</p>
<p>(i)
2 ...g
</p>
<p>(i)
K }, where K
</p>
<p>is the number of groups that contain word i. To
initialize Es, for each dimension j of each word
embedding ei, we use a hash function hi to map
</p>
<p>1The idea of sharing weights to reflect known similarity is
general and could be applied with other neural architectures.
</p>
<p>It’s a good movie
</p>
<p>CNN Layer
</p>
<p>Ep Es
</p>
<p>Pooling Layer
</p>
<p>Softmax Layer
</p>
<p>Feature 
Maps
</p>
<p>Classes
</p>
<p>Figure 2: Proposed two-channel model. The first
channel input is a standard pre-trained embedding
matrix. The second channel receives a partially
shared embedding matrix constructed using exter-
nal linguistic resources.
</p>
<p>(hash) the index j to one of the K group IDs:
hi : N → {g(i)1 , g
</p>
<p>(i)
2 ...g
</p>
<p>(i)
K }. Following (Wein-
</p>
<p>berger et al., 2009; Shi et al., 2009), we use a
second hash function b to remove bias induced by
hashing. This is a signing function, i.e., it maps
(i, j) tuples to {+1,−1} 2. We then set ei,j to the
product of ghi(j),j and b(i, j). h and b are both ap-
proximately uniform hash functions. Algorithm 1
provides the full initialization procedure.
</p>
<p>Algorithm 1 Initialization of Es
</p>
<p>1: for i in {1, . . . , V } do
2: {g(i)1 , g
</p>
<p>(i)
2 , . . . , g
</p>
<p>(i)
K } := G(i).
</p>
<p>3: for j ∈ {1, . . . , d} do
4: ei,j := ghi(j),j · b(i, j)
5: end for
6: end for
</p>
<p>For illustration, consider Figure 1. Here g1 con-
tains three words: good, nice and amazing, while
g2 has two words: good and interesting. The group
embeddings gg1 , gg2 are initialized as averages
over the pre-trained embeddings of the words they
comprise. Here, embedding parameters e1,1 and
e2,1 are both mapped to gg1,1, and thus share this
value. Similarly, e1,3 and e2,3 will share value at
gg1,3. We have elided the second hash function b
from this figure for simplicity.
</p>
<p>2Empirically, we found that using this signing function
does not affect performance.
</p>
<p>156</p>
<p />
</div>
<div class="page"><p />
<p>During training, we update Ep as usual using
back-propagation (Rumelhart et al., 1986). We up-
date Es and group embeddings g in a manner sim-
ilar to Chen et al. (2015). In the forward propa-
gation before each training step (mini-batch), we
derive the value of ei,j from g:
</p>
<p>ei,j := ghi(j),j ∗ b(i, j) (1)
</p>
<p>We use this newly updated ei,j to perform for-
ward propagation in the CNN.
</p>
<p>During backward propagation, we first compute
the gradient of Es, and then we use this to derive
the gradient w.r.t gs. To do this, for each dimen-
sion j in ggk , we aggregate the gradients w.r.t E
</p>
<p>s
</p>
<p>whose elements are mapped to this dimension:
</p>
<p>∇ggk,j :=
∑
</p>
<p>(i,j)
</p>
<p>∇Esi,j · δhi(j)=gk · b(i, j) (2)
</p>
<p>where δhi(j)=gk = 1 when h
i(j) = gk, and 0
</p>
<p>otherwise. Each training step involves execut-
ing Equations 1 and 2. Once the shared gradient
is calculated, gradient descent proceeds as usual.
We update all parameters aside from the shared
weights in the standard way.
</p>
<p>The number of parameters in our approach
scales linearly with the number of channels. But
the gradients can actually be back-propagated in
a distributed way for each channel, since the con-
volutional and embedding layers are independent
across these. Thus training time scales approxi-
mately linearly with the number of parameters in
one channel (if the gradient is back-propagated in
a distributed way).
</p>
<p>3 Experimental Setup
</p>
<p>3.1 Datasets
</p>
<p>We use three sentiment datasets: a movie review
(MR) dataset (Pang and Lee, 2005)3; a customer
review (CR) dataset (Hu and Liu, 2004)4; and an
opinion dataset (MPQA) (Wiebe et al., 2005)5.
</p>
<p>We also use four biomedical datasets, which
concern systematic reviews. The task here is to
classify published articles describing clinical tri-
als as relevant or not to a well-specified clinical
question. Articles deemed relevant are included in
</p>
<p>3www.cs.cornell.edu/people/pabo/
movie-review-data/
</p>
<p>4www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html
</p>
<p>5mpqa.cs.pitt.edu/corpora/mpqa_corpus/
</p>
<p>the corresponding review, which is a synthesis of
all pertinent evidence (Wallace et al., 2010). We
use data from reviews that concerned: clopido-
grel (CL) for cardiovascular conditions (Dahabreh
et al., 2013); biomarkers for assessing iron de-
ficiency in anemia (AN) experienced by patients
with kidney disease (Chung et al., 2012); statins
(ST) (Cohen et al., 2006); and proton beam (PB)
therapy (Terasawa et al., 2009).
</p>
<p>3.2 Implementation Details and Baselines
</p>
<p>We use SentiWordNet (Baccianella et al., 2010)6
</p>
<p>for the sentiment tasks. SentiWordNet assigns
to each synset of wordnet three sentiment scores:
positivity, negativity and objectivity, constrained
to sum to 1. We keep only the synsets with pos-
itivity or negativity scores greater than 0, i.e., we
remove synsets deemed objective. The synsets in
SentiWordNet constitute our groups. We also use
the Brown clustering algorithm7 on the three senti-
ment datasets. We generate 1000 clusters and treat
each as a group.
</p>
<p>For the biomedical datasets, we use the Medical
Subject Headings (MeSH) terms8 attached to each
abstract to classify them. Each MeSH term has a
tree number indicating the path from the root in the
UMLS. For example, ‘Alagille Syndrome’ has tree
number ‘C06.552.150.125’; periods denote tree
splits, numbers are nodes. We induce groups com-
prising MeSH terms that share the same first three
parent nodes, e.g., all terms with ‘C06.552.150’ as
their tree number prefix constitute one group.
</p>
<p>We compare our approach to several baselines.
All use pre-trained embeddings to initialize Ep,
but we explore several approaches to exploiting
Es: (1) randomly initialize Es; (2) initialize Es
</p>
<p>to reflect the group embedding g, but do not
share weights during the training process, i.e., do
not constrain their weights to be equal when we
perform back-propagation; (3) use linguistic re-
sources to retro-fit (Faruqui et al., 2014) the pre-
trained embeddings, and use these to initialize Es.
For retro-fitting, we first construct a graph de-
rived from SentiWordNet. Then we run belief-
propagation on the graph to encourage linked
words to have similar vectors. This is a pre-
processing step only; we do not impose weight
sharing constraints during training.
</p>
<p>6sentiwordnet.isti.cnr.it
7github.com/percyliang/brown-cluster
8www.nlm.nih.gov/bsd/disted/
</p>
<p>meshtutorial/
</p>
<p>157</p>
<p />
</div>
<div class="page"><p />
<p>total #instances vocabulary size #positive instances #negative instances
MR 10662 18765 5331 5331
CR 3773 5340 2406 1367
</p>
<p>MPQA 10604 6246 3311 7293
AN 5653 5554 653 5000
CL 8288 3684 768 7520
ST 3464 2965 173 3291
PB 4749 3086 243 4506
</p>
<p>Table 1: Corpora statistics.
</p>
<p>For the sentiment datasets we use three filter
heights (3,4,5) for each of the two CNN channels.
For the biomedical datasets, we use only one fil-
ter height (1), because the inputs are unstructured
MeSH terms.9 In both cases we use 100 filters of
each unique height. For the sentiment datasets, we
use Google word2vec (Mikolov et al., 2013)10 to
initialize Ep. For the biomedical datasets, we use
word2vec trained on biomedical texts (Moen and
Ananiadou, 2013)11 to initialize Ep. For param-
eter estimation, we use Adadelta (Zeiler, 2012).
Because the biomedical datasets are imbalanced,
we use downsampling (Zhang et al., 2016a; Zhang
and Wallace, 2015) to effectively train on balanced
subsets of the data.
</p>
<p>We developed our approach using the MR senti-
ment dataset, tuning our approach to constructing
groups from the available resources – experiments
on other sentiment datasets were run after we fi-
nalized the model and hyperparameters. Similarly,
we used the anemia (AN) review as a development
set for the biomedical tasks, especially w.r.t. con-
structing groups from MeSH terms using UMLS.
</p>
<p>4 Results
</p>
<p>We replicate each experiment five times (each is
a 10-fold cross validation), and report the mean
(min, max) across these replications. Results on
the sentiment and biomedical corpora in are pre-
sented in Tables 2 and 3, respectively.12 These
exploit different external resources to induce the
word groupings that in turn inform weight shar-
ing. We report AUC for the biomedical datasets
because these are highly imbalanced (see Table 1).
</p>
<p>Our method improves performance compared to
all relevant baselines (including an approach that
</p>
<p>9For this work we are ignoring title and abstract texts.
10code.google.com/archive/p/word2vec/
11bio.nlplab.org/
12Sentiment task results are not directly comparable to
</p>
<p>prior work due to different preprocessing steps.
</p>
<p>also exploits external knowledge via retrofitting)
in six of seven cases. Informing weight initial-
ization using external resources improves perfor-
mance independently, but additional gains are re-
alized by also enforcing sharing during training.
</p>
<p>We note that our aim here is not necessarily to
achieve state-of-art results on any given dataset,
but rather to evaluate the proposed method for in-
corporating external linguistic resources into neu-
ral models via weight sharing. We have therefore
compared to baselines that enable us to assess this.
</p>
<p>5 Related Work
</p>
<p>Neural Models for NLP. Recently there has been
enormous interest in neural models for NLP gener-
ally (Collobert et al., 2011; Goldberg, 2016). Most
relevant to this work, simple CNN based mod-
els (which we have built on here) have proven
extremely effective for text categorization (Kim,
2014; Zhang and Wallace, 2015).
Exploiting Linguistic Resources. A potential
drawback to learning from scratch in end-to-end
neural models is a failure to capitalize on exist-
ing knowledge sources. There have been efforts to
exploit such resources specifically to induce bet-
ter word vectors (Yu and Dredze, 2014; Faruqui
et al., 2014; Yu et al., 2016; Xu et al., 2014). But
these models do not attempt to exploit external
resources jointly during training for a particular
downstream task (which uses word embeddings as
inputs), as we do here.
</p>
<p>Past work on sparse linear models has shown
the potential of exploiting linguistic knowledge in
statistical NLP models. For example, Yogatama
and Smith (2014) used external resources to in-
form structured, grouped regularization of log-
linear text classification models, yielding improve-
ments over standard regularization approaches.
Elsewhere, Doshi-Velez et al. (2015) proposed a
variant of LDA that exploits a priori known tree-
</p>
<p>158</p>
<p />
</div>
<div class="page"><p />
<p>Method MR CR MPQA
p only 81.02 (80.84,81.24) 84.34 (84.21,84.53) 89.41 (89.22,89.58)
p + r 81.25 (81.19,81.32) 84.33 (84.24,84.38) 89.63 (89.58,89.71)
</p>
<p>p + retro 81.35 (81.23,81.51) 84.16 (84.09,84.28) 89.61 (89.48,89.77)
p + S (no sharing) 81.39 (81.32,81.43) 84.13 (84.06,84.21) 89.71 (89.67,89.75)
p + B (no sharing) 81.50 (81.29,81.63) 84.60 (84.53,84.66) 89.57 (89.52,89.61)
</p>
<p>p + S (sharing) 81.69 (81.60,81.78) 84.34 (84.24,84.43) 89.84 (89.74,90.13)
p + B (sharing) 81.83 (81.80,81.87) 84.68 (84.64,84.72) 89.97 (89.74,90.13)
</p>
<p>Table 2: Accuracy mean (min, max) on sentiment datasets. ‘p’: channel initialized with the pre-trained
embeddings Ep. ‘r’: channel randomly initialized. ‘retro’: initialized with retofitted embeddings. ‘S/B
(no sharing)’: channel initialized with Es (using SentiWordNet or Brown clusters), but weights are not
shared during training. ‘S/B (sharing)’: proposed weight-sharing method.
</p>
<p>Method AN CL ST PB
p only 86.63 (86.57,86.67) 88.73 (88.51,89.00) 67.15 (66.00, 67.91) 90.11 (89.46, 91.03)
p + r 85.67 (85.46,85.95) 88.87 (88.56,89.03) 67.72 (67.65,67.86) 90.12 (89.87,90.47)
</p>
<p>p + retro 86.46 (86.32,86.65) 89.27 (88.89,90.01) 67.78 (67.56,68.00) 90.07 (89.92,90.20)
p + U 86.60 (86.32,87.01) 88.93 (88.67,89.13) 67.78 (67.71,67.85) 90.23 (89.84,90.47)
</p>
<p>p + U(s) 87.15 (87.00,87.29) 89.29 (89.09,89.51) 67.73 (67.58,67.88) 90.99 (90.46,91.59)
</p>
<p>Table 3: AUC mean (min, max) on the biomedical datasets. Abbreviations are as in Table 2, except here
the external resource is the UMLS MeSH ontology (‘U’).‘U(s)’ is the proposed weight sharing method
utilizing ULMS.
</p>
<p>structured relations between tokens (e.g., derived
from the UMLS) in topic modeling.
Weight-sharing in NNs. Recent work has con-
sidered stochastically sharing weights in neural
models. Notably, Chen et al. (2015). proposed
randomly sharing weights in neural networks.
Elsewhere, Han et al. (2015) proposed quantized
weight sharing as an intermediate step in their
deep compression model. In these works, the pri-
mary motivation was model compression, whereas
here we view the hashing trick as a mechanism to
encode domain knowledge.
</p>
<p>6 Conclusion
</p>
<p>We have proposed a novel method for incorporat-
ing prior semantic knowledge into neural models
via stochastic weight sharing. We have showed it
generally improves text classification performance
vs. model variants which do not exploit external
resources and vs. an approach based on retrofitting
prior to training. In future work, we will inves-
tigate generalizing our approach beyond classifi-
cation, and to inform weight sharing using other
varieties and sources of linguistic knowledge.
</p>
<p>Acknowledgements. This work was made possible by NPRP
grant NPRP 7-1313-1-245 from the Qatar National Research
</p>
<p>Fund (a member of Qatar Foundation). The statements made
</p>
<p>herein are solely the responsibility of the authors.
</p>
<p>References
</p>
<p>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC. volume 10, pages 2200–2204.
</p>
<p>Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research 32(suppl 1):D267–
D270.
</p>
<p>Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics 18(4):467–479.
</p>
<p>Wenlin Chen, James T Wilson, Stephen Tyree, Kilian Q
Weinberger, and Yixin Chen. 2015. Compressing
neural networks with the hashing trick. In ICML.
pages 2285–2294.
</p>
<p>Mei Chung, Denish Moorthy, Nira Hadar, Priyanka
Salvi, Ramon C Iovin, and Joseph Lau. 2012.
Biomarkers for Assessing and Managing Iron De-
ficiency Anemia in Late-Stage Chronic Kidney Dis-
ease. AHRQ Comparative Effectiveness Reviews.
Agency for Healthcare Research and Quality (US),
Rockville (MD).
</p>
<p>159</p>
<p />
</div>
<div class="page"><p />
<p>Aaron M Cohen, William R Hersh, K Peterson, and Po-
Yin Yen. 2006. Reducing workload in systematic re-
view preparation using automated citation classifica-
tion. Journal of the American Medical Informatics
Association 13(2):206–219.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12(Aug):2493–2537.
</p>
<p>Issa J Dahabreh, Denish Moorthy, Jenny L Lamont,
Minghua L Chen, David M Kent, and Joseph Lau.
2013. Testing of cyp2c19 variants and platelet reac-
tivity for guiding antiplatelet treatment .
</p>
<p>Finale Doshi-Velez, Byron C Wallace, and Ryan
Adams. 2015. Graph-sparse lda: A topic model with
structured sparsity. In AAAI Conference on Artificial
Intelligence. pages 2575–2581.
</p>
<p>Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons.
arXiv preprint arXiv:1411.4166 .
</p>
<p>Yoav Goldberg. 2016. A primer on neural network
models for natural language processing. Journal of
Artificial Intelligence Research 57:345–420.
</p>
<p>Song Han, Huizi Mao, and William J Dally. 2015.
Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huff-
man coding. arXiv preprint arXiv:1510.00149 .
</p>
<p>Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining. pages 168–177.
</p>
<p>Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41.
</p>
<p>SPFGH Moen and Tapio Salakoski2 Sophia Anani-
adou. 2013. Distributional semantics resources for
biomedical text processing.
</p>
<p>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proc. of the ACL.
</p>
<p>D.E. Rumelhart, G.E. Hintont, and R.J. Williams.
1986. Learning representations by back-
propagating errors. Nature 323(6088):533–536.
</p>
<p>Qinfeng Shi, James Petterson, Gideon Dror, John
Langford, Alex Smola, and SVN Vishwanathan.
2009. Hash kernels for structured data. Journal of
Machine Learning Research 10(Nov):2615–2637.
</p>
<p>T. Terasawa, T. Dvorak, S. Ip, G. Raman, J. Lau, and
T. A. Trikalinos. 2009. Charged Particle Radiation
Therapy for Cancer: A Systematic Review. Ann.
Intern. Med. .
</p>
<p>Byron C Wallace, Thomas A Trikalinos, Joseph
Lau, Carla Brodley, and Christopher H Schmid.
2010. Semi-automated screening of biomedical ci-
tations for systematic reviews. BMC bioinformatics
11(1):55.
</p>
<p>Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In Pro-
ceedings of the 26th Annual International Confer-
ence on Machine Learning. pages 1113–1120.
</p>
<p>Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion 39(2):165–210.
</p>
<p>Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proceedings of
the 23rd ACM International Conference on Confer-
ence on Information and Knowledge Management.
ACM, pages 1219–1228.
</p>
<p>Dani Yogatama and Noah A Smith. 2014. Linguistic
structured sparsity in text categorization. In Meet-
ing of the Association for Computational Linguis-
tics. pages 786–796.
</p>
<p>Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In ACL. pages
545–550.
</p>
<p>Zhiguo Yu, Trevor Cohen, Elmer V Bernstam, and
Byron C Wallace. 2016. Retrofitting word vectors
of mesh terms to improve semantic similarity mea-
sures. Intl. Workshop on Health Text Mining and
Information Analysis at EMNLP pages 43–51.
</p>
<p>Matthew D. Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method.
</p>
<p>Ye Zhang, Iain Marshall, and Byron C Wallace.
2016a. Rationale-augmented convolutional neu-
ral networks for text classification. arXiv preprint
arXiv:1605.04469 .
</p>
<p>Ye Zhang, Stephen Roller, and Byron C Wallace.
2016b. MGNC-CNN: A simple approach to exploit-
ing multiple word embeddings for sentence classifi-
cation pages 1522–1527.
</p>
<p>Ye Zhang and Byron C Wallace. 2015. A sensitiv-
ity analysis of (and practitioners’ guide to) convo-
lutional neural networks for sentence classification.
arXiv preprint arXiv:1510.03820 .
</p>
<p>160</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161–166
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2025
</p>
<p>Improving Neural Parsing by
Disentangling Model Combination and Reranking Effects
</p>
<p>Daniel Fried∗ Mitchell Stern∗ Dan Klein
Computer Science Division
</p>
<p>University of California, Berkeley
{dfried,mitchell,klein}@cs.berkeley.edu
</p>
<p>Abstract
</p>
<p>Recent work has proposed several genera-
tive neural models for constituency pars-
ing that achieve state-of-the-art results.
Since direct search in these generative
models is difficult, they have primarily
been used to rescore candidate outputs
from base parsers in which decoding is
more straightforward. We first present an
algorithm for direct search in these gen-
erative models. We then demonstrate that
the rescoring results are at least partly due
to implicit model combination rather than
reranking effects. Finally, we show that
explicit model combination can improve
performance even further, resulting in new
state-of-the-art numbers on the PTB of
94.25 F1 when training only on gold data
and 94.66 F1 when using external data.
</p>
<p>1 Introduction
</p>
<p>Recent work on neural constituency parsing (Dyer
et al., 2016; Choe and Charniak, 2016) has found
multiple cases where generative scoring models
for which inference is complex outperform base
models for which inference is simpler. Let A be
a parser that we want to parse with (here one of
the generative models), and let B be a base parser
that we use to propose candidate parses which are
then scored by the less-tractable parser A. We de-
note this cross-scoring setup by B→ A. The pa-
pers above repeatedly saw that the cross-scoring
setup B→ A under which their generative mod-
els were applied outperformed the standard single-
parser setup B→ B. We term this a cross-scoring
gain.
</p>
<p>This paper asks two questions. First, why do re-
cent discriminative-to-generative cross-scoring se-
</p>
<p>∗Equal contribution.
</p>
<p>tups B→ A outperform their base parsers B? Per-
haps generative models A are simply superior to
the base models B and direct generative parsing
(A→ A) would be better still if it were feasi-
ble. If so, we would characterize the cross-scoring
gain from B→ B to B→ A as a reranking gain.
However, it’s also possible that the hybrid sys-
tem B→ A shows gains merely from subtle model
combination effects. If so, scoring candidates us-
ing some combined score A + B would be even
better, which we would characterize as a model
combination gain. It might even be the case that B
is a better parser overall (i.e. B→ B outperforms
A→ A).
</p>
<p>Of course, many real hybrids will exhibit both
reranking and model combination gains. In this
paper, we present experiments to isolate the de-
gree to which each gain occurs for each of two
state-of-the-art generative neural parsing models:
the Recurrent Neural Network Grammar genera-
tive parser (RG) of Dyer et al. (2016), and the
LSTM language modeling generative parser (LM)
of Choe and Charniak (2016).
</p>
<p>In particular, we present and use a beam-based
search procedure with an augmented state space
that can search directly in the generative models,
allowing us to explore A→ A for these genera-
tive parsers A independent of any base parsers.
Our findings suggest the presence of model com-
bination effects in both generative parsers: when
parses found by searching directly in the genera-
tive parser are added to a list of candidates from
a strong base parser (the RNNG discriminative
parser, RD (Dyer et al., 2016)), performance de-
creases when compared to using just candidates
from the base parser, i.e., B ∪ A→ A has lower
evaluation performance than B→ A (Section 3.1).
</p>
<p>This result suggests that both generative models
benefit from fortuitous search errors in the rescor-
ing setting – there are trees with higher probability
</p>
<p>161</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2025">https://doi.org/10.18653/v1/P17-2025</a></div>
</div>
<div class="page"><p />
<p>under the generative model than any tree proposed
by the base parser, but which would decrease eval-
uation performance if selected. Because of this,
we hypothesize that model combination effects be-
tween the base and generative models are partially
responsible for the high performance of the gener-
ative reranking systems, rather than the generative
model being generally superior.
</p>
<p>Here we consider our second question: if cross-
scoring gains are at least partly due to implicit
model combination, can we gain even more by
combining the models explicitly? We find that this
is indeed the case: simply taking a weighted aver-
age of the scores of both models when selecting
a parse from the base parser’s candidate list im-
proves over using only the score of the generative
model, in many cases substantially (Section 3.2).
Using this technique, in combination with ensem-
bling, we obtain new state-of-the-art results on the
Penn Treebank: 94.25 F1 when training only on
gold parse trees and 94.66 F1 when using external
silver data.
</p>
<p>2 Decoding in generative neural models
</p>
<p>All of the parsers we investigate in this work (the
discriminative parser RD, and the two generative
parsers RG and LM, see Section 1) produce parse
trees in a depth-first, left-to-right traversal, using
the same basic actions: NT(X ), which opens a
new constituent with the non-terminal symbol X;
SHIFT / GEN(w), which adds a word; and RE-
DUCE, which closes the current constituent. We
refer to Dyer et al. (2016) for a complete descrip-
tion of these actions, and the constraints on them
necessary to ensure valid parse trees.1
</p>
<p>The primary difference between the actions in
the discriminative and generative models is that,
whereas the discriminative model uses a SHIFT ac-
tion which is fixed to produce the next word in
the sentence, the generative models use GEN(w)
to define a distribution over all possible words w
in the lexicon. This stems from the generative
model’s definition of a joint probability p(x, y)
over all possible sentences x and parses y. To use
a generative model as a parser, we are interested in
finding the maximum probability parse for a given
sentence. This is made more complicated by not
</p>
<p>1The action space for LM differs from RG in two ways: 1)
LM has separate reduce actions REDUCE(X ) for each non-
terminal X , and 2) LM allows any action to have non-zero
probability at all times, even those that may be structurally
invalid.
</p>
<p>having an explicit representation for p(y|x), as we
do in the discriminative setting. However, we can
start by applying similar approximate search pro-
cedures as are used for the discriminative parser,
constraining the set of actions such that it is only
possible to produce the observed sentence: i.e.
only allow a GEN(w) action when w is the next
terminal in the sentence, and prohibit GEN actions
if all terminals have been produced.
</p>
<p>2.1 Action-synchronous beam search
Past work on discriminative neural constituency
parsers has shown the effectiveness of beam
search with a small beam (Vinyals et al., 2015)
or even greedy search, as in the case of RD (Dyer
et al., 2016). The standard beam search procedure,
which we refer to as action-synchronous, main-
tains a beam of K partially-completed parses that
all have the same number of actions taken. At each
stage, a pool of successors is constructed by ex-
tending each candidate in the beam with each of its
possible next actions. The K highest-probability
successors are chosen as the next beam.
</p>
<p>Unfortunately, we find that action-synchronous
beam search breaks down for both generative
models we explore in this work, failing to find
parses that are high scoring under the model. This
stems from the probabilities of the actions NT(X )
for all labels X almost always being greater than
the probability of GEN(w) for the particular word
w which must be produced next in a given sen-
tence. Qualitatively, the search procedure prefers
to open constituents repeatedly up until the max-
imum number allowed by the model. While
these long chains of non-terminals will usually
have lower probability than the correct sequence
at the point where they finally generate the next
word, they often have higher probability up un-
til the word is generated, and so they tend to
push the correct sequence off the beam before
this point is reached. This search failure produces
very low evaluation performance: with a beam of
size K = 100, action-synchronous beam search
achieves 29.1 F1 for RG and 27.4 F1 for LM on
the development set.
</p>
<p>2.2 Word-synchronous beam search
To deal with this issue, we force partial parse can-
didates to compete with each other on a word-
by-word level, rather than solely on the level of
individual actions. The word-synchronous beam
search we apply is very similar to approximate
</p>
<p>162</p>
<p />
</div>
<div class="page"><p />
<p>Word-synchronous beam size, Kw
model 10 20 40 60 80 100
</p>
<p>RG 74.1 80.1 85.3 87.5 88.7 89.6
LM 83.7 88.6 90.9 91.6 92.0 92.2
</p>
<p>Table 1: F1 on the development set for word-synchronous
beam search when searching in the RNNG generative (RG)
and LSTM generative (LM) models. Ka is set to 10×Kw.
</p>
<p>decoding procedures developed for other genera-
tive models (Henderson, 2003; Titov and Hender-
son, 2010; Buys and Blunsom, 2015) and can be
viewed as a simplified version of the procedure
used in the generative top-down parsers of Roark
(2001) and Charniak (2010).
</p>
<p>In word-synchronous search, we augment the
beam state space, identifying beams by tuples
(|W |, |Aw|), where |W | is the number of words
that have been produced so far in the sentence, and
|Aw| is the number of structural actions that have
been taken since the last word was produced. Intu-
itively, we want candidates with the same |W | =
w to compete against each other. For a beam of
partial parses in the state (|W | = w, |Aw| = a),
we generate a beam of successors by taking all of
the next possible actions for each partial parse in
the beam. If the action is NT(X) or REDUCE,
we place the resulting partial parse in the beam
for state (|W | = w, |Aw| = a + 1); other-
wise, if the action is GEN, we place it in a list
for (|W | = w + 1, |Aw| = 0). After all par-
tial parses in the beam have been processed, we
check to see if there are a sufficient number of
partial parses that have produced the next word:
if the beam (|W | = w + 1, |Aw| = 0) con-
tains at least Kw partial parses (the word beam
size), we prune it to this size and continue search
using this beam. Otherwise, we continue build-
ing candidates for this word by pruning the beam
(|W | = w, |Aw| = a + 1) to size Ka (the action
beam size), and continuing search from there.
</p>
<p>In practice, we found it to be most effective to
use a value for Kw that is a fraction of the value
for Ka. In all the experiments we present here,
we fix Ka = 10 × Kw, with Kw ranging from
10 to 100. Table 1 shows F1 for decoding in
both generative models on the development set,
using the top-scoring parse found for a sentence
when searching with the given beam size. RG
has comparatively larger gains in performance be-
tween the larger beam sizes, while still underper-
forming LM, suggesting that more search is nec-
essary in this model.
</p>
<p>3 Experiments
</p>
<p>Using the above decoding procedures, we attempt
to separate reranking effects from model combina-
tion effects through a set of reranking experiments.
Our base experiments are performed on the Penn
Treebank (Marcus et al., 1993), using sections
2-21 for training, section 22 for development, and
section 23 for testing. For the LSTM generative
model (LM), we use the pre-trained model re-
leased by Choe and Charniak (2016). We train
RNNG discriminative (RD) and generative (RG)
models, following Dyer et al. (2016) by using
the same hyperparameter settings, and using pre-
trained word embeddings from Ling et al. (2015)
for the discriminative model. The automatically-
predicted part-of-speech tags we use as input for
RD are the same as those used by Cross and Huang
(2016).
</p>
<p>In each experiment, we obtain a set of candi-
date parses for each sentence by performing beam
search in one or more parsers. We use action-
synchronous beam search (Section 2.1) with beam
size K = 100 for RD and word-synchronous
beam (Section 2.2) with Kw = 100 and Ka =
1000 for the generative models RG and LM.
</p>
<p>In the case that we are using only the scores
from a single generative model to rescore can-
didates taken from the discriminative parser, this
setup is close to the reranking procedures origi-
nally proposed for these generative models. For
RG, the original work also used RD to produce
candidates, but drew samples from it, whereas
we use a beam search to approximate its k-best
list. The LM generative model was originally used
to rerank a 50-best list taken from the Charniak
parser (Charniak, 2000). In comparison, we found
higher performance for the LM model when using
a candidate list from the RD parser: 93.66 F1 ver-
sus 92.79 F1 on the development data. This may
be attributable to having a stronger set of candi-
dates: with beam size 100, RD has an oracle F1
of 98.2, compared to 95.9 for the 50-best list from
the Charniak parser.
</p>
<p>3.1 Augmenting the candidate set
</p>
<p>We first experiment with combining the candidate
lists from multiple models, which allows us to
look for potential model errors and model com-
bination effects. Consider the standard rerank-
ing setup B→ A, where we search in B to get
a set of candidate parses for each sentence, and
</p>
<p>163</p>
<p />
</div>
<div class="page"><p />
<p>Scoring models
Candidates RD RG RD + RG
</p>
<p>RD 92.22 93.45 93.87
RG 90.24 89.55 90.53
</p>
<p>RD ∪ RG 92.22 92.78 93.92
Scoring models
</p>
<p>Candidates RD LM RD + LM
RD 92.22 93.66 93.99
LM 92.57 92.20 93.07
</p>
<p>RD ∪ LM 92.24 93.47 94.15
Table 2: Development F1 scores on section 22 of the PTB
when using various models to produce candidates and to
score them. ∪ denotes taking the union of candidates from
each of two models; + denotes using a weighted average of
the models’ log-probabilities.
</p>
<p>choose the top scoring candidate from these under
A. We extend this by also searching directly in A
to find high-scoring candidates for each sentence,
and combining them with the candidate list pro-
posed by B by taking the union, A ∪ B. We then
choose the highest scoring candidate from this list
under A. If A generally prefers parses outside of
the candidate list from B, but these decrease eval-
uation performance (i.e., if B ∪ A→ A is worse
than B→ A), this suggests a model combination
effect is occurring: A makes errors which are hid-
den by having a limited candidate list from B.
</p>
<p>This does seem to be the case for both genera-
tive models, as shown in Table 2, which presents
F1 scores on the development set when varying
the models used to produce the candidates and
to score them. Each row is a different candidate
set, where the third row in each table presents re-
sults for the augmented candidate sets; each col-
umn is a different scoring model, where the third
column is the score combination setting described
below. Going from RD→ RG to the augmented
candidate setting RD ∪ RG→ RG decreases per-
formance from 93.45 F1 to 92.78 F1 on the devel-
opment set. This difference is statistically signifi-
cant at the p &lt; 0.05 level under a paired bootstrap
test. We see a smaller, but still significant, effect in
the case of LM: RD→ LM achieves 93.66, com-
pared to 93.47 for RD ∪ LM→ LM.
</p>
<p>We can also consider the performance of
RG→ RG and LM→ LM (where we do not use
candidates from RD at all, but return the highest-
scoring parse from searching directly in one of
the generative models) as an indicator of rerank-
ing effects: absolute performance is higher for LM
(92.20 F1) than for RG (89.55). Taken together,
</p>
<p>these results suggest that model combination con-
tributes to the success of both models, but to a
larger extent for RG. A reranking effect may be
a larger contributor to the success of LM, as this
model achieves stronger performance on its own
for the described search setting.
</p>
<p>3.2 Score combination
</p>
<p>If the cross-scoring setup exhibits an implicit
model combination effect, where strong perfor-
mance results from searching in one model and
scoring with the other, we might expect substan-
tial further improvements in performance by ex-
plicitly combining the scores of both models. To
do so, we score each parse by taking a weighted
sum of the log-probabilities assigned by both mod-
els (Hayashi et al., 2013), using an interpolation
parameter which we tune to maximize F1 on the
development set.
</p>
<p>These results are given in columns RD + RG
and RD + LM in Table 2. We find that com-
bining the scores of both models improves on
using the score of either model alone, regard-
less of the source of candidates. These im-
provements are statistically significant in all cases.
Score combination also more than compensates
for the decrease in performance we saw previ-
ously when adding in candidates from the gen-
erative model: RD ∪ RG→ RD + RG improves
upon both RD→ RG and RD ∪ RG→ RG, and
the same effect holds for LM.
</p>
<p>3.3 Strengthening model combination
</p>
<p>Given the success of model combination between
the base model and a single generative model, we
also investigate the hypothesis that the generative
models are complementary. The Model Combina-
tion block of Table 3 shows full results on the test
set for these experiments, in the PTB column. The
same trends we observed on the development data,
on which the interpolation parameters were tuned,
hold here: score combination improves results for
all models (row 3 vs. row 2; row 6 vs. row 5), with
candidate augmentation from the generative mod-
els giving a further increase (rows 4 and 7).2 Com-
bining candidates and scores from all three models
(row 9), we obtain 93.94 F1.
</p>
<p>2These increases, from adding score combination and
candidate augmentation, are all significant with p &lt; 0.05
in the PTB setting. In the +S data setting, all are significant
except for the difference between row 5 and row 6.
</p>
<p>164</p>
<p />
</div>
<div class="page"><p />
<p>Model PTB +S
Liu and Zhang (2017) 91.7 –
Dyer et al. (2016)-discriminative 91.7 –
Dyer et al. (2016)-generative 93.3 –
Choe and Charniak (2016) 92.6 93.8
Model Combination
1) RD→ RD 91.51 91.73
2) RD→ RG 92.73 93.29
3) RD→ RD + RG 93.27 93.64
4) RD ∪ RG→ RD + RG 93.45 93.75
5) RD→ LM 93.31 94.18
6) RD→ RD + LM 93.71 94.27
7) RD ∪ LM→ RD + LM 93.89 94.63
8) RD→ RD + RG + LM 93.63 94.33
9) RD ∪ RG ∪ LM→ RD + RG + LM 93.94 94.66
Ensembling
10) RD (8)→ RD (8) 92.72 92.53
11) RD (8)→ RD (8) + RG (8) 94.09 94.22
12) RD (8)→ RD (8) + LM 93.97 94.56
13) RD (8)→ RD (8) + RG (8) + LM 94.25 94.62
</p>
<p>Table 3: Test F1 scores on section 23 of the PTB, by tree-
bank training data conditions: either using only the training
sections of the PTB, or using additional silver data (+S).
</p>
<p>Semi-supervised silver data Choe and Char-
niak (2016) found a substantial increase in per-
formance by training on external data in addi-
tion to trees from the Penn Treebank. This silver
dataset was obtained by parsing the entire New
York Times section of the fifth Gigaword corpus
using a product of eight Berkeley parsers (Petrov,
2010) and ZPar (Zhu et al., 2013), then retain-
ing 24 million sentences on which both parsers
agreed. For our experiments we train RD and RG
using the same silver dataset.3 The +S column
in Table 3 shows these results, where we observe
gains over the PTB models in nearly every case.
As in the PTB training data setting, using all mod-
els for candidates and score combinations is best,
achieving 94.66 F1 (row 9).
</p>
<p>Ensembling Finally, we compare to another
commonly used model combination method: en-
sembling multiple instances of the same model
type trained from different random initializations.
We train ensembles of 8 copies each of RD and
RG in both the PTB and silver data settings, com-
bining scores from models within an ensemble by
</p>
<p>3When training with silver data, we use a 1-to-1 ratio of
silver data updates per gold data updates, which we found to
give significantly faster convergence times on development
set perplexity for RD and RG compared to the 10-to-1 ratio
used by Choe and Charniak (2016) for LM.
</p>
<p>averaging the models’ distributions for each action
(in beam search as well as rescoring). These re-
sults are shown in the bottom section, Ensembling,
of Table 3.
</p>
<p>Performance when using only the ensembled
RD models (row 10) is lower than rescoring a
single RD model with score combinations of sin-
gle models, either RD + RG (row 3) or RD + LM
(row 6). In the PTB setting, ensembling with
score combination achieves the best overall result
of 94.25 (row 13). In the silver training data set-
ting, while this does improve on the analogous
unensembled result (row 8), it is not better than
the combination of single models when candi-
dates from the generative models are also included
(row 9).
</p>
<p>4 Discussion
</p>
<p>Searching directly in the generative models yields
results that are partly surprising, as it reveals the
presence of parses which the generative models
prefer, but which lead to lower performance than
the candidates proposed by the base model. How-
ever, the results are also unsurprising in the sense
that explicitly combining scores allows the rerank-
ing setup to achieve better performance than im-
plicit combination, which uses only the scores of a
single model. Additionally, we see support for the
hypothesis that the generative models can achieve
good results on their own, with the LSTM gener-
ative model showing particularly strong and self-
contained performance.
</p>
<p>While this search procedure allows us to explore
these generative models, disentangling reranking
and model combination effects, the increase in
performance from augmenting the candidate lists
with the results of the search may not be worth the
required computational cost in a practical parser.
However, we do obtain a gain over state-of-the-
art results using simple model score combination
on only the base candidates, which can be imple-
mented with minimal cost over the basic reranking
setup. This provides a concrete improvement for
these particular generative reranking procedures
for parsing. More generally, it supports the idea
that hybrid systems, which rely on one model to
produce a set of candidates and another to deter-
mine which candidates are good, should explore
combining their scores and candidates when pos-
sible.
</p>
<p>165</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgments
</p>
<p>We would like to thank Adhiguna Kuncoro and
Do Kook Choe for their help providing data and
answering questions about their work, as well as
Jacob Andreas, John DeNero, and the anonymous
reviewers for their suggestions. DF is supported
by an NDSEG fellowship. MS is supported by an
NSF Graduate Research Fellowship.
</p>
<p>References
Jan Buys and Phil Blunsom. 2015. Generative incre-
</p>
<p>mental dependency parsing with neural networks. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics.
</p>
<p>Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics.
</p>
<p>Eugene Charniak. 2010. Top-down nearly-context-
sensitive parsing. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
</p>
<p>Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
</p>
<p>James Cross and Liang Huang. 2016. Span-based con-
stituency parsing with a structure-label system and
provably optimal dynamic oracles. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A Smith. 2016. Recurrent neural network
grammars. In Proceedings of the Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics.
</p>
<p>Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-
sumoto. 2013. Efficient stacked dependency parsing
by forest reranking. Transactions of the Association
for Computational Linguistics 1:139–150.
</p>
<p>James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
</p>
<p>Wang Ling, Chris Dyer, Alan Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics.
</p>
<p>Jiangming Liu and Yue Zhang. 2017. Shift-reduce
constituent parsing with neural lookahead features.
Transactions of the Association for Computational
Linguistics 5:45–58.
</p>
<p>Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics 19(2):313–330.
</p>
<p>Slav Petrov. 2010. Products of random latent vari-
able grammars. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
</p>
<p>Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics
27(2):249–276.
</p>
<p>Ivan Titov and James Henderson. 2010. A latent vari-
able model for generative dependency parsing. In
Trends in Parsing Technology, Springer, pages 35–
55.
</p>
<p>Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems. pages 2773–2781.
</p>
<p>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics.
</p>
<p>166</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 167–171
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2026
</p>
<p>Information-Theory Interpretation of the
Skip-Gram Negative-Sampling Objective Function
</p>
<p>Oren Melamud
IBM Research
</p>
<p>Yorktown Heights, NY, USA
oren.melamud@ibm.com
</p>
<p>Jacob Goldberger
Faculty of Engineering
</p>
<p>Bar-Ilan University, Israel
jacob.goldberger@biu.ac.il
</p>
<p>Abstract
</p>
<p>In this paper, we define a measure of de-
pendency between two random variables,
based on the Jensen-Shannon (JS) diver-
gence between their joint distribution and
the product of their marginal distributions.
Then, we show that word2vec’s skip-gram
with negative sampling embedding algo-
rithm finds the optimal low-dimensional
approximation of this JS dependency mea-
sure between the words and their contexts.
The gap between the optimal score and the
low-dimensional approximation is demon-
strated on a standard text corpus.
</p>
<p>1 Introduction
</p>
<p>Continuous word representations, derived from un-
labeled text, have proven useful in many NLP tasks.
Such word representations (or embeddings) asso-
ciate a low-dimensional, real-valued vector with
each word, typically induced via neural language
models or matrix factorization.
</p>
<p>Substantial benefit arises when embeddings can
be efficiently trained on large volumes of data.
Hence the recent considerable interest in the con-
tinuous bag-of-words (CBOW) and skip-gram with
negative sampling (SGNS) models, described in
(Mikolov et al., 2013), as implemented in the open-
source toolkit word2vec. These models are based
on a relatively simple log-linear method and avoid
hidden layers typical to neural networks. Conse-
quently, they can be trained to produce high-quality
word embeddings on large corpora like the entirety
of English Wikipedia in several hours, compared to
days or even weeks in the case of other continuous
models. Recent studies obtained state-of-the-art
results by using skip-gram embeddings on a va-
riety of natural language processing tasks, such
as named entity extraction (Passos et al., 2014)
</p>
<p>and dependency parsing (Bansal et al., 2014). In
recent years, there were several attempts to mathe-
matically interpret word embedding models (Arora
et al., 2016; Pennington et al., 2014; Stratos et al.,
2015). Our study pursues this established line of
work, attempting to explain the objective function
of the SGNS word embedding algorithm.
</p>
<p>In the SGNS model, the energy function takes
the form of a dot product between the vectors of an
observed word and an observed context. The objec-
tive function is a binary logistic regression classifier
that treats a word and its observed context as a pos-
itive example, and a word and a randomly sampled
context as a negative example. Levy and Goldberg
(2014) offered a motivation for this function by
showing that it obtains its global maximum value
at the word-context pointwise mutual information
(PMI) matrix. In this study, we take their analy-
sis one step further and provide an information-
theoretical interpretation of the SGNS objective
function. In Section 2, we define a new measure
of mutual information between random variables
based the Jensen-Shennon divergence (Lin, 1991)
instead of the KL divergence. In Section 3, we
show that the value of the SGNS objective com-
puted at the PMI matrix is this information measure.
We then derive an explicit expression for the infor-
mation loss caused by the low-dimensional embed-
ding learned by the SGNS algorithm. Finally, in
Section 4, we illustrate this by computing the infor-
mation loss caused by actual SGNS embeddings
learned on a standard text corpus.
</p>
<p>2 A Dependency Measure based on
Jensen-Shannon
</p>
<p>In this section, we define a dependency measure be-
tween two random variables, which is based on the
Jensen-Shannon divergence. Later, in Section 3, we
show how it relates to the SGNS objective function.
</p>
<p>167</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2026">https://doi.org/10.18653/v1/P17-2026</a></div>
</div>
<div class="page"><p />
<p>There are several standard methods of measuring
the distance between two discrete probability dis-
tributions, defined on a given finite set A. The
Kullback-Leibler (KL) divergence of a distribu-
tion p from a distribution q is defined as follows:
KL(p||q) =∑i∈A pi log piqi . The mutual informa-
tion between two jointly distributed random vari-
ables X and Y is defined as the KL divergence
of the joint distribution p(x, y) from the product
p(x)p(y) of the marginal distributions of X and Y,
i.e. I(X;Y ) = KL(p(x, y)||p(x)p(y)).
</p>
<p>The Jensen-Shannon (JS) divergence (Lin, 1991)
between distributions p and q is:
</p>
<p>JSα(p, q) =αKL(p||r) + (1−α)KL(q||r) (1)
</p>
<p>= H(r)− αH(p)− (1−α)H(q)
such that 0 &lt; α &lt; 1, r = αp+ (1− α)q and H is
the entropy function (i.e. H(p) = −∑i pi log pi).
Unlike KL divergence, JS divergence is bounded
from above and 0 ≤ JSα(p, q) ≤ 1.
</p>
<p>We next propose a new measure for mutual-
information using the JS-divergence between
p(x, y) and p(x)p(y) instead of the KL-divergence.
We define the Jensen-Shannon Mutual information
(JSMI) as follows:
</p>
<p>JSMIα(X,Y ) = JSα(p(x, y), p(x)p(y)). (2)
</p>
<p>It can be easily verified that X and Y are indepen-
dent if and only if JSMIα(X,Y ) = 0.
</p>
<p>We next derive an alternative definition of the
JSMI dependency measure. Assume we choose be-
tween the two distributions, p(x, y) and the product
of marginal distributions p(x)p(y), according to a
binary random variableZ, such that p(Z = 1) = α.
We first sample a binary value for Z and next, we
sample a r.v. W as follows:
</p>
<p>p(W =(x, y)|Z)=
{
p(x)p(y) if Z=0
p(x, y) if Z=1.
</p>
<p>(3)
The divergence measure JSMIα(X,Y ) can be al-
ternatively defined in terms of mutual information
between W and Z. The mutual-information be-
tween W and Z is:
</p>
<p>I(W;Z) = H(W )−
∑
</p>
<p>i=0,1
</p>
<p>p(Z= i)H(W |Z= i)
</p>
<p>= H(αp(x, y) + (1−α)p(x)p(y))
</p>
<p>−αH(p(x, y))− (1−α)H(p(x)p(y)).
</p>
<p>Eq. (1) thus implies that:
</p>
<p>JSMIα(X,Y ) = I(W ;Z). (4)
</p>
<p>Applying Bayes rule we obtain:
</p>
<p>p(Z=1|W =(x, y)) (5)
</p>
<p>=
αp(x, y)
</p>
<p>αp(x, y) + (1−α)p(x)p(y)
</p>
<p>=
1
</p>
<p>1 + exp(− log( αp(x,y)(1−α)p(x)p(y)))
= σ(pmix,y)
</p>
<p>such that σ(u) = 11+exp(−u) is the sigmoid func-
tion and
</p>
<p>pmix,y = log
p(x, y)
</p>
<p>p(x)p(y)
+ log
</p>
<p>α
</p>
<p>1−α (6)
</p>
<p>is a shifted version of the PMI function. Equa-
tions (4) and (5) imply that:
</p>
<p>JSMIα(X,Y ) = H(Z)−H(Z|W ) (7)
</p>
<p>= h(α)+α
∑
</p>
<p>x,y
</p>
<p>p(x, y) log σ(pmix,y)
</p>
<p>+(1−α)
∑
</p>
<p>x,y
</p>
<p>p(x)p(y) log σ(−pmix,y)
</p>
<p>such that h(α) = −α log(α) − (1−α) log(1−α)
is the binary entropy function.
</p>
<p>3 The Skip-Gram Embedding Algorithm
</p>
<p>The SGNS embedding algorithm (Mikolov et al.,
2013) represents each word x and each context y
as d-dimensional vectors ~x and ~y, with the purpose
that words that are “similar” to each other will have
similar vector representations. We can represent
a given d-dimensional embedding by a matrix m,
such that m(x, y) = ~x · ~y. The rank of the embed-
ding matrix m is (at most) d.
</p>
<p>Let p(x, y) be the normalized number of co-
occurrences of word x and context-word y in a
given corpus and let p(x) and p(y) be the corre-
sponding unigram distributions. Consider a binary
classifier that treats a word and its observed con-
text as a positive example, and a word and a ran-
domly sampled context as a negative example. The
classification is made based on the embedding in
such a way that the probability that (x, y) is a pos-
itive example is σ(~x · ~y). The objective function
ideally maximized by the SGNS word embedding
</p>
<p>168</p>
<p />
</div>
<div class="page"><p />
<p>algorithm is the expectation of the log-likelihood
function of the embedding:
</p>
<p>S(m) = h(
1
</p>
<p>k+1
) +
</p>
<p>1
</p>
<p>k+1
</p>
<p>∑
</p>
<p>x,y
</p>
<p>p(x, y) log σ(~x · ~y)
</p>
<p>+
k
</p>
<p>k+1
</p>
<p>∑
</p>
<p>x,y
</p>
<p>p(x)p(y) log σ(−~x · ~y).
</p>
<p>(8)
Note that the term h( 1k+1), which does not appear
in the original SGNS objective function (Mikolov
et al., 2013), is a constant number that was added
here to simplify the following presentation.
</p>
<p>The sparsity of p(x, y) (which is obtained as
normalized counts from a given learning corpus)
makes it feasible to compute the second term of (8).
The number of summed-over elements in the third
term of (8), however, is quadratic in the size of the
vocabulary, making it hard to compute. Therefore,
in practice, we can approximate the expectation by
sampling of ‘negative’ examples. The actual SGNS
score, then, is:
</p>
<p>S(m) ≈ h( 1
k+1
</p>
<p>) +
1
</p>
<p>k+1
· 1
n
</p>
<p>n∑
</p>
<p>t=1
</p>
<p>(log σ(~xt · ~yt)
</p>
<p>+
</p>
<p>k∑
</p>
<p>i=1
</p>
<p>log σ(−~xt · ~yti)).
</p>
<p>(9)
such that t goes over all the word-context pairs
in a given corpus. The negative examples yti are
created for each pair (xt, yt) by drawing k random
contexts from the context-word distribution p(y).
</p>
<p>As pointed out in (Levy et al., 2015), k has two
distinct functions in the SGNS objective function.
First, it is used to better estimate the distribution of
negative examples. Second, it is used as a weight
on the probability of observing a positive example
versus a negative example; a higher k means that
negative examples are more probable.
</p>
<p>We can compute the SGNS score function S(m)
for every real-valued matrix m = (mx,y). Levy
and Goldberg (2014) showed that the function
achieves its global maximal value when for each
word-pair (x, y) the inner product of the embed-
ding vectors ~x · ~y is equal to pmi(x, y). In other
words they showed that S(m) ≤ S(pmi) for every
matrix m. We next show that the value of the func-
tion S(m) at its maximum point, the PMI matrix,
has a concrete interpretation, namely it is exactly
the Jensen-Shannon Mutual Information (JSMI)
between words and their contexts.
</p>
<p>Theorem 1: The value of the SGNS score with k
negative samples (8) at the PMI matrix satisfies:
</p>
<p>S(pmi) = JSMIα(X,Y )
</p>
<p>such that α = 1k+1 .
Proof: It can be easily verified that by substituting
α = 1k+1 in the definition of JSMI (Eq. (7)), we ex-
actly obtain the SGNS score (8) at the PMI matrix.
2
</p>
<p>Levy and Goldberg (2014) showed that SGNS’s
objective achieves its maximal value at the PMI ma-
trix. However, this result reveals nothing about the
more interesting lower dimensional case, where the
PMI matrix factorization is forced to compress the
joint distribution and thereby learn a meaningful
embedding. We next derive an explicit description
of the approximation criterion that quantifies the
gap between S(m) and S(pmi).
</p>
<p>Given the word co-occurrences joint distribution
p(x, y), we obtained in Eq. (5) a conditional distri-
bution on the alphabet of (Z,W ) as follows:
</p>
<p>p(Z=1|W =(x, y)) = σ(pmix,y).
</p>
<p>In a similar way, given any matrixm, we can define
a conditional distribution pm on the alphabet of
(Z,W ) as follows:
</p>
<p>pm(Z=1|W =(x, y)) = σ(mx,y).
</p>
<p>Note that in the special case where m is the PMI
matrix, ppmi(z|w) coincides with the original
p(z|w) that was defined in Eq. (5).
Theorem 2: The difference between the SGNS
score at the PMI matrix and the SGNS score at a
given matrix m can be written as:
</p>
<p>S(pmi)− S(m) = KL(ppmi(Z|W )||pm(Z|W ))
(10)
</p>
<p>Proof:
</p>
<p>S(pmi)−S(m) =
∑
</p>
<p>x,y
</p>
<p>(αp(x, y) log
σ(pmix,y)
σ(mx,y)
</p>
<p>+(1−α)p(x)p(y) log
σ(−pmix,y)
σ(−mx,y)
</p>
<p>)
</p>
<p>=
∑
</p>
<p>x,y
</p>
<p>(αp(x, y) log
ppmi(Z=1|x, y)
pm(Z=1|x, y)
</p>
<p>+(1−α)p(x)p(y) log
ppmi(Z=0|x, y)
pm(Z=0|x, y)
</p>
<p>)
</p>
<p>169</p>
<p />
</div>
<div class="page"><p />
<p>=
∑
</p>
<p>w,z
</p>
<p>p(W =w,Z=z) log
ppmi(Z=z|W =w)
pm(Z=z|W =w)
</p>
<p>= KL(ppmi(Z|W )||pm(Z|W )).2
The KL divergence between two distributions is
always non-negative and is zero only if the two
distributions are the same. Therefore, we red-
erive the results of (Levy and Goldberg, 2014)
that S(pmi) = maxm S(m). Theorem 2 can be
viewed as an instance of the well-known connec-
tion between maximizing log-likelihood and mini-
mizing KL divergence between the estimated and
the true data-generating distribution. In this case,
the true distribution is the pmi-based classifier
ppmi(Z|W ).
</p>
<p>Combining theorems 1 and 2 we obtain that
S(m) ≤ JSMIα(X,Y ) for every low-dimensional
embedding matrix. The difference JSMIα(X,Y )−
S(m) is the information loss caused by the low-
dimensional embedding. We can view it as a
Jensen-Shannon variant of the information bottle-
neck principle (Tishby et al., 1999; Globerson et al.,
2007) that is defined in terms of the KL divergence.
The optimal d-dimensional embedding, is the best
d-dimensional approximation of the JSMI depen-
dency measure in the sense that it minimizes the
information loss. The JSMI is the upper bound
that any embedding can obtain. To illustrate that,
in the next section we compute the JSMI between
words and their contexts based on a standard text
corpus and show the information gap between the
JSMI and the actual SGNS score as a function of
the embedding dimension d.
</p>
<p>From Theorem 2 we can also derive an explicit
information-theoretic interpretation of the score
function S(m) (7) as the difference between two
KL-divergence terms:
</p>
<p>S(m) = S(pmi)− (S(pmi)− S(m)) =
</p>
<p>I(Z;W )− (S(pmi)− S(m)) =
KL(p(Z|W )||p(Z))− KL(p(Z|W )||pm(Z|W ))
</p>
<p>The word embedding problem can be also
viewed as a factorization of the PMI matrix. Previ-
ous works suggested other criteria for matrix fac-
torization such as least-squares (Eckart and Young,
1936) and KL-divergence between the original ma-
trix and the low-rank matrix approximation (Lee
and Seung, 2000). We have shown that the SGNS
algorithm factorizes the PMI matrix based on the
JSMI-based criterion stated in Eq. (10).
</p>
<p>Figure 1: SGNS objective function score of trained
embeddings models, compared to the optimal PMI-
based score. dim and iter denote the dimensionality
and training iterations used for each model.
</p>
<p>4 Experiments
</p>
<p>In this section we use word2vec to train real skip-
gram with negative sampling (SGNS) embedding
models. By measuring the value of their objec-
tive function and comparing it against the optimal
one using exact PMI values, we demonstrate how
a well-trained model minimizes the difference in
Eq. (10). We note that this is an intrinsic measure
that does not necessarily reflect the usefulness of
the learned embeddings for other tasks.
</p>
<p>We used the Penn Tree Bank (PTB), a popu-
lar small-scale corpus, for our experiments. A
version of this dataset is available from Tomas
Mikolov.1 It consists of 929K training words with
a 10K word vocabulary, which we used to train our
models. To learn the SGNS word embeddings, we
used word2vec’s default parameter values: window-
size = 5, min-count = 5, and number of negative
samples k = 5. We varied the dimensionality of
the embeddings and the number of training itera-
tions performed. Once the models were trained, we
measured their score (9) on the training corpus.
</p>
<p>Based on the same learning corpus, we computed
S(pmi) = JSMIα(X,Y ) for α = 1k+1 = 1/6.
Note that p(x, y) = 0 implies that pmix,y = −∞
and therefore log σ(−pmix,y) = 0. Hence, as in
the second term, to compute the third term of S(m)
(8) for the case of m = pmi, we can sum only
</p>
<p>1http://www.fit.vutbr.cz/~imikolov/
rnnlm/simple-examples.tgz
</p>
<p>170</p>
<p />
</div>
<div class="page"><p />
<p>over the positive pairs (x, y) that actually appear
in the corpus.2 In other words, for the special
case m = pmi, it is feasible to compute the ex-
act score (8) and not just its approximation (9) that
is based on negative sampling. Figure 1 illustrates
the optimal PMI-based score, compared with the
scores obtained by different models with varied
embedding dimensionality and number of training
iterations. As can be seen, the embeddings score
gets close to the optimal value using higher dimen-
sionality and more training iterations, but doesn’t
surpass it.
</p>
<p>5 Conclusion
</p>
<p>In this study, we developed a new correlation mea-
sure between random variables, denoted JSMI.
This measure is based on the JS divergence and dif-
fers from the standard mutual information measure
that is based on the KL divergence. We showed that
the optimization of skip-gram embeddings with
negative sampling finds the best low-dimensional
approximation of the JSMI measure. Thus, we pro-
vided an information theory framework that hope-
fully contributes to a better understanding of this
embedding algorithm. Furthermore, although we
focused here on the case of word-context joint dis-
tributions, the connection we haven shown between
the PMI matrix and the JSMI function is valid for
every joint distribution of two random variables.
</p>
<p>Acknowledgments
</p>
<p>This work is supported by the Intel Collaborative
Research Institute for Computational Intelligence
(ICRI-CI).
</p>
<p>References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
</p>
<p>and Andrej Risteski. 2016. A latent variable model
approach to pmi-based word embeddings. Transac-
tions of the Association for Computational Linguis-
tics 4:385–399.
</p>
<p>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring continuous word representations for depen-
dency parsing. In Association for Computational
Linguistics (ACL).
</p>
<p>Carl Eckart and Gale Young. 1936. The approximation
of one matrix by another of lower rank. Psychome-
trika 1:211–218.
2We used the exact same positive co-occurrence pairs sam-
</p>
<p>pled by word2vec during the training of the SGNS embeddings
to compute S(pmi).
</p>
<p>Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftaly Tishby. 2007. Euclidean embedding of co-
occurrence data. Journal of Machine Learning Re-
search 8:2265–2295.
</p>
<p>Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for nonnegative matrix factorization. In Ad-
vances in Neural Information Processing Systems.
</p>
<p>Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems.
</p>
<p>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Trans. of the Association
for Computational Linguistics 3:211–225.
</p>
<p>Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory 37(1):145–151.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems.
</p>
<p>Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. In Conference on Natural
Language Learning (CoNLL).
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.
</p>
<p>Karl Stratos, Michael Collins, and Daniel J Hsu. 2015.
Model-based word embeddings from decomposi-
tions of count matrices. In ACL (1). pages 1282–
1291.
</p>
<p>Naftaly Tishby, Fernando C. Pereira, and William
Bialek. 1999. The information bottleneck method.
In Allerton Conf. on Communication, Control, and
Computing.
</p>
<p>171</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 172–177
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2027
</p>
<p>Implicitly-Defined Neural Networks for Sequence Labeling ∗
</p>
<p>Michaeel Kazi, Brian Thompson
MIT Lincoln Laboratory
</p>
<p>244 Wood St, Lexington, MA, 02420, USA
{first.last}@ll.mit.edu
</p>
<p>Abstract
</p>
<p>In this work, we propose a novel,
implicitly-defined neural network archi-
tecture and describe a method to compute
its components. The proposed architec-
ture forgoes the causality assumption used
to formulate recurrent neural networks and
instead couples the hidden states of the
network, allowing improvement on prob-
lems with complex, long-distance depen-
dencies. Initial experiments demonstrate
the new architecture outperforms both the
Stanford Parser and baseline bidirectional
networks on the Penn Treebank Part-of-
Speech tagging task and a baseline bidi-
rectional network on an additional artifi-
cial random biased walk task.
</p>
<p>1 Introduction
</p>
<p>Feedforward neural networks were designed to ap-
proximate and interpolate functions. Recurrent
Neural Networks (RNNs) were developed to pre-
dict sequences. RNNs can be ‘unwrapped’ and
thought of as very deep feedforward networks,
with weights shared between each layer. Com-
putation proceeds one step at a time, like the tra-
jectory of an ordinary differential equation when
solving an initial value problem. The path of an
initial value problem depends only on the current
state and the current value of the forcing func-
tion. In a RNN, the analogy is the current hidden
state and the current input sequence. However, in
certain applications in natural language process-
ing, especially those with long-distance dependen-
cies or where grammar matters, sequence predic-
</p>
<p>∗This work is sponsored by the Air Force Research Lab-
oratory under Air Force contract FA-8721-05-C-0002. Opin-
ions, interpretations, conclusions and recommendations are
those of the authors and are not necessarily endorsed by the
United States Government.
</p>
<p>tion may be better thought of as a boundary value
problem. Changing the value of the forcing func-
tion (analogously, of an input sequence element)
at any point in the sequence will affect the val-
ues everywhere else. The bidirectional recurrent
network (Schuster and Paliwal, 1997) attempts to
addresses this problem by creating a network with
two recurrent hidden states – one that progresses
in the forward direction and one that progresses
in the reverse. This allows information to flow in
both directions, but each state can only consider
information from one direction. In practice many
algorithms require more than two passes through
the data to determine an answer. We provide a
novel mechanism that is able to process informa-
tion in both directions, with the motivation being
a program which iterates over itself until conver-
gence.
</p>
<p>1.1 Related Work
</p>
<p>Bidirectional, long-distance dependencies in se-
quences have been an issue as long as there have
been NLP tasks, and there are many approaches to
dealing with them.
</p>
<p>Hidden Markov models (HMMs) (Rabiner,
1989) have been used extensively for sequence-
based tasks, but they rely on the Markov assump-
tion – that a hidden variable changes its state
based only on its current state and observables.
In finding maximum likelihood state sequences,
the Forward-Backward algorithm can take into ac-
count the entire set of observables, but the under-
lying model is still local.
</p>
<p>In recent years, popularity of the Long Short-
Term Memory (LSTM) (Hochreiter and Schmid-
huber, 1997) and variants such as the Gated Recur-
rent Unit (GRU) (Cho et al., 2014) has soared, as
they enable RNNs to process long sequences with-
out the problem of vanishing or exploding gradi-
ents (Pascanu et al., 2013). However, these models
</p>
<p>172</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2027">https://doi.org/10.18653/v1/P17-2027</a></div>
</div>
<div class="page"><p />
<p>only allow for information/gradient information to
flow in the forward direction.
</p>
<p>The Bidirectional LSTM (b-LSTM) (Graves
and Schmidhuber, 2005), a natural extension of
(Schuster and Paliwal, 1997), incorporates past
and future hidden states via two separate recurrent
networks, allowing information/gradients to flow
in both directions of a sequence. This is a very
loose coupling, however.
</p>
<p>In contrast to these methods, our work goes a
step further, fully coupling the entire sequences of
hidden states of an RNN. Our work is similar to
(Finkel et al., 2005), which augments a CRF with
long-distance constraints. However, our work dif-
fers in that we extend an RNN and uses Netwon-
Krylov (Knoll and Keyes, 2004) instead of Gibbs
Sampling.
</p>
<p>2 The Implicit Neural Network (INN)
</p>
<p>2.1 Traditional Recurrent Neural Networks
</p>
<p>A typical recurrent neural network has a (pos-
sibly transformed) input sequence [ξ1, ξ2, . . . , ξn]
and initial state hs and iteratively produces future
states:
</p>
<p>h1 = f(ξ1, hs)
h2 = f(ξ2, h1)
. . .
hn = f(ξn, hn−1)
</p>
<p>The LSTM, GRU, and related variants follow
this formula, with different choices for the state
transition function. Computation proceeds lin-
early, with each next state depending only on in-
puts and previously computed hidden states.
</p>
<p>Figure 1: Traditional RNN structure.
</p>
<p>2.2 Proposed Architecture
</p>
<p>In this work, we relax this assumption by allowing
ht = f(ξt, ht−1, ht+1)1. This leads to an implicit
set of equations for the entire sequence of hidden
states, which can be thought of as a single tensor
</p>
<p>1A wider stencil can also be used, e.g. f(ht−2, ht−1, . . .).
</p>
<p>H:
H = [h1, h2, . . . , hn]
</p>
<p>This yields a system of nonlinear equations. This
setup has the potential to arrive at nonlocal, whole
sequence-dependent results. We also hope such a
system is more ‘stable’, in the sense that the pre-
dicted sequence may drift less from the true mean-
ing, since errors will not compound with each time
step in the same way.
</p>
<p>There are many potential ways to architect a
neural network – in fact, this flexibility is one of
deep learning’s best features – but we restrict our
discussion to the structure depicted in Figure 2. In
this setup, we have the following variables:
</p>
<p>data X
labels Y
parameters θ
</p>
<p>and functions:
</p>
<p>input layer transformation ξ = g(θ,X)
implicit hidden layer def. H = F (θ, ξ,H)
loss function L = `(θ,H, Y )
</p>
<p>Our implicit definition function, F , is made up
of local state transitions and forms a system of
nonlinear equations that require solving, denoting
n as the length of the input sequence and hs, he as
boundary states:
</p>
<p>h1 = f(hs, h2, ξ1)
. . .
hi = f(hi−1, hi+1, ξi)
. . .
hn = f(hn−1, he, ξn)
</p>
<p>Figure 2: Proposed INN Architecture
</p>
<p>173</p>
<p />
</div>
<div class="page"><p />
<p>2.3 Computing the forward pass
</p>
<p>To evaluate the network, we must solve the equa-
tionH = F (H). We computed this via an approx-
imate Newton solve, where we successively refine
an approximation Hn of H:
</p>
<p>Hn+1 = Hn − (I −∇HF )−1(Hn − F (Hn))
</p>
<p>Let k be the dimension of a single hidden state.
(I −∇HF ) is a sparse matrix, since∇HF is zero
except for k pairs of n × n block matrices, cor-
responding to the influence of the left and right
neighbors of each state.
</p>
<p>Because of this sparsity, we can apply Krylov
subspace methods (Knoll and Keyes, 2004),
specifically the BiCG-STAB method (Van der
Vorst, 1992), since the system is non-symmetric.
This has the added advantage of only relying on
matrix-vector multiplies of the gradient of F .
</p>
<p>2.4 Gradients
</p>
<p>In order to train the model, we perform stochastic
gradient descent. We take the gradient of the loss
function:
</p>
<p>∇θL = ∇θ`+∇H`∇θH
</p>
<p>The gradient of the hidden units with respect to the
parameters can found via the implicit definition:
</p>
<p>∇θH = ∇θF +∇HF∇θH +∇ξF∇θξ
= (I −∇HF )−1 (∇θF +∇ξF∇θξ)
</p>
<p>where the factorization follows from the noting
that
</p>
<p>(I −∇HF )∇θH = ∇θF +∇ξF∇θξ.
</p>
<p>The entire gradient is thus:
</p>
<p>∇θL =∇H`(I −∇HF )−1 (∇θF +∇ξF∇θξ)
+∇θ`
</p>
<p>(1)
Once again, the inverse of I −∇HF appears, and
we can compute it via Krylov subspace methods.
It is worth mentioning the technique of computing
parameter updates by implicit differentiation and
conjugate gradients have been applied before, in
the context of energy minimization models in im-
age labeling and denoising (Domke, 2012).
</p>
<p>2.5 Transition Functions
</p>
<p>Recall the original GRU equations (Cho et al.,
2014), with slight notational modifications:
</p>
<p>final h ht = (1− zt)ĥt + zth̃t
candidate h h̃t = tanh(Wxt + U(rtĥt) + b̃)
update weight zt = σ(Wzxt + Uzĥt + bz)
reset gate rt = σ(Wrxt + Urĥt + br)
</p>
<p>We make the following substitution for ĥt
(which was set to ht−1 in the original GRU def-
inition):
</p>
<p>state comb. ĥt = sht−1 + (1− s)ht+1
switch s = spsp+sn
prev. switch sp = σ(Wpxt + Upht−1 + bp)
next switch sn = σ(Wnxt + Unht+1 + bn)
</p>
<p>(2)
This modification makes the architecture both
</p>
<p>implicit and bidirectional, since ĥt is a linear com-
bination of previous and future hidden states. The
switch variable s is determined by a competition
between two sigmoidal units sp and sn, represent-
ing the contributions of the previous and next hid-
den states, respectively.
</p>
<p>2.6 Implementation Details
</p>
<p>We implemented the implicit GRU structure us-
ing Theano (Bergstra et al., 2011). The product
∇HFv for various v, required for the BiCG-STAB
method, was computed via the Rop operator. In
computing ∇θL (Equation 1), we noted it is more
efficient to compute ∇H`(I −∇HF )−1 first, and
thus used the Lop operator.
</p>
<p>All experiments used a batch size of 20. To
batch solve the linear equations, we simply solved
a single, very large block diagonal system of equa-
tions: each sequence in the batch was a single
block matrix, and we input the encompassing ma-
trix into our Theano BiCG solver. (In practice the
block diagonal system is represented as a 3-tensor,
but it is equivalent.) In this setup, each step does
receive separate update directions, but one global
step length. hS and he were fixed at zero, but
could be trained as parameters.
</p>
<p>In solving multiple simultaneous systems of
equations, we noted some elements converged sig-
nificantly faster than others. For this reason, we
found it helpful to run Newton’s method from
two separate initializations for each element in our
batch, one selected randomly and the other set to a
</p>
<p>174</p>
<p />
</div>
<div class="page"><p />
<p>“one-step” approximation: Hidden states of a tra-
ditional GRU were computed in both forward (hfi )
and reverse (hbi ) directions, and hi was initialized
to f(hfi−1, h
</p>
<p>b
i+1, ξi). If either of the two candidates
</p>
<p>converged, we took its value and stopped comput-
ing the other. We also limited both the number
Newton iterations and BiCG-STAB iterations per
Newton iteration to 40.
</p>
<p>3 Experiments
</p>
<p>3.1 Biased random walks
</p>
<p>We developed an artificial task with bidirectional
sequence-level dependencies to explore the perfor-
mance of our model. Our task was to find the
point at which a random walk, in the spirit of
the Wiener Process (Durrett, 2010), changes from
a zero to nonzero mean. We trained a network
to predict when the walk is no longer unbiased.
We generated algorithmic data for this problem,
the specifics of which are as follows: First, we
chose an integer interval lengthN uniformly in the
range 1 to 40. Then, we chose a (continuous) time
t′ ∈ [0, N), and a direction v ∈ Rd. We produced
the input sequence xi ∈ Rd, setting x0 = 0 and
iteratively computing xi+1 = xi +N (0, 1). After
time t, a bias term of b · v was added at each time
step (b·v·(t′−t)) for the first time step greater than
t′. b is a global scalar parameter. The network was
fed in these elements, and asked to predict y = 0
for times t ≤ t′ and y = 1 for times t &gt; t′.
</p>
<p>For each architecture, ξ was simply the unmod-
ified input vectors, zero-padded to the embedding
dimension size. The output was a simple binary
logistic regression. We produced 50,000 random
training examples, 2500 random validation exam-
ples, and 5000 random test examples. The implicit
algorithm used a hidden dimension of 200, and
the b-LSTM had an embedding dimension rang-
ing from 100 to 1000. b-LSTM dimension of 300
was the point where the total number of parame-
ters were roughly equal.
</p>
<p>The results are shown in Table 1. The b-LSTM
scores reported are the maximum over sweeps
from 100 to 1500 hidden dimension size. The INN
outperforms the best b-LSTM in the more chal-
lenging cases where the bias size b is small.
</p>
<p>3.2 Part-of-speech tagging
</p>
<p>We next applied our model to a real-world prob-
lem. Part-of-speech tagging fits naturally in the se-
quence labeling framework, and has the advantage
</p>
<p>b INN Error b-LSTM Error
</p>
<p>2.0 0.0226 0.0210
1.0 0.0518 0.0589
0.75 0.0782 0.0879
0.5 0.119 0.132
0.25 0.189 0.205
</p>
<p>Table 1: Biased walk classification performance.
</p>
<p>of a standard dataset that we can use to compare
our network with other techniques. To train a part-
of-speech tagger, we simply let L be a softmax
layer transforming each hidden unit output into a
part of speech tag. Our input encoding ξ, is a con-
catenation of three sets of features, adapted from
(Huang et al., 2015): first, word vectors for 39,000
case-insensitive vocabulary words; second, six ad-
ditional ‘word vector’ components indicating the
presence of the top-2000 most common prefixes
and suffixes of words, for affix lengths 2 to 4; and
finally, eight other binary features to indicate the
presence of numbers, symbols, punctuation, and
more rich case data.
</p>
<p>We trained the Part of Speech (POS) tagger
on the Penn Treebank Wall Street Journal cor-
pus (Marcus et al., 1993), blocks 0-18, validated
on 19-21, and tested on 22-24, per convention.
Training was done using stochastic gradient de-
scent, with an initial learning rate of 0.5. The
learning rate was halved if validation perplexity
increased. Word vectors were of dimension 320,
prefix and suffix vectors were of dimension 20.
Hidden unit size was equal to feature input size,
so in this case, 448.
</p>
<p>As shown in Table 2, the INN outperformed
baseline GRU, bidirectional GRU, LSTM, and b-
LSTM networks, all with 628-dimensional hidden
layers (1256 for the bidirectional architectures),
The INN also outperforms the Stanford Part-of-
Speech tagger (Toutanova et al., 2003) (model
wsj-0-18-bidirectional-distsim.tagger
</p>
<p>from 10-31-2016). Note that performance gains
past approximately 97% are difficult due to er-
rors/inconsistencies in the dataset, ambiguity, and
complex linguistic constructions including depen-
dencies across sentence boundaries (Manning,
2011).
</p>
<p>175</p>
<p />
</div>
<div class="page"><p />
<p>Architecture WSJ Accuracy
</p>
<p>GRU 96.43
LSTM 96.47
Bidirectional GRU 97.28
b-LSTM 97.25
INN 97.37
Stanford POS Tagger 97.33
</p>
<p>Table 2: Tagging performance relative to recur-
rent architectures and Stanford POS Tagger.
</p>
<p>4 Time Complexity
</p>
<p>The implicit experiments in this paper took ap-
proximately 3-5 days to run on a single Tesla K40,
while the explicit experiments took approximately
1-3 hours. Running time of the solver is approx-
imately nn × nb × tb where nn is the number
of Newton iterations, nb is the number of BiCG-
STAB iterations, and tb is the time for a single
BiCG-STAB iteration. tb is proportional to the
number of non-zero entries in the matrix (Van der
Vorst, 1992), in our case n(2k2 + 1). New-
ton’s method has second order convergence (Isaac-
son and Keller, 1994), and while the specific
bound depends on the norm of (I −∇HF )−1 and
the norm of its derivatives, convergence is well-
behaved. For nb, however, we are not aware of
a bound. For symmetric matrices, the Conjugate
Gradient method is known to take O(
</p>
<p>√
κ) itera-
</p>
<p>tions (Shewchuk et al., 1994), where κ is the con-
dition number of the matrix. However, our matrix
is nonsymmetric, and we expect κ to vary from
problem to problem. Because of this, we empiri-
cally estimated the correlation between sequence
length and total time to compute a batch of 20 hid-
den layer states.
</p>
<p>For the random walk experiment with b = 0.5,
we found the the average run time for a given se-
quence length to be approximately 0.17n1.8, with
r2 = 0.994. Note that the exponent would have
been larger had we not truncated the number of
BiCG-STAB iterations to 40, as the inner itera-
tion frequently hit this limit for larger n. How-
ever, the average number of Newton iterations did
not go above 10, indicating that exiting early from
the BiCG-STAB loop did not prevent the New-
ton solver from converging. Run times for the
other random walk experiments were very similar,
indicating run time does not depend on b; How-
ever, for the POS task runtime was 0.29n1.3, with
</p>
<p>r2 = 0.910.
</p>
<p>5 Conclusion and Future Work
</p>
<p>We have introduced a novel, implicitly defined
neural network architecture based on the GRU
and shown that it outperforms a b-LSTM on an
artificial random walk task and slightly outper-
forms both the Stanford Parser and a baseline bidi-
rectional network on the Penn Treebank Part-of-
Speech tagging task.
</p>
<p>In future work, we intend to consider im-
plicit variations of other architectures, such as
the LSTM, as well as additional, more challeng-
ing, and/or data-rich applications. We also plan
to explore ways to speed up the computation of
(I−∇HF )−1. Potential speedups include approx-
imating the hidden state values by reducing the
number of Newton and/or BiCG-STAB iterations,
using cached previous solutions as initial values,
and modifying the gradient update strategy to keep
the batch full at every Newton iteration.
</p>
<p>6 Acknowledgements
</p>
<p>This work would not be possible without the sup-
port and funding of the Air Force Research Labo-
ratory. We also acknowledge Nick Malyska, Eliz-
abeth Salesky, and Jonathan Taylor at MIT Lin-
coln Lab for interesting technical discussions re-
lated to this work.
</p>
<p>Cleared for Public Release on 29 Jul 2016. Originator
reference number: RH-16-115722. Case Number: 88ABW-
2016-3809.
</p>
<p>176</p>
<p />
</div>
<div class="page"><p />
<p>References
James Bergstra, Frédéric Bastien, Olivier Breuleux,
</p>
<p>Pascal Lamblin, Razvan Pascanu, Olivier Delalleau,
Guillaume Desjardins, David Warde-Farley, Ian
Goodfellow, Arnaud Bergeron, et al. 2011. Theano:
Deep learning on gpus with python. In NIPS 2011,
BigLearning Workshop, Granada, Spain.
</p>
<p>Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. Syntax, Semantics and Structure in Statis-
tical Translation page 103.
</p>
<p>Justin Domke. 2012. Generic methods for
optimization-based modeling. In AISTATS.
volume 22, pages 318–326.
</p>
<p>Richard Durrett. 2010. Probability : theory and exam-
ples. Cambridge University Press, Cambridge New
York.
</p>
<p>Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
Association for Computational Linguistics, pages
363–370.
</p>
<p>Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works 18(5):602–610.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991 .
</p>
<p>Eugene Isaacson and Herbert Bishop Keller. 1994.
Analysis of numerical methods. Courier Corpora-
tion, New York.
</p>
<p>Dana A Knoll and David E Keyes. 2004. Jacobian-free
newton–krylov methods: a survey of approaches
and applications. Journal of Computational Physics
193(2):357–397.
</p>
<p>Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some lin-
guistics? In International Conference on Intelli-
gent Text Processing and Computational Linguistics.
Springer, pages 171–189.
</p>
<p>Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.
</p>
<p>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. ICML (3) 28:1310–1318.
</p>
<p>Lawrence R Rabiner. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. Proceedings of the IEEE 77(2):257–
286.
</p>
<p>Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on 45(11):2673–2681.
</p>
<p>Jonathan Richard Shewchuk et al. 1994. An introduc-
tion to the conjugate gradient method without the ag-
onizing pain.
</p>
<p>Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics, pages 173–180.
</p>
<p>Henk A Van der Vorst. 1992. Bi-cgstab: A fast and
smoothly converging variant of bi-cg for the solution
of nonsymmetric linear systems. SIAM Journal on
scientific and Statistical Computing 13(2):631–644.
</p>
<p>177</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 178–183
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2028
</p>
<p>The Role of Prosody and Speech Register in Word Segmentation: A
Computational Modelling Perspective
</p>
<p>Bogdan Ludusan
LSCP
</p>
<p>EHESS/ENS/PSL/CNRS
29 rue d’Ulm, 75005 Paris, France
bogdan.ludusan@ens.fr
</p>
<p>Reiko Mazuka
Language Development Lab
</p>
<p>RIKEN Brain Science Institute
2-1 Hirosawa, Wako, 351-0198, Japan
</p>
<p>mazuka@brain.riken.jp
</p>
<p>Mathieu Bernard
Alejandrina Cristia
Emmanuel Dupoux
</p>
<p>LSCP - EHESS/ENS/PSL/CNRS
29 rue d’Ulm, 75005 Paris, France
</p>
<p>{mmathieubernardd, alecristia,
emmanuel.dupoux}@gmail.com
</p>
<p>Abstract
</p>
<p>This study explores the role of speech reg-
ister and prosody for the task of word seg-
mentation. Since these two factors are
thought to play an important role in early
language acquisition, we aim to quan-
tify their contribution for this task. We
study a Japanese corpus containing both
infant- and adult-directed speech and we
apply four different word segmentation
models, with and without knowledge of
prosodic boundaries. The results showed
that the difference between registers is
smaller than previously reported and that
prosodic boundary information helps more
adult- than infant-directed speech.
</p>
<p>1 Introduction
</p>
<p>Infants start learning their native language even
before birth and, already during their first year of
life, they succeed in acquiring linguistic structure
at several levels, including phonetic and lexical
knowledge. One extraordinary aspect of the learn-
ing process is infants’ ability to segment contin-
uous speech into words, while having little or no
knowledge of the sounds of their native language.
</p>
<p>Several hypotheses have been proposed in
the experimental literature to explain how they
achieve this feat. Among the main classes of
cues put forward, prosodic cues (e.g. stress,
prosodic boundaries) have been shown to be par-
ticularly useful in early-stage word segmentation
(Christophe et al., 2003; Curtin et al., 2005; Seidl
and Johnson, 2006). Previous work suggests that
</p>
<p>these cues may be emphasized in the speech reg-
ister often used when addressing infants (infant-
directed speech; IDS). This register is character-
ized by shorter utterances, repeated words and ex-
aggerated prosody (see (Cristia, 2013) for a re-
view). It has been shown that IDS can facilitate
segmentation performance in infants (Thiessen
et al., 2005), when compared to the register that
parents use when talking to adults (adult-directed
speech; ADS).
</p>
<p>The process of word segmentation has received
considerable attention also from the computational
linguistics community, where various computa-
tional models have been proposed (e.g. (Brent
and Cartwright, 1996; Goldwater et al., 2009)).
Yet, despite the role that prosodic cues play in
early word segmentation, only lexical stress has
been addressed in detail, in the computational
modelling literature (e.g. (Börschinger and John-
son, 2014; Doyle and Levy, 2013; Lignos, 2011)).
As for prosodic boundary information, it was in-
vestigated in only one previous study (Ludusan
et al., 2015). That study found that that an Adap-
tor Grammar model (Johnson et al., 2007) per-
formed better on both English and Japanese cor-
pora when prosodic boundary information was
added to its grammar. These previous studies in-
vestigated the effect of prosodic cues while keep-
ing register constant, investigating either IDS (e.g.
(Börschinger and Johnson, 2014)) or ADS (Ludu-
san et al., 2015). Other work focuses on register
only. For instance, (Fourtassi et al., 2013) used the
Adaptor Grammar framework to examine English
and Japanese corpora of infant- and adult-directed
speech, concluding that IDS was easier to segment
</p>
<p>178</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2028">https://doi.org/10.18653/v1/P17-2028</a></div>
</div>
<div class="page"><p />
<p>than ADS. However, the corpora were not parallel
or necessarily directly comparable, as, the ADS in
Japanese was transcribed from academic presenta-
tion speeches, whereas the IDS came from spon-
taneous conversational speech.
</p>
<p>We aim to put together these two lines of
research, by conducting the first computational
study of word segmentation that takes into ac-
count both variables: speech register and prosodic
boundary information. This investigation extends
the previously mentioned studies, by allowing us
to observe not only the effect of each individ-
ual variable, but also any interaction between the
two. More importantly, it is performed in a more
controlled manner as it makes use of a large cor-
pus of spontaneous verbal interactions, containing
both IDS and ADS uttered by the same speakers.
Furthermore, we do not limit ourselves to a spe-
cific model, but test several, different, unsuper-
vised segmentation models in order to increase the
generalizability of the findings.
</p>
<p>2 Methods
</p>
<p>Several unsupervised segmentation algorithms
were employed. We selected 2 sub-lexical and 2
lexical models, all of which are made freely avail-
able through the CDSwordSeg package1.
</p>
<p>The first model performs transition-probability-
based segmentation (TP) employing the relative
algorithm of Saksida et al. (2016). It takes in input
transcribed utterances, segmented at the syllable
level and computes the forward transitional prob-
abilities between every pair of syllables in the cor-
pus. The transition probability between two sylla-
bles X and Y is defined as the frequency of the pair
(X,Y) divided by the frequency of the syllable X.
Once probabilities are computed, word boundaries
are posited using local minima of the probability
function. As this algorithm only attempts to posit
boundaries based on phonological information it
is called a ‘sub-lexical’ model.
</p>
<p>Diphone-based segmentation (DiBS) is another
sub-lexical model, which uses diphones instead
of syllables pairs (Daland and Pierrehumbert,
2011). The input is represented as a sequence
of phonemes and the model tries to place bound-
aries based on the identity of each consecutive
sequence of two phonemes. The goal is accom-
plished by computing the probability of a word
boundary falling within such a sequence, with the
</p>
<p>1https://github.com/alecristia/CDSwordSeg
</p>
<p>probability being rewritten using Bayes’ rule. The
information needed for the computation of the
word boundary probability is estimated on a small
subset of the corpus, using the gold word bound-
aries. Thereafter, a boundary is placed between
every diphone whose probability is above a pre-
determined threshold.
</p>
<p>Monaghan and Christiansen (2010)’s PUDDLE
is a lexical model which utilizes previously seen
utterances to extract lexical and phonotactic in-
formation knowledge later used to “chunk” se-
quences. In a nutshell, it is an incremental al-
gorithm that initially memorizes whole utterances
into its long-term lexical storage, from which pos-
sible word-final and word-initial diphones are ex-
tracted. The model continues to consider each ut-
terance as a lexical unit, unless sub-sequences of
the given utterance have already been stored in the
word list. In that case, it cuts the utterance based
on the words which it already knows and considers
the newly segmented chunks as word candidates.
In order for the word candidates to be added to
the lexical list, they have to respect two rules: 1)
the final diphones of the left chunk and the be-
ginning diphones of the right chunk must be on
the list of permissible final diphones; and 2) both
chunks have to contain at least one vowel. Once
a candidate is added to the lexical list, its begin-
ning and final diphones are included into the list
of permissible diphones.
</p>
<p>The last model was a unigram implementation
of Adaptor Grammar (AG) (Johnson et al., 2007).
AG is a hierarchical Bayesian model based on an
extension of probabilistic context free grammars.
It alternates between using the previously learned
grammar to parse an utterance into a hierarchical
tree structure made up of words and phonemes,
and updating the grammar by learning probabili-
ties associated to rules and entire tree fragments,
called adapted non-terminals. The unigram model
is the simplest grammar, considering utterances as
being composed of words, which are represented
as a sequence of phonemes.
</p>
<p>3 Materials
</p>
<p>The RIKEN corpus (Mazuka et al., 2006) contains
recordings of 22 Japanese mothers interacting
with their 18 to 24-month old infants, while play-
ing with toys or reading a book. The same moth-
ers were then recorded while talking to an experi-
menter. Out of the total 14.5 hours of recordings,
</p>
<p>179</p>
<p />
</div>
<div class="page"><p />
<p>about 11 hours represent infant-directed speech,
while the rest adult-directed speech.
</p>
<p>The corpus was annotated at both segmental and
prosodic levels. We made use in this study of
the prosodic boundary annotation, labelled using
the X-JToBI standard (Maekawa et al., 2002). X-
JToBI defines prosodic breaks based on the de-
gree of their perceived disjuncture, ranging from
level 0 (the weakest) to level 3 (the strongest).
We use here level 2 and level 3 prosodic breaks,
which in the Japanese prosodic organization (Ven-
ditti, 2005) correspond, respectively, to accen-
tual phrases and intonational phrases. Accentual
phrases are sequences of words that carry at most
one pitch accent; for instance, a noun with a post-
position will typically only have one accent. Into-
national phrases are made up of sequences of ac-
centual phrases, and constitute the domain where
pitch range is defined such that, for instance, the
onset of an intonational phrase will be marked by
a reset the pitch level.
</p>
<p>An additional dataset, part of the Corpus of
Spontaneous Japanese (CSJ) (Maekawa, 2003),
was considered as control. It contains academic
speech and was previously used to investigate ei-
ther the effect of speech register (Fourtassi et al.,
2013) or that of prosodic boundaries (Ludusan
et al., 2015) on unsupervised word segmentation.
The same levels of annotations are available as for
the RIKEN corpus. Statistics about the number of
utterances and word token and types, for all three
corpora, can be found in Table 1.
</p>
<p>4 Experimental settings
</p>
<p>The transitional probabilities used by TP were
computed on the entire input dataset, while the es-
timation of the probabilities needed by DiBS was
performed on the first 200 utterances of the cor-
pus. PUDDLE, being an incremental algorithm,
was evaluated using a five-fold cross-validation.
For AG, the process was repeated five times for
each register and prosodic boundary condition,
and the average across the five runs was reported.
</p>
<p>Dataset #utts #tokens #types
CSJ 20,052 216,932 7,340
ADS 3,582 22,844 2,022
IDS 14,570 51,315 2,850
</p>
<p>Table 1: Statistics regarding the utterances and
words contained in the investigated corpora.
</p>
<p>Each run had 2000 iterations and Minimum Bayes
Risk (Johnson and Goldwater, 2009) decoding
was used for the evaluation.
</p>
<p>Each algorithm was run on the ADS, IDS and
CSJ datasets for each of the 3 cases considered:
no prosody (base), level 3 prosodic breaks (brk3)
and level 2 and level 3 prosodic breaks (brk23).
For the base case, the system had in input a file
containing on each line an utterance, defined as
being an intonational phrase or a filler phrase fol-
lowed by a pause longer than 200 ms. In the brk3
and brk23 cases, each prosodic phrase was consid-
ered as a standalone utterance, and thus was writ-
ten on a separate line. During the evaluation of the
brk3 and brk23 cases, the original utterances were
rebuilt by concatenating all the prosodic phrases
contained in them, after which they were com-
pared against the reference.
</p>
<p>Additionally, we checked whether the size dif-
ference between the ADS and IDS datasets might
have an effect on the results obtained. For this,
we created two additional, balanced, subsets of the
IDS data. The first one contained an equal number
of words from each speaker as in their ADS data,
while the second one an equal number of utter-
ances, for each speaker, as in their ADS produc-
tion. As there was no significant difference be-
tween the results with the two balanced subsets
and the entire IDS corpus, we will present here
only the latter results.
</p>
<p>5 Results and discussion
</p>
<p>The segmentation evaluation was performed
against the gold word segmentation, provided with
the corpus. A classical metric, the token F-score,
was used as evaluation measure. It is defined as
the harmonic average between the token precision
(how many word tokens, out of the total number of
segmented words, were correct) and token recall
(how many word tokens, out of the total number
of words in the reference data, were found).
</p>
<p>Next, we illustrate the obtained token F-score
for the three corpora (IDS, ADS and CSJ) in Fig-
ure 1, for the three cases (base, brk3 and brk23)
and for the four algorithms investigated (TP, DiBS,
PUDDLE and AG). We observe that the largest
differences are between algorithms. It appears that
models employing sub-lexical information fare
worse than the ones working at the lexical level.
DiBS gives the lowest performance (.132 token F-
score for CSJ base), followed by TP, PUDDLE
</p>
<p>180</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Segmentation results obtained using four algorithms: TP, DiBS, PUDDLE and AG on the
IDS, ADS and CSJ datasets, when no prosodic information was provided (base), when utterances where
additionally broken at boundaries of type 3 (brk3), and when utterances where additionally broken at
boundaries of type 2 and 3 (brk23).
</p>
<p>and AG giving the best performance (.567 token
F-score for ADS brk23). The goal of the present
study, however, is not to pit algorithms against
each other, but rather to sample from plausible
segmentation strategies that infants could poten-
tially use so as to provide more representative and
generalizable results.
</p>
<p>Register effects found in the comparison be-
tween IDS and CSJ with the AG model replicate
previous work (Fourtassi et al., 2013). We consid-
erably extend knowledge by additionally includ-
ing a casual ADS sample matched to the IDS, and
investigating 3 additional algorithms. This allows
us to conclude that differences between IDS and
ADS are considerably smaller than previous work
could have suggested. This is expected in view
of previous reports that using un-matched mate-
rials leads to an overestimation of the differences
between IDS and ADS (Batchelder, 2002). Inter-
estingly, we also found that the size and direction
of this difference was dependent on the algorithm
used. An important advantage can be observed in
the IDS-ADS comparison for the sub-lexical al-
gorithms (maximally 9% for TP and 10.3% for
DiBS), which decreases for PUDDLE and AG
(maximally 1-1.1%), and can sometimes reverse
when prosodic information is taken into account
(DiBS brk23, AG brk3 and brk23).
</p>
<p>Turning to prosodic boundaries, breaking utter-
ances using internal prosodic breaks seems to help
to a different degree the two classes of segmenta-
tion models and the three corpora, in ways that re-
</p>
<p>semble a crossed interaction. The performance of
sub-lexical models improves more with the use of
prosodic information than that of lexical models,
and this for all corpora. By and large, performance
is boosted by additional prosodic breaks more for
CSJ and ADS than IDS. This boost is, however,
rather variable for PUDDLE, with apparent de-
clines when, for instance, type 3 breaks are added
for ADS. These results only partially replicate
those reported in (Ludusan et al., 2015). Overall,
the improvement brought by prosodic boundaries
is smaller. TP brk23 brings an absolute improve-
ment of 17.3% over TP base, for CSJ, but the im-
provement brought for AG (3.6%) is modest com-
pared to what was previously reported (12.3%).2
</p>
<p>Overall, we observe that some of our conclu-
sions are dependent on the actual corpus being
used. For this reason, we further analysed several
measures which could play a role in the segmenta-
tion process. The first one, the average number of
words per utterance was highest for CSJ, followed
by ADS and the lowest for IDS. This would be
expected taken into account the characteristics of
IDS (Cristia, 2013). It is important to note that the
smallest difference with respect to utterance length
</p>
<p>2These differences might stem from the model used (we
used here a unigram model, while a colloc3-syll model was
previously used) or from the way in which the prosodic in-
formation was integrated (at the input level, in the current
study, compared to at the grammar level, before). Indeed, a
model that makes explicit in its grammar the prosodic bound-
aries and, thus, learns word boundaries jointly with prosodic
boundaries could be more powerful. These aspects will have
to be investigated in a future study.
</p>
<p>181</p>
<p />
</div>
<div class="page"><p />
<p>Set cond phn typ wrd ambig
</p>
<p>CSJ
base
</p>
<p>3.498 .584
10.82 .02918
</p>
<p>brk3 5.25 .01996
brk23 2.75 .01195
</p>
<p>ADS
base
</p>
<p>3.089 .579
6.38 .02981
</p>
<p>brk3 3.57 .02217
brk23 2.53 .01746
</p>
<p>IDS
base
</p>
<p>3.402 .522
3.52 .03099
</p>
<p>brk3 2.48 .02681
brk23 2.06 .02425
</p>
<p>Table 2: Detailed statistics on the three cor-
pora used: average number of phonemes per word
token (phn), average number of types per to-
kens (typ), average number of words per utterance
(wrd), and segmentation ambiguity (ambig).
</p>
<p>between the base and brk23 was obtained for IDS,
the same register that seems to take advantage the
least by the information on prosodic boundaries.
</p>
<p>Besides the length of the utterance, the length of
the words plays an important role in the segmen-
tation task. Longer words would increase the pos-
sibility of having substrings which are words on
their own, thus decreasing the segmentation per-
formance. As expected, CSJ has the highest aver-
age word length, but IDS was found to have a very
similar word length, followed by ADS. The un-
expected value obtained for IDS might be due to
the high number of long onomatopoeia present in
the corpus. Thus, any IDS advantage due to hav-
ing shorter utterances might be reversed by having
longer words. We computed also the average num-
ber of types per token, which can give information
about the distribution of the words in the corpora.
In order not to have a measure biased by the size
of the corpus, we computed it as a moving average
over a window of 100 words. It shows a slightly
higher vocabulary diversity for CSJ and ADS, than
IDS, suggesting a more difficult segmentation.
</p>
<p>The segmental ambiguity score (Fourtassi et al.,
2013) measures the number of different parses of
a sentence given the gold lexicon, by computing
the average entropy in parses, taken into account
the probability of each parse. Fourtassi and col-
leagues argue that this measure captures the intrin-
sic difficulty of the segmentation problem and pre-
dicts segmentation scores across languages (but
see Phillips and Pearl (2014)). Here, we found that
segmentation ambiguity decreases with the use
of prosodic information (by preventing segmen-
tations that would straddle a prosodic break). In
</p>
<p>contrast, there is not much difference between reg-
isters; if anything, IDS is more ambiguous than the
two adult corpora; we speculate that this may be
due to the presence of many onomatopoeia in IDS
(over 8% of the total word tokens) some of which
contain a lot of reduplications, which would in-
crease segmentation ambiguity. This may explain
why, when prosody equates sentence lengths, the
advantage of IDS over ADS becomes small or
even reverts to a detrimental effect.
</p>
<p>6 Conclusions
</p>
<p>We examined the performance of 4 different word
segmentation algorithms on two matched corpora
of spontaneous ADS and IDS, and a control cor-
pus of more formal ADS, all of them with and
without prosodic breaks. We found that, overall,
sub-lexical algorithms perform less well than lexi-
cal algorithms, that IDS was overall slightly easier
or equal to informal ADS, itself easier than formal
ADS. In addition, across all algorithms and regis-
ters, we observed that prosody helped word seg-
mentation. However, the impact of prosody was
unequal and showed an interaction with register: It
helped more ADS than IDS to the point that, with
prosody taken into account, spontaneous ADS and
IDS yield somewhat similar scores.
</p>
<p>This has impact for theories of language acqui-
sition, since IDS has been assumed to provide in-
fants with ‘hyperspeech’, i.e. a simplified kind of
input that facilitates language acquisition. If our
observations are true, as far as word segmentation
goes, it is not the case that IDS is massively easier
to segment than ADS, at least at the stage when
infants have acquired the ability to use prosodic
breaks to constrain word segmentation. Of course,
our observations would need to be confirmed and
replicated with other languages and recording pro-
cedures. To conclude, our study illustrates the in-
terest of testing theories of language acquisition
using quantitative tools.
</p>
<p>Acknowledgments
</p>
<p>BL, MB and ED’s research was funded by
the European Research Council (ERC-2011-AdG-
295810 BOOTPHON), and AC by the Agence Na-
tionale pour la Recherche (ANR-14-CE30-0003
MechELex). It was also supported by the Canon
Foundation in Europe and the Agence Nationale
pour la Recherche (ANR-10-LABX-0087 IEC,
ANR-10-IDEX-0001-02 PSL*).
</p>
<p>182</p>
<p />
</div>
<div class="page"><p />
<p>References
Eleanor Olds Batchelder. 2002. Bootstrapping the lex-
</p>
<p>icon: A computational model of infant speech seg-
mentation. Cognition 83(2):167–206.
</p>
<p>Benjamin Börschinger and Mark Johnson. 2014. Ex-
ploring the role of stress in bayesian word segmen-
tation using adaptor grammars. Transactions of
the Association for Computational Linguistics 2:93–
104.
</p>
<p>Michael R Brent and Timothy A Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. Cognition 61(1):93–
125.
</p>
<p>Anne Christophe, Ariel Gout, Sharon Peperkamp, and
James Morgan. 2003. Discovering words in the con-
tinuous speech stream: the role of prosody. Journal
of phonetics 31(3):585–598.
</p>
<p>Alejandrina Cristia. 2013. Input to language: The
phonetics and perception of infant-directed speech.
Language and Linguistics Compass 7(3):157–170.
</p>
<p>Suzanne Curtin, Toben H Mintz, and Morten H Chris-
tiansen. 2005. Stress changes the representational
landscape: Evidence from word segmentation. Cog-
nition 96(3):233–262.
</p>
<p>Robert Daland and Janet B Pierrehumbert. 2011.
Learning diphone-based segmentation. Cognitive
science 35(1):119–155.
</p>
<p>Gabriel Doyle and Roger Levy. 2013. Combining mul-
tiple information types in bayesian word segmenta-
tion. In Proceedings of NAACL-HLT . pages 117–
126.
</p>
<p>Abdellah Fourtassi, Benjamin Börschinger, Mark
Johnson, and Emmanuel Dupoux. 2013. Whyisen-
glishsoeasytosegment. In Proceedings of the Fourth
Annual Workshop on Cognitive Modeling and Com-
putational Linguistics. pages 1–10.
</p>
<p>Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition
112(1):21–54.
</p>
<p>Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL-HLT . pages 317–
325.
</p>
<p>Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. Advances in neural information processing
systems 19:641.
</p>
<p>Constantine Lignos. 2011. Modeling infant word seg-
mentation. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning.
pages 29–38.
</p>
<p>Bogdan Ludusan, Gabriel Synnaeve, and Emmanuel
Dupoux. 2015. Prosodic boundary information
helps unsupervised word segmentation. In Proceed-
ings of NAACL-HLT . pages 953–963.
</p>
<p>K. Maekawa. 2003. Corpus of Spontaneous Japanese:
Its design and evaluation. In ISCA &amp; IEEE Work-
shop on Spontaneous Speech Processing and Recog-
nition.
</p>
<p>Kikuo Maekawa, Hideaki Kikuchi, Yosuke Igarashi,
and Jennifer Venditti. 2002. X-JToBI: an extended
J-ToBI for spontaneous speech. In Proceedings of
INTERSPEECH. pages 1545–1548.
</p>
<p>Reiko Mazuka, Yosuke Igarashi, and Ken’ya
Nishikawa. 2006. Input for learning Japanese:
RIKEN Japanese mother-infant conversation corpus
(COE Workshop session 2). IEICE Technical
Report 106(165):11–15.
</p>
<p>Padraic Monaghan and Morten H Christiansen. 2010.
Words in puddles of sound: modelling psycholin-
guistic effects in speech segmentation. Journal of
child language 37(03):545–564.
</p>
<p>Lawrence Phillips and Lisa Pearl. 2014. Bayesian in-
ference as a viable cross-linguistic word segmenta-
tion strategy: It’s all about what’s useful. In Pro-
ceedings of CogSci. pages 2775–2780.
</p>
<p>Amanda Saksida, Alan Langus, and Marina Nes-
por. 2016. Co-occurrence statistics as a language-
dependent cue for speech segmentation. Develop-
mental science .
</p>
<p>Amanda Seidl and Elizabeth K Johnson. 2006. In-
fant word segmentation revisited: Edge alignment
facilitates target extraction. Developmental science
9(6):565–573.
</p>
<p>Erik D Thiessen, Emily A Hill, and Jenny R Saffran.
2005. Infant-directed speech facilitates word seg-
mentation. Infancy 7(1):53–71.
</p>
<p>Jennifer Venditti. 2005. The J-ToBI model of Japanese
intonation. In Sun-Ah Jun, editor, Prosodic typol-
ogy: The phonology of intonation and phrasing, Ox-
ford University Press, Oxford, pages 172–200.
</p>
<p>183</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 184–188
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2029
</p>
<p>A Two-Stage Parsing Method for Text-Level Discourse Analysis
</p>
<p>Yizhong Wang Sujian Li Houfeng Wang
Key Laboratory of Computational Linguistics, Peking University, MOE, China
</p>
<p>{yizhong, lisujian, wanghf}@pku.edu.cn
</p>
<p>Abstract
</p>
<p>Previous work introduced transition-based
algorithms to form a unified architecture
of parsing rhetorical structures (including
span, nuclearity and relation), but did not
achieve satisfactory performance. In this
paper, we propose that transition-based
model is more appropriate for parsing the
naked discourse tree (i.e., identifying span
and nuclearity) due to data sparsity. At the
same time, we argue that relation labeling
can benefit from naked tree structure and
should be treated elaborately with consid-
eration of three kinds of relations includ-
ing within-sentence, across-sentence and
across-paragraph relations. Thus, we de-
sign a pipelined two-stage parsing method
for generating an RST tree from text. Ex-
perimental results show that our method
achieves state-of-the-art performance, es-
pecially on span and nuclearity identifica-
tion.
</p>
<p>1 Introduction
</p>
<p>A typical document is usually organized in a co-
herent way that each text unit is relevant to its
context and plays a role in the entire semantics.
Text-level discourse analysis tries to identify such
discourse structure of a document and its success
can benefit many downstream tasks, such as sen-
timent analysis (Polanyi and van den Berg, 2011)
and document summarization (Louis et al., 2010).
</p>
<p>One most influential text-level discourse pars-
ing theory is Rhetorical Structure Theory (RST)
(Mann and Thompson, 1988), under which a text
is parsed to a hierarchical discourse tree. The leaf
nodes of this tree correspond to Elementary Dis-
course Units (EDUs, usually clauses) and then leaf
nodes are recursively connected by rhetorical rela-
</p>
<p>tions to form larger text spans until the final tree is
built. RST also depicts which part is more impor-
tant in a relation by tagging Nucleus or Satellite.
Generally, each relation at least includes a Nucleus
and there are three nuclearity types: Nucleus-
Satellite (NS), Satellite-Nucleus (SN) and Nucleus-
Nucleus (NN). Therefore, the performance of RST
discourse parsing can be evaluated from three as-
pects: span, nuclearity and relation.
</p>
<p>To parse discourse trees, transition-based pars-
ing model, which gains significant success in de-
pendency parsing (Yamada and Matsumoto, 2003;
Nivre et al., 2006) , was introduced to discourse
analysis. Marcu (1999) first employed a transi-
tion system to derive a discourse parse tree. In
such a system, action labels are designed by com-
bining shift-reduce action with nuclearity and re-
lation labels, so that one classifier can determine
span, nuclearity and relation simultaneously via
judging actions. More recent studies followed
this research line and enhanced the performance
by either tuning the models (Sagae, 2009) or
using more effective features (Ji and Eisenstein,
2014; Heilman and Sagae, 2015). Though these
transition-based models show advantages in the
unified processing of span, nuclearity and rela-
tion, they report weaker performance than other
methods, like CYK-like algorithms (Li et al.,
2014, 2016) or greedy bottom-up algorithms
that merge adjacent spans (Hernault et al., 2010;
Feng and Hirst, 2014).
</p>
<p>In such cases, we analyze that the labelled data
can not sufficiently support the classifier to dis-
tinguish among the information-rich actions (e.g.,
Reduce-NS-Contrast) , since there exist very few
labelled text-level discourse corpus available for
training. The limited training data will cause un-
balanced actions and lead to the problems of data
sparsity and overfitting. Thus, we propose to use
the transition-based model to parse a naked dis-
</p>
<p>184</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2029">https://doi.org/10.18653/v1/P17-2029</a></div>
</div>
<div class="page"><p />
<p>course tree (i.e., identifying span and nuclearity)
in the first stage. The benefits are three-fold. First,
we can still use the transition based model which is
a good tree construction tool. Second, much fewer
actions need to be identified in the tree construc-
tion process. Third, we could separately label re-
lations, which needs careful consideration.
</p>
<p>In the second stage, relation labels for each
span are determined independently. Prior stud-
ies (Joty et al., 2013; Feng and Hirst, 2014) have
found that rhetorical relations distribute differ-
ently intra-sententially vs. multi-sententially.
They discriminate the two levels by training two
models with different feature sets. We take a fur-
ther step and argue that relations between para-
graphs are usually more loosely connected than
those between sentences within the same para-
graph. Therefore we train three separate classi-
fiers for labeling relations at three levels: within-
sentence, across-sentence and across-paragraph.
Different features are used for each classifier and
the naked tree structure generated in the first stage
is also leveraged as features. Experiments on the
RST-DT corpus demonstrate the effectiveness of
our pipelined two-stage discourse parsing model.
</p>
<p>2 Our Method
</p>
<p>Our discourse parsing process is composed of two
stages: tree structure construction and relation la-
beling. In this work, we follow the convention to
use the gold standard EDU segmentations and fo-
cus on building a tree with nuclearity and relation
labels assigned for each inner node.
</p>
<p>2.1 Tree Structure Construction
</p>
<p>In a typical transition-based system for discourse
parsing, the parsing process is modeled as a se-
quence of shift and reduce actions, which are ap-
plied to a stack and a queue. The stack is initial-
ized to be empty and the queue contains all EDUs
in the document. At each step, the parser performs
either shift or reduce. Shift pushes the first EDU
in the queue to the top of the stack, while reduce
pops and merges the top elements in the stack to
get a new subtree, which is then pushed back to
the top of the stack. A parse tree can be finally
constructed until the queue is empty and the stack
only contains the complete tree. Only one classi-
fier is learned to judge the actions at each step.
</p>
<p>To derive a discourse tree in a unified frame-
work, prior systems design multiple reduce actions
</p>
<p>with consideration of both nuclearity and relation
types. With 3 nuclearity types and 18 relation
types, the number of reduce actions exceeds 40,
leading to the data sparsity problem.
</p>
<p>In our parsing model, a transition-based system
is responsible for building the naked tree without
relation labels. We only design four types of ac-
tions, including: Shift, Reduce-NN, Reduce-NS,
Reduce-SN. We identify span and nuclearity si-
multaneously in the transition-based tree construc-
tion, since nuclearity is actually closely related to
the tree structure, just as the left-arc and right-
arc action in dependency parsing. The number
of the four actions on the training set of RST-DT
corpus is shown in Table 1. Though the four ac-
tions still have an unbalanced distribution, the rel-
atively large number of occurrences assures that
the classifier in our system can be trained more
sufficiently.
</p>
<p>Shift Reduce-NN Reduce-NS Reduce-SN
19443 4329 11702 3065
</p>
<p>Table 1: Statistics of the Four Actions
</p>
<p>2.2 Relation Labeling
</p>
<p>The most challenging subtask of discourse parsing
is relation labeling. In a binarized RST discourse
tree, a relation label can be determined for each
internal node, describing the relation between its
left and right subtrees1.
</p>
<p>We conduct relation labeling after the naked
tree structure has been constructed. On one hand,
the naked tree structure can provide more infor-
mation to support relation classification, verified
in (Feng and Hirst, 2014). For example, some re-
lations tend to appear around the tree root while
other relaitons would like to keep away from
the root. On the other hand, we can elabo-
rately distinguish relations at different levels, in-
cluding within-sentence, across-sentence, across-
paragraph. We add across-paragraph level be-
cause some relations, like textual-organization and
topic-change are observed to mainly occur be-
tween paragraphs.
</p>
<p>Therefore, we adopt three classifiers for label-
ing relations at different levels. We first traverse
the naked tree in post order and ignore leaf nodes,
since we only need to judge relations for internal
nodes. Next, for each internal node, we determine
</p>
<p>1Relation label is actually assigned to the satellite subtree
and a “Span” label is assigned to the nucleus substree.
</p>
<p>185</p>
<p />
</div>
<div class="page"><p />
<p>whether its left and right subtrees are in different
paragraphs, or the same paragraph, or the same
sentence. For each level, we predict a relation la-
bel using the corresponding classifier.
</p>
<p>2.3 Training
</p>
<p>We use SVM classifiers for the four classification
tasks (one action classifier and three relation clas-
sifiers). We take the linear kernel for fast training
and use squared hinge loss with L1 penalty on the
error term. The penalty coefficient C is set to 1.
</p>
<p>The four classifiers are learned with offline
training. Training instances for the action classi-
fier are generated by converting gold parse trees
into a sequence of actions. Then we extract fea-
tures for each action before it is performed. Train-
ing instances for relation classifiers are prepared
by traversing the gold parse trees and extracting
features for the relation of each internal node.
</p>
<p>3 Features
</p>
<p>This section details the features used in our model,
which are a key to the four classifiers in discourse
parsing.
</p>
<p>For the action classifier, features are extracted
from the top 2 elements S1, S2 in the stack and the
first EDU Q1 in the queue. We design the feature
sets for the action classifier as follows:
• Status features: the previous action; number of
</p>
<p>elements in the stack and queue.
</p>
<p>• Position features: whether S1, S2 or S1, Q1
are in the same sentence or paragraph; whether
they are start or end of a sentence, paragraph or
document; distance from S1, S2, Q1 to the start
and end of document.
</p>
<p>• Structural features: nuclearity type (NN, NS
or SN) of S1, S2; number of EDUs and sen-
tences in S1, S2; length comparison of S1, S2
with respect to EDUs and sentences.
</p>
<p>• Dependency features: whether dependency re-
lations exist between S1, S2 or between S1, Q1;
the dependency direction and relation type.
</p>
<p>• N-gram features: the first and the last n words
and their POS tags in the text of S1, S2, Q1,
where n ∈ {1, 2}.
</p>
<p>• Nucleus features: the dependency heads of
the nucleus EDUs2 for S1, S2, Q1 and their
POS tags; brown clusters (Brown et al., 1992;
2Nucleus EDU is defined by recursively selecting the Nu-
</p>
<p>cleus in the binary tree until an EDU (leaf node) is reached.
</p>
<p>Turian et al., 2010) of all the words in the nu-
cleus EDUs of S1, S2, Q1.
Next, we list all the features used for the three
</p>
<p>relation classifiers. Given an internal node P in the
naked tree, we aim to predict the relation between
its left child Cleft and right child Cright. Depen-
dency features, N-gram features and nucleus fea-
tures discussed above are also needed, the only
difference is that these features are applied to the
left and right children. Other features include:
• Refined Structural features: nuclearity type
</p>
<p>of node P ; distance from P , Cleft, Cright to
the start and end of the document / paragraph /
sentence with respect to paragraphs / sentences
/ EDUs; number of paragraphs / sentences /
EDUs in Cleft and Cright; length comparison
of Cleft and Cright with respect to paragraphs /
sentences / EDUs.
</p>
<p>• Tree features: depth and height of the node P
in the tree; nuclearity type of P and P ’s grand-
parent node, if they exist. This feature type ben-
efits from our stagewise parsing method.
Relation labeling classifiers at different levels
</p>
<p>pick somewhat different features from all the fea-
tures. N-gram and structural features work for the
three classifiers. Dependency features are only
used for within-sentence classifier. Nucleus fea-
tures and tree features are only used for across-
sentence and across-paragraph classifiers.
</p>
<p>4 Experiments
</p>
<p>We evaluate our parser on RST Discourse Tree-
bank (RST-DT) (Carlson et al., 2003) and thor-
oughly analyze different components of our
method. Results show our parsing model achieves
state-of-the-art performance on the text-level dis-
course parsing task.
</p>
<p>4.1 Setup
RST-DT annotates 385 documents (347 for
training and 38 for testing) from the Wall
Street Journal using Rhetorical Structure The-
ory (Mann and Thompson, 1988). Convention-
ally, we use 18 coarse-grained relations and bi-
narize non-binary relations with right-branching
(Sagae and Lavie, 2005). For preprocessing, we
use the Stanford CoreNLP toolkit (Manning et al.,
2014) to lemmatize words, get POS tags, segment
sentences and syntactically parse them.
</p>
<p>To directly compare with other discourse pars-
ing systems, we employ the same evaluation met-
</p>
<p>186</p>
<p />
</div>
<div class="page"><p />
<p>rics, i.e. the precision, recall and F-score 3 with re-
spect to span (S), nuclearity (N) and relation (R),
as defined by Marcu (2000).
</p>
<p>4.2 Results and Analysis
</p>
<p>We compare our system against other state-
of-the-art discourse parsers, shown in Table 2.
Among them, Joty et al. (2013), Li et al. (2014)
and Li et al. (2016) all employ CKY-like algo-
rithms to search global optimal parsing result.
Ji and Eisenstein (2014) and Heilman and Sagae
(2015) use transition-based parsing systems with
improvements on the feature representation.
Feng and Hirst (2014) adopts a greedy approach
that merges two adjacent spans at each step and
two CRFs are used to predict the structure and the
relation separately.
</p>
<p>From Table 2, we can see that our method
outperforms all the others with respect to span
and nuclearity, and exceeds most systems on re-
lation labeling. Especially, our method signif-
icantly outperforms other transition-based mod-
els (Ji and Eisenstein, 2014; Heilman and Sagae,
2015) on building the naked tree structure (span
and nuclearity). This is mainly due to the proper
design of actions in our transition-based system.
The reason that Ji and Eisenstein (2014) achieve a
high score of relation labeling may be that their la-
tent representations are more advantageous in cap-
turing semantics, which will inspire us to refine
our features in future work.
</p>
<p>Model S N R
Joty et al. (2013) 82.7 68.4 55.7
Li et al. (2014) 84.0 70.8 58.6
</p>
<p>Ji and Eisenstein (2014) 82.1 71.1 61.6
Feng and Hirst (2014) 85.7 71.0 58.2
</p>
<p>Heilman and Sagae (2015) 83.5 69.3 57.4
Li et al. (2016) 85.8 71.1 58.9
</p>
<p>Ours 86.0 72.4 59.7
Human 4 88.7 77.7 65.8
</p>
<p>Table 2: Performance comparison with state-of-
the-art parsers.
</p>
<p>To further explore the influence of different
components in our model, we implement three
simplified versions (i.e., Simp-1/2/3), as is shown
in Table 3. Stage means whether two-stage strat-
</p>
<p>3Precision, recall and F-score are the same when manual
segmentation is used.
</p>
<p>4The human agreement on the annotations of RST corpus
</p>
<p>egy is adopted, Level denotes whether three kinds
of relations (i.e., within-sentence, across-sentence,
and across-paragraph) are differently classified,
and Tree represents whether relation labeling uses
tree features generated in the first stage.
</p>
<p>The simplest model Simp-1 is almost the same
as (Heilman and Sagae, 2015) except that we em-
ploy more features. That Simp-1 has a high per-
formance also means that transition-based method
has potentials for constructing discourse trees.
Simp-2 adopts the two-stage strategy, but uses only
one classifier to classify all the relations. We can
observe that the pipelined two stages bring a sig-
nificant improvement with respect to all the as-
pects, compared to Simp-1. The difference be-
tween Simp-3 and Ours is that Simp-3 does not
exploit the tree structure features generated in the
first stage. We can see that the three-level rela-
tion classification and tree features together bring
an improvement of about 1 percent on relation la-
beling. Compared with prior work, this slight im-
provement is also valuable and more efficacious
features need to be explored.
</p>
<p>Model Stage Level Tree S N R
Simp-1 No No No 84.4 70.7 57.7
Simp-2 Yes No No 86.0 72.4 58.6
Simp-3 Yes Yes No 86.0 72.4 59.4
Ours Yes Yes Yes 86.0 72.4 59.7
</p>
<p>Table 3: Comparison with simplified versions.
</p>
<p>Though the three-level relation labeling does
not achieve prominent improvement, we get some
interesting results via analyzing the performance
on each relation. The Attribution and Same-Unit
relations are the top 2 relations that we success-
fully classify with F-score as 0.87 and 0.83 re-
spectively and over 90 percent of these two re-
lations occur within sentences. This means that
within-sentence relations are relatively easy to
cope with. We also compare our final model
with Simp-1 and results show that the Textual-
Organization and Topic-Comment relaitons gain
an increase by 20% and 8% respectively. Most
of the Textual-Organization and Topic-Comment
relations are loosely across paragraphs and their
numbers (i.e., 148 and 130 instances in training
data) are also relatively small. We can see that our
method can improve on predicting infrequent re-
lations and partly solve the data sparsity problem.
At the same time, we infer that relations indeed be-
long to different levels and deserve fine treatment.
</p>
<p>187</p>
<p />
</div>
<div class="page"><p />
<p>5 Conclusion
</p>
<p>In this paper, we design a novel two-stage method
for text-level discourse analysis. The first stage
adopts the transition-based algorithm to construct
naked trees with consideration of span and nu-
clearity. The second stage categorizes relations
into three levels and uses three classifiers for re-
lation labeling. This pipelined design can mitigate
the data sparsity problem in tree construction, and
provide a new view of elaborately treating rela-
tions. Comprehensive experiments show the ef-
fectiveness of our proposed method.
</p>
<p>Acknowledgments
</p>
<p>We thank the anonymous reviewers for their in-
sightful comments on this paper. This work was
partially supported by National Natural Science
Foundation of China (61572049 and 61333018).
The correspondence author of this paper is Sujian
Li.
</p>
<p>References
Peter F Brown, Peter V Desouza, Robert L Mercer,
</p>
<p>Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics 18(4):467–479.
</p>
<p>Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Current and new directions in discourse and dia-
logue, Springer, pages 85–112.
</p>
<p>Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In ACL. pages 511–521.
</p>
<p>Michael Heilman and Kenji Sagae. 2015. Fast rhetor-
ical structure theory discourse parsing. arXiv
preprint arXiv:1505.02425 .
</p>
<p>Hugo Hernault, Helmut Prendinger, David A DuVerle,
Mitsuru Ishizuka, and Tim Paek. 2010. Hilda: a dis-
course parser using support vector machine classifi-
cation. Dialogue and Discourse 1(3):1–33.
</p>
<p>Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
ACL. pages 13–24.
</p>
<p>Shafiq R Joty, Giuseppe Carenini, Raymond T Ng, and
Yashar Mehdad. 2013. Combining intra-and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In ACL (1). pages 486–496.
</p>
<p>Jiwei Li, Rumeng Li, and Eduard H Hovy. 2014.
Recursive deep models for discourse parsing. In
EMNLP. pages 2061–2069.
</p>
<p>Qi Li, Tianshi Li, and Baobao Chang. 2016. Discourse
parsing with attention-based hierarchical neural net-
works pages 362–371.
</p>
<p>Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue. Association for Computational Linguistics,
pages 147–156.
</p>
<p>William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse 8(3):243–281.
</p>
<p>Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.
pages 55–60.
</p>
<p>Daniel Marcu. 1999. A decision-based approach to
rhetorical parsing. In Proceedings of the 37th an-
nual meeting of the Association for Computational
Linguistics on Computational Linguistics. Associa-
tion for Computational Linguistics, pages 365–372.
</p>
<p>Daniel Marcu. 2000. The theory and practice of dis-
course parsing and summarization. MIT press.
</p>
<p>Joakim Nivre, Johan Hall, Jens Nilsson, Gülşen Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics, pages 221–
225.
</p>
<p>Livia Polanyi and Martin van den Berg. 2011. Dis-
course structure and sentiment. In Data Min-
ing Workshops (ICDMW), 2011 IEEE 11th Interna-
tional Conference on. IEEE, pages 97–102.
</p>
<p>Kenji Sagae. 2009. Analysis of discourse structure
with syntactic dependencies and data-driven shift-
reduce parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies. Associ-
ation for Computational Linguistics, pages 81–84.
</p>
<p>Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology. Association for Computational Linguis-
tics, pages 125–132.
</p>
<p>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL. Association
for Computational Linguistics, pages 384–394.
</p>
<p>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT. volume 3, pages
195–206.
</p>
<p>188</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 189–195
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2030
</p>
<p>Error-repair Dependency Parsing for Ungrammatical Texts
</p>
<p>Keisuke Sakaguchi† and Matt Post‡ and Benjamin Van Durme†‡
†Center for Language and Speech Processing, Johns Hopkins University
</p>
<p>‡Human Language Technology Center of Excellence, Johns Hopkins University
{keisuke,post,vandurme}@cs.jhu.edu
</p>
<p>Abstract
</p>
<p>We propose a new dependency pars-
ing scheme which jointly parses a sen-
tence and repairs grammatical errors by
extending the non-directional transition-
based formalism of Goldberg and El-
hadad (2010) with three additional ac-
tions: SUBSTITUTE, DELETE, INSERT. Be-
cause these actions may cause an infinite
loop in derivation, we also introduce sim-
ple constraints that ensure the parser ter-
mination. We evaluate our model with re-
spect to dependency accuracy and gram-
maticality improvements for ungrammat-
ical sentences, demonstrating the robust-
ness and applicability of our scheme.
</p>
<p>1 Introduction
</p>
<p>Robustness has always been a desirable property
for natural language parsers: humans generate a
variety of noisy outputs, such as ungrammatical
webpages, speech disfluencies, and the text in lan-
guage learner’s essays. Such non-canonical text
contains grammatical errors such as substitutions,
insertions, and deletions. For example, a non-
native speaker of English might write “*I look in
forward hear from you”, where in is inserted, to is
deleted, and hearing is substituted incorrectly.
</p>
<p>We propose a novel dependency parsing scheme
that jointly parses and repairs ungrammatical sen-
tences with these sorts of errors. The parser is
based on the non-directional easy-first (EF) parser
introduced by Goldberg and Elhadad (2010) (GE
herein), which iteratively adds the most probable
arc until the parse tree is completed. These ac-
tions are called ATTACHLEFT and ATTACHRIGHT
depending on the direction of the arc. We ex-
tend the EF parsing scheme to be robust for un-
grammatical inputs by correcting grammatical er-
</p>
<p>I look in forward hear from youATTACHRIGHT
</p>
<p>fromin forward hearlook youATTACHLEFT
</p>
<p>hear fromSUBSTITUTE look in forward
</p>
<p>look in forward hearing fromDELETE
</p>
<p>I
</p>
<p>youI
</p>
<p>I you
</p>
<p>look forward hearing fromINSERT
</p>
<p>fromforward to hearinglookATTACHLEFT
</p>
<p>I
</p>
<p>I you
</p>
<p>you
</p>
<p>Figure 1: Illustrative example of partial derivation under
error-repair easy-first non-directional dependency parsing.
Solid arrows represent ATTACHRIGHT and ATTACHLEFT in
Goldberg and Elhadad (2010). Dotted arcs correspond to ac-
tions for each step. Following the notation by GE: arcs are
directed from a child to its parent.
</p>
<p>rors with three new actions: SUBSTITUTE, INSERT,
and DELETE. These new actions do not add an arc
between tokens but instead they edit a single to-
ken. As a result, the parser is able to jointly parse
and correct grammatical errors in the input sen-
tence. We call this new scheme Error-Repair Non-
Directional Easy-First parsing (EREF). Since the
new actions may greatly increase the search space
(e.g., infinite substitutions), we also introduce sim-
ple constraints to avoid such issues.
</p>
<p>We first describe the technical details of EREF
(§2) and then evaluate our EREF parser with re-
spect to dependency accuracy (robustness) and
grammaticality improvements (§3). Finally, we
</p>
<p>189</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2030">https://doi.org/10.18653/v1/P17-2030</a></div>
</div>
<div class="page"><p />
<p>position this effort at the intersection of noisy text
parsing and grammatical error correction (§4).
</p>
<p>2 Model
Non-directional Easy-first Parsing Let us be-
gin with a brief review of a non-directional easy-
first (EF) parsing scheme proposed by GE, which
is the foundation of our proposed scheme de-
scribed in the following sections.
</p>
<p>The EF parser has a list of partial structures
p1, ..., pk (called pending) initialized with sen-
tence tokens w1, ..., wn, and it keeps updating
pending through derivations. Unlike left-to-right
(e.g., shift-reduce) parsing algorithms (Yamada
and Matsumoto, 2003; Nivre, 2004), EF itera-
tively selects the best pair of adjoining tokens and
chooses the direction of attachment: ATTACHLEFT
or ATTACHRIGHT. Once the action is committed,
the corresponding dependency arc is added and the
child token is removed from pending. The first
two derivations in Figure 1 depict ATTACHRIGHT
and ATTACHLEFT. Pseudocode is shown in Algo-
rithm 1 (lines 1, 3-12).
</p>
<p>The parser is trained using the structured per-
ceptron (Collins, 2002) to choose actions to take
given a set of features expanded from templates.
The cost of actions is computed at every step by
checking the validity: whether a new arc is in-
cluded in the gold parse and whether the child al-
ready has all its children. See GE for further de-
scription of feature templates and structured per-
ceptron training. Since it is possible that there are
multiple valid sequence of actions and it is impor-
tant to examine a large search space, the parser
is allowed to explore (possibly incorrect) actions
with a certain probability, termed learning with ex-
ploration by Goldberg and Nivre (2013).
</p>
<p>Error-repair variant of EF Error-repair non-
directional easy-first parsing scheme (EREF) is
a variant of EF. We add three new actions:
SUBSTITUTE, DELETE, INSERT as ActsER. We do
not deal with a swapping action (Nivre, 2009) to
deal with word reordering errors, since the errors
are even less frequent than other error types (Lea-
cock et al., 2014). SUBSTITUTE replaces a token
to a grammatically more probable token, DELETE
removes an unnecessary token, and INSERT in-
serts a new token at a designated index. These
actions are shown in Figure 1 and Algorithm
1 (lines 13-25). Because the length of pend-
ing decreases as an attachment occurs, the parser
</p>
<p>Algorithm 1: Error-repair non-directional parsing
Input: ungrammatical sentence= w1 ... wn
Output: a set of dependency arcs (Arcs),
</p>
<p>repaired sentence (ŵ1 ... ŵm)
1 Acts = { ATTACHLEFT, ATTACHRIGHT }
2 ActsER = { DELETE, INSERT, SUBSTITUTE }
3 Arcs = { }
4 pending = p1...pn ← w1...wn
5 repaired = ŵ1...ŵn ← w1...wn
6 while len (pending) &gt; 1 do
7 best ← argmax
</p>
<p>act∈Acts∪ActsER
score (act (i))
</p>
<p>8 s.t. 1 ≤ i ≤ len(pending) ∩ isLegal(act, pending)
9 if best ∈ Acts then
</p>
<p>10 (parent, child)← edgeFor(best)
11 Arcs.add((parent, child))
12 pending.remove(child)
13 else if best = SUBSTITUTE then
14 c = bestCandidate(best, repaired)
15 pending.replace(pi, c)
16 repaired.replace(ŵpi.idx, c)
17 else if best = DELETE then
18 pending.remove(pi)
19 repaired.remove(ŵpi.idx)
20 Arcs.updateIndex()
21 else if best = INSERT then
22 c = bestCandidate(best, repaired)
23 pending.insert(i, c)
24 repaired.insert(pi.idx, c)
25 Arcs.updateIndex()
26 end
27 return Arcs, repaired
</p>
<p>also keeps the token indices in repaired (line 5),
which holds all tokens in a sentence throughout
the parsing process. Furthermore, the parser up-
dates token indices in pending and repaired when
INSERT or DELETE occurs. Technically, when a
token at i is deleted/inserted, the parser decre-
ments/increments the indices that are k &gt;= i (be-
fore executing the action) in pending, repaired,
and parents and children in a (partial) dependency
tree (Arcs).
</p>
<p>To find the best candidate for SUBSTITUTE and
INSERT efficiently, we restrict candidates to the
same part-of-speech or pre-defined candidate list.
We select the best candidate by comparing each
n-gram language model score with the same sur-
rounding context.
</p>
<p>Similar to EF, while training the parser, the cost
</p>
<p>190</p>
<p />
</div>
<div class="page"><p />
<p>Algorithm 2: Check validity during training
1 Function isValid(act, repaired, Gold)
2 d before = editDistance(repaired, Gold)
3 repaired + = repaired.apply(act)
4 d after = editDistance(repaired +, Gold)
5 if d before &gt; d after then return true;
6 else return false;
7 end
</p>
<p>for ActsER is based on validity. The validity of
the new actions is computed by taking the edit dis-
tance (d) between the Gold tokens (w∗1 ... w
</p>
<p>∗
r ) and
</p>
<p>the sentence state that the parser stores in repaired
(ŵ1 ... ŵm). When the edit distance after taking
an action (d after) is smaller than before (d before),
we regard the action as valid (Algorithm 2).
</p>
<p>One serious concern of EREF is that the new
actions may cause an infinite loop during pars-
ing (e.g., infinite SUBSTITUTE, or an alternative
DELETE and INSERT sequence.). To avoid this,
we introduce two constraints: (1) edit flag and
(2) edit limit. Edit flag is assigned for each to-
ken as a property, and a parser is not allowed to
execute ActsER on a token if its flag is on. The
flag is turned on when a parser executes ActsER on
a token whose flag is off. In INSERT action, the
flag of the inserted token is activated, while the
subsequent token (which gave rise to the INSERT)
is not. Edit limit is set to be the number of to-
kens in a sentence, and the parser is not allowed
to perform ActsER when the total number of ex-
ecution of ActsER exceeds the limit. These two
constraints prevent the parser from falling into an
infinite loop as well as parsing in the same order
of time complexity as GE. We also add the follow-
ing constraints to avoid unreasonable derivations:
(i) a word with a dependent cannot be deleted and
(ii) any child words cannot be substituted. All the
constraints are implemented in the isLegal() func-
tion in Algorithm 1 (line 8). We note that these
constraints not only prevent undesirable deriva-
tions but also leads to an efficiency in exploring
the search space during training.
</p>
<p>3 Experiment
</p>
<p>Data and Evaluation We evaluate EREF with
respect to dependency parsing accuracy (Exp1)
and grammaticality improvement (Exp2).1
</p>
<p>1Code for the experiments is available at http://
github.com/keisks/error-repair-parsing
</p>
<p>In the first experiment, as in GE, we train and
evaluate our parser on the English dataset from
the Penn Treebank (Marcus et al., 1993) with the
Penn2Malt conversion program (Sections 2-21 for
training, 22 for tuning, and 23 for test). We use the
PTB for the dependency experiment, since there
are no ungrammatical text corpora that has depen-
dency annotation on the corrected texts by human.
</p>
<p>We choose the following most frequent error
types that are used in CoNLL 2013 shared task
(Ng et al., 2013):
</p>
<p>1. Determiner (substitution, deletion, insertion)
2. Preposition (substitution, deletion, insertion)
3. Noun number (singular vs. plural)
4. Verb form (tense and aspect)
5. Subject verb agreement
</p>
<p>Regarding the candidate sets for INSERT and
SUBSTITUTE actions, following Rozovskaya and
Roth (2014), we focus on the most common can-
didates for each error type, setting the determiner
candidates to be {a, an, the, φ (as deletion)},
preposition candidates to be {on, about, from, for,
of, to, at, in, with, by, φ}, and verb forms to be
{VB(P|Z|G|D|N)}. We build a 5-gram language
model on English Gigaword with the KenLM
Toolkit (Heafield, 2011) for EREF to select the
best candidate.
</p>
<p>We manually inject grammatical errors into
PTB with certain error-rates similarly to the Gen-
ERRate toolkit by Foster and Andersen (2009),
which is designed to create synthetic errors into
sentences to improve grammatical error detection.
</p>
<p>We train and tune EREF models with different
token-level error injection rates from 5% (E05) to
20% (E20), because language learner corpora have
generally around 5% to 15% of token level errors
depending on learners’ proficiency (Leacock et al.,
2014). Since the error injection is stochastic, we
train each model with 10 runs and take an average
of parser performance on the test set.
</p>
<p>As a baseline, we use the original parser as de-
scribed by GE, which is equivalent to EREF with
training on an error-free corpus (E00). Since the
EF baseline does not allow error correction dur-
ing parsing, we pre-process the test data with a
grammatical error correction system similar to Ro-
zovskaya and Roth (2014), where a combination
of classifiers for each error type corrects grammat-
ical errors.
</p>
<p>For evaluation, we jointly parse and correct
grammatical errors in the test set with different
</p>
<p>191</p>
<p />
</div>
<div class="page"><p />
<p>(%) Baseline E05 E10 E15 E20
0 91.43 91.12 90.87 90.61 90.29
5 89.99 90.00 89.87 89.72 89.48
10 87.84 87.99 88.07 88.14 88.04
15 85.64 86.18 86.54 86.75 86.82
20 84.12 84.78 85.28 85.50 85.76
∇ -0.37 -0.32 -0.28 -0.26 -0.23
</p>
<p>Table 1: Unlabeled dependency accuracy results with the 5x5
models and test sets. ∇ shows the slope of deterioration in
parser performance.
</p>
<p>E05 E10 E15 E20
# edited sents (out of 5,124) 175 391 583 861
grammaticality (source) 2.92 2.95 2.95 2.89
grammaticality (this work) 2.96 2.99 3.27 2.98
</p>
<p>Table 2: Grammaticality scores by 1-4 scale regression model
(Heilman et al., 2014). The first row shows the number of
sentences that are made (at least one) change. Bold numbers
show statistically significant improvements.
</p>
<p>error injection rates (from 0% to 20%). It is im-
portant to note that the number of tokens between
the parser output and the oracle may be differ-
ent because of error injection into the test set and
ActsER during parsing. To handle this mismatch,
we evaluate the dependency accuracy with align-
ment (Favre et al., 2010) in the spirit of SParseval
(Roark et al., 2006), where tokens between a hy-
pothesis and oracle are aligned prior to calculating
the dependency accuracy.
</p>
<p>In the second experiment, we use the Treebank
of Learner English (TLE) (Berzak et al., 2016) to
see the grammaticality improvement in a real sce-
nario. TLE contains 5,124 sentences and 2.69 (std
1.9) token errors per sentence. The average sen-
tence length is 19.06 (std 9.47). TLE also pro-
vides dependency labels and POS tags on the raw
(ungrammatical) sentences. It is important to note
that TLE has dependency annotation only for the
original ungrammatical sentences, and therefore
we do not compute the accuracy of dependency
parse in this experiment. Since the corpus size is
small, we train EREF (E05 to E20) on 100k sen-
tences from Annotated Gigaword (Napoles et al.,
2012) and used TLE as a test set. Spelling errors
are ignored because EREF can use the POS infor-
mation. Grammaticality is evaluated by a regres-
sion model (Heilman et al., 2014), which scores
grammaticality on the ordinal scale (from 1 to 4).
</p>
<p>Results Table 1 shows the result of unlabeled
dependency accuracy (UAS).2 As previously pre-
</p>
<p>2Technically, it is possible to train the model with learning
labels simultaneously (LAS), but there is a trade-off between
</p>
<p>Successful cases
I ’m looking forward to [-see-] {+seeing+} you next summer
I ’ve never [-approve-] {+approved+} his deal
There is not {+a+} possibility to travel
</p>
<p>Failure cases
I ’ve [-assisted-] {+assisting+} your new musical show
I am writing to complain [-about-] {+with+} somethings
I hope you liked {+the+} everything I said
</p>
<p>Table 3: Successful and failure examples by EREF. The edits
are represented by [-deletion-] and {+insertion+}. Adjacent
pairs of deletion and insertion are considered as substitution.
</p>
<p>sented (Foster, 2007; Cahill, 2015), our experi-
ment also shows that parser performance deterio-
rated as the error rate in the test corpus increased.
On the error-free test set (0%), the baseline (EF
pipeline) outperforms other EREF models; the ac-
curacy is lower when the parser is trained on nois-
ier data. The difference among the models be-
comes small when the test set has 10% error injec-
tion rate. As the rate increases further, the trend
of parser accuracy reverses. When the test set has
15% or higher noise, the E20 is the most accu-
rate parser. This trend is presented by the slope
of deterioration ∇ = accuracy20%−accuracy0%20 in Ta-
ble 1; a parser trained on noisier training data
shows smaller decline and more robustness.3 This
indicates that the EREF is more robust than the
vanilla EF on ungrammatical texts by jointly pars-
ing and correcting errors.
</p>
<p>Table 2 demonstrates the result of grammati-
cality improvement (1-4 scale) on the TLE cor-
pus, and Table 3 shows successful and failure
corrections by EREF. Minimally trained models
(E05 and E10) show little improvement in gram-
maticality because the models are too conser-
vative to make edits. The models with higher
error-injection rates (E15 and E20) achieve 0.1
to 0.3 improvements that are statistically signifi-
cant. There is still room to improve regarding the
amount of corrections. This is probably because
TLE contains a variety of errors (e.g., collocation,
punctuation) in addition to the five error types we
focus. To deal with other error types, we can ex-
tend EREF by adding more actions, although it in-
creases the search space.
</p>
<p>From a practical perspective, the level of un-
grammaticality should be realized ahead of time.
This is an issue to be addressed in future research.
search space and training time. The primary goal of this ex-
periment is to see if the EREF is able to detect and correct
grammatical errors.
</p>
<p>3Baseline model without preprocessing always underper-
formed the preprocessed baseline.
</p>
<p>192</p>
<p />
</div>
<div class="page"><p />
<p>4 Related Work
</p>
<p>Our work lies at the intersection of parsing non-
canonical texts and grammatical error correction.
</p>
<p>Joint dependency parsing and disfluency de-
tection has been pursued (Rasooli and Tetreault,
2013, 2014; Honnibal and Johnson, 2014; Wu
et al., 2015; Yoshikawa et al., 2016), where a
parser jointly parses and detects disfluency (e.g.,
reparandum and interregnum) for a given speech
utterance. Our work could be considered an exten-
sion via adding SUBSTITUTE and INSERT actions,
although we depend on easy-first non-directional
parsing framework instead of a left-to-right strat-
egy. Importantly, the DELETE action is easier to
handle than the SUBSTITUTE and INSERT actions,
because they bring us challenging issues about a
process of candidate word generation and avoiding
an infinite loop in derivation. We have addressed
these issues as explained in §2.
</p>
<p>In terms of the literature from grammatical
error correction, this work is closely related to
Dahlmeier and Ng (2012), where they show an er-
ror correction decoder with the easy-first strategy.
The decoder iteratively corrects the most probable
ungrammatical token by applying different classi-
fiers for each error type. The EREF parser also de-
pends on the easy-first strategy to find ungrammat-
ical index to be deleted, inserted, or substituted,
but it parses and corrects errors jointly whereas the
decoder is designed as a grammatical error correc-
tion framework rather than a parser.
</p>
<p>There is a line of work for parsing ungrammati-
cal sentences (e.g., web forum) by adapting an ex-
isting parsing scheme on domain specific annota-
tions (Petrov and McDonald, 2012; Cahill, 2015;
Berzak et al., 2016; Nagata and Sakaguchi, 2016).
Although we share an interest with respect to deal-
ing with ungrammatical sentences, EREF focuses
on the parsing scheme for repairing grammatical
errors instead of adapting a parser with a domain
specific annotation scheme.
</p>
<p>More broadly, our work can also be regarded
as one of the joint parsing and text normalization
tasks such as joint spelling correction and POS
tagging (Sakaguchi et al., 2012), word segmen-
tation and POS tagging (Kaji and Kitsuregawa,
2014; Qian et al., 2015).
</p>
<p>5 Conclusions
</p>
<p>We have presented an error-repair variant of the
non-directional easy-first dependency parser. We
</p>
<p>have introduced three new actions, SUBSTITUTE,
INSERT, and DELETE into the parser so that it
jointly parses and corrects grammatical errors in
a sentence. To address the issue of parsing incom-
pletion due to the new actions, we have proposed
simple constraints that keep track of editing his-
tory for each token and the total number of ed-
its during derivation. The experimental result has
demonstrated robustness of EREF parsers against
EF and grammaticality improvement. Our work is
positioned at the intersection of noisy text parsing
and grammatical error correction. The EREF is a
flexible formalism not only for grammatical error
correction but other tasks with jointly editing and
parsing a given sentence.
</p>
<p>Acknowledgments
</p>
<p>This work was supported in part by the JHU Hu-
man Language Technology Center of Excellence
(HLTCOE), and DARPA LORELEI. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
</p>
<p>References
Yevgeni Berzak, Jessica Kenney, Carolyn Spadine,
</p>
<p>Jing Xian Wang, Lucia Lam, Keiko Sophie Mori,
Sebastian Garza, and Boris Katz. 2016. Universal
dependencies for learner english. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers). As-
sociation for Computational Linguistics, pages 737–
746.
</p>
<p>Aoife Cahill. 2015. Parsing learner text: to shoehorn or
not to shoehorn. In Proceedings of The 9th Linguis-
tic Annotation Workshop. Association for Compu-
tational Linguistics, Denver, Colorado, USA, pages
144–147.
</p>
<p>Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1–8.
</p>
<p>Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics, Jeju Island,
Korea, pages 568–578.
</p>
<p>193</p>
<p />
</div>
<div class="page"><p />
<p>Benoit Favre, Bernd Bohnet, and Dilek Hakkani-Tür.
2010. Evaluation of semantic role labeling and de-
pendency parsing of automatic speech recognition
output. In 2010 IEEE International Conference
on Acoustics, Speech and Signal Processing. pages
5342–5345.
</p>
<p>Jennifer Foster. 2007. Treebanks gone bad. Interna-
tional Journal of Document Analysis and Recogni-
tion (IJDAR) 10(3):129–145.
</p>
<p>Jennifer Foster and Oistein Andersen. 2009. Gen-
errate: Generating errors for use in grammatical er-
ror detection. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications. Association for Computational
Linguistics, Boulder, Colorado, pages 82–90.
</p>
<p>Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
Los Angeles, California, pages 742–750.
</p>
<p>Yoav Goldberg and Joakim Nivre. 2013. Training de-
terministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics 1:403–414.
</p>
<p>Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation. Edinburgh, Scotland, United
Kingdom, pages 187–197.
</p>
<p>Michael Heilman, Aoife Cahill, Nitin Madnani,
Melissa Lopez, Matthew Mulholland, and Joel
Tetreault. 2014. Predicting grammaticality on an
ordinal scale. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Association for
Computational Linguistics, Baltimore, Maryland,
pages 174–180.
</p>
<p>Matthew Honnibal and Mark Johnson. 2014. Joint
incremental disfluency detection and dependency
parsing. Transactions of the Association for Com-
putational Linguistics 2:131–142.
</p>
<p>Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Accu-
rate word segmentation and pos tagging for japanese
microblogs: Corpus annotation and joint modeling
with lexical normalization. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP). Association for
Computational Linguistics, Doha, Qatar, pages 99–
109.
</p>
<p>Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2014. Automated grammatical
error detection for language learners. Synthesis lec-
tures on human language technologies 7(1):1–170.
</p>
<p>Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.
</p>
<p>Ryo Nagata and Keisuke Sakaguchi. 2016. Phrase
structure annotation and parsing for learner english.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics, Berlin, Germany, pages 1837–1847.
</p>
<p>Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction. Association for Computational Linguis-
tics, pages 95–100.
</p>
<p>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-
tian Hadiwinoto, and Joel Tetreault. 2013. The
conll-2013 shared task on grammatical error cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task. Association for Computational
Linguistics, Sofia, Bulgaria, pages 1–12.
</p>
<p>Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineer-
ing and Cognition Together. Association for Com-
putational Linguistics, Stroudsburg, PA, USA, In-
crementParsing ’04, pages 50–57.
</p>
<p>Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. Asso-
ciation for Computational Linguistics, Suntec, Sin-
gapore, pages 351–359.
</p>
<p>Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL) .
</p>
<p>Tao Qian, Yue Zhang, Meishan Zhang, Yafeng Ren,
and Donghong Ji. 2015. A transition-based model
for joint segmentation, pos-tagging and normaliza-
tion. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Lisbon,
Portugal, pages 1837–1846.
</p>
<p>Mohammad Sadegh Rasooli and Joel Tetreault. 2013.
Joint parsing and disfluency detection in linear time.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, Seattle, Wash-
ington, USA, pages 124–129.
</p>
<p>Mohammad Sadegh Rasooli and Joel Tetreault. 2014.
Non-monotonic parsing of fluent umm i mean dis-
fluent sentences. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
</p>
<p>194</p>
<p />
</div>
<div class="page"><p />
<p>Computational Linguistics, volume 2: Short Papers.
Association for Computational Linguistics, Gothen-
burg, Sweden, pages 48–53.
</p>
<p>Brian Roark, Mary Harper, Eugene Charniak, Bon-
nie Dorr, Mark Johnson, Jeremy Kahn, Yang Liu,
Mari Ostendorf, John Hale, Anna Krasnyanskaya,
Matthew Lease, Izhak Shafran, Matthew Snover,
Robin Stewart, and Lisa Yung. 2006. Sparseval:
Evaluation metrics for parsing speech. In Proceed-
ings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC’06). Euro-
pean Language Resources Association (ELRA).
</p>
<p>Alla Rozovskaya and Dan Roth. 2014. Building a
state-of-the-art grammatical error correction system.
Transactions of the Association for Computational
Linguistics 2:414–434.
</p>
<p>Keisuke Sakaguchi, Tomoya Mizumoto, Mamoru Ko-
machi, and Yuji Matsumoto. 2012. Joint English
spelling error correction and POS tagging for lan-
guage learners writing. In Proceedings of COLING
2012. The COLING 2012 Organizing Committee,
Mumbai, India, pages 2357–2374.
</p>
<p>Shuangzhi Wu, Dongdong Zhang, Ming Zhou, and
Tiejun Zhao. 2015. Efficient disfluency detection
with transition-based parsing. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 495–503.
</p>
<p>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In In Proceedings of IWPT . pages 195–206.
</p>
<p>Masashi Yoshikawa, Hiroyuki Shindo, and Yuji Mat-
sumoto. 2016. Joint transition-based dependency
parsing and disfluency detection for automatic
speech recognition texts. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Austin, Texas, pages 1036–1041.
</p>
<p>195</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 196–202
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2031
</p>
<p>Attention Strategies for Multi-Source Sequence-to-Sequence Learning
</p>
<p>Jindřich Libovický and Jindřich Helcl
Charles University, Faculty of Mathematics and Physics
</p>
<p>Institute of Formal and Applied Linguistics
Malostranské náměstı́ 25, 118 00 Prague, Czech Republic
{libovicky, helcl}@ufal.mff.cuni.cz
</p>
<p>Abstract
</p>
<p>Modeling attention in neural multi-source
sequence-to-sequence learning remains a
relatively unexplored area, despite its use-
fulness in tasks that incorporate multiple
source languages or modalities. We pro-
pose two novel approaches to combine the
outputs of attention mechanisms over each
source sequence, flat and hierarchical. We
compare the proposed methods with exist-
ing techniques and present results of sys-
tematic evaluation of those methods on the
WMT16 Multimodal Translation and Au-
tomatic Post-editing tasks. We show that
the proposed methods achieve competitive
results on both tasks.
</p>
<p>1 Introduction
</p>
<p>Sequence-to-sequence (S2S) learning with atten-
tion mechanism recently became the most suc-
cessful paradigm with state-of-the-art results in
machine translation (MT) (Bahdanau et al., 2014;
Sennrich et al., 2016a), image captioning (Xu
et al., 2015; Lu et al., 2016), text summariza-
tion (Rush et al., 2015) and other NLP tasks.
</p>
<p>All of the above applications of S2S learning
make use of a single encoder. Depending on the
modality, it can be either a recurrent neural net-
work (RNN) for textual input data, or a convolu-
tional network for images.
</p>
<p>In this work, we focus on a special case of S2S
learning with multiple input sequences of possibly
different modalities and a single output-generating
recurrent decoder. We explore various strategies
the decoder can employ to attend to the hidden
states of the individual encoders.
</p>
<p>The existing approaches to this problem do not
explicitly model different importance of the inputs
to the decoder (Firat et al., 2016; Zoph and Knight,
</p>
<p>2016). In multimodal MT (MMT), where an im-
age and its caption are on the input, we might ex-
pect the caption to be the primary source of in-
formation, whereas the image itself would only
play a role in output disambiguation. In automatic
post-editing (APE), where a sentence in a source
language and its automatically generated transla-
tion are on the input, we might want to attend to
the source text only in case the model decides that
there is an error in the translation.
</p>
<p>We propose two interpretable attention strate-
gies that take into account the roles of the indi-
vidual source sequences explicitly—flat and hier-
archical attention combination.
</p>
<p>This paper is organized as follows: In Sec-
tion 2, we review the attention mechanism in
single-source S2S learning. Section 3 introduces
new attention combination strategies. In Section 4,
we evaluate the proposed models on the MMT and
APE tasks. We summarize the related work in Sec-
tion 5, and conclude in Section 6.
</p>
<p>2 Attentive S2S Learning
</p>
<p>The attention mechanism in S2S learning allows
an RNN decoder to directly access information
about the input each time before it emits a sym-
bol. Inspired by content-based addressing in Neu-
ral Turing Machines (Graves et al., 2014), the at-
tention mechanism estimates a probability distri-
bution over the encoder hidden states in each de-
coding step. This distribution is used for comput-
ing the context vector—the weighted average of
the encoder hidden states—as an additional input
to the decoder.
</p>
<p>The standard attention model as described
by Bahdanau et al. (2014) defines the attention en-
ergies eij , attention distribution αij , and the con-
</p>
<p>196</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2031">https://doi.org/10.18653/v1/P17-2031</a></div>
</div>
<div class="page"><p />
<p>text vector ci in i-th decoder step as:
</p>
<p>eij = v
&gt;
a tanh(Wasi + Uahj), (1)
</p>
<p>αij =
exp(eij)∑Tx
k=1 exp(eik)
</p>
<p>, (2)
</p>
<p>ci =
</p>
<p>Tx∑
</p>
<p>j=1
</p>
<p>αijhj . (3)
</p>
<p>The trainable parameters Wa and Ua are projec-
tion matrices that transform the decoder and en-
coder states si and hj into a common vector space
and va is a weight vector over the dimensions of
this space. Tx denotes the length of the input se-
quence. For the sake of clarity, bias terms (applied
every time a vector is linearly projected using a
weight matrix) are omitted.
</p>
<p>Recently, Lu et al. (2016) introduced sentinel
gate, an extension of the attentive RNN decoder
with LSTM units (Hochreiter and Schmidhuber,
1997). We adapt the extension for gated recurrent
units (GRU) (Cho et al., 2014), which we use in
our experiments:
</p>
<p>ψi = σ(Wyyi +Wssi−1) (4)
</p>
<p>where Wy and Ws are trainable parameters, yi is
the embedded decoder input, and si−1 is the pre-
vious decoder state.
</p>
<p>Analogically to Equation 1, we compute a
scalar energy term for the sentinel:
</p>
<p>eψi = v
&gt;
a tanh
</p>
<p>(
Wasi + U
</p>
<p>(ψ)
a (ψi � si)
</p>
<p>)
(5)
</p>
<p>where Wa, U
(ψ)
a are the projection matrices, va is
</p>
<p>the weight vector, and ψi�si is the sentinel vector.
Note that the sentinel energy term does not depend
on any hidden state of any encoder. The sentinel
vector is projected to the same vector space as the
encoder state hj in Equation 1. The term eψi is
added as an extra attention energy term to Equa-
tion 2 and the sentinel vector ψi� si is used as the
corresponding vector in the summation in Equa-
tion 3.
</p>
<p>This technique should allow the decoder to
choose whether to attend to the encoder or to fo-
cus on its own state and act more like a language
model. This can be beneficial if the encoder does
not contain much relevant information for the cur-
rent decoding step.
</p>
<p>3 Attention Combination
</p>
<p>In S2S models with multiple encoders, the decoder
needs to be able to combine the attention informa-
tion collected from the encoders.
</p>
<p>A widely adopted technique for combining mul-
tiple attention models in a decoder is concatena-
tion of the context vectors c(1)i , . . . , c
</p>
<p>(N)
i (Zoph
</p>
<p>and Knight, 2016; Firat et al., 2016). As men-
tioned in Section 1, this setting forces the model to
attend to each encoder independently and lets the
attention combination to be resolved implicitly in
the subsequent network layers.
</p>
<p>In this section, we propose two alternative
strategies of combining attentions from multiple
encoders. We either let the decoder learn the αi
distribution jointly over all encoder hidden states
(flat attention combination) or factorize the dis-
tribution over individual encoders (hierarchical
combination).
</p>
<p>Both of the alternatives allow us to explicitly
compute distribution over the encoders and thus
interpret how much attention is paid to each en-
coder at every decoding step.
</p>
<p>3.1 Flat Attention Combination
</p>
<p>Flat attention combination projects the hidden
states of all encoders into a shared space and then
computes an arbitrary distribution over the projec-
tions. The difference between the concatenation of
the context vectors and the flat attention combina-
tion is that the αi coefficients are computed jointly
for all encoders:
</p>
<p>α
(k)
ij =
</p>
<p>exp(e
(k)
ij )
</p>
<p>∑N
n=1
</p>
<p>∑T (n)x
m=1 exp
</p>
<p>(
e
(n)
im
</p>
<p>) (6)
</p>
<p>where T (n)x is the length of the input sequence of
the n-th encoder and e(k)ij is the attention energy
of the j-th state of the k-th encoder in the i-th
decoding step. These attention energies are com-
puted as in Equation 1. The parameters va andWa
are shared among the encoders, and Ua is different
for each encoder and serves as an encoder-specific
projection of hidden states into a common vector
space.
</p>
<p>The states of the individual encoders occupy
different vector spaces and can have a different di-
mensionality, therefore the context vector cannot
be computed as their weighted sum. We project
</p>
<p>197</p>
<p />
</div>
<div class="page"><p />
<p>them into a single space using linear projections:
</p>
<p>ci =
N∑
</p>
<p>k=1
</p>
<p>T
(k)
x∑
</p>
<p>j=1
</p>
<p>α
(k)
ij U
</p>
<p>(k)
c h
</p>
<p>(k)
j (7)
</p>
<p>where U (k)c are additional trainable parameters.
The matrices U (k)c project the hidden states into
</p>
<p>a common vector space. This raises a question
whether this space can be the same as the one
that is projected into in the energy computation
using matrices U (k)a in Equation 1, i.e., whether
U
</p>
<p>(k)
c = U
</p>
<p>(k)
a . In our experiments, we explore both
</p>
<p>options. We also try both adding and not adding
the sentinel α(ψ)i U
</p>
<p>(ψ)
c (ψi � si) to the context vec-
</p>
<p>tor.
</p>
<p>3.2 Hierarchical Attention Combination
The hierarchical attention combination model
computes every context vector independently,
similarly to the concatenation approach. Instead
of concatenation, a second attention mechanism is
constructed over the context vectors.
</p>
<p>We divide the computation of the attention dis-
tribution into two steps: First, we compute the
context vector for each encoder independently us-
ing Equation 3. Second, we project the context
vectors (and optionally the sentinel) into a com-
mon space (Equation 8), we compute another dis-
tribution over the projected context vectors (Equa-
tion 9) and their corresponding weighted average
(Equation 10):
</p>
<p>e
(k)
i = v
</p>
<p>&gt;
b tanh(Wbsi + U
</p>
<p>(k)
b c
</p>
<p>(k)
i ), (8)
</p>
<p>β
(k)
i =
</p>
<p>exp(e
(k)
i )∑N
</p>
<p>n=1 exp(e
(n)
i )
</p>
<p>, (9)
</p>
<p>ci =
N∑
</p>
<p>k=1
</p>
<p>β
(k)
i U
</p>
<p>(k)
c c
</p>
<p>(k)
i (10)
</p>
<p>where c(k)i is the context vector of the k-th en-
coder, additional trainable parameters vb and Wb
are shared for all encoders, and U (k)b and U
</p>
<p>(k)
c are
</p>
<p>encoder-specific projection matrices, that can be
set equal and shared, similarly to the case of flat
attention combination.
</p>
<p>4 Experiments
</p>
<p>We evaluate the attention combination strategies
presented in Section 3 on the tasks of multi-
modal translation (Section 4.1) and automatic
post-editing (Section 4.2).
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>30
</p>
<p>35
</p>
<p>0 10 20 30 40 50
</p>
<p>B
L
</p>
<p>E
U
</p>
<p>Epochs
</p>
<p>concatenation
flat
</p>
<p>hierarchical
</p>
<p>Figure 1: Learning curves on validation data for
context vector concatenation (blue), flat (green)
and hierarchical (red) attention combination with-
out sentinel and without sharing the projection ma-
trices.
</p>
<p>The models were implemented using the
Neural Monkey sequence-to-sequence learning
toolkit (Helcl and Libovický, 2017).12 In both
setups, we process the textual input with bidirec-
tional GRU network (Cho et al., 2014) with 300
units in the hidden state in each direction and 300
units in embeddings. For the attention projection
space, we use 500 hidden units. We optimize the
network to minimize the output cross-entropy us-
ing the Adam algorithm (Kingma and Ba, 2014)
with learning rate 10−4.
</p>
<p>4.1 Multimodal Translation
</p>
<p>The goal of multimodal translation (Specia et al.,
2016) is to generate target-language image cap-
tions given both the image and its caption in the
source language.
</p>
<p>We train and evaluate the model on the
Multi30k dataset (Elliott et al., 2016). It con-
sists of 29,000 training instances (images together
with English captions and their German trans-
lations), 1,014 validation instances, and 1,000
test instances. The results are evaluated us-
ing the BLEU (Papineni et al., 2002) and ME-
TEOR (Denkowski and Lavie, 2011).
</p>
<p>In our model, the visual input is processed with
a pre-trained VGG 16 network (Simonyan and Zis-
serman, 2014) without further fine-tuning. Atten-
</p>
<p>1http://github.com/ufal/neuralmonkey
2The trained models can be downloaded from
</p>
<p>http://ufallab.ms.mff.cuni.cz/
˜libovicky/acl2017_att_models/
</p>
<p>198</p>
<p />
</div>
<div class="page"><p />
<p>tion distribution over the visual input is computed
from the last convolutional layer of the network.
The decoder is an RNN with 500 conditional GRU
units (Firat and Cho, 2016) in the recurrent layer.
We use byte-pair encoding (Sennrich et al., 2016b)
with a vocabulary of 20,000 subword units shared
between the textual encoder and the decoder.
</p>
<p>The results of our experiments in multimodal
MT are shown in Table 1. We achieved the best
results using the hierarchical attention combina-
tion without the sentinel mechanism, which also
showed the fastest convergence. The flat com-
bination strategy achieves similar results eventu-
ally. Sharing the projections for energy and con-
text vector computation does not improve over the
concatenation baseline and slows the training al-
most prohibitively. Multimodal models were not
able to surpass the textual baseline (BLEU 33.0).
</p>
<p>Using the conditional GRU units brought an
improvement of about 1.5 BLEU points on aver-
age, with the exception of the concatenation sce-
nario where the performance dropped by almost 5
BLEU points. We hypothesize this is caused by
the fact the model has to learn the implicit atten-
tion combination on multiple places – once in the
output projection and three times inside the con-
ditional GRU unit (Firat and Cho, 2016, Equa-
tions 10-12). We thus report the scores of the in-
troduced attention combination techniques trained
with conditional GRU units and compare them
with the concatenation baseline trained with plain
GRU units.
</p>
<p>4.2 Automatic MT Post-editing
</p>
<p>Automatic post-editing is a task of improving
an automatically generated translation given the
source sentence where the translation system is
treated as a black box.
</p>
<p>We used the data from the WMT16 APE
Task (Bojar et al., 2016; Turchi et al., 2016), which
consists of 12,000 training, 2,000 validation, and
1,000 test sentence triplets from the IT domain.
Each triplet contains an English source sentence,
an automatically generated German translation of
the source sentence, and a manually post-edited
German sentence as a reference. In case of this
dataset, the MT outputs are almost perfect in and
only little effort was required to post-edit the sen-
tences. The results are evaluated using the human-
targeted error rate (HTER) (Snover et al., 2006)
and BLEU score (Papineni et al., 2002).
</p>
<p>sh
ar
</p>
<p>e
</p>
<p>se
nt
</p>
<p>. MMT APE
BLEU METEOR BLEU HTER
</p>
<p>concat. 31.4 ± .8 48.0 ± .7 62.3 ± .5 24.4 ± .4
</p>
<p>fla
t
</p>
<p>× × 30.2 ± .8 46.5 ± .7 62.6 ± .5 24.2 ± .4
× X 29.3 ± .8 45.4 ± .7 62.3 ± .5 24.3 ± .4
X × 30.9 ± .8 47.1 ± .7 62.4 ± .6 24.4 ± .4
X X 29.4 ± .8 46.9 ± .7 62.5 ± .6 24.2 ± .4
</p>
<p>hi
er
</p>
<p>ar
ch
</p>
<p>ic
al × × 32.1 ± .8 49.1 ± .7 62.3 ± .5 24.1 ± .4
</p>
<p>× X 28.1 ± .8 45.5 ± .7 62.6 ± .6 24.1 ± .4
X × 26.1 ± .7 42.4 ± .7 62.4 ± .5 24.3 ± .4
X X 22.0 ± .7 38.5 ± .6 62.5 ± .5 24.1 ± .4
</p>
<p>Table 1: Results of our experiments on the test sets
of Multi30k dataset and the APE dataset. The col-
umn ‘share’ denotes whether the projection matrix
is shared for energies and context vector computa-
tion, ‘sent.’ indicates whether the sentinel vector
has been used or not.
</p>
<p>Following Libovický et al. (2016), we encode
the target sentence as a sequence of edit operations
transforming the MT output into the reference. By
this technique, we prevent the model from para-
phrasing the input sentences. The decoder is a
GRU network with 300 hidden units. Unlike in
the MMT setup (Section 4.1), we do not use the
conditional GRU because it is prone to overfitting
on the small dataset we work with.
</p>
<p>The models were able to slightly, but signifi-
cantly improve over the baseline – leaving the MT
output as is (HTER 24.8). The differences be-
tween the attention combination strategies are not
significant.
</p>
<p>5 Related Work
</p>
<p>Attempts to use S2S models for APE are relatively
rare (Bojar et al., 2016). Niehues et al. (2016) con-
catenate both inputs into one long sequence, which
forces the encoder to be able to work with both
source and target language. Their attention is then
similar to our flat combination strategy; however,
it can only be used for sequential data.
</p>
<p>The best system from the WMT’16 competi-
tion (Junczys-Dowmunt and Grundkiewicz, 2016)
trains two separate S2S models, one translating
from MT output to post-edited targets and the
second one from source sentences to post-edited
targets. The decoders average their output dis-
tributions similarly to decoder ensembling. The
biggest source of improvement in this state-of-the-
art posteditor came from additional training data
generation, rather than from changes in the net-
work architecture.
</p>
<p>199</p>
<p />
</div>
<div class="page"><p />
<p>Source: a man sleeping in a green room on a
couch .
Reference: ein Mann schläft in einem grünen
Raum auf einem Sofa .
Output with attention:
</p>
<p>e
i
n
</p>
<p>M
a
n
n
</p>
<p>s
c
h
l
ä
f
t
</p>
<p>a
u
f
</p>
<p>e
i
n
e
m
</p>
<p>g
r
ü
n
e
n
</p>
<p>S
o
f
a
</p>
<p>i
n
e
i
n
e
m
</p>
<p>g
r
ü
n
e
n
</p>
<p>R
a
u
m
</p>
<p>.
</p>
<p>(1)
(2)
(3)
</p>
<p>(1) source, (2) image, (3) sentinel
</p>
<p>Figure 2: Visualization of hierarchical attention in
MMT. Each column in the diagram corresponds
to the weights of the encoders and sentinel. Note
that the despite the overall low importance of the
image encoder, it gets activated for the content
words.
</p>
<p>Caglayan et al. (2016) used an architecture very
similar to ours for multimodal translation. They
made a strong assumption that the network can be
trained in such a way that the hidden states of the
encoder and the convolutional network occupy the
same vector space and thus sum the context vec-
tors from both modalities. In this way, their mul-
timodal MT system (BLEU 27.82) remained far
bellow the text-only setup (BLEU 32.50).
</p>
<p>New state-of-the-art results on the Multi30k
dataset were achieved very recently by Calixto
et al. (2017). The best-performing architecture
uses the last fully-connected layer of VGG-19
network (Simonyan and Zisserman, 2014) as de-
coder initialization and only attends to the text en-
coder hidden states. With a stronger monomodal
baseline (BLEU 33.7), their multimodal model
achieved a BLEU score of 37.1. Similarly to
Niehues et al. (2016) in the APE task, even fur-
ther improvement was achieved by synthetically
extending the dataset.
</p>
<p>6 Conclusions
</p>
<p>We introduced two new strategies of combining
attention in a multi-source sequence-to-sequence
setup. Both methods are based on computing
a joint distribution over hidden states of all en-
coders.
</p>
<p>We conducted experiments with the proposed
strategies on multimodal translation and automatic
post-editing tasks, and we showed that the flat and
hierarchical attention combination can be applied
to these tasks with maintaining competitive score
to previously used techniques.
</p>
<p>Unlike the simple context vector concatenation,
the introduced combination strategies can be used
with the conditional GRU units in the decoder. On
top of that, the hierarchical combination strategy
exhibits faster learning than than the other strate-
gies.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank Ondřej Dušek, Rudolf
Rosa, Pavel Pecina, and Ondřej Bojar for a fruit-
ful discussions and comments on the draft of the
paper.
</p>
<p>This research has been funded by the Czech
Science Foundation grant no. P103/12/G084, the
EU grant no. H2020-ICT-2014-1-645452 (QT21),
and Charles University grant no. 52315/2014 and
SVV project no. 260 453. This work has been
using language resources developed and/or stored
and/or distributed by the LINDAT-Clarin project
of the Ministry of Education of the Czech Repub-
lic (project LM2010013).
</p>
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
</p>
<p>Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473. http://arxiv.org/abs/1409.0473.
</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Yepes, Philipp Koehn, Varvara Logacheva,
Christof Monz, Matteo Negri, Aurelie Névéol, Mar-
iana Neves, Martin Popel, Matt Post, Raphael Ru-
bino, Carolina Scarton, Lucia Specia, Marco Turchi,
Karin Verspoor, and Marcos Zampieri. 2016. Find-
ings of the 2016 conference on machine transla-
tion (WMT16). In Proceedings of the First Con-
ference on Machine Translation (WMT). Volume 2:
Shared Task Papers. Association for Computational
Linguistics, Association for Computational Linguis-
tics, Stroudsburg, PA, USA, volume 2, pages 131–
198.
</p>
<p>200</p>
<p />
</div>
<div class="page"><p />
<p>Ozan Caglayan, Walid Aransa, Yaxing Wang,
Marc Masana, Mercedes Garcı́a-Martı́nez, Fethi
Bougares, Loı̈c Barrault, and Joost van de Wei-
jer. 2016. Does multimodality help human and
machine for translation and image captioning?
In Proceedings of the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 627–633.
http://www.aclweb.org/anthology/W16-2358.
</p>
<p>Iacer Calixto, Qun Liu, and Nick Campbell.
2017. Incorporating global visual features into
attention-based neural machine translation. CoRR
abs/1701.06521. http://arxiv.org/abs/1701.06521.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the proper-
ties of neural machine translation: Encoder–decoder
approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in
Statistical Translation. Association for Computa-
tional Linguistics, Doha, Qatar, pages 103–111.
http://www.aclweb.org/anthology/W14-4012.
</p>
<p>Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation. Association for Computational
Linguistics, Edinburgh, United Kingdom, pages 85–
91. http://www.aclweb.org/anthology/W11-2107.
</p>
<p>Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-
cia Specia. 2016. Multi30k: Multilingual english-
german image descriptions. CoRR abs/1605.00459.
http://arxiv.org/abs/1605.00459.
</p>
<p>Orhan Firat and Kyunghyun Cho. 2016. Con-
ditional gated recurrent unit with attention
mechanism. https://github.com/nyu-dl/dl4mt-
tutorial/blob/master/docs/cgru.pdf. Published
online, version adbaeea.
</p>
<p>Orhan Firat, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. Multi-way, multilingual neural ma-
chine translation with a shared attention mecha-
nism. In Proceedings of the 2016 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies. Association for Computational Lin-
guistics, San Diego, CA, USA, pages 866–875.
http://www.aclweb.org/anthology/N16-1101.
</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
Neural turing machines. CoRR abs/1410.5401.
http://arxiv.org/abs/1410.5401.
</p>
<p>Jindřich Helcl and Jindřich Libovický. 2017. Neural
monkey: An open-source tool for sequence learn-
ing. The Prague Bulletin of Mathematical Lin-
guistics (107):5–17. https://doi.org/10.1515/pralin-
2017-0001.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9:1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735.
</p>
<p>Marcin Junczys-Dowmunt and Roman Grund-
kiewicz. 2016. Log-linear combinations of
monolingual and bilingual neural machine trans-
lation models for automatic post-editing. In
Proceedings of the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 751–758.
http://www.aclweb.org/anthology/W/W16/W16-
2378.
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.
</p>
<p>Jindřich Libovický, Jindřich Helcl, Marek Tlustý,
Ondřej Bojar, and Pavel Pecina. 2016. CUNI
system for WMT16 automatic post-editing
and multimodal translation tasks. In Pro-
ceedings of the First Conference on Machine
Translation. Association for Computational
Linguistics, Berlin, Germany, pages 646–654.
http://www.aclweb.org/anthology/W/W16/W16-
2361.
</p>
<p>Jiasen Lu, Caiming Xiong, Devi Parikh, and
Richard Socher. 2016. Knowing when to
look: Adaptive attention via a visual sentinel
for image captioning. CoRR abs/1612.01887.
http://arxiv.org/abs/1612.01887.
</p>
<p>Jan Niehues, Eunah Cho, Thanh-Le Ha, and
Alex Waibel. 2016. Pre-translation for neu-
ral machine translation. CoRR abs/1610.05243.
http://arxiv.org/abs/1610.05243.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-Jing Zhu. 2002. Bleu: a method
for automatic evaluation of machine transla-
tion. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA, pages 311–318.
https://doi.org/10.3115/1073083.1073135.
</p>
<p>Alexander M. Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for abstrac-
tive sentence summarization. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Lisbon, Portugal, pages 379–389.
https://aclweb.org/anthology/D/D15/D15-1044.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016a. Edinburgh neural machine
translation systems for WMT 16. In Pro-
ceedings of the First Conference on Machine
Translation. Association for Computational
Linguistics, Berlin, Germany, pages 371–376.
http://www.aclweb.org/anthology/W/W16/W16-
2323.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016b. Neural machine translation of
rare words with subword units. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
</p>
<p>201</p>
<p />
</div>
<div class="page"><p />
<p>1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1715–1725.
http://www.aclweb.org/anthology/P16-1162.
</p>
<p>Karen Simonyan and Andrew Zisserman. 2014.
Very deep convolutional networks for large-
scale image recognition. CoRR abs/1409.1556.
http://arxiv.org/abs/1409.1556.
</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas. volume 200.
</p>
<p>Lucia Specia, Stella Frank, Khalil Sima’an, and
Desmond Elliott. 2016. A shared task on multi-
modal machine translation and crosslingual image
description. In Proceedings of the First Conference
on Machine Translation. Association for Computa-
tional Linguistics, Berlin, Germany, pages 543–553.
http://www.aclweb.org/anthology/W16-2346.
</p>
<p>Marco Turchi, Rajen Chatterjee, and Matteo Negri.
2016. WMT16 APE shared task data. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics, Charles University in
Prague. http://hdl.handle.net/11372/LRT-1632.
</p>
<p>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. 2015. Show, at-
tend and tell: Neural image caption gener-
ation with visual attention. In David Blei
and Francis Bach, editors, Proceedings of the
32nd International Conference on Machine Learn-
ing (ICML-15). JMLR Workshop and Confer-
ence Proceedings, Lille, France, pages 2048–2057.
http://jmlr.org/proceedings/papers/v37/xuc15.pdf.
</p>
<p>Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, CA, USA, pages 30–34.
http://www.aclweb.org/anthology/N16-1004.
</p>
<p>202</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 203–208
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2032
</p>
<p>Understanding and Detecting Supporting Arguments of Diverse Types
</p>
<p>Xinyu Hua and Lu Wang
College of Computer and Information Science
</p>
<p>Northeastern University
Boston, MA 02115
</p>
<p>hua.x@husky.neu.edu luwang@ccs.neu.edu
</p>
<p>Abstract
</p>
<p>We investigate the problem of sentence-level
supporting argument detection from relevant
documents for user-specified claims. A dataset
containing claims and associated citation ar-
ticles is collected from online debate web-
site idebate.org. We then manually label
sentence-level supporting arguments from the
documents along with their types as STUDY,
FACTUAL, OPINION, or REASONING. We fur-
ther characterize arguments of different types,
and explore whether leveraging type informa-
tion can facilitate the supporting arguments
detection task. Experimental results show
that LambdaMART (Burges, 2010) ranker that
uses features informed by argument types
yields better performance than the same ranker
trained without type information.
</p>
<p>1 Introduction
</p>
<p>Argumentation plays a crucial role in persuasion
and decision-making processes. An argument usu-
ally consists of a central claim (or conclusion) and
several supporting premises. Constructing argu-
ments of high quality would require the inclusion
of diverse information, such as factual evidence
and solid reasoning (Rieke et al., 1997; Park and
Cardie, 2014). For instance, as shown in Figure 1,
the editor on idebate.org – a Wikipedia-style
website for gathering pro and con arguments on
controversial issues, utilizes arguments based on
study, factual evidence, and expert opinion to sup-
port the anti-gun claim “legally owned guns are
frequently stolen and used by criminals”. How-
ever, it would require substantial human effort to
collect information from diverse resources to sup-
port argument construction. In order to facilitate
this process, there is a pressing need for tools that
can automatically detect supporting arguments.
</p>
<p>To date, most of the argument mining research
focuses on recognizing argumentative components
</p>
<p>- A June 2013 IOM report states that “almost all guns
used in criminal acts enter circulation via initial legal
transaction”. [study]
- Between 2005 and 2010, 1.4 million guns were
stolen from US homes during property crimes (in-
cluding bulglary and car theft), a yearly average of
232,400. [factual]
- Ian Ayres, JD, PhD, . . . states, “with guns being a
product that can be easily carried away and quickly
sold at a relatively high fraction of the initial cost, the
presence of more guns can actually serve as a stimu-
lus to burglary and theft.” [expert opinion]
</p>
<p>Figure 1: Three different types of arguments used to
support the claim “Legally owned guns are frequently
stolen and used by criminals”.
</p>
<p>and their structures from constructed arguments
based on curated corpus (Mochales and Moens,
2011; Stab and Gurevych, 2014; Feng and Hirst,
2011; Habernal and Gurevych, 2015; Nguyen and
Litman, 2016). Limited work has been done for
retrieving supporting arguments from external re-
sources. Initial effort by Rinott et al. (2015) inves-
tigates the detection of relevant factual evidence
from Wikipedia articles. However, it is unclear
whether their method can perform well on docu-
ments of different genres (e.g. news articles vs.
blogs) for detecting distinct types of supporting in-
formation.
</p>
<p>In this work, we present a novel study on the
task of sentence-level supporting argument detec-
tion from relevant documents for a user-specified
claim. Take Figure 2 as an example: assume we
are given a claim on the topic of “banning cos-
metic surgery” and a relevant article (cited for
argument construction), we aim to automatically
pinpoint the sentence(s) (in italics) among all sen-
tences in the cited article that can be used to back
up the claim. We define such tasks as supporting
argument detection. Furthermore, another goal of
</p>
<p>203</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2032">https://doi.org/10.18653/v1/P17-2032</a></div>
</div>
<div class="page"><p />
<p>- Topic: This house would ban cosmetic surgery
- Claim: An outright ban would be easier than the
partial bans that have been enacted in some places.
- Human Constructed Argument: . . .This poten-
tially leaves difficulty drawing the line for what is al-
lowed.[1] . . .
Citation Article
[1]: “Australian State Ban Cosmetic Surgery for
Teens”
- . . . It is unfortunate that a parent would consider let-
ting a 16-year-old daughter have a breast augmenta-
tion.”
- But others worry that similar legislation, if it ever
comes to pass in the United States, would draw a
largely arbitrary line – and could needlessly restrict
some teens from procedures that would help their self-
esteem.
- Dr. Malcolm Z. Roth, director of plastic surgery at
Maimondes Medical Center in Brooklyn, N.Y. , said
he believes that some teens are intelligent and mature
enough to comprehend the risks and benefits of cos-
metic surgery.. . .
</p>
<p>Figure 2: A typical debate motion consists of a Topic,
Claims, and Human Constructed Arguments. Citation
article is marked at the end of sentence. Our goal is to
find out supporting argument (in italics) from citation
article that can back up the given claim.
</p>
<p>this work is to understand and characterize differ-
ent types of supporting arguments. Indeed, hu-
man editors do use different types of information
to promote persuasiveness as we will show in Sec-
tion 3. Prediction performance also varies among
different types of supporting arguments.
</p>
<p>Given that none of the existing datasets is suit-
able for our study, we collect and annotate a cor-
pus from Idebate, which contains hundreds of de-
bate topics and corresponding claims.1 As is
shown in Figure 2, each claim is supported with
some human constructed argument, with cited ar-
ticles marked on sentence level. After careful in-
spection on the supporting arguments, we propose
to label them as STUDY, FACTUAL, OPINION, or
REASONING. Substantial inter-annotator agree-
ment rate is achieved for both supporting argu-
ment labeling (with Cohen’s κ of 0.8) and argu-
ment type annotation, on 200 topics with 621 ref-
erence articles.
</p>
<p>Based on the new corpus, we first carry out
a study on characterizing arguments of different
types via type prediction. We find that arguments
</p>
<p>1The labeled dataset along with the annotation guideline
will be released at xyhua.me.
</p>
<p>of STUDY and FACTUAL tend to use more con-
crete words, while arguments of OPINION contain
more named entities of person names. We then in-
vestigate whether argument type can be leveraged
to assist supporting argument detection. Experi-
mental results based on LambdaMART (Burges,
2010) show that utilizing features composite with
argument types achieves a Mean Reciprocal Rank
(MRR) score of 57.65, which outperforms an un-
supervised baseline and the same ranker trained
without type information. Feature analysis also
demonstrates that salient features have signifi-
cantly different distribution over different argu-
ment types.
</p>
<p>For the rest of the paper, we summarize related
work in Section 2. The data collection and anno-
tation process is described in Section 3, which is
followed by argument type study (Section 4). Ex-
periment on supporting argument detection is pre-
sented in Section 5. We finally conclude in Sec-
tion 6.
</p>
<p>2 Related Work
</p>
<p>Our work is in line with argumentation min-
ing, which has recently attracted significant re-
search interest. Existing work focuses on argu-
ment extraction from news articles, legal docu-
ments, or online comments without given user-
specified claim (Moens et al., 2007; Palau and
Moens, 2009; Mochales and Moens, 2011; Park
and Cardie, 2014). Argument scheme classifi-
cation is also widely studied (Biran and Ram-
bow, 2011; Feng and Hirst, 2011; Rooney et al.,
2012; Stab and Gurevych, 2014; Al Khatib et al.,
2016), which emphasizes on distinguishing differ-
ent types of arguments. To the best of our knowl-
edge, none of them studies the interaction between
types of arguments and their usage to support a
user-specified claim. This is the gap we aim to fill.
</p>
<p>3 Data and Annotation
</p>
<p>We rely on data from idebate.org, where hu-
man editors construct paragraphs of arguments, ei-
ther supporting or opposing claims under contro-
versial topics. We also extract textual citation arti-
cles as source of information used by editors dur-
ing argument construction. In total we collected
383 unique debates, out of which 200 debates are
randomly selected for study. After removing in-
valid ones, our final dataset includes 450 claims
</p>
<p>204</p>
<p />
</div>
<div class="page"><p />
<p>STUDY: Results and discoveries, usually quantita-
tive, as a result of some research investment.
FACTUAL: Description of some occurred events or
facts, or chapters in law or declaration.
OPINION: Quotes from some person or group, ei-
ther direct or indirect. It usually contains subjective,
judgemental and evaluative languages, and might re-
flect the position or stance of some entity.
REASONING: Logical structures. It usually can be
further broken down into causal or conditional sub-
structures.
</p>
<p>Table 1: Annotation scheme for our dataset. Due to
space limit, we do not show detailed explanations and
examples.
</p>
<p>and 621 citation articles with about 53,000 sen-
tences.
Annotation Process. As shown in Figure 2, we
first annotate which sentence(s) from a citation
articles is used by the editor as supporting argu-
ments. Then we annotate the type for each of them
as STUDY, FACTUAL, OPINION, or REASONING,
based on the scheme in Table 1.2 For instance, the
highlighted supporting argument in Figure 2 is la-
beled as REASONING.
</p>
<p>Two experienced annotators were hired to iden-
tify supporting arguments by reading through the
whole cited article and locating the sentences that
best match the reference human constructed argu-
ment. This task is rather complicated since hu-
man do not just repeat or directly quote the origi-
nal sentences from citation articles, they also para-
phrase, summarize, and generalize. For instance,
the original sentence is “The global counterfeit
drug trade, a billion-dollar industry, is thriving in
Africa”, which is paraphrased to “This is exploited
by the billion dollar global counterfeit drug trade”
in human constructed argument.
</p>
<p>The annotators were asked to annotate indepen-
dently, then discuss and resolve disagreements and
give feedback about current scheme. We compute
inter-annotator agreement based on Cohen’s κ for
both supporting arguments labeling and argument
type annotation. For supporting arguments we
have a high degree of consensus, with Cohen’s κ
ranges from 0.76 to 0.83 in all rounds and 0.80
overall. For argument type annotation, we achieve
Cohen’s κ of 0.61 for STUDY, 0.75 for FACTUAL,
0.71 for OPINION, and 0.29 for REASONING3
</p>
<p>2We end up with the four-type scheme as a trade-off be-
tween complexity and its coverage of the arguments.
</p>
<p>3Many times annotators have different interpretation on
REASONING, and frequently label it as OPINION. This results
</p>
<p>study factual opinion reasoning0.00
</p>
<p>0.05
</p>
<p>0.10
</p>
<p>0.15
</p>
<p>0.20
</p>
<p>PERCENTAGE OF EACH DOMAIN TYPE
Organization
Scientific
</p>
<p>Blog Reference
</p>
<p>Figure 3: For each supporting argument type, from
left to right shows the percentage of domain names of
organizations, scientific, blog, and reference. We do
not display statistics for news, because news articles
take the same portion in all types (about 50%).
</p>
<p>Statistics. In total 995 sentences are identified as
supporting arguments. Among those, 95 (9.55%)
are labeled as STUDY, 497 (49.95%) as FACTUAL,
363 (36.48%) as OPINION, and 40 (4.02%) as
REASONING.
</p>
<p>We further analyze the source of the supporting
arguments. Domain names of the citation articles
are collected based on their URL, and then cat-
egorized into “news”, “organization”, “scientific”,
“blog”, “reference”, and others, according to a tax-
onomy provided by Alexa4 with a few edits to fit
our dataset. News articles are the major source
for all types, which account for roughly 50% for
each. We show the distribution of other four types
in Figure 3. Arguments of STUDY and REASON-
ING are mostly from “scientific” websites (14.9%
and 22.9%), whereas “organization” websites con-
tribute a large portion of arguments of FACTUAL
(18.5%) and OPINION (16.7%).
</p>
<p>4 A Study On Argument Type Prediction
</p>
<p>Here we characterize arguments of different types
based on diverse features under the task of predict-
ing argument types. Supporting arguments identi-
fied from previous section are utilized for experi-
ments. We also leverage the learned classifier in
this section to label the sentences that are not sup-
porting arguments, which will be used for support-
ing argument detection in the next section. Four
major types of features are considered.
Basic Features. We calculate frequencies of un-
igram and bigram words, number of four major
types of part-of-speech tags (verb, noun, adjective,
and adverb), number of dependency relations, and
</p>
<p>in a low agreement for REASONING.
4http://www.alexa.com/topsites/category
</p>
<p>205</p>
<p />
</div>
<div class="page"><p />
<p>Acc F1
Majority class 0.520 0.171
Random 0.240 0.199
Log-linear (ngrams) 0.535 0.277
Log-linear (all features) 0.622 0.436
</p>
<p>Table 2: Results for argument type prediction. One-
vs-rest classifiers are learned for Log-linear models.
</p>
<p>number of seven types of named entities (Chin-
chor and Robinson, 1997).
Sentiment Features. We also compute number
of positive, negative and neutral words in MPQA
lexicon (Wilson et al., 2005), and number of words
from a subset of semantic categories from General
Inquirer (Stone et al., 1966).5
</p>
<p>Discourse Features. We use the number of dis-
course connectives from the top two levels of Penn
Discourse Tree Bank (Prasad et al., 2007).
Style Features. We measure word attributes for
their concreteness (perceptible vs. conceptual),
valence (or pleasantness), arousal (or intensity of
emotion), and dominance (or degree of control)
based on the lexicons collected by Brysbaert et al.
(2014) and Warriner et al. (2013).
</p>
<p>We utilize Log-linear model for argument type
prediction with one-vs-rest setup. Three baselines
are considered: (1) random guess, (2) majority
class, and (3) unigrams and bigrams as features
for Log-linear model. Identified supporting argu-
ments are used for experiments, and divided into
training set (50%), validation set (25%) and test
set (25%). From Table 2, we can see that Log-
linear model trained with all features outperforms
the ones trained with ngram features. To further
characterize arguments of different types, we dis-
play sample features with significant different val-
ues in Figure 4. As can be seen, arguments of
STUDY and FACTUAL tend to contain more con-
crete words and named entities. Arguments of
OPINION mention more person names, which im-
plies that expert opinions are commonly quoted.
</p>
<p>5 Supporting Argument Detection
</p>
<p>We cast the sentence-level supporting argument
detection problem as a ranking task.6 Features
</p>
<p>5Categories used: Strong, Weak, Virtue, Vice, Ovrst
(Overstated), Undrst (Understated), Academ (Academic),
Doctrin (Doctrine), Econ (Economic), Relig (Religious),
Causal, Ought, and Perceiv (Perception).
</p>
<p>6Many sentences in the citation article is relevant to the
topic to various degrees. We focus on detecting the most rel-
evant ones, and thus treat it as a ranking problem instead of a
</p>
<p>study factual opinion reasoning
</p>
<p>CO
N
</p>
<p>CR
ET
</p>
<p>EN
ES
</p>
<p>S
</p>
<p>2.633 2.573
2.442 2.407
</p>
<p>study factual opinion reasoning
</p>
<p>CO
N
</p>
<p>D
IT
</p>
<p>IO
N
</p>
<p>(P
D
</p>
<p>TB
)
</p>
<p>0.037
</p>
<p>0.096 0.097
</p>
<p>0.167
</p>
<p>study factual opinion reasoning
</p>
<p>#
N
</p>
<p>AM
ED
</p>
<p> E
N
</p>
<p>TI
TY
</p>
<p>3.537
4.181
</p>
<p>2.725
</p>
<p>1.976
</p>
<p>study factual opinion reasoning
</p>
<p>#
PE
</p>
<p>RS
O
</p>
<p>N
(N
</p>
<p>E)
</p>
<p>0.120
</p>
<p>0.341
</p>
<p>0.516
</p>
<p>0.167
</p>
<p>study factual opinion reasoning
</p>
<p>CA
U
</p>
<p>SA
L(
</p>
<p>G
I)
</p>
<p>0.648
0.489
</p>
<p>0.702
</p>
<p>1.714
</p>
<p>study factual opinion reasoning
</p>
<p>#
PO
</p>
<p>SI
TI
</p>
<p>VE
 S
</p>
<p>EN
TI
</p>
<p>M
EN
</p>
<p>T
</p>
<p>1.102
1.190
</p>
<p>1.723 1.714
</p>
<p>Figure 4: Average features values for different ar-
gument types. Numbers in boldface are significantly
higher than the others based on paired t-test (p &lt; 0.05).
</p>
<p>in Section 4 are also utilized here as “Sentence
features” with additional features considering the
sentence position in the article. We further employ
features that measure similarity between claims
and sentences, and the composite features that
leverage argument type information.
</p>
<p>Similarity Features. We compute similarity be-
tween claim and candidate sentence based on TF-
IDF and average word embeddings. We also con-
sider ROUGE (Lin, 2004), a recall oriented met-
ric for summarization evaluation. In particular,
ROUGE-L, a variation based on longest common
subsequence, is computed by treating claim as
reference and each candidate sentence as sample
summary. In similar manner we use BLEU (Pap-
ineni et al., 2002), a precision oriented metric.
</p>
<p>Composite Features. We adopt composite fea-
tures to study the interaction of other features with
type of the sentence. Given claim c and sentence
s with any feature mentioned above, a composite
feature function φM(type, feature)(s, c) is set
to the actual feature value if and only if the
argument type matches. For instance, if the
ROUGE-L score is 0.2, and s is of type STUDY,
then φM(study, ROUGE)(s, c) = 0.2
φM(factual, ROUGE)(s, c), φM(opinion, ROUGE)(s, c),
</p>
<p>φM(reasoning, ROUGE)(s, c) are all set to 0.
</p>
<p>binary classification task.
</p>
<p>206</p>
<p />
</div>
<div class="page"><p />
<p>Feature set MRR NDCG
Baselines
TFIDF similarity 45.48 56.48
W2V similarity 47.65 59.00
Ngrams 27.26 43.83
Separate feature sets
Sentence (Sen) 55.38* 65.09*
Similarity (Simi) 43.13 55.16
Comp(type, Sen) + Comp(type, Simi) 55.75* 64.91*
Additive Feature Test
Sen + Ngrams + Simi 56.43* 65.79*
</p>
<p>+ Comp(type, Sen) + Comp(type, Simi) 57.65* 66.51*
+ Comp(type, Claim) 56.58* 65.68*
</p>
<p>Table 3: Supporting argument detection results.
Comp(type, Sen) stands for composite features of
argument type and sentence features, similarly for
Comp(type,Simi). Comp(type,Claim) represents com-
posite features of type and claim features. Results that
are statistically significantly better than all three base-
lines are marked with ∗ (paired t-test, p &lt; 0.05).
</p>
<p>We choose LambdaMART (Burges, 2010) for
experiments, which is shown to be successful for
many text ranking problems (Chapelle and Chang,
2011). Our model is evaluated by Mean Recipro-
cal Rank (MRR) and Normalized Discounted Cu-
mulative Gain (NDCG) using 5-fold cross valida-
tion. We compare to TFIDF and Word embedding
similarity baselines, and LambdaMART trained
with ngrams (unigrams and bigrams).
</p>
<p>Results in Table 3 show that using com-
posite features with argument type information
(Comp(type, Sen) + Comp(type, Simi)) can im-
prove the ranking performance. Specifically, the
best performance is achieved by adding composite
features to sentence features, similarity features,
and ngram features. As can be seen, supervised
methods outperform unsupervised baseline meth-
ods. And similarity features have similar perfor-
mance as those baselines. The best performance is
achieved by combination of sentence features, N-
grams, similarity, and two composite types, which
is boldfaced. Feature sets that significantly outper-
form all three baselines are marked with ∗.
</p>
<p>For feature analysis, we conduct t-test for in-
dividual feature values between supporting argu-
ments and the others. We breakdown features
according to their argument types and show top
salient composite features in Table 4. For all sen-
tences of type STUDY, relevant ones tend to con-
tain more “percentage” and more concrete words.
We also notice those sentences with more hedging
words are more likely to be considered. For sen-
tences of FACTUAL, position of sentence in article
</p>
<p>Feature STUDY FACTUAL OPINION REASONING
# PERC, NE ∗∗ ↑↑↑↑ – – –
# LOC, NE – ∗∗ ↑↑ – ∗∗ ↑
position
of sentence
</p>
<p>∗∗ ↓↓ ∗ ∗ ∗∗ ↓↓ – ∗∗∗∗ ↓↓↓↓
</p>
<p>concreteness
of sentence
</p>
<p>∗ ∗ ∗ ↑↑ – ∗∗ ↑↑ ∗ ∗ ∗ ↓
</p>
<p>arousal
of sentence
</p>
<p>∗ ∗ ∗ ↑↑ – ∗∗ ↑↑ ∗∗ ↓
</p>
<p># hedging
word
</p>
<p>∗∗ ↑↑↑ – – –
</p>
<p>ROUGE ∗ ∗ ∗↑↑ ∗ ∗ ∗ ↑ ∗∗ ↑↑ –
concreteness
of claim
</p>
<p>∗ ∗ ∗ ↑↑ – ∗∗ ↑↑ ∗ ∗ ∗ ↓
</p>
<p>arousal
of claim
</p>
<p>∗ ∗ ∗ ↑↑ – ∗∗ ↑↑ ∗ ∗ ∗ ↓
</p>
<p>Table 4: Comparison of feature significance under
composition with different types. The number of ∗
stands for the p-value based on t-test between support-
ing argument sentences and the others after Bonferroni
correction. From one ∗ to four, the p-value scales as:
0.05, 1e-3, 1e-5, and 1e-10. When mean value of sup-
porting argument sentences is larger, ↑ is used; other-
wise, ↓ is displayed. Number of arrows represents the
ratio of the larger value over smaller one. “-” indicates
no significant difference.
</p>
<p>plays an important role, as well as their similarity
to the claim based on ROUGE scores. For type
OPINION, unlike all other types, position of sen-
tence seems to be insignificant. As we could imag-
ine, opinionated information might scatter around
the whole documents. For sentences of REASON-
ING, the ones that can be used as supporting argu-
ments tend to be less concrete and less emotional,
as opposed to opinion.
</p>
<p>6 Conclusion
</p>
<p>We presented a novel study on the task of
sentence-level supporting argument detection
from relevant documents for a user-specified
claim. Based on our newly-collected dataset, we
characterized arguments of different types with a
rich feature set. We also showed that leveraging
argument type information can further improve the
performance of supporting argument detection.
</p>
<p>Acknowledgments
</p>
<p>This work was supported in part by National Sci-
ence Foundation Grant IIS-1566382 and a GPU
gift from Nvidia. We thank Kechen Qin for his
help on data collection. We also appreciate the
valuable suggestions on various aspects of this
work from three anonymous reviewers.
</p>
<p>207</p>
<p />
</div>
<div class="page"><p />
<p>References
Khalid Al Khatib, Henning Wachsmuth, Johannes
</p>
<p>Kiesel, Matthias Hagen, and Benno Stein. 2016.
A news editorial corpus for mining argumentation
strategies. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers. Osaka, Japan, pages
3433–3443.
</p>
<p>Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialogs by classifying text as argu-
mentative. International Journal of Semantic Com-
puting 5(04):363–381.
</p>
<p>Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2014. Concreteness ratings for 40 thousand
generally known english word lemmas. Behavior
research methods 46(3):904–911.
</p>
<p>Christopher JC Burges. 2010. From ranknet to lamb-
darank to lambdamart: An overview. Learning
11(23-581):81.
</p>
<p>Olivier Chapelle and Yi Chang. 2011. Yahoo! learning
to rank challenge overview. In Yahoo! Learning to
Rank Challenge. pages 1–24.
</p>
<p>Nancy Chinchor and Patricia Robinson. 1997. Muc-
7 named entity task definition. In Proceedings of
the 7th Conference on Message Understanding. vol-
ume 29.
</p>
<p>Vanessa Wei Feng and Graeme Hirst. 2011. Clas-
sifying arguments by scheme. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computa-
tional Linguistics, pages 987–996.
</p>
<p>Ivan Habernal and Iryna Gurevych. 2015. Exploit-
ing debate portals for semi-supervised argumenta-
tion mining in user-generated web discourse. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Lisbon, Portu-
gal, pages 2127–2137.
</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop. Barcelona, Spain, volume 8.
</p>
<p>Raquel Mochales and Marie-Francine Moens. 2011.
Argumentation mining. Artificial Intelligence and
Law 19(1):1–22.
</p>
<p>Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
of arguments in legal texts. In Proceedings of the
11th international conference on Artificial intelli-
gence and law. ACM, pages 225–230.
</p>
<p>Huy Nguyen and Diane Litman. 2016. Context-aware
argumentative relation mining. In Proceedings of
</p>
<p>the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).
Association for Computational Linguistics, Berlin,
Germany, pages 1127–1137.
</p>
<p>Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the 12th international conference on ar-
tificial intelligence and law. ACM, pages 98–107.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.
</p>
<p>Joonsuk Park and Claire Cardie. 2014. Identifying
appropriate support for propositions in online user
comments. In Proceedings of the First Workshop on
Argumentation Mining. pages 29–38.
</p>
<p>Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie L
Webber. 2007. The penn discourse treebank 2.0 an-
notation manual .
</p>
<p>Richard D Rieke, Malcolm Osgood Sillars, and
Tarla Rai Peterson. 1997. Argumentation and crit-
ical decision making. New York: Longman.
</p>
<p>Ruty Rinott, Lena Dankin, Carlos Alzate Perez,
Mitesh M Khapra, Ehud Aharoni, and Noam
Slonim. 2015. Show me your evidence-an automatic
method for context dependent evidence detection. In
EMNLP. pages 440–450.
</p>
<p>Niall Rooney, Hui Wang, and Fiona Browne. 2012.
Applying kernel methods to argumentation mining.
In Twenty-Fifth International FLAIRS Conference.
</p>
<p>Christian Stab and Iryna Gurevych. 2014. Identifying
argumentative discourse structures in persuasive es-
says. In EMNLP. pages 46–56.
</p>
<p>Philip J Stone, Dexter C Dunphy, and Marshall S
Smith. 1966. The general inquirer: A computer ap-
proach to content analysis. .
</p>
<p>Amy Beth Warriner, Victor Kuperman, and Marc Brys-
baert. 2013. Norms of valence, arousal, and dom-
inance for 13,915 english lemmas. Behavior re-
search methods 45(4):1191–1207.
</p>
<p>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing. Associ-
ation for Computational Linguistics, pages 347–354.
</p>
<p>208</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 209–216
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2033
</p>
<p>A Neural Model for User Geolocation and Lexical Dialectology
</p>
<p>Afshin Rahimi Trevor Cohn Timothy Baldwin
School of Computing and Information Systems
</p>
<p>The University of Melbourne
arahimi@student.unimelb.edu.au
{t.cohn,tbaldwin}@unimelb.edu.au
</p>
<p>Abstract
</p>
<p>We propose a simple yet effective text-
based user geolocation model based on
a neural network with one hidden layer,
which achieves state of the art performance
over three Twitter benchmark geolocation
datasets, in addition to producing word and
phrase embeddings in the hidden layer that
we show to be useful for detecting dialectal
terms. As part of our analysis of dialectal
terms, we release DAREDS, a dataset for
evaluating dialect term detection methods.
</p>
<p>1 Introduction
</p>
<p>Many services such as web search (Leung et al.,
2010), recommender systems (Ho et al., 2012), tar-
geted advertising (Lim and Datta, 2013), and rapid
disaster response (Ashktorab et al., 2014) rely on
the location of users to personalise information
and extract actionable knowledge. Explicit user ge-
olocation metadata (e.g. GPS tags, WiFi footprint,
IP address) is not usually available to third-party
consumers, giving rise to the need for geoloca-
tion based on profile data, text content, friendship
graphs (Jurgens et al., 2015) or some combination
of these (Rahimi et al., 2015b,a). The strong geo-
graphical bias, most obviously at the language level
(e.g. Finland vs. Japan), and more subtly at the di-
alect level (e.g. in English used in north-west Eng-
land vs. north-east USA vs. Texas, USA), clearly
reflected in language use in social media services
such as Twitter, has been used extensively either for
geolocation of users (Eisenstein et al., 2010; Roller
et al., 2012; Rout et al., 2013; Han et al., 2014;
Wing and Baldridge, 2014) or dialectology (Cook
et al., 2014; Eisenstein, 2015). In these methods,
a user is often represented by the concatenation of
their tweets, and the geolocation model is trained
on a very small percentage of explicitly geotagged
</p>
<p>tweets, noting the potential biases implicit in geo-
tagged tweets (Pavalanathan and Eisenstein, 2015).
</p>
<p>Lexical dialectology is (in part) the converse of
user geolocation (Eisenstein, 2015): given text as-
sociated with a variety of regions, the task is to
identify terms that are distinctive of particular re-
gions. The complexity of the task is two-fold: (1)
localised named entities (e.g. sporting team names)
are not of interest; and (2) without semantic knowl-
edge it is difficult to detect terms that are in general
use but have a special meaning in a region.
</p>
<p>In this paper we propose a text-based geolocation
method based on neural networks. Our contribu-
tions are as follows: (1) we achieve state-of-the-art
results on benchmark Twitter geolocation datasets;
(2) we show that the model is less sensitive to the
specific location discretisation method; (3) we re-
lease the first broad-coverage dataset for evaluation
of lexical dialectology models; (4) we incorporate
our text-based model into a network-based model
(Rahimi et al., 2015a) and improve the performance
utilising both network and text; and (5) we use the
model’s embeddings for extraction of local terms
and show that it outperforms two baselines.
</p>
<p>2 Related Work
</p>
<p>Related work on Twitter user geolocation falls
into two categories: text-based and network-based
methods. Text-based methods make use of the
geographical biases of language use, and network-
based methods rely on the geospatial homophily
of user–user interactions. In both cases, the as-
sumption is that users who live in the same geo-
graphic area share similar features (linguistic or
interactional). Three main text-based approaches
are: (1) the use of gazetteers (Lieberman et al.,
2010; Quercini et al., 2010); (2) unsupervised text
clustering based on topic models or similar (Eisen-
stein et al., 2010; Hong et al., 2012; Ahmed et al.,
</p>
<p>209</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2033">https://doi.org/10.18653/v1/P17-2033</a></div>
</div>
<div class="page"><p />
<p>2013); and (3) supervised classification (Ding et al.,
2000; Backstrom et al., 2008; Cheng et al., 2010;
Hecht et al., 2011; Kinsella et al., 2011; Wing
and Baldridge, 2011; Han et al., 2012; Rout et al.,
2013), which unlike gazetteers can be applied to
informal text and compared to topic models, scales
better. The classification models often rely on
less than 1% of geotagged tweets for supervision
and discretise real-valued coordinates into equal-
sized grids (Serdyukov et al., 2009), administra-
tive regions (Cheng et al., 2010; Hecht et al.,
2011; Kinsella et al., 2011; Han et al., 2012, 2014),
or flat (Wing and Baldridge, 2011) or hierarchi-
cal k-d tree clusters (Wing and Baldridge, 2014).
Network-based methods also use either real-valued
coordinates (Jurgens et al., 2015) or discretised
regions (Rahimi et al., 2015a) as labels, and use
label propagation over the interaction graph (e.g.
@-mentions). More recent methods have focused
on representation learning by using sparse cod-
ing (Cha et al., 2015) or neural networks (Liu and
Inkpen, 2015), utilising both text and network in-
formation (Rahimi et al., 2015a).
</p>
<p>Dialect is a variety of language shared by a group
of speakers (Wolfram and Schilling, 2015). Our
focus here is on geographical dialects which are
spoken (and written in social media) by people
from particular areas. The traditional approach to
dialectology is to find the geographical distribution
of known lexical alternatives (e.g. you, yall and
yinz: (Labov et al., 2005; Nerbonne et al., 2008;
Gonçalves and Sánchez, 2014; Doyle, 2014; Huang
et al., 2015; Nguyen and Eisenstein, 2016)), the
shortcoming of which is that the alternative lexi-
cal variables must be known beforehand. There
have also been attempts to automatically identify
such words from geotagged documents (Eisenstein
et al., 2010; Ahmed et al., 2013; Cook et al., 2014;
Eisenstein, 2015). The main idea is to find lexical
variables that are disproportionately distributed in
different locations either via model-based or sta-
tistical methods (Monroe et al., 2008). There is a
research gap in evaluating the geolocation models
in terms of their usability in retrieving dialect terms
given a geographic region.
</p>
<p>We use a text-based neural approach trained on
geotagged Twitter messages that: (a) given a geo-
graphical region, identifies the associated lexical
terms; and (b) given a text, predicts its location.
</p>
<p>3 Data
</p>
<p>We use three existing Twitter user geolocation
datasets: (1) GEOTEXT (Eisenstein et al., 2010),
(2) TWITTER-US (Roller et al., 2012), and (3)
TWITTER-WORLD (Han et al., 2012). These
datasets have been used widely for training and
evaluation of geolocation models. They are all pre-
partitioned into training, development and test sets.
Each user is represented by the concatenation of
their tweets, and labeled with the latitude/longitude
of the first collected geotagged tweet in the case
of GEOTEXT and TWITTER-US, and the centre of
the closest city in the case of TWITTER-WORLD.1
</p>
<p>GEOTEXT and TWITTER-US cover the continen-
tal US, and TWITTER-WORLD covers the whole
world, with 9k, 449k and 1.3m users, respec-
tively as shown in Figure 1.2 DAREDS is a
dialect-term dataset novel to this research, created
from the Dictionary of American Regional English
(DARE) (Cassidy et al., 1985). DARE consists
of dialect regions, their terms and meaning.3 It is
based on dialectal surveys from different regions of
the U.S., which are then postprocessed to identify
dialect regions and terms. In order to construct a
dataset based on DARE, we downloaded the web
version of DARE, cleaned it, and removed multi-
word expressions and highly-frequent words (any
word which occurred in the top 50k most frequent
words, based on a word frequency list (Norvig,
2009). For dialect regions that don’t correspond
to a single state or set of cities (e.g. South), we
mapped it to the most populous cities within each
region. For example, within the Pacific Northwest
dialect region, we manually extracted the most pop-
ulous cities (Seattle, Tacoma, Portland, Salem, Eu-
gene) and added those cities to DAREDS as sub-
regions.
</p>
<p>The resulting dataset (DAREDS) consists of
around 4.3k dialect terms from 99 U.S. dialect re-
gions. DAREDS is the largest standardised dialec-
tology dataset.
</p>
<p>4 Methods
</p>
<p>We use a multilayer perceptron (MLP) with one
hidden layer as our location classifier, where the
</p>
<p>1The decision as to how a given user is labeled was made
by the creators of the original datasets, and has been preserved
in this work, despite misgivings about the representativeness
of the label for some users.
</p>
<p>2The datasets can be obtained from https://github.
com/utcompling/textgrounder
</p>
<p>3http://www.daredictionary.com/
</p>
<p>210</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: The geographical distribution of training points in GEOTEXT (left), TWITTER-US (middle)
and TWITTER-WORLD (right). Each red point is a training user. The number of training users is 5.6K,
429K and 1.3M in each dataset respectively. Despite the number of training instances being higher in
TWITTER-WORLD (right) compared to TWITTER-US (middle), there is greater user density in the case of
the latter.
</p>
<p>−120 −60
Longitude
</p>
<p>25
</p>
<p>50
</p>
<p>L
at
</p>
<p>it
ud
</p>
<p>e
</p>
<p>(a) k-d tree
</p>
<p>−120 −60
Longitude
</p>
<p>25
</p>
<p>50
</p>
<p>L
at
</p>
<p>it
ud
</p>
<p>e
</p>
<p>(b) k-means
</p>
<p>Figure 2: Discretisation of TWITTER-US training coordinates using k-d tree and k-means clustering
shown by the convex hulls around the training points within each cluster.
</p>
<p>input is l2 normalised bag-of-words features for a
given user. We exclude @-mentions, words with
document frequency less than 10, and stop words.
The output is either a k-d tree leaf node or k-means
discretisation of real-valued coordinates of train-
ing locations, the output of which is visualised for
TWITTER-US in Figure 2. The hidden layer out-
put provides word (and phrase, as bags of words)
embeddings for dialectal analysis.
</p>
<p>The number of regions, regularisation strength,
hidden layer and mini-batch size are tuned over
development data and set to (32, 10−5, 896,
100), (256, 10−6, 2048, 10000) and (930, 10−6,
3720, 10000) for GEOTEXT, TWITTER-US and
TWITTER-WORLD, respectively. The parame-
ters are optimised using Adamx (Kingma and Ba,
2014) using Lasagne/Theano (Theano Develop-
ment Team, 2016). Following Cheng (2010) and
Eisenstein (2010), we evaluated the geolocation
model using mean and median error in km (“Mean”
and “Median” resp.) and accuracy within 161km
of the actual location (“Acc@161”). Note that
lower numbers are better for Mean and Median,
and higher numbers better for Acc@161.
</p>
<p>4The results reported in Rahimi et al. (2015b; 2015a) for
TWITTER-WORLD were over a superset of the dataset; the
results reported here are based on the actual dataset.
</p>
<p>While the focus of this paper is text-based
user geolocation, state-of-the-art results for the
three datasets have been achieved with hybrid
text+network-based models, where the predictions
of the text-based model are fed into a mention net-
work as “dongle” nodes to each user node, pro-
viding a personalised geolocation prior for each
user (Rahimi et al., 2015a). Note that it would, of
course, be possible to combine text and network
information in a joint deep learning model (Yang
et al., 2016; Kipf and Welling, 2016), which we
leave to future work (noting that scalability will
potentially be a major issue for the larger datasets).
</p>
<p>To test the applicability of the model’s embed-
dings in dialectology, we created DAREDS. The
output of the hidden layer of the model is used as
embeddings for both location names and dialect
terms. Given a dialect region name, we retrieve
its nearest neighbours in the embedding space, and
compare them to dialect terms associated with that
location. We also compare the quality of the em-
beddings with pre-trained word2vec embeddings
and the embeddings from the output layer of LR
(logistic regression) (Rahimi et al., 2015b) as base-
lines. Regions in DAREDS can be very broad (e.g.
SouthWest), meaning that words associated with
those locations will be used across a large number
</p>
<p>211</p>
<p />
</div>
<div class="page"><p />
<p>GEOTEXT TWITTER-US TWITTER-WORLD
</p>
<p>Acc@161 Mean Median Acc@161 Mean Median Acc@161 Mean Median
TEXT-BASED METHODS
</p>
<p>Proposed method (MLP + k-d tree) 38 844 389 54 554 120 34 1456 415
Proposed method (MLP + k-means) 40 856 380 55 581 91 36 1417 373
</p>
<p>(Rahimi et al., 2015b) (LR) 38 880 397 50 686 159 32 1724 530
(Wing and Baldridge, 2014) (uniform) — — — 49 703 170 32 1714 490
(Wing and Baldridge, 2014) (k-d tree) — — — 48 686 191 31 1669 509
(Melo and Martins, 2015) — — — — 702 208 — 1507 502
(Cha et al., 2015) — 581 425 — — — — — —
(Liu and Inkpen, 2015) — — — — 733 377 — — —
</p>
<p>NETWORK-BASED METHODS
</p>
<p>(Rahimi et al., 2015a) MADCEL-W 58 586 60 54 705 116 45 2525 279
</p>
<p>TEXT+NETWORK-BASED METHODS
</p>
<p>(Rahimi et al., 2015a) MADCEL-W-LR 59 581 57 60 529 78 53 1403 111
MADCEL-W-MLP 59 578 61 61 515 77 53 1280 104
</p>
<p>Table 1: Geolocation results over the three Twitter datasets, based on the text-based MLP with k-d tree or
k-means discretisation and text+network model MADCEL-W-MLP using MLP with k-d tree for text-based
predictions. We compare with state-of-the-art results for each dataset.4“—” signifies that no results were
reported for the given metric or dataset.
</p>
<p>of cities contained within that region. We gener-
ate a region-level embedding by simply taking the
city names associated with the region, and feeding
them as BoW input for LR and MLP and averaging
their embeddings for word2vec. We evaluate the
retrieved terms by computing recall of DAREDS
terms existing in TWITTER-US (1071 terms) at
k ∈ {0.05%, 0.1%, 0.2%, 0.5%, 1%, 2%, 5%} of
vocabulary size. The code and the DAREDS
dataset are available at https://github.
com/afshinrahimi/acl2017.
</p>
<p>5 Results
</p>
<p>5.1 Geolocation
The performance of the text-based MLP model with
k-d tree and k-means discretisation over the three
datasets is shown in Table 1. The results are also
compared with state-of-the-art text-based methods
based on a flat (Rahimi et al., 2015b; Cha et al.,
2015) or hierarchical (Wing and Baldridge, 2014;
Melo and Martins, 2015; Liu and Inkpen, 2015)
geospatial representation. Our method outperforms
both the flat and hierarchical text-based models
by a large margin. Comparing the two discretisa-
tion strategies, k-means outperforms k-d tree by a
reasonable margin. We also incorporated the MLP
predictions into a network-based model based on
the method of Rahimi et al. (2015a), and improved
upon their work. We analysed the Median error
of MLP (k-d tree) over the development users of
TWITTER-US in each of the U.S. states as shown
</p>
<p>NYC LA Chicago Philadelphia
</p>
<p>manhattan lapd chi septa
ny wiltern uic erked
</p>
<p>soho ralphs metra philly
mets ucla depaul phillies
nycc weho bears jawn
nyu lausd chitown #udproblems
</p>
<p>#electriczoo #hollywoodbowl cta dickhead
yorkers lmu bogus flyers
</p>
<p>mta asf lbs irked
#thingswhitepeopledo lacma lbvvs erkin
</p>
<p>Table 2: Nearest neighbours of place names.
</p>
<p>in Figure 3. The error is highest in states with lower
training coverage (e.g. Maine, Montana, Wiscon-
sin, Iowa and Kansas). We also randomly sampled
50 development samples from the 1000 samples
with highest prediction errors to check the biases
of the model. Most of the errors are the result of ge-
olocating users from Eastern U.S. in Western U.S.
particularly in Los Angeles and San Francisco.
</p>
<p>5.2 Dialectology
We quantitatively tested the quality of the geograph-
ical embeddings by calculating the micro-average
recall of the k-nearest dialect terms (in terms of
the proportion of retrieved dialect terms) given a di-
alect region, as shown in Figure 4. Recall at 0.5% is
about 3.6%, meaning that we were able to retrieve
3.6% of the dialect terms given the dialect region
name in the geographical embedding space. The
embeddings slightly outperform the output layer
of logistic regression (LR) (Rahimi et al., 2015b)
</p>
<p>212</p>
<p />
</div>
<div class="page"><p />
<p>MN
WA MT
</p>
<p>ID
</p>
<p>ND
</p>
<p>MEWI
OR
</p>
<p>SD
MI NH
</p>
<p>VT
</p>
<p>NY
WY
</p>
<p>IANE MA
</p>
<p>IL
</p>
<p>PA
CT
</p>
<p>RI
</p>
<p>CA
</p>
<p>NV
UT
</p>
<p>OH
IN
</p>
<p>NJ
CO WVMO
</p>
<p>KS DE
MD
</p>
<p>VAKY
</p>
<p>DC
</p>
<p>AZ
</p>
<p>OKNM
TN NC
</p>
<p>TX
</p>
<p>AR
SC
</p>
<p>AL
GA
</p>
<p>MS
LA
</p>
<p>FL
</p>
<p>75
</p>
<p>100
</p>
<p>125
</p>
<p>150
</p>
<p>175
</p>
<p>200
</p>
<p>225
</p>
<p>250
</p>
<p>275
</p>
<p>er
ro
</p>
<p>ri
n
</p>
<p>km
</p>
<p>Figure 3: The geographical distribution of Median error of MLP (k-d tree) in each state over the
development set of TWITTER-US. The colour indicates error and the size indicates the number of
development users within the state.
</p>
<p>0.05% 0.1% 0.2% 0.5% 1% 2% 5%
0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>k
</p>
<p>R
ec
</p>
<p>al
l@
k word2vec
</p>
<p>LR
MLP
</p>
<p>Figure 4: Micro-averaged Recall@k results for re-
trieving dialect terms given the dialect region name
using LR embeddings, pre-trained word2vec em-
beddings as baselines, and our embeddings (MLP).
For example at k = 0.5% of vocabulary (1292
words), recall is 3.6% for MLP compared to 2.3%
for LR and less than 1% for word2vec.
</p>
<p>and word2vec pre-trained embeddings, but there
is still substantial room for improvement.
</p>
<p>Our model is slightly better than both baselines,
and can retrieve 3.6% of the correct dialect terms
given the region name at 0.5% of the total vocab-
ulary, noting the significant performance gap left
for future research. It is worth noting that the re-
trieved terms that are not included in DAREDS
are not irrelevant: many of them are toponyms (e.g.
city names, rivers, companies, or companies) as-
sociated with the given region which are not of
interest in dialectology. Equally, some are terms
that don’t exist in the DARE dictionary but might
be of interest for dialectologists because language
use in social media is so dynamic that they won’t
be captured by traditional survey-like approaches.
A major shortcoming of this work is that it doesn’t
</p>
<p>incorporate sense distinctions and so can’t recover
dialect terms that are uniformly distributed but have
an idiomatic usage in a particular region.
</p>
<p>6 Conclusion and Future Work
</p>
<p>We proposed a new text geolocation model based
on the multilayer perceptron (MLP), and evalu-
ated it over three benchmark Twitter geolocation
datasets. We achieved state-of-the-art text-based
results over all datasets. We used the parameters
of the hidden layer of the neural network as word
and phrase embeddings. We performed a nearest
neighbour search on a sample of city names and
dialect terms, and showed that the embeddings can
be used both to discover dialect terms from a geo-
graphic area and to find the geographic area a di-
alect term is spoken. To evaluate the geographical
embeddings quantitatively, we created DAREDS,
a machine-readable version of the DARE dictio-
nary and compared the performance of dialect term
retrieval given dialect region name in terms of re-
call (Figure 4), and compared the performance to
the performance in pre-trained word2vec and LR
embeddings.
</p>
<p>Acknowledgments
</p>
<p>We thank the anonymous reviewers for their in-
sightful comments and valuable suggestions. This
work was funded in part by the Australian Govern-
ment Research Training Program Scholarship, and
the Australian Research Council.
</p>
<p>213</p>
<p />
</div>
<div class="page"><p />
<p>References
Amr Ahmed, Liangjie Hong, and Alexander J. Smola.
</p>
<p>2013. Hierarchical geographical modeling of user
locations from social media posts. In Proceedings
of the 22nd International Conference on World Wide
Web (WWW 2013). Rio de Janeiro, Brazil, pages 25–
36.
</p>
<p>Zahra Ashktorab, Christopher Brown, Manojit Nandi,
and Aron Culotta. 2014. Tweedr: Mining Twit-
ter to inform disaster response. In Proceedings of
the 11th International Conference on Information
Systems for Crisis Response and Management (IS-
CRAM 2014). University Park, USA, pages 354–
358.
</p>
<p>Lars Backstrom, Jon Kleinberg, Ravi Kumar, and Jas-
mine Novak. 2008. Spatial variation in search en-
gine queries. In Proceedings of the 17th Interna-
tional Conference on World Wide Web (WWW 2008).
Beijing, China, pages 357–366.
</p>
<p>Frederic Gomes Cassidy et al. 1985. Dictionary of
American Regional English. Belknap Press of Har-
vard University Press.
</p>
<p>Miriam Cha, Youngjune Gwon, and HT Kung. 2015.
Twitter geolocation and regional classification via
sparse coding. In Proceedings of the 9th Inter-
national Conference on Weblogs and Social Media
(ICWSM 2015). Oxford, UK, pages 582–585.
</p>
<p>Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based ap-
proach to geo-locating Twitter users. In Proceedings
of the 19th ACM International Conference Infor-
mation and Knowledge Management (CIKM 2010).
Toronto, Canada, pages 759–768.
</p>
<p>Paul Cook, Bo Han, and Timothy Baldwin. 2014. Sta-
tistical methods for identifying local dialectal terms
from gps-tagged documents. Dictionaries: Jour-
nal of the Dictionary Society of North America
35(35):248–271.
</p>
<p>Junyan Ding, Luis Gravano, and Narayanan Shivaku-
mar. 2000. Computing geographical scopes of web
resources. In Proceedings of the 26th International
Conference on Very Large Data Bases (VLDB 2000).
Cairo, Egypt, pages 545–556.
</p>
<p>Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In Proceedings of the 14th
Conference of the EACL (EACL 2014). Gothenburg,
Sweden, pages 98–106.
</p>
<p>Jacob Eisenstein. 2015. Written dialect variation in
online social media. In Charles Boberg, John Ner-
bonne, and Dom Watt, editors, Handbook of Dialec-
tology, Wiley.
</p>
<p>Jacob Eisenstein, Brendan O’Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
2010 Conference on Empirical Methods in Natural
</p>
<p>Language Processing (EMNLP 2010). Boston, USA,
pages 1277–1287.
</p>
<p>Bruno Gonçalves and David Sánchez. 2014. Crowd-
sourcing dialect characterization through twitter.
PloS one 9(11):e112074.
</p>
<p>Bo Han, Paul Cook, and Timothy Baldwin. 2012. Ge-
olocation prediction in social media data by find-
ing location indicative words. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING 2012). Mumbai, India, pages
1045–1062.
</p>
<p>Bo Han, Paul Cook, and Timothy Baldwin. 2014. Text-
based Twitter user geolocation prediction. Journal
of Artificial Intelligence Research 49:451–500.
</p>
<p>Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H
Chi. 2011. Tweets from Justin Bieber’s heart: the
dynamics of the location field in user profiles. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. Vancouver, Canada,
pages 237–246.
</p>
<p>Shen-Shyang Ho, Mike Lieberman, Pu Wang, and
Hanan Samet. 2012. Mining future spatiotemporal
events and their sentiment from online news arti-
cles for location-aware recommendation system. In
Proceedings of the First ACM SIGSPATIAL Interna-
tional Workshop on Mobile Geographic Information
Systems. Redondo Beach, California, pages 25–32.
</p>
<p>Liangjie Hong, Amr Ahmed, Siva Gurumurthy, Alexan-
der J. Smola, and Kostas Tsioutsiouliklis. 2012. Dis-
covering geographical topics in the Twitter stream.
In Proceedings of the 21st International Conference
on World Wide Web (WWW 2012). Lyon, France,
pages 769–778.
</p>
<p>Yuan Huang, Diansheng Guo, Alice Kasakoff, and Jack
Grieve. 2015. Understanding US regional linguistic
variation with Twitter data analysis. Computers, En-
vironment and Urban Systems 59:244–255.
</p>
<p>David Jurgens, Tyler Finethy, James McCorriston,
Yi Tian Xu, and Derek Ruths. 2015. Geolocation
prediction in Twitter using social networks: A crit-
ical analysis and review of current practice. In
Proceedings of the 9th International Conference on
Weblogs and Social Media (ICWSM 2015). Oxford,
UK, pages 188–197.
</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
</p>
<p>Sheila Kinsella, Vanessa Murdock, and Neil O’Hare.
2011. ”I’m eating a sandwich in Glasgow”: Model-
ing locations with tweets. In Proceedings of the 3rd
International Workshop on Search and Mining User-
generated Contents. Glasgow, UK, pages 61–68.
</p>
<p>Thomas N. Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907 .
</p>
<p>214</p>
<p />
</div>
<div class="page"><p />
<p>William Labov, Sharon Ash, and Charles Boberg. 2005.
The Atlas of North American English: Phonetics,
Phonology and Sound Change. Walter de Gruyter.
</p>
<p>Kenneth Wai-Ting Leung, Dik Lun Lee, and Wang-
Chien Lee. 2010. Personalized web search with
location preferences. In 2010 IEEE 26th Inter-
national Conference on Data Engineering (ICDE
2010). Long Beach, USA, pages 701–712.
</p>
<p>Michael D Lieberman, Hanan Samet, and Jagan
Sankaranarayanan. 2010. Geotagging with local lex-
icons to build indexes for textually-specified spatial
data. In Proceedings of the 26th International Con-
ference on Data Engineering (ICDE 2010). IEEE,
Long Beach, USA, pages 201–212.
</p>
<p>Kwan Hui Lim and Amitava Datta. 2013. A topo-
logical approach for detecting Twitter communities
with common interests. In Martin Atzmueller, Alvin
Chin, and Denis Helic, editors, Ubiquitous Social
Media Analysis, Springer, pages 23–43.
</p>
<p>Ji Liu and Diana Inkpen. 2015. Estimating user lo-
cation in social media with stacked denoising auto-
encoders. In Proceedings of the 2015 Conference
of the North American Chapter of the Association
for Computational Linguistics — Human Language
Technologies (NAACL HLT 2015). Denver, USA,
pages 201–210.
</p>
<p>Fernando Melo and Bruno Martins. 2015. Geocoding
textual documents through the usage of hierarchical
classifiers. In Proceedings of the 9th Workshop on
Geographic Information Retrieval (GIR 2015). Paris,
France, pages 7:1–7:9.
</p>
<p>Burt L. Monroe, Michael P. Colaresi, and Kevin M.
Quinn. 2008. Fightin’words: Lexical feature selec-
tion and evaluation for identifying the content of po-
litical conflict. Political Analysis 16(4):372–403.
</p>
<p>John Nerbonne, Peter Kleiweg, Wilbert Heeringa, and
Franz Manni. 2008. Projecting dialect distances to
geography: Bootstrap clustering vs. noisy clustering.
In Proceedings of the 31st Annual Conference of the
Gesellschaft für Klassifikation. pages 647–654.
</p>
<p>Dong Nguyen and Jacob Eisenstein. 2016. A kernel in-
dependence test for geographical language variation.
CoRR abs/1601.06579.
</p>
<p>Peter Norvig. 2009. Natural language corpus data.
In Toby Segaran and Jeff Hammerbacher, editors,
Beautiful Data: The Stories Behind Elegant Data So-
lutions, O’Reilly Media, pages 219–242.
</p>
<p>Umashanthi Pavalanathan and Jacob Eisenstein. 2015.
Confounds and consequences in geotagged Twitter
data. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2015). Lisbon, Portugal, pages 2138–2148.
</p>
<p>Gianluca Quercini, Hanan Samet, Jagan Sankara-
narayanan, and Michael D. Lieberman. 2010. De-
termining the spatial reader scopes of news sources
</p>
<p>using local lexicons. In Proceedings of the 18th
SIGSPATIAL International Conference on Advances
in Geographic Information Systems (SIGSPATIAL
2010). New York, USA, pages 43–52.
</p>
<p>Afshin Rahimi, Trevor Cohn, and Timothy Baldwin.
2015a. Twitter user geolocation using a unified
text and network prediction model. Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics — 7th International Joint
Conference on Natural Language Processing (ACL-
IJCNLP 2015) pages 630–636.
</p>
<p>Afshin Rahimi, Duy Vu, Trevor Cohn, and Timo-
thy Baldwin. 2015b. Exploiting text and network
context for geolocation of social media users. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics — Human Language Technolo-
gies (NAACL HLT 2015). Denver, USA, pages 1362–
1367.
</p>
<p>Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CONLL 2012). Jeju, Ko-
rea, pages 1500–1510.
</p>
<p>Dominic Rout, Kalina Bontcheva, Daniel Preoţiuc-
Pietro, and Trevor Cohn. 2013. Where’s @wally?:
A classification approach to geolocating users based
on their social ties. In Proceedings of the 24th ACM
Conference on Hypertext and Social Media (Hyper-
text 2013). Paris, France, pages 11–20.
</p>
<p>Pavel Serdyukov, Vanessa Murdock, and Roelof
Van Zwol. 2009. Placing flickr photos on a map. In
Proceedings of the 32nd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. Boston, USA, pages 484–491.
</p>
<p>Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints abs/1605.02688.
</p>
<p>Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1 (ACL-HLT
2011). Portland, USA, pages 955–964.
</p>
<p>Benjamin P. Wing and Jason Baldridge. 2014. Hier-
archical discriminative classification for text-based
geolocation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP 2014). Doha, Qatar, pages 336–348.
</p>
<p>Walt Wolfram and Natalie Schilling. 2015. American
English: Dialects and Variation. John Wiley &amp;
Sons.
</p>
<p>215</p>
<p />
</div>
<div class="page"><p />
<p>Zhilin Yang, William Cohen, and Ruslan Salakhut-
dinov. 2016. Revisiting semi-supervised learn-
ing with graph embeddings. arXiv preprint
arXiv:1603.08861 .
</p>
<p>216</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 217–223
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2034
</p>
<p>A Corpus of Natural Language for Visual Reasoning
</p>
<p>Alane Suhr†, Mike Lewis‡, James Yeh†, and Yoav Artzi†
</p>
<p>† Dept. of Computer Science and Cornell Tech, Cornell University, New York, NY 10044
{suhr, yoav}@cs.cornell.edu, jamesclyeh@gmail.com
</p>
<p>‡ Facebook AI Research, Menlo Park, CA 94025
mikelewis@fb.com
</p>
<p>Abstract
</p>
<p>We present a new visual reasoning lan-
guage dataset, containing 92,244 pairs of
examples of natural statements grounded
in synthetic images with 3,962 unique sen-
tences. We describe a method of crowd-
sourcing linguistically-diverse data, and
present an analysis of our data. The data
demonstrates a broad set of linguistic phe-
nomena, requiring visual and set-theoretic
reasoning. We experiment with various
models, and show the data presents a
strong challenge for future research.
</p>
<p>1 Introduction
</p>
<p>Understanding complex compositional language
in context is a challenge shared by many tasks.
Visual question answering and robot instruction
systems require reasoning about sets of objects,
quantities, comparisons, and spatial relations; for
example, when instructing home assistance or
assembly-line robots to manipulate objects in clut-
tered environments. This reasoning requires ro-
bust language understanding, and is only partially
addressed by existing datasets. VQA (Antol et al.,
2015), while lexically and visually diverse, in-
cludes relatively short sentences with limited cov-
erage of such phenomena. CLEVR (Johnson et al.,
2016) and SHAPES (Andreas et al., 2016b), in
contrast, display complex compositional structure,
but include only synthetic language.
</p>
<p>In this paper, we introduce the Cornell Natural
Language Visual Reasoning (NLVR) corpus and
task. We define the binary prediction task of judg-
ing if a statement is true for an image or not, and
introduce a corpus of annotated pairs of natural
language statements and synthetic images.
</p>
<p>Collecting this kind of language presents two
challenges. First, we must design environments to
</p>
<p>There are two towers with the same height but their base
</p>
<p>is not the same in color.
</p>
<p>There is a box with 2 triangles of same color nearly
</p>
<p>touching each other.
</p>
<p>Figure 1: Example sentences and images from our cor-
pus. Each image includes three boxes with different ob-
ject types. The truth value of the top sentence is true,
while the bottom is false.
</p>
<p>support such descriptions. We use simple visual
environments displaying objects with complex vi-
sual relations between them. Figure 1 shows two
generated images. The second challenge is elic-
iting complex descriptions displaying a range of
syntactic and semantic phenomena. We use a two-
stage crowdsourcing process. In the first stage, we
present sets of images and ask workers to write de-
scriptive statements that distinguish them. Using
synthetic images with abstract shapes allows us
to control the potential distinctions between them;
for example, by discouraging simple statements
about object existence. In the second stage, we
ask workers to label the truth value for the sen-
tences and images generated in the first stage.
</p>
<p>Our data includes 92,244 sentence-image pairs
with 3,962 unique sentences. We include both
images and the structured representation used to
generate them to support research using both raw
visual information and structured data. Figure 1
shows two examples. To assess the difficulty of
NLVR, we experiment with multiple baselines.
The best model using images achieves an accu-
racy of 66.12, demonstrating remaining challenges
</p>
<p>217</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2034">https://doi.org/10.18653/v1/P17-2034</a></div>
</div>
<div class="page"><p />
<p>in the data. We also analyze the language in
our data for presence of certain linguistic phe-
nomena, and compare this analysis with related
datasets. The data and leaderboard are available
at http://lic.nlp.cornell.edu/nlvr.
</p>
<p>2 Related Work and Datasets
</p>
<p>Several datasets have been created to study vi-
sual reasoning and language. VQA (Antol et al.,
2015; Zitnick and Parikh, 2013) includes crowd-
sourced questions and answers for photographs
and abstract scenes, and has been studied exten-
sively (e.g., Lu et al., 2016; Xu and Saenko, 2016;
Zhou et al., 2015; Chen et al., 2015a; Andreas
et al., 2016b,a; Ray et al., 2016). In contrast to
VQA, we use synthetic images and emphasize rep-
resenting a broad range of language phenomena.
Our motivation is similar to that of SHAPES (An-
dreas et al., 2016b) and CLEVR (Johnson et al.,
2016). Both datasets also use synthetic images and
emphasize representing diverse spatial language.
However, unlike our approach, they include only
automatically generated language.
</p>
<p>Visual reasoning has also been addressed in in-
structional language corpora (e.g., MacMahon
et al., 2006; Chen and Mooney, 2011; Bisk et al.,
2016), where executable instructions are grounded
in manipulable environments. The language we
observe is similar to the type of language studied
for understanding and generation of referential ex-
pressions (Mitchell et al., 2010; Matuszek et al.,
2012; FitzGerald et al., 2013).
</p>
<p>Our task is related to caption generation,
which has been studied extensively (e.g., Peder-
soli et al., 2016; Carrara et al., 2016; Chen et al.,
2016) with MSCOCO (Chen et al., 2015b) and
Flickr30K (Young et al., 2014; Plummer et al.,
2015). In contrast to caption generation, our task
does not require approximate metrics like BLEU.
</p>
<p>Several existing datasets focus on natural lan-
guage querying of structured representations, in-
cluding GeoQuery (Zelle, 1995) and WikiTa-
bles (Pasupat and Liang, 2015). Our work is com-
plementary to these resources. While our corpus
was collected using images, we also provide struc-
tured representations. When used with these rep-
resentations, our corpus is similar to WikiTables,
where questions are paired with small web tables.
Instead of web tables, we use object sets and focus
on visual language.
</p>
<p>3 Task
</p>
<p>Statements in our data are grounded in synthetic
images rendered from structured representations.
Given an example, the task is to determine whether
a statement is true or false for the image or struc-
tured representation. While we describe the im-
age, the structured representation is equivalent.
We provide examples of the structured represen-
tation in the supplementary material. Images are
divided into three boxes. Figure 1 shows two
images. Each box contains 1-8 objects. Each
object has four properties: position (x/y coordi-
nates), color (black, blue, yellow), shape (triangle,
square, circle), and size (small, medium, large).
Objects within a box cannot overlap and must be
contained entirely in the box. We distinguish be-
tween images containing scattered objects and im-
ages containing only squares arranged in towers
up to four blocks tall. The top image in Figure 1 is
a tower example; the bottom is a scatter example.
</p>
<p>This design encourages compositional language
with complex visual reasoning. We divide the im-
age into boxes to encourage set theoretic reasoning
within and between boxes. We also use a relatively
limited number of values for each property. While
a large number of properties provides a more di-
verse image, it is likely to result in descriptions
that refer to property differences. We find that the
limited number of properties elicits descriptions
with rich compositional structure.
</p>
<p>4 Data Collection
</p>
<p>We generate images following the structure de-
scribed in Section 3, and collect grounded natu-
ral language descriptions. Data is collected in two
phases: sentence writing and validation. During
sentence writing, workers are asked to write con-
trasting descriptions about a set of images. To val-
idate sentences, the description is paired with each
of the images. We execute the collection process
four times to collect training, development, and
two test sets (Test-P and Test-U). We retain one
test set as unreleased (Test-U).
Generating Images We generate images by ren-
dering a randomly sampled structured representa-
tion. The number of objects in each box and their
properties are sampled uniformly. We generate an
equal number of scatter and tower images. To gen-
erate the sets of images presented to annotators,
we generate two images independently, a third im-
age by using the set of objects in the first im-
</p>
<p>218</p>
<p />
</div>
<div class="page"><p />
<p>(A)
</p>
<p>(B)
</p>
<p>(C)
</p>
<p>(D)
</p>
<p>Write one sentence. This sentence must meet all of the
following requirements:
</p>
<p>• It describes A.
• It describes B.
• It does not describe C.
• It does not describe D.
• It does not mention the images explicitly (e.g. “In im-
</p>
<p>age A, ...”).
• It does not mention the order of the light grey squares
</p>
<p>(e.g. “In the rightmost square...”)
There is no one correct sentence for this image. There
may be multiple sentences which satisfy the above re-
quirements. If you can think of more than one sentence,
submit only one.
</p>
<p>Figure 2: Sentence writing prompt. The bottom sen-
tence in Figure 1 was generated from this prompt.
</p>
<p>age and randomly re-shuffling them between the
boxes, and a fourth image by re-shuffling the ob-
jects in the second image. For images with towers,
we constrain the re-shuffling to form towers.
</p>
<p>Phase 1 – Sentence Writing Each writing task
presents an annotator with four images. Figure 2
shows the sentence writing prompt, including the
set of constraints, which is shown for all writing
tasks. The constraints force the worker to contrast
two pairs by referring to similarities and differ-
ences between the images, but not to refer to the
position of the image in the prompt, or of each
box in each image. These constraints are placed
to elicit more set-theoretic language, and to allow
us to divide the result of each task into four exam-
ples, pairing the annotator’s sentence with each of
the four images it was presented with.
</p>
<p>Phase 2 – Validation In the second phase, we
pair each sentence with the four images used to
generate it. We re-label all sentence-image pairs
as true or false, correcting for any violations of
the constraints in the first phase. We do not use
the original position of the image as any part of
the final label to neutralize any ordering effect.
In practice, 8.2% of examples had a different la-
bel than inferred from their original position in
</p>
<p>Unique sentences Examples
Train 3,163 74,460
Dev 267 5,940
Test-P 266 5,934
Test-U 266 5,910
Total 3,962 92,244
</p>
<p>Table 1: Data statistics.
</p>
<p>the first phase. During validation, boxes are ran-
domly permuted to ensure the last constraint was
followed. We allow workers to annotate a sentence
as nonsensical with regard to the image, and in-
struct annotators to ignore grammar errors.
Post-processing We prune pairs when their ma-
jority class is nonsensical. When collecting mul-
tiple annotations for a pair, we prune pairs if the
gap between the classes is less than two votes.
</p>
<p>5 Data Statistics and Analysis
</p>
<p>We use the crowdsourcing platform Upwork,1 and
select ten annotators using a small set of exam-
ple questions. We collect 3,974 task instances and
28,723 total validation judgments at a total cost
of $5,526. From these 3,974 task instances we
extract 15,896 sentence-image pairs. We prune
522 pairs in post-processing. For the training set
we collect a single validation annotation for each
sentence-image pair; for the rest of the data we
collect five annotations each. Finally, we gener-
ate six sentence-image pairs from each sample by
permuting the boxes. The validation step ensures
this permutation does not change the label. Table 1
shows the number of sentences and pairs, includ-
ing permutations, for each split.
</p>
<p>We merge the development and test splits to cal-
culate agreement statistics. We calculate Krippen-
dorf’s α and Fleiss’ κ (Cocos et al., 2015) on both
the full and pruned datasets. To calculate Fleiss’
κ, we randomly permute the five annotations to
be assigned to five “raters” and compute average
kappa from 100 iterations. Before pruning, we ob-
serve α = 0.768 and κ = 0.709, indicating sub-
stantial agreement (Landis and Koch, 1977). Prun-
ing improves agreement to α = 0.831 (indicating
almost-perfect agreement) and κ = 0.808.
</p>
<p>We analyze 200 development sentences to iden-
tify the distribution of semantic phenomena and
syntactic ambiguity (Table 2). For comparison,
we apply this analysis to 200 abstract-image and
200 real-image sentences from VQA (Antol et al.,
2015). The difference in the distribution illustrates
the complexity of our data. The mean sentence
</p>
<p>1http://upwork.com
</p>
<p>219</p>
<p />
</div>
<div class="page"><p />
<p>VQA VQA Our NMN Example
(abs) (real) Data Correct
</p>
<p>Semantics
Cardinality (hard) 12 11.5 66 63.8 There are exactly four objects not touching any edge
Cardinality (soft) 0 1 16 63.4 There is a box with at least one square and at least three
</p>
<p>triangles.
Existential 4.5 11.5 88 64.2 There is a tower with yellow base.
Universal 1 1 7.5 67.8 There is a black item in every box.
Coordination 3 5 17 58.5 There are 2 blue circles and 1 blue triangle
Coreference 8.5 6.5 3 55.3 There is a blue triangle touching the wall with its side.
Spatial Relations 31 42.5 66 61.6 there is one tower with a yellow block above a yellow block
Comparative 1.5 1 3 73.6 There is a box with multiple items and only one item has a
</p>
<p>different color.
Presupposition2 79 80 19.5 54.0 There is a box with seven items and the three black items
</p>
<p>are the same in shape.
Negation 0 1 9.5 51.0 there is exactly one black triangle not touching the edge
Syntax
Coordination 0 0 4.5 53.4 There is a box with at least one square and at least three
</p>
<p>triangles.
PP Attachment 7 3 23 70.9 There is a black block on a black block as the base of a
</p>
<p>tower with three blocks.
</p>
<p>Table 2: Qualitative and empirical analysis of our data and VQA (Antol et al., 2015). We analyze 200 sentences
for each dataset. The data is categorized to semantic and syntactic categories. We use the terms hard and soft
cardinality to differentiate between language using exact numerical values and ranges. For each dataset, we show
the percentage of the samples analyzed that demonstrate the phenomena. We analyze abstract (abs) and real images
from VQA separately. For our data, we also include the accuracy using the NMN system (Section 6) for the subset
of images we tagged with this category.
</p>
<p>5 10 15 20 25 30 35 40
0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>30
</p>
<p>Sentence length
</p>
<p>%
of
</p>
<p>se
nt
</p>
<p>en
ce
</p>
<p>s
</p>
<p>VQA (real)
VQA (abstract)
</p>
<p>MSCOCO
CLEVR
</p>
<p>Our dataset
</p>
<p>Figure 3: Distribution of sentence lengths.
</p>
<p>length in our data is 11.22 tokens and the vocabu-
lary size is 262. In Figure 3, we compare sentence
length distribution to VQA, MSCOCO (Chen
et al., 2015b), and CLEVR (Johnson et al., 2016).
Our sentences are generally longer than VQA and
more similar in length to MSCOCO. However, our
task is more similar to VQA, where context is used
to understand language, rather than to generate.
</p>
<p>6 Methods
</p>
<p>We evaluate multiple methods on the rendered
images and structured representations. Hyper-
parameters and initialization details are described
in the supplementary material.
</p>
<p>2We say a statement or question uses presupposition when
it assumes the truth value of some proposition in order for its
entire truth value to be defined. In this example, an image
which does not have three black items will have no defined
truth value for this statement.
</p>
<p>6.1 Majority Class and Single Modality
</p>
<p>We use image- and text-only models to measure
how well biases in our data can be used to solve
the task. If the model is able to do well on the
text- or image-only baselines, this implies our data
does not require the two modalities. Antol et al.
(2015) performed a similar analysis of VQA with
the questions only to gauge how and if background
knowledge of the domain could aid performance.
</p>
<p>Majority Assign the most common label (true)
to all examples.
</p>
<p>Text Only Encode the sentence with a recurrent
neural network (RNN; Elman, 1990) with long
short-term memory units (LSTM; Hochreiter and
Schmidhuber, 1997) and a binary softmax com-
puted from the final output.
</p>
<p>Image Only Encode the image with a convo-
lutional neural network (CNN) with three layers.
The CNN output is used by a three-layer percep-
tron with a softmax on the final layer.3
</p>
<p>6.2 Structured Representation
</p>
<p>We use the structured representations described in
Sections 3 and 4.
</p>
<p>3We also experimented using the ImageNet-trained Incep-
tion v4 model (Szegedy et al., 2017), but found it did not im-
prove performance, possibly due to the difference between
our images and ImageNet.
</p>
<p>220</p>
<p />
</div>
<div class="page"><p />
<p>Train Dev Test-P Test-U
Majority 56.37 55.31 56.16 55.43
Text only 58.36 ±0.6 56.61 ±0.5 57.18 ±0.6 56.21 ±0.4
Image Only 56.79 ±1.3 55.35 ±0.1 56.05 ±0.3 55.33 ±0.3
</p>
<p>Structured representation
MaxEnt 99.99 68.04 67.68 67.82
MLP 96.15 ±1.3 67.50 ±0.5 66.28 ±0.4 65.32 ±0.4
Image features+RNN 59.71 ±1.0 57.72 ±1.4 57.62 ±1.3 56.29 ±0.9
</p>
<p>Raw image CNN+RNN 58.85 ±0.2 56.59 ±0.3 58.01 ±0.3 56.30 ±0.6NMN 98.37±0.6 63.06 ±0.1 66.12 ±0.4 61.99 ±0.8
</p>
<p>Table 3: Mean accuracy and standard deviation results. We report accuracy for the train, development, and both
test sets. Three systems use the structured representation. Two systems (and Image Only) use the raw image.
</p>
<p>MaxEnt Train a MaxEnt classifier. We use
the text and structured representation to compute
property- and count-based features. Property-
based features trigger when some property (e.g.,
an object is touching a wall) is true in the structure.
We create features by crossing triggered proper-
ties with each n-grams from the sentence, up to
n = 6. Count-based features trigger when a count
we observe in the image (e.g., the number of black
triangles) is present in the sentence. We generate
features combining the type of item counted (e.g.,
black triangles) with the n-grams surrounding the
count in the sentence, up to n = 6. We provide
details in the supplementary material.
MLP Train a single-layer perceptron with a soft-
max layer. The input to the perceptron is the mean
of the feature embeddings. We use the same fea-
ture set as the MaxEnt model.
Image Features+RNN Compute features from
the structure representation only, and encode the
text with an LSTM RNN. The two representations
are concatenated, and used as input to a two-layer
perceptron and a softmax layer.
</p>
<p>6.3 Image Representation
CNN+RNN Concatenate the CNN and RNN
representations (Section 6.1) and apply a multi-
layer perceptron with a softmax.
NMN The neural module networks approach of
Andreas et al. (2016b). We experiment with the
default maximum leaves of two, and with allowing
for more expressive representations with a max-
imum leaves of five. We observe higher devel-
opment accuracy with the trees using maximum
leaves of five (63.06% vs. 62.4% with the default
of two), which we use in our experiments.
</p>
<p>7 Results
</p>
<p>We run each experiment ten times and report mean
accuracy as well as standard deviation for ran-
domly initialized models. Table 3 shows our re-
</p>
<p>sults. NMN is the best performing model using
images. Table 2 shows the NMN accuracy for
each category in our qualitative analysis sample.
While the number of sentences in some categories
is relatively small, we observe a higher number
of failures in sentences that include negations and
coordinations. For models using the structured
representation, the MaxEnt model provides the
best performance. When ablating count-based fea-
tures from the MaxEnt model, development accu-
racy decreases from 68.04 to 57.7. This indicates
counting is an important aspect of the problem.
</p>
<p>8 Discussion
</p>
<p>We introduce the Cornell Natural Language Vi-
sual Reasoning dataset and task. The data in-
cludes complex compositional language grounded
in images and structured representations. The
task requires addressing challenges in visual and
set-theoretic reasoning. We experiment with
multiple systems and, in general, observe rel-
atively low performance. Together with our
qualitative analysis, this exemplifies the com-
plexity of the data. We release our annotated
training and development sets, and create two
test sets. The public test set will be released
along with its annotation. Computing results on
the unreleased test data will require submitting
trained models. Procedures for submitting mod-
els and the task leader board are available at
http://lic.nlp.cornell.edu/nlvr.
</p>
<p>Acknowledgments
</p>
<p>This research was supported by a Microsoft Re-
search Women’s Fellowship, a Google Faculty
Award, and an Amazon Web Services Cloud Cred-
its for Research Grant. We thank the Cornell and
University of Washington NLP groups for their
support and helpful comments. We thank the
anonymous reviewers for their feedback.
</p>
<p>221</p>
<p />
</div>
<div class="page"><p />
<p>References
</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016a. Learning to compose neu-
ral networks for question answering. In Proceed-
ings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
https://doi.org/10.18653/v1/N16-1181.
</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016b. Neural module networks. In
Conference on Computer Vision and Pattern Recog-
nition.
</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual question an-
swering. In International Journal of Computer Vi-
sion.
</p>
<p>Yonatan Bisk, Deniz Yuret, and Daniel Marcu. 2016.
Natural language communication with robots. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
https://doi.org/10.18653/v1/N16-1089.
</p>
<p>Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fab-
rizio Falchi, and Alejandro Moreo. 2016. Pic-
ture it in your mind: Generating high level visual
representations from textual descriptions. CoRR
abs/1606.07287.
</p>
<p>David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
</p>
<p>Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan
Gao, Wei Xu, and Ramakant Nevatia. 2015a.
ABC-CNN: An attention based convolutional neu-
ral network for visual question answering. CoRR
abs/1511.05960.
</p>
<p>Wenhu Chen, Aurélien Lucchi, and Thomas Hofmann.
2016. Bootstrap, review, decode: Using out-of-
domain textual data to improve image captioning.
CoRR abs/1611.05321.
</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C. Lawrence Zitnick. 2015b. Microsoft COCO cap-
tions: Data collection and evaluation server. CoRR
abs/1504.00325.
</p>
<p>Anne Cocos, Aaron Masino, Ting Qian, Ellie Pavlick,
and Chris Callison-Burch. 2015. Effectively crowd-
sourcing radiology report annotations. In Pro-
ceedings of the Sixth International Workshop on
Health Text Mining and Information Analysis.
https://doi.org/10.18653/v1/W15-2614.
</p>
<p>Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science 14:179–211.
</p>
<p>Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logi-
cal forms for referring expression generation. In
Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing.
http://www.aclweb.org/anthology/D13-1197.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9:1735–
1780.
</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C. Lawrence Zitnick, and
Ross B. Girshick. 2016. CLEVR: A diagnostic
dataset for compositional language and elementary
visual reasoning. CoRR abs/1612.06890.
</p>
<p>J. Richard Landis and Gary Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics 33 1:159–74.
</p>
<p>Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi
Parikh. 2016. Hierarchical question-image co-
attention for visual question answering. In Neural
Information Processing Systems.
</p>
<p>Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting
language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tificial Intelligence.
</p>
<p>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
</p>
<p>Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2010. Natural reference to objects in a vi-
sual domain. In Proceedings of the 6th Interna-
tional Natural Language Generation Conference.
http://aclweb.org/anthology/W10-4210.
</p>
<p>Panupong Pasupat and Percy Liang. 2015. Com-
positional semantic parsing on semi-structured ta-
bles. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
https://doi.org/10.3115/v1/P15-1142.
</p>
<p>Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and
Jakob Verbeek. 2016. Areas of attention for image
captioning. CoRR abs/1612.01033.
</p>
<p>Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,
Juan C. Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. 2015. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-
to-sentence models. In The IEEE International Con-
ference on Computer Vision.
</p>
<p>Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Ba-
tra, and Devi Parikh. 2016. Question relevance
in VQA: Identifying non-visual and false-premise
</p>
<p>222</p>
<p />
</div>
<div class="page"><p />
<p>questions. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing. http://aclweb.org/anthology/D16-1090.
</p>
<p>Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,
and Alexander A. Alemi. 2017. Inception-v4,
inception-resnet and the impact of residual connec-
tions on learning. In Association for the Advance-
ment of Artificial Intelligence.
</p>
<p>Huijuan Xu and Kate Saenko. 2016. Ask, attend and
answer: Exploring question-guided spatial attention
for visual question answering. In European Confer-
ence on Computer Vision.
</p>
<p>Peter Young, Alice Lai, Micah Hodosh, and
Julia Hockenmaier. 2014. From image de-
scriptions to visual denotations. Transactions
of the Association of Computational Linguistics
http://aclweb.org/anthology/Q14-1006.
</p>
<p>John M. Zelle. 1995. Using inductive logic program-
ming to automate the construction of natural lan-
guage parsers. Ph.D. thesis, University of Texas,
Austin.
</p>
<p>Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar,
Arthur Szlam, and Rob Fergus. 2015. Simple
baseline for visual question answering. CoRR
abs/1512.02167.
</p>
<p>C. Lawrence Zitnick and Devi Parikh. 2013. Bring-
ing semantics into focus using visual abstraction. In
2013 IEEE Conference on Computer Vision and Pat-
tern Recognition.
</p>
<p>223</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2035
</p>
<p>Neural Architecture for Temporal Relation Extraction:
A Bi-LSTM Approach for Detecting Narrative Containers
</p>
<p>Julien Tourille
LIMSI, CNRS
</p>
<p>Univ. Paris-Sud
Université Paris-Saclay
</p>
<p>julien.tourille@limsi.fr
</p>
<p>Olivier Ferret
CEA, LIST,
</p>
<p>Gif-sur-Yvette,
F-91191 France.
</p>
<p>olivier.ferret@cea.fr
</p>
<p>Xavier Tannier
LIMSI, CNRS
</p>
<p>Univ. Paris-Sud
Université Paris-Saclay
</p>
<p>xavier.tannier@limsi.fr
</p>
<p>Aurélie Névéol
LIMSI, CNRS
</p>
<p>Université Paris-Saclay
aurelie.neveol@limsi.fr
</p>
<p>Abstract
</p>
<p>We present a neural architecture for con-
tainment relation identification between
medical events and/or temporal expres-
sions. We experiment on a corpus of de-
identified clinical notes in English from
the Mayo Clinic, namely the THYME cor-
pus. Our model achieves an F-measure
of 0.613 and outperforms the best result
reported on this corpus to date.
</p>
<p>1 Introduction
</p>
<p>Temporal information extraction from clinical
health records allows for a fine-grained analysis of
patient health history. Providing medical staff with
patient timelines could lead to improved diagnos-
tic and care. Important temporal information (such
as when a patient started a treatment, or when they
started experiencing side effects from a treatment)
can be found only within the narrative portion of
records and needs the development of new Natu-
ral Language Processing methods in order to be
accessed.
</p>
<p>In this paper, we present a neural architec-
ture for narrative container identification between
medical events (EVENT) and/or temporal expres-
sions (TIMEX3). We experiment on the THYME
corpus (Styler IV et al., 2014), a corpus of de-
identified clinical notes in English from the Mayo
Clinic. We use the Gold Standard annotations for
EVENT and TIMEX3 entities and we focus on con-
tainment relation extraction where the objective is
to identify temporal relations between pairs of en-
tities formalized as narrative container relations.
</p>
<p>2 Related Work
</p>
<p>SemEval has been offering a shared task related
to temporal relation extraction from clinical nar-
ratives over the past two years (Bethard et al.,
2015, 2016). Relying on the THYME corpus,
the task challenged participants to extract EVENT
and TIMEX3 entities and then to extract narra-
tive container relations and document creation
time relations. Herein, we focus on the second
part of the challenge, temporal relation extrac-
tion and more specifically the narrative container
relations. Different approaches have been im-
plemented by the participants, including Support
Vector Machine (SVM) classifiers (AAl Abdul-
salam et al., 2016; Cohan et al., 2016; Lee et al.,
2016; Tourille et al., 2016), Conditional Random
Fields (CRF) and convolutional neural networks
(CNNs) (Chikka, 2016). Beyond the challenges,
Leeuwenberg and Moens (2017) propose a model
based on a structured perceptron to jointly predict
both types of temporal relations. Lin et al. (2016)
performs training instance augmentation to in-
crease the number of training examples and imple-
ment a SVM based model for containment relation
extraction. Dligach et al. (2017) implement mod-
els based on CNNs and Long Short-Term Mem-
ory Networks (LSTMs) (Hochreiter and Schmid-
huber, 1997) to extract containment relations from
the THYME corpus.
</p>
<p>From a more general perspective, relation ex-
traction and classification is a task explored by
many approaches, from fully unsupervised to fully
supervised. Recent years have seen an increas-
ing interest for the use of neural approaches.
</p>
<p>224</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2035">https://doi.org/10.18653/v1/P17-2035</a></div>
</div>
<div class="page"><p />
<p>Recursive neural networks (Socher et al., 2011,
2013) have proved useful for tasks involving long-
distance relations, such as semantic relation ex-
traction (Hashimoto et al., 2013; Li et al., 2015).
Convolutional networks have also been used (dos
Santos et al., 2015; Zeng et al., 2014) and more re-
cently, recurrent networks such as LSTM showed
to be more robust for learning long-distance se-
mantic information (Miwa and Bansal, 2016; Xu
et al., 2015; Zhou et al., 2016).
</p>
<p>3 Data
</p>
<p>3.1 Corpus Presentation
</p>
<p>The THYME corpus is a collection of clinical texts
written in English from a cancer department that
have been released during the Clinical TempEval
campaigns (Bethard et al., 2015, 2016). This cor-
pus contains documents annotated with medical
events and temporal expressions as well as narra-
tive container relations.
</p>
<p>According to the annotation guidelines of the
THYME corpus, a medical event is anything that
could be of interest on the patient’s clinical time-
line. It could be for instance a medical procedure,
a disease or a diagnosis. There are five attributes
given to each event: Contextual Modality, Degree,
Polarity, Type and DocTimeRel.
</p>
<p>Temporal expressions are assigned a Class at-
tribute. Possible values for these attributes are pre-
sented in Table 2.
</p>
<p>Narrative containers can be apprehended as
temporal buckets in which several events may be
included. These containers are anchored by tem-
poral expressions, medical events or other con-
cepts. Styler IV et al. (2014) argue that the use
of narrative containers instead of classical tem-
poral relations (Allen, 1983) yields better anno-
tation while keeping most of the useful temporal
information intact. The concept of narrative con-
tainer is illustrated in Figure 1 and described fur-
ther in Pustejovsky and Stubbs (2011).
</p>
<p>We identify two types of narrative container re-
lations: relations that stay within sentence bound-
aries (≈75% of the instances) and relations that
spread over several sentences. In the rest of the
paper, we will refer to them as intra- and inter-
sentence relations. Descriptive statistics on the
corpus are presented in Table 1.
</p>
<p>The
-
</p>
<p>last time
TIMEX
</p>
<p>the
-
</p>
<p>dose
EVENT
</p>
<p>was
-
</p>
<p>increased
EVENT
</p>
<p>was
-
</p>
<p>in
-
</p>
<p>February 2010
TIMEX
</p>
<p>.
-
</p>
<p>CONTAINSCONTAINS
</p>
<p>CONTAINS
</p>
<p>Figure 1: Examples of intra-sentence narrative
container relations.
</p>
<p>Train Test
EVENT 49,147 15,503
TIMEX3 5,791 1,917
Intra-sentence relations 12,855 4,365
Inter-sentence relations 4,582 1,565
</p>
<p>Table 1: Descriptive statistics about the train and
test parts of the THYME corpus.
</p>
<p>3.2 Preprocessing
</p>
<p>We preprocessed the corpus using cTAKES
(Savova et al., 2010), an open-source natural lan-
guage processing system for the extraction of in-
formation from electronic health records. We ex-
tracted sentence and token boundaries, as well as
token types and semantic types of the entities that
have a span overlap with a least one gold stan-
dard EVENT entity of the THYME corpus. Seman-
tic types are a set of subject categories (organized
as a tree) that are used to categorize concepts in
the UMLS® (Unified Medical Language System)
Metathesaurus. There are currently 135 types (Bo-
denreider, 2004). This information was added to
the set of Gold Standard attributes available for
EVENT entities in the corpus. An overview of the
attributes available for each token is presented in
Table 2.
</p>
<p>4 Task Description
</p>
<p>The container relation extraction task can be cast
as a 3-class classification problem. For each com-
bination of EVENT and/or TIMEX3 from left to
right, three cases are possible:
</p>
<p>• the first entity temporally contains the second
entity,
</p>
<p>• the first entity is temporally contained by the
second entity,
</p>
<p>• there is no temporal containment relation be-
tween the entities.
</p>
<p>Intra- and inter-sentence relation detection can
be seen as two different tasks with specific fea-
tures. Intra-sentence relations can benefit from
intra-sentential clues such as adverbs (e.g. during)
or pronouns (e.g. which) which are not available
at the inter-sentence level. Furthermore, past work
on the topic seems to indicate that this differentia-
</p>
<p>225</p>
<p />
</div>
<div class="page"><p />
<p>Source Attribute Values
</p>
<p>Corpus
</p>
<p>Contextual Modality Actual, Hypothetical, Hedged, Generic or no-value
Degree Most, Little, N/A or no-value
Polarity Pos, Neg or no-value
Type Aspectual, Evidential, N/A or no-value
DocTimeRel Before, Before-Overlap, Overlap, After or no-value
Entity EVENT, TIMEX3 or no-entity
</p>
<p>cTAKES
Entity Typea DiseaseDisorderMention, LabMention, MedicationEventMention, MedicationMen-
</p>
<p>tion, ProcedureMention, SignSymptomMention or no-value
Semantic Typea list of semantic types extracted from the training corpus or no-value
</p>
<p>a If the token is not part of an EVENT entity, the value is automatically no-value.
</p>
<p>Table 2: Attributes available for each token of the corpus after preprocessing.
</p>
<p>tion improves overall performance (Tourille et al.,
2016). We have adopted this approach by build-
ing two separate classifiers, one for intra-sentence
relations and one for inter-sentence relations.
</p>
<p>If we were to consider all combinations of enti-
ties within documents for inter-sentence relations,
it would result in a very large training corpus with
very few positive examples. In order to cope
with this issue, we limit our experiments to inter-
sentence relations that do not span over more than
three sentences. By doing so, we obtain a man-
ageable training corpus size with less unbalanced
classes while keeping a good coverage. It results
in 2,085 inter-sentences relations for the training
corpus and 743 for the test corpus.
</p>
<p>5 Neural Network Model
</p>
<p>First, we present the main component of our model
in Section 5.1. Then, we describe the word embed-
dings that we use as input for the model in Sec-
tion 5.2. Finally, we present the parameters used
for network training in Section 5.3.
</p>
<p>5.1 Temporal Relation Extraction
</p>
<p>Our approach relies on Long Short-Term Memory
Networks (LSTMs) (Hochreiter and Schmidhuber,
1997). The architecture of our model is presented
in Figure 2. For a given sequence of tokens sepa-
rating two entities (EVENT and/or TIMEX3), repre-
sented as vectors, we compute a representation by
going from left to right in the sequence (forward
LSTM in figure 2).
</p>
<p>As LSTMs tend to be biased toward the most re-
cent inputs, this implementation would be biased
toward the second entity of each pair processed by
the network. To counteract this effect, we compute
the reverse representation with an LSTM reading
the sequence backwards, from right to left (back-
ward LSTM in figure 2). By doing so, we keep as
</p>
<p>much information as possible about the two enti-
ties.
</p>
<p>The two final states are then concatenated and
linearly transformed to a 3-dimensional vector
representing the number of categories (concatena-
tion and projection in figure 2). Finally, a softmax
function is applied.
</p>
<p>Figure 2: Neural architecture for containment re-
lation extraction.
</p>
<p>5.2 Input Embeddings
Vectors representing tokens are built by concate-
nating a character-based embedding, a word em-
bedding, one embedding per Gold Standard at-
tribute and one embedding per cTAKES attribute.
While the word embedding is a classical option in
the context of neural models, the embeddings for
Gold Standard and cTAKES attributes are a way of
integrating in such model features that have been
demonstrated as useful in previous work. Finally,
temporal clues such as verbs, and more particu-
larly their tense, which are important in assessing
if one entity temporally contains another, are taken
into account by our character-based representation
</p>
<p>226</p>
<p />
</div>
<div class="page"><p />
<p>Intra-sentence classifier Inter-sentence classifier
</p>
<p>ref pred corr P R F1 ref pred corr P R F1
</p>
<p>No Features 4365 4529 3035 0.670 0.681 0.675 743 895 377 0.421 0.498 0.456
+ GS 4365 4253 2980 0.701 0.661 0.680 743 692 349 0.504 0.462 0.482
+ cTAKES 4365 4780 3170 0.663 0.704 0.683 743 628 305 0.486 0.408 0.443
</p>
<p>Table 3: Results obtained by the intra-sentence and inter-sentence classifiers for each model of this paper.
We report the number of Gold Standard relations (ref), the number of relations predicted by our system
(pred), the number of true positives (corr), the precision (P), the recall (R) and the F1-measure (F1).
</p>
<p>of tokens.
An overview of the embedding computation is
</p>
<p>presented in Figure 3. Following Lample et al.
(2016), the character-based representation is con-
structed with a Bi-LSTM. First, a random embed-
ding is generated for every character present in the
training corpus. Token characters are then pro-
cessed with a forward and backward LSTM sim-
ilar to the one we use in our general architecture.
The final character-based representation is the re-
sult of the concatenation of the forward and back-
ward representations.
</p>
<p>Figure 3: Neural architecture for input embed-
dings.
</p>
<p>We use a character embedding size of 8 and hid-
den dimensions of 25 for the forward and back-
ward LSTMs, resulting in a final representation
size of 50 after concatenation. This representation
is randomly initialized and incrementally defined
by the training of the whole network.
</p>
<p>For word embeddings, our vectors are pre-
trained by applying word2vec (Mikolov et al.,
2013) on the Mimic 3 corpus (Johnson et al.,
2016)1. In order to account for unknown tokens
during the test phase, we train a special embed-
ding UNK by replacing randomly some singletons
with the UNK token (probability of replacement =
0.5).
</p>
<p>1Parameters used during computation: algorithm =
CBOW; min-count = 4; vector size = 100; window = 8.
</p>
<p>In the inter-sentence relation classifier, we in-
troduce a specific token for identifying sentence
breaks. This token is composed of one distinc-
tive character and it is associated to a specific word
embedding.
</p>
<p>Similarly to the character embeddings, we ran-
domly initialize one embedding per token attribute
value, with an embedding size of 4. All these em-
beddings are then concatenated in a final represen-
tation.
</p>
<p>5.3 Network Training
We implemented the network using Tensor-
Flow (Abadi et al., 2015). We trained our network
with mini-batch Stochastic Gradient Descent us-
ing Adam (Kingma and Ba, 2014) with a batch-
size of 256. The learning rate was set to 0.001.
The hidden layers of our forward and backward
LSTMs have a size of 512. We kept 10% of the
training corpus for a development corpus and we
implemented early stopping with a patience of 10
epochs without performance improvement. Fi-
nally, we used dropout training to avoid overfit-
ting. We applied dropout on input embeddings
with a rate of 0.5.
</p>
<p>6 Experiments and Discussion
</p>
<p>We experimented with three configurations. In the
first one, we used only word embeddings and char-
acter embeddings. In the second one, we added
the feature embeddings related to the Gold Stan-
dard (GS) attributes. Finally, in a third experi-
ment, we added the feature embeddings related to
cTAKES. For each experiment, we report preci-
sion (P), recall (R) and F1-measure (F1) computed
with the official evaluation script2 provided during
the Clinical TempEval challenges. Results of the
experiments are presented in Table 4. For com-
parison, we report the baseline provided as refer-
ence during the Clinical TempEval shared tasks,
</p>
<p>2https://github.com/bethard/
anaforatools
</p>
<p>227</p>
<p />
</div>
<div class="page"><p />
<p>P R F1
</p>
<p>baseline (closest) 0.459 0.154 0.231
Lee et al. (2016) 0.588 0.559 0.573
Lin et al. (2016) 0.669 0.534 0.594
</p>
<p>No features 0.646 0.568 0.605
+ GS features 0.687 0.549 0.610
+ cTAKES features 0.657 0.575 0.613
</p>
<p>Table 4: Experimentation results. We report preci-
sion (P), recall (R) and F1-measure (F1) for each
configuration of our model, for the best system of
the Clinical TempEval 2016 challenge (Lee et al.,
2016) and for the best result obtained so far on the
corpus (Lin et al., 2016).
</p>
<p>the results of the best system of the Clinical Tem-
pEval 2016 challenge (Lee et al., 2016) and the
best scores obtained after the challenge (Lin et al.,
2016) on the test portion of the corpus. Both
Lee et al. (2016) and Lin et al. (2016) rely on
SVM classifiers using hand-engineered linguistic
features.
</p>
<p>All three of our models perform better in terms
of F1-measure than Lee et al. (2016) and Lin et al.
(2016). Our two best models also outperform
Leeuwenberg and Moens (2017), who report an
F-measure of .608 using a structured perceptron.
Interestingly, their model did not distinguish be-
tween intra- and inter- sentence relations, but in-
stead considered that related entities had to occur
within a window of 30 tokens. We see that the ad-
dition of attribute embeddings slightly improves
the overall performance of our system (+0.008).
Adding the embeddings of GS features contributes
to the major part of this improvement but tends to
increase the imbalance between recall and preci-
sion. On the contrary, while the attribute embed-
dings related to cTAKES seem to have little impact
on the overall performance, they tend to restore
more balanced precision and recall.
</p>
<p>The results for respectively intra- and inter-
sentence relations are presented in Table 3. Simi-
larly to our global results, the intra-sentence clas-
sifier benefits from the addition of feature embed-
dings with a small increase for GS features and
only a very little improvement for cTAKES fea-
tures.
</p>
<p>The inter-sentence classifier exhibits the same
trend: GS features do improve the performance.
However, adding cTAKES features degrades it
slightly (-0.013).
</p>
<p>The closest work compared to ours is clearly
</p>
<p>Dligach et al. (2017) as it also heavily relies on
neural models for extracting temporal contain-
ment relations between medical events. Dligach
et al. (2017) tested both CNN and LSTM mod-
els and found CNN superior to LSTM. However,
this work addressed intra-sentence relations only.
Moreover, its LSTM model was not a Bi-LSTM
model as ours and it did not include character-
based or attribute embeddings. Finally, it distin-
guished EVENT-TIMEX3 and EVENT-EVENT rela-
tions while we have only one model for the two
types of relations.
</p>
<p>7 Conclusion and Perspectives
</p>
<p>From a global perspective, the work we have pre-
sented in this article shows that in accordance with
a more general trend, our neural model for ex-
tracting containment relations clearly outperforms
classical approaches based on feature engineering.
However, it also shows that incorporating classi-
cal features in such a model is a way to improve
it, even if all kinds of features do not contribute
equally to such improvement. A more fine-grained
study has now to be performed to determine the
most meaningful features in this perspective and
to measure the contribution of each feature to the
overall performance, with a specific emphasis on
character-based embeddings.
</p>
<p>Beyond a further analysis of the characteristics
of our model, we are interested in two main ex-
tensions. The first one will investigate whether
training two models, one for EVENT-TIMEX3 rela-
tions and one for EVENT-EVENT relations, as done
by Dligach et al. (2017), is a better option than
training one model for all types of containment
relations as presented herein. The second exten-
sion consists in transposing the model we have de-
fined in this work for English to French, as done
by Tourille et al. (2017) for a more traditional ap-
proach based on a feature engineering approach.
</p>
<p>Acknowledgements
</p>
<p>We thank the anonymous reviewers for their in-
sightful comments. This work was supported by
Labex Digicosme, operated by the Foundation for
Scientific Cooperation (FSC) Paris-Saclay, under
grant CÔT and by the French National Agency for
Research under grant CABeRneT ANR-13-JS02-
0009-01.
</p>
<p>228</p>
<p />
</div>
<div class="page"><p />
<p>References
Abdulrahman AAl Abdulsalam, Sumithra Velupil-
</p>
<p>lai, and Stephane Meystre. 2016. UtahBMI at
SemEval-2016 Task 12: Extracting Temporal In-
formation from Clinical Text. In Proceedings of
the 10th International Workshop on Semantic Eval-
uation (SemEval-2016). Association for Computa-
tional Linguistics, San Diego, California, pages
1256–1262.
</p>
<p>Martín Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete War-
den, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. 2015. TensorFlow: Large-
Scale Machine Learning on Heterogeneous Sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.
</p>
<p>James F. Allen. 1983. Maintaining Knowledge About
Temporal Intervals. Communications of the ACM
26(11):832–843.
</p>
<p>Steven Bethard, Leon Derczynski, Guergana Savova,
James Pustejovsky, and Marc Verhagen. 2015.
SemEval-2015 Task 6: Clinical TempEval. In
Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015). Association
for Computational Linguistics, Denver, USA, pages
806–814.
</p>
<p>Steven Bethard, Guergana Savova, Wei-Te Chen, Leon
Derczynski, James Pustejovsky, and Marc Verhagen.
2016. SemEval-2016 Task 12: Clinical TempEval.
In Proceedings of the 10th International Workshop
on Semantic Evaluation (SemEval-2016). Associa-
tion for Computational Linguistics, San Diego, Cal-
ifornia, pages 1052–1062.
</p>
<p>Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research 32:267–270.
</p>
<p>Veera Raghavendra Chikka. 2016. CDE-IIITH at
SemEval-2016 Task 12: Extraction of Temporal
Information from Clinical documents using Ma-
chine Learning techniques. In Proceedings of the
10th International Workshop on Semantic Evalu-
ation (SemEval-2016). Association for Computa-
tional Linguistics, San Diego, California, pages
1237–1240.
</p>
<p>Arman Cohan, Kevin Meurer, and Nazli Goharian.
2016. GUIR at SemEval-2016 task 12: Temporal
Information Processing for Clinical Narratives. In
Proceedings of the 10th International Workshop on
</p>
<p>Semantic Evaluation (SemEval-2016). Association
for Computational Linguistics, San Diego, Califor-
nia, pages 1248–1255.
</p>
<p>Dmitriy Dligach, Timothy Miller, Chen Lin, Steven
Bethard, and Guergana Savova. 2017. Neural tem-
poral relation extraction. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers. Association for Computational Linguistics,
Valencia, Spain, pages 746–751.
</p>
<p>Cicero dos Santos, Bing Xiang, and Bowen Zhou.
2015. Classifying Relations by Ranking with Con-
volutional Neural Networks. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 626–634.
</p>
<p>Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple Cus-
tomization of Recursive Neural Networks for Se-
mantic Relation Classification. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Seattle, Washington, USA, pages
1372–1376.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Alistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-
wei H. Lehman, Mengling Feng, Mohammad Ghas-
semi, Benjamin Moody, Peter Szolovits, Leo A.
Celi, and Roger G. Mark. 2016. MIMIC-III, a freely
accessible critical care database. Scientific Data 3.
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A Method for Stochastic Optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.
</p>
<p>Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural Architectures for Named Entity Recogni-
tion. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. Association for Computational Linguis-
tics, San Diego, California, pages 260–270.
</p>
<p>Hee-Jin Lee, Hua Xu, Jingqi Wang, Yaoyun Zhang,
Sungrim Moon, Jun Xu, and Yonghui Wu. 2016.
UTHealth at SemEval-2016 Task 12: an End-to-End
System for Temporal Information Extraction from
Clinical Notes. In Proceedings of the 10th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2016). Association for Computational Linguistics,
San Diego, California, pages 1292–1297.
</p>
<p>Artuur Leeuwenberg and Marie-Francine Moens.
2017. Structured Learning for Temporal Relation
Extraction from Clinical Records. In Proceedings
</p>
<p>229</p>
<p />
</div>
<div class="page"><p />
<p>of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Vol-
ume 1, Long Papers. Association for Computational
Linguistics, Valencia, Spain, pages 1150–1158.
</p>
<p>Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard
Hovy. 2015. When Are Tree Structures Necessary
for Deep Learning of Representations? In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics, Lisbon, Portugal, pages
2304–2314.
</p>
<p>Chen Lin, Timothy Miller, Dimitry Dligach, Steven
Bethard, and Guergana Savova. 2016. Improv-
ing Temporal Relation Extraction with Training In-
stance Augmentation. In Proceedings of the 15th
Workshop on Biomedical Natural Language Pro-
cessing. Association for Computational Linguistics,
Berlin, Germany, pages 108–113.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. CoRR abs/1301.3781.
</p>
<p>Makoto Miwa and Mohit Bansal. 2016. End-to-End
Relation Extraction using LSTMs on Sequences and
Tree Structures. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1105–1116.
</p>
<p>James Pustejovsky and Amber Stubbs. 2011. Increas-
ing Informativeness in Temporal Annotation. In
Proceedings of the 5th Linguistic Annotation Work-
shop. Association for Computational Linguistics,
Stroudsburg, PA, USA, LAW V ’11, pages 152–160.
</p>
<p>Guergana K. Savova, James J. Masanz, Philip V.
Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C.
Kipper-Schuler, and Christopher G. Chute. 2010.
Mayo clinical Text Analysis and Knowledge Ex-
traction System (cTAKES): architecture, component
evaluation and applications. Journal of the Amer-
ican Medical Informatics Association 17(5):507–
513.
</p>
<p>Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Edinburgh, Scotland, UK., pages
151–161.
</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive Deep Models
for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
Seattle, Washington, USA, pages 1631–1642.
</p>
<p>William Styler IV, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Erick-
son, Timothy Miller, Chen Lin, Guergana Savova,
and James Pustejovsky. 2014. Temporal Annotation
in the Clinical Domain. Transactions of the Associ-
ation for Computational Linguistics 2:143–154.
</p>
<p>Julien Tourille, Olivier Ferret, Aurélie Névéol, and
Xavier Tannier. 2016. LIMSI-COT at SemEval-
2016 Task 12: Temporal relation identification us-
ing a pipeline of classifiers. In Proceedings of
the 10th International Workshop on Semantic Eval-
uation (SemEval-2016). Association for Computa-
tional Linguistics, San Diego, California, pages
1136–1142.
</p>
<p>Julien Tourille, Olivier Ferret, Xavier Tannier, and Au-
rélie Névéol. 2017. Temporal information extrac-
tion from clinical text. In Proceedings of the 15th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2017),
short paper session. Valencia, Spain, pages 739–
745.
</p>
<p>Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying Relations via Long
Short Term Memory Networks along Shortest De-
pendency Paths. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Lisbon, Portugal, pages 1785–1794.
</p>
<p>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation Classification via
Convolutional Deep Neural Network. In Proceed-
ings of COLING 2014, the 25th International Con-
ference on Computational Linguistics: Technical
Papers. Dublin City University and Association for
Computational Linguistics, Dublin, Ireland, pages
2335–2344.
</p>
<p>Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
Based Bidirectional Long Short-Term Memory Net-
works for Relation Classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers).
Association for Computational Linguistics, Berlin,
Germany, pages 207–212.
</p>
<p>230</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 231–236
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2036
</p>
<p>How to Make Context More Useful?
An Empirical Study on Context-Aware Neural Conversational Models
</p>
<p>Zhiliang Tian,1 Rui Yan,2∗ Lili Mou,3 Yiping Song,4 Yansong Feng,2 Dongyan Zhao2
1Baidu Inc., China tianzhiliang@baidu.com
</p>
<p>2Institute of Computer Science and Technology, Peking University, China
3Key Laboratory of High Confidence Software Technologies, MoE, China
</p>
<p>Institute of Software, Peking University, China
4Institute of Network Computing and Information Systems, Peking University, China
{ruiyan,songyiping,yansong,zhaody}@pku.edu.cn doublepower.mou@gmail.com
</p>
<p>Abstract
</p>
<p>Generative conversational systems are at-
tracting increasing attention in natural lan-
guage processing (NLP). Recently, re-
searchers have noticed the importance of
context information in dialog processing,
and built various models to utilize context.
However, there is no systematic compar-
ison to analyze how to use context effec-
tively. In this paper, we conduct an empir-
ical study to compare various models and
investigate the effect of context informa-
tion in dialog systems. We also propose a
variant that explicitly weights context vec-
tors by context-query relevance, outper-
forming the other baselines.
</p>
<p>1 Introduction
</p>
<p>Recently, human-computer conversation is attract-
ing increasing attention due to its promising poten-
tials and alluring commercial values. Researchers
have proposed both retrieval methods (Ji et al.,
2014; Yan et al., 2016) and generative meth-
ods (Ritter et al., 2011; Shang et al., 2015) for
automatic conversational systems. With the suc-
cess of deep learning techniques, neural networks
have demonstrated powerful capability of learning
human dialog patterns; given a user-issued utter-
ance as an input query q, the network can gen-
erate a reply r, which is usually accomplished
in a sequence-to-sequence (Seq2Seq) manner
(Shang et al., 2015).
</p>
<p>In the literature, there are two typical research
setups for dialog systems: single-turn and multi-
turn. Single-turn conversation is, perhaps, the sim-
plest setting where the model only takes q into
consideration when generating r (Shang et al.,
</p>
<p>∗Corresponding author.
</p>
<p>2015; Mou et al., 2016). However, most real-
world dialogs comprise multiple turns. Previous
utterances (referred to as context in this paper)
could also provide useful information about the di-
alog status and are the key to coherent multi-turn
conversation.
</p>
<p>Existing studies have realized the importance
of context, and proposed several context-aware
conversational systems. For example, Yan et al.
(2016) directly concatenate context utterances
and the current query; others use hierarchical
models, first capturing the meaning of individ-
ual utterances and then integrating them as dis-
courses (Serban et al., 2016). There could be
several ways of combining context and the cur-
rent query, e.g., pooling or concatenation (Sordoni
et al., 2015). Unfortunately, previous literature
lacks a systematic comparison of the above meth-
ods.
</p>
<p>In this paper, we conduct an empirical study
on context modeling in Seq2Seq-like conversa-
tional systems. We emphasize the following re-
search questions:
• RQ1. How can we make better use of con-
</p>
<p>text information? Our study shows that hier-
archical models are generally better than non-
hierarchical ones. We also propose a variant
of context integration that explicitly weights
a context vector by its relevance measure,
outperforming simple vector pooling or con-
catenation.
• RQ2. What is the effect of context on neural
</p>
<p>dialog systems? We find context information
is useful to neural conversational models. It
yields longer, more informative and diversi-
fied replies.
</p>
<p>To sum up, the contributions of this paper are
two-fold: (1) We conduct a systematic study on
context modeling in neural conversational mod-
els. (2) We further propose an explicitly con-
</p>
<p>231</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2036">https://doi.org/10.18653/v1/P17-2036</a></div>
</div>
<div class="page"><p />
<p>text weighting approach, outperforming the other
baselines.
</p>
<p>2 Models
</p>
<p>2.1 Non-Hierarchical Model
</p>
<p>To model a few utterances before the current
query, several studies directly concatenate these
sentences together and use a single model to cap-
ture the meaning of context and the query (Yan
et al., 2016; Sordoni et al., 2015). They are re-
ferred to as non-hierarchical models in our pa-
per. Such method is also used in other NLP
tasks, e.g., document-level sentiment analysis (Xu
et al., 2016) and machine comprehension (Wang
and Jiang, 2017).
</p>
<p>Following the classic encode-decoder frame-
work, we use a Seq2Seq network, which trans-
forms the query and context into a fixed-length
vector venc by a recurrent neural network (RNN)
during encoding; then, in the decoding phase, it
generates a reply r with another RNN in a word-
by-word fashion. (See Figure 1a.)
</p>
<p>In our study, we adopt RNNs with gated re-
current units (Cho et al., 2014, GRUs), which al-
leviates the long propagation problem of vanilla
RNNs. When decoding, we apply beam search
with a size of 5.
</p>
<p>2.2 Hierarchical Model
</p>
<p>A more complicated approach to context model-
ing is to build hierarchical models with a two-step
strategy: an utterance-level model captures the
meaning of each individual sentences, and then an
inter-utterance model integrates context and query
information (Figure 1b).
</p>
<p>Researchers have tried different ways of com-
bining information during inter-utterance model-
ing; this paper evaluates several prevailing meth-
ods.
</p>
<p>Sum pooling. Sum pooling (denoted as Sum)
integrates information over a candidate set by
summing the values in each dimension (Figure
2a). Given context vectors vc1 , · · · ,vcn and the
query vector vq, the encoded vector venc is
</p>
<p>venc =
</p>
<p>n∑
</p>
<p>i=1
</p>
<p>vci + vq (1)
</p>
<p>Sum pooling is used in Sordoni et al. (2015),
where bag-of-words (BoW) features of context
</p>
<p>(a) Non-hierarchical model.
</p>
<p>(b) Hierarchical model.
</p>
<p>Figure 1: Seq2Seq-like neural networks generate
a reply r based on context C = {c1, . . . , cn} and
the current query q with (a) non-hierarchical or (b)
hierarchical models.
</p>
<p>and the query are simply added. In our experi-
ments, sum pooling operates on the features ex-
tracted by sentence-level RNNs of context and
query utterances, as modern neural networks pre-
serve more information than BoW features.
</p>
<p>Concatenation. Concatenation (Concat) is
yet another method used in Sordoni et al. (2015).
This strategy concatenates every utterance-level
vectors vci and vq as a long vector, i.e., venc =
[vc0 ; . . . ;vcn ;vq]. (See Figure 2b.)
</p>
<p>Compared with sum pooling, vector concatena-
tion can distinguish different roles of the context
and query, as this operation keeps input separately.
One potential shortcoming, however, is that con-
catenation only works with fixed-length context.
</p>
<p>Sequential integration. Yao et al. (2015) and
Serban et al. (2015) propose hierarchical dialog
systems, where an inter-utterance RNN is built
upon utterance-level RNNs’ features (last hidden
state). Training is accomplished by end-to-end
gradient propagation, and the process is illustrated
in Figure 2c.
</p>
<p>Using an RNN to integrate context and query
vectors in a sequential manner enables compli-
cated information interaction. Based on the
RNN’s hidden states, Sum and Concat could
also be applied to obtain the encoded vector venc.
</p>
<p>232</p>
<p />
</div>
<div class="page"><p />
<p>(a) Sum pooling. (b) Concatenation.
</p>
<p>(c) Sequential Integration.
</p>
<p>(d) Weighted Sequential Integration.
</p>
<p>Figure 2: The inter-utterance modeling in hierar-
chical models. vci and vq are the utterance-level
vectors, hci and hq are the utterance-level hidden
states, αci and αq are the explicitly weights and
venc is the output of the encoder.
</p>
<p>However, we found their performance is worse
than only using the last hidden state (denoted as
Seq). One plausible reason might be that the
inter-sentence RNN is not long and that RNN can
preserve these information well. Therefore, this
variant is adopted in our experiments, as shown in
Figure 2c.
</p>
<p>2.3 Explicitly Weighting by Context-Query
Relevance
</p>
<p>In conversation, context utterances may vary in
content and semantics: context utterances that are
relevant to the query may be useful, while irrele-
vant ones may bring more about noise. Following
this intuition, we propose a variant that explicitly
weights the context vector by an attention score of
context-query relevance.
</p>
<p>First, we compute the similarity between the
context and query by the cosine measure
</p>
<p>sci = sim(ci, q) =
eci · eq
‖eci‖ · ‖eq‖
</p>
<p>(2)
</p>
<p>where
</p>
<p>eci =
∑
</p>
<p>w∈ci
ew and eq =
</p>
<p>∑
</p>
<p>w′∈q
ew′ (3)
</p>
<p>that is, the sentence embedding is the sum of word
embeddings.
</p>
<p>Following the spirit of attention mecha-
nisms (Bahdanau et al., 2014), we would like to
normalize these similarities by a softmax function
and obtain attention probabilities:
</p>
<p>αci =
exp(sci)∑n
</p>
<p>j=0 exp(scj ) + exp(sq)
(4)
</p>
<p>αq =
exp(sq)∑n
</p>
<p>j=0 exp(scj ) + exp(sq)
(5)
</p>
<p>where sq is computed in the same manner as sci
and is always 1, which is the cosine of two same
vectors. The intuition is that, if the context is less
relevant, we should mainly focus on the query it-
self, but if the context is relevant, we should focus
more evenly across context and the query.
</p>
<p>In other words, our explicitly weighting ap-
proach could be viewed as heuristic attention.
Akin to Subsection 2.2, we aggregate the weighted
context and query vectors by pooling and concate-
nation, resulting in the following two variants.
</p>
<p>• WSeq (sum), where weighted vectors are
summed together
</p>
<p>venc =
n∑
</p>
<p>i=0
</p>
<p>αcihci + αqhq (6)
</p>
<p>• WSeq (concat), where weighted vectors
are concatenated
</p>
<p>venc = [αc0hc0 ; . . . ;αcnhcn ;αqhq] (7)
</p>
<p>Notice that the explicitly weighting approach
can also be applied to sentence embeddings (with-
out inter-sentence RNN). We denote the variants
by WSum and WConcat, respectively; details are
not repeated. They are included for comparison in
Section 3.2.
</p>
<p>3 Experiments
</p>
<p>3.1 Setup
We conducted all experiments on a Chinese
dataset crawled from an online free chatting plat-
form, Baidu Tieba.1 To facilitate the research
of context’s effect, we established a multi-turn
conversational corpus following Sordoni et al.
(2015) and Serban et al. (2015). A data sam-
ple contains three utterances, being a triple
〈last context, query, reply〉. In total, we had
</p>
<p>1https://tieba.baidu.com
</p>
<p>233</p>
<p />
</div>
<div class="page"><p />
<p>Method BLEU-1 BLEU-2 BLEU-3 BLEU-4
</p>
<p>Context-Insensitive 4.611 1.488 0.657 0.311
</p>
<p>Non-Hierarchical 4.805 1.507 0.678 0.343
</p>
<p>Hierarchical
</p>
<p>Sum 4.440 1.367 0.505 0.042
WSum 5.055 1.667 0.741 0.378
Concat 5.107 1.688 0.747 0.420
WConcat 5.181 1.763 0.745 0.342
Seq 5.355 1.771 0.916 0.387
WSeq (sum) 5.134 1.586 0.7429 0.4359
WSeq (concat) 5.322 1.883 0.9966 0.6897
</p>
<p>Table 1: Performance of different models.
</p>
<p>500,000 samples for training, 2000 for validation,
and 4000 for testing. The hyperparameters of
neural networks were mainly derived from Shang
et al. (2015) and Song et al. (2016): embeddings
620d and hidden states 1000d; we used AdaDelta
for optimization.
</p>
<p>3.2 Results and Analysis
</p>
<p>We evaluated model performance by BLEU
scores. As this paper compares various models,
it is unaffordable for us to hire workers to man-
ually annotate their satisfaction. BLEU scores,
albeit imperfect for open-domain dialog systems,
exhibits more or less correlation with human sat-
isfaction (Liu et al., 2016; Tao et al., 2017). We
present in Table 1 the overall performance of the
models introduced in Section 2, and answer our
research questions as follows.
</p>
<p>RQ1: How can we make better use of context in-
formation?
</p>
<p>We first observe that context-aware methods
generally outperform the context-insensitive one.
This implies context is indeed useful in open-
domain, chit-chat-style dialog systems. The re-
sults are consistent with previous studies (Sordoni
et al., 2015; Serban et al., 2015).
</p>
<p>Among context-aware neural conversational
models, we have the following findings.
</p>
<p>• Hierarchical structures outperform the non-
hierarchical one.
</p>
<p>Comparing the non-hierarchical and hierar-
chical structures, we find it obvious that
(most) hierarchical models outperform the
non-hierarchical one by a large margin. The
results show that, dialog systems are differ-
</p>
<p>ent from other NLP applications, e.g., com-
prehension (Wang and Jiang, 2017), where
non-hierarchical recurrent neural networks
are adopted to better integrate information
across different sentences. A plausible expla-
nation, as indicated by Meng et al. (2017), is
that conversational sentences are not neces-
sarily uttered by a same speaker, and litera-
ture shows consistent evidence of the effec-
tiveness of hierarchical RNNs in dialog sys-
tems.
</p>
<p>• Keeping the roles of different utterances sep-
arately is important.
</p>
<p>As mentioned in Section 2, the concatena-
tion operation (Concat) distinguishes the
roles of different utterances, while sum pool-
ing Sum aggregates information in a homo-
geneous way. We see that the former outper-
forms the latter in both sentence-embedding
and inter-sentence RNN levels, showing that
sum pooling is not suitable for treating dialog
context. Our conjecture is that sum pooling
buries illuminating query information under
less important context. Hence, keeping them
separately will generally help.
</p>
<p>• The context-query relevance score benefits
conversational systems.
</p>
<p>Our explicitly weighting approach computes
an attention probability by context-query rel-
evance. In all variants (Sum, Concat, and
Seq), explicitly weighting improves the per-
formance by a large margin (except BLEU-1
for Seq). The results indicate that context-
query relevance is useful, as it emphasizes
</p>
<p>234</p>
<p />
</div>
<div class="page"><p />
<p>Method Length Entropy Diversity
</p>
<p>Context-Insensitive 4.008 7.648 0.917
Context-Aware 4.204 7.863 0.927
</p>
<p>Ground Truth 9.735 9.277 0.949
</p>
<p>Table 2: The length, entropy, and diversity of
the replies on the context-insensitive and context-
aware (WSeq,concat) methods.
</p>
<p>relevant context utterances as well as weak-
ens irrelevant contexts.
</p>
<p>RQ2: What is the effect of context on neural dia-
log systems?
</p>
<p>We are now curious about how context informa-
tion affects neural conversational systems. In Ta-
ble 2, we present three auxiliary metrics, i.e., sen-
tence length, entropy, and diversity. The former
two are used in Serban et al. (2016) and Mou et al.
(2016), whereas the latter one is used in Zhang and
Hurley (2008).
</p>
<p>As shown, content-aware conversational mod-
els tend to generate longer, more meaningful and
diverse replies compared with content-insensitive
models, given that they also improve BLEU
scores.2
</p>
<p>This shows an interesting phenomenon of neu-
ral sequence generation: an encoder-decoder
framework needs sufficient source information for
meaningful generation of the target; it simply does
not fall into meaningful content from less mean-
ingful input. A similar phenomenon is also re-
ported in our previous work (Mou et al., 2016);
we show that, a same network will generate more
meaningful sentences if it starts from a given
(meaningful) keyword. These results also partially
explain why a seq2seq neural network tends
to generate short and universally relevant replies
in open-domain conversation, despite its success
in machine translation, abstractive summarization,
etc.
</p>
<p>4 Conclusion
</p>
<p>In this work, we analyzed the effect of context in
generative conversational models. We conducted
a systematic comparison among existing meth-
</p>
<p>2This condition is important when we draw conclusions.
The length, entropy and diversity metrics do not make sense
by themselves alone, because a system can achieve very high
scores by repetitively generating random words.
</p>
<p>ods and our newly proposed variant that explic-
itly weights context vectors by context-query rele-
vance.
</p>
<p>We show that hierarchical RNNs generally out-
perform non-hierarchical ones, and that explicitly
weighting context information can emphasize the
relevant context utterances and weaken less rele-
vant ones.
</p>
<p>Our experiments also reveal an interesting phe-
nomenon: with context information, neural net-
works tend to generate longer, more meaningful
and diverse replies, which sheds light on neural
sequence generation.
</p>
<p>Acknowledgments
</p>
<p>We thank the reviewers for insightful com-
ments. This research is partially supported by
863 Grant No. 2015AA015403 and NSFC Grant
No. 61672058.
</p>
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2014. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations.
</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .
</p>
<p>Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint arXiv:1408.6988 .
</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How NOT to evaluate your dialogue system: An
empirical study of unsupervised evaluation metrics
for dialogue response generation. In Proceedings
of the 2016 Conference on Empirical Methods in
Natural Language Processing. pages 2122–2132.
https://doi.org/10.18653/v1/D16-1230.
</p>
<p>Zhao Meng, Lili Mou, and Zhi Jin. 2017. Hierar-
chical RNN with static sentence-level attention for
text-based speaker change detection. arXiv preprint
arXiv:1703.07713 .
</p>
<p>Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,
and Zhi Jin. 2016. Sequence to backward and
forward sequences: A content-introducing ap-
proach to generative short-text conversation. In
Proceedings of the 26th International Conference
on Computational Linguistics. pages 3349–3358.
http://aclweb.org/anthology/C16-1316.
</p>
<p>235</p>
<p />
</div>
<div class="page"><p />
<p>Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. pages 583–
593. http://aclweb.org/anthology/D11-1054.
</p>
<p>Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2015. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the 30th AAAI Conference on Artificial Intelligence.
pages 3776–3783.
</p>
<p>Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. arXiv preprint arXiv:1605.06069 .
</p>
<p>Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conver-
sation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
pages 1577–1586. https://doi.org/10.3115/v1/P15-
1152.
</p>
<p>Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and
Ming Zhang. 2016. Two are better than one: An en-
semble of retrieval-and generation-based dialog sys-
tems. arXiv preprint arXiv:1610.07149 .
</p>
<p>Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
196–205. https://doi.org/10.3115/v1/N15-1020.
</p>
<p>Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui
Yan. 2017. RUBER: An unsupervised method for
automatic evaluation of open-domain dialog sys-
tems. arXiv preprint arXiv:1701.03079 .
</p>
<p>Shuohang Wang and Jing Jiang. 2017. Machine com-
prehension using match-LSTM and answer pointer.
In Proceedings of the International Conference on
Learning Representations.
</p>
<p>Jiacheng Xu, Danlu Chen, Xipeng Qiu, and Xuanjing
Huang. 2016. Cached long short-term memory neu-
ral networks for document-level sentiment classifi-
cation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing. pages
1660–1669. https://doi.org/10.18653/v1/D16-1172.
</p>
<p>Rui Yan, Yiping Song, and Hua Wu. 2016. Learning
to respond with deep neural networks for retrieval-
based human-computer conversation system. In
Proceedings of the 39th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. pages 55–64.
</p>
<p>Kaisheng Yao, Geoffrey Zweig, and Baolin Peng.
2015. Attention with intention for a neural network
conversation model. In NIPS Workshop.
</p>
<p>Mi Zhang and Neil Hurley. 2008. Avoiding monotony:
Improving the diversity of recommendation lists. In
Proceedings of the 2008 ACM Conference on Rec-
ommender Systems. pages 123–130.
</p>
<p>236</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 237–243
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2037
</p>
<p>Cross-lingual and cross-domain discourse segmentation
of entire documents
</p>
<p>Chloé Braud
CoAStaL DIKU
</p>
<p>University of Copenhagen
University Park 5,
2100 Copenhagen
</p>
<p>chloe.braud@gmail.com
</p>
<p>Ophélie Lacroix
CoAStaL DIKU
</p>
<p>University of Copenhagen
University Park 5,
2100 Copenhagen
</p>
<p>lacroix@di.ku.dk
</p>
<p>Anders Søgaard
CoAStaL DIKU
</p>
<p>University of Copenhagen
University Park 5,
2100 Copenhagen
</p>
<p>soegaard@di.ku.dk
</p>
<p>Abstract
</p>
<p>Discourse segmentation is a crucial step
in building end-to-end discourse parsers.
However, discourse segmenters only exist
for a few languages and domains. Typi-
cally they only detect intra-sentential seg-
ment boundaries, assuming gold standard
sentence and token segmentation, and re-
lying on high-quality syntactic parses and
rich heuristics that are not generally avail-
able across languages and domains. In
this paper, we propose statistical discourse
segmenters for five languages and three
domains that do not rely on gold pre-
annotations. We also consider the problem
of learning discourse segmenters when
no labeled data is available for a lan-
guage. Our fully supervised system ob-
tains 89.5% F1 for English newswire, with
slight drops in performance on other do-
mains, and we report supervised and un-
supervised (cross-lingual) results for five
languages in total.
</p>
<p>1 Introduction
</p>
<p>Discourse segmentation is the first step in building
a discourse parser. The goal is to identify the min-
imal units — called Elementary Discourse Units
(EDU) — in the documents that will then be linked
by discourse relations. For example, the sentences
(1a) and (1b)1 are each segmented into two EDUs,
then respectively linked by a CONTRAST and an
ATTRIBUTION relation. The EDUs are mostly
clauses and may cover a full sentence. This step
is crucial: making a segmentation error leads to an
error in the final analysis. Discourse segmentation
can also inform other tasks, such as argumentation
</p>
<p>1The examples come from the RST Discourse Treebank.
</p>
<p>mining, anaphora resolution, or speech act assign-
ment (Sidarenka et al., 2015).
</p>
<p>(1) a. [Such trappings suggest a glorious past]
[but give no hint of a troubled present.]
</p>
<p>b. [He said] [the thrift will to get regulators
to reverse the decision.]
</p>
<p>We focus on the Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988) – and re-
sources such as the RST Discourse Treebank
(RST-DT) (Carlson et al., 2001) – in which dis-
course structures are trees covering the docu-
ments. Most recent works on RST discourse pars-
ing focuses on the task of tree building, relying
on a gold discourse segmentation (Ji and Eisen-
stein, 2014; Feng and Hirst, 2014; Li et al., 2014;
Joty et al., 2013). However, discourse parsers’
performance drops by 12-14% when relying on
predicted segmentation (Joty et al., 2015), un-
derscoring the importance of discourse segmen-
tation. State-of-the-art performance for discourse
segmentation on the RST-DT is about 91% in F1
with predicted parses (Xuan Bach et al., 2012),
but these systems rely on a gold segmentation of
sentences and words, therefore probably overesti-
mating performance in the wild. We propose to
build discourse segmenters without making any
data assumptions. Specifically, rather than seg-
menting sentences, our systems segment docu-
ments directly.
</p>
<p>Furthermore, only a few systems have been de-
veloped for languages other than English and do-
mains other than the Wall Street Journal texts from
the RST-DT. We are the first to perform exper-
iments across 5 languages, and 3 non-newswire
English domains. Since our goal is to provide
a system usable for low-resource languages, we
only use language-independent resources: here,
the Universal Dependencies (UD) (Nivre et al.,
</p>
<p>237</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2037">https://doi.org/10.18653/v1/P17-2037</a></div>
</div>
<div class="page"><p />
<p>2016) Part-of-Speech (POS) tags, for which an-
notations exist for about 50 languages. For the
cross-lingual experiments, we also rely on cross-
lingual word embeddings induced from parallel
data. With a shared representation, we can transfer
model parameters across languages, or learn mod-
els jointly through multi-task learning.
</p>
<p>Contributions: We (i) propose a general sta-
tistical discourse segmenter (ii) that does not as-
sume gold sentences and tokens, and (iii) evaluate
it across 5 languages and 3 domains.
</p>
<p>We make our code available at https://bitbucket.
org/chloebt/discourse.
</p>
<p>2 Related work
</p>
<p>For English RST-DT, the best discourse segmen-
tation results were presented in Xuan Bach et al.
(2012) (F1 91.0% with automatic parse, 93.7
with gold parse) – and in Joty et al. (2015) for
the Instructional corpus (Subba and Di Euge-
nio, 2009) (F1 80.9% on 10-fold). Segmenters
based on handwritten rules have been developed
for Brazilian Portuguese (Pardo and Nunes, 2008)
(51.3% to 56.8%, depending on the genre), Span-
ish (da Cunha et al., 2010, 2012) (80%) and
Dutch (van der Vliet, 2010) (73% with automatic
parse, 82% with gold parse).2
</p>
<p>Most statistical discourse segmenters are based
on classifiers (Fisher and Roark, 2007; Joty et al.,
2015). Subba and Di Eugenio (2007) were the first
to use a neural network, and Sporleder and Lapata
(2005) to model the task as a sequence prediction
problem. In this work, we do sequence prediction
using a neural network.
</p>
<p>All these systems rely on a quite large range
of lexical and syntactic features (e.g. token, POS
tags, lexicalized production rules). Sporleder and
Lapata (2005) present arguments for a knowledge-
lean system that can be used for low-resourced
languages. Their system, however, still relies on
several tools and gold annotations (e.g. POS tag-
ger, chunker, list of connectives, gold sentences).
In contrast, we present what is to the best of our
knowledge the first work on discourse segmenta-
tion that is directly applicable to low-resource lan-
guages, presenting results for scenarios where no
labeled data is available for the target language.
</p>
<p>Previous work, relying on gold sentence bound-
aries, also only considers intra-sentential segment
</p>
<p>2 For German (Sidarenka et al., 2015) propose a seg-
menter in clauses (that may be EDU or not).
</p>
<p>boundaries. We move to processing entire docu-
ments, motivated by the fact that sentence bound-
aries are not easily detected across all languages.
</p>
<p>3 Discourse segmentation
</p>
<p>Nature of the EDUs Discourse segmentation is
the first step in annotating a discourse corpus. The
annotation guidelines define what is the nature of
the EDUs, broadly relying on lexical and syntac-
tic clues. If sentences and independent clauses are
always minimal units, some fine distinctions make
the task difficult.
</p>
<p>In the English RST-DT (Carlson and Marcu,
2001), lexical information is crucial: for instance,
the presence of the discourse connective “but” in
example (1a)3 indicates the beginning of an EDU.
In addition, clausal complements of verbs are gen-
erally not treated as EDUs. Exceptions are the
complements of attribution verbs, as in (1b), and
the infinitival clauses marking a PURPOSE rela-
tion as the second EDU in (2a). Note that, in
this latter example, the first infinitival clause (“to
cover up . . .”) is, however, not considered as an
EDU. This fine distinction corresponds to one of
the main difficulties of the task. Another one is
linked to coordination: coordinated clauses are
generally segmented as in (2b), but not coordi-
nated verb phrases as in (2c).
</p>
<p>(2) a. [A grand jury has been investigating
whether officials at Southern Co. ac-
counting conspired to cover up their ac-
counting for spare parts] [to evade federal
income taxes.]
</p>
<p>b. [they parcel out money] [so that their
clients can find temporary living
quarters,] [buy food] (. . .) [and replaster
walls.]
</p>
<p>c. [Under Superfund, those] [who owned,
generated or transported hazardous waste]
[are liable for its cleanup, (. . .)]
</p>
<p>Finally, in a multi-lingual and multi-domain set-
ting, note that all the corpora do not follow the
same rules: for example, the relation ATTRIBU-
TION is only annotated in the English RST-DT
and the corpora for Brazilian Portuguese, conse-
quently, complements of attribution verbs are not
segmented in the other corpora.
</p>
<p>3All the examples given come from (Carlson et al., 2001).
</p>
<p>238</p>
<p />
</div>
<div class="page"><p />
<p>Binary task As in previous studies, we view
segmentation as a binary task at the word level:
a word is either an EDU boundary (label B, begin-
ning an EDU) or not (label I, inside an EDU). This
design choice is motivated by the fact that, in RST
corpora, the EDUs cover the documents entirely,
and that EDUs mostly are adjacent spans of text.
An exception is when embedded EDUs break up
another EDU, as in Example (3). The units 1 and
3 form in fact one EDU. We follow previous work
on treating this as three segments, but note that this
may not be the optimal solution.
</p>
<p>(3) [But maintaining the key components (. . .)]1
[– a stable exchange rate and high levels of imports –]2
[will consume enormous amounts (. . .).]3
</p>
<p>Document-level segmentation Contrary to pre-
vious studies, we do not assume gold sentences:
Since sentence boundaries are EDU boundaries,
our system jointly predicts sentence and intra-
sentential EDU boundaries.
</p>
<p>4 Cross-lingual/-domain segmentation
</p>
<p>Data is scarce for discourse. In order to build
statistical segmenters for new, low-resourced lan-
guages and domains, we propose to combine cor-
pora within a multi-task learning setting (Sec-
tion 5) leveraging data from well-resourced lan-
guages or domains. Models are trained on several
(source) languages (resp. domains) – each viewed
as an auxiliary task – for building a system for a
(target) language (resp. domain).
</p>
<p>Cross-domain For cross-domain experiments,
the models are trained on all the other (source) do-
mains and parameters are tuned on data for the
target domain. This allows us to improve per-
formance when only few data points (i.e. devel-
opment set) are annotated for a specific domain
(semi-supervised setting).
</p>
<p>Cross-lingual For cross-lingual experiments,
we tune our system’s parameters by training a sys-
tem on the data for three languages with sufficient
amounts of data (namely, German, Spanish and
Brazilian Portuguese), and using English data as a
development set. We then train a new model also
using multi-task learning (with these tuned param-
eters) using only source training data, and report
performance on the target test set. This allows us
to estimate performance when no data is available
for the language of interest (unsupervised adapta-
tion).
</p>
<p>5 Multi-task learning
</p>
<p>Our models perform sequence labeling based on
a stacked k-layer bi-directional LSTM, a variant
of LSTMs (Hochreiter and Schmidhuber, 1997)
that reads the input in both regular and reversed
order, allowing to take into account both left and
right contexts (Graves and Schmidhuber, 2005).
For our task, this enables us, for example, to dis-
tinguish between coordinated nouns and clauses.
This model takes as input a sequence of words
(and, here, POS tags) represented by vectors (ini-
tialized randomly or, for words, using pre-trained
embedding vectors). The sequence goes through
an embedding layer, and we compute the predic-
tions of the forward and backward states for the
k stacked layers. At the upper level, we compute
the softmax predictions for each word based on a
linear transformation. We use a logistic loss.
</p>
<p>We also investigate joint training of multiple
languages and domains for discourse segmenta-
tion. We thus try to leverage languages and do-
mains regularities by sharing the architecture and
parameters through multi-task training, where an
auxiliary task is a source language (resp. domain)
different from the target language (resp. domain)
of interest. Specifically, we train models based
on hard parameters sharing (Caruana, 1993; Col-
lobert et al., 2011; Klerke et al., 2016; Plank et al.,
2016):4 each task is associated with a specific out-
put layer, whereas the inner layers – the stacked
LSTMs – are shared across the tasks. At train-
ing time, we randomly sample data points from
one task and do forward predictions. During back-
propagation, we modify the weights of the shared
layers and the task-specific outer layer. The model
is optimized for one target task (corresponding to
the development data used). Except for the outer
layer, the target task model is thus regularized by
the induction of auxiliary models.
</p>
<p>6 Corpora
</p>
<p>Table 1 summarizes statistics about the data. For
English, we use four corpora, allowing us to eval-
uate cross-domain performance: the RST-DT (En-
DT) composed of Wall Street Journal articles;
the SFU review corpus5 (En-SFU-DT) contain-
ing product reviews; the instructional corpus (En-
Instr-DT) (Subba and Di Eugenio, 2009) built
</p>
<p>4We used a modified version of (Plank et al., 2016) fixing
the random seed and using standard SGD.
</p>
<p>5https://www.sfu.ca/∼mtaboada
</p>
<p>239</p>
<p />
</div>
<div class="page"><p />
<p>Corpus #Doc #EDU #Sent #Words
</p>
<p>En-SFU-DT 400 28, 260 16, 827 328, 362
En-DT 385 21, 789 9, 074 210, 584
Pt-DT 330 12, 594 4, 385 136, 346
Es-DT 266 3, 325 1, 816 57, 768
En-Instr-DT 176 5, 754 3, 090 56, 197
De-DT 174 2, 979 1, 805 33, 591
</p>
<p>En-Gum-DT 54 3, 151 2, 400 44, 577
Nl-DT 80 2, 345 1, 692 25, 095
</p>
<p>Table 1: Number of documents, EDUs, sentences
and words (according to UDPipe, see Section 7).
</p>
<p>on instruction manuals; and the GUM corpus6
</p>
<p>(En-Gum-DT) containing interviews, news, travel
guides and how-tos.
</p>
<p>For cross-lingual experiments, we use anno-
tated corpora for Spanish (Es-DT) (da Cunha
et al., 2011),7 German (De-DT) (Stede, 2004;
Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet
et al., 2011; Redeker et al., 2012) and, for Brazil-
ian Portuguese, we merged four corpora (Pt-
DT) (Cardoso et al., 2011; Collovini et al., 2007;
Pardo and Seno, 2005; Pardo and Nunes, 2003,
2004) as done in (Maziero et al., 2015).
</p>
<p>Three other RST corpora exist, but we were not
able to obtain cross-lingual word embeddings for
Basque (Iruskieta et al., 2013) and Chinese (Wu
et al., 2016), and could not obtain the data for
Tamil (Subalalitha and Parthasarathi, 2012).
</p>
<p>7 Experiments
</p>
<p>Data We use the official test sets for the En-DT
(38 documents) and the Es-DT (84). For the oth-
ers, we randomly choose 38 documents as test set,
and either keep the rest as development set (Nl-
DT) or split it into a train and a development set.
</p>
<p>Baselines As baselines at the document level,
we report the scores obtained (a) when only con-
sidering the sentence boundaries predicted using
UDPipe (Straka et al., 2016) (UDP-S),8 and (b)
when EDU boundaries are added after each token
PoS-tagged with “PUNCT” (UDP-P), marking ei-
ther an inter- or an intra-sentential boundary.
</p>
<p>Systems As described in Section 3, our systems
are either mono-lingual or mono-domain (mono),
or based on a joint training across languages or
domains (cross). The “mono” systems are built for
</p>
<p>6https://corpling.uis.georgetown.edu/gum/
7We only use the test set from the annotator A.
8http://ufal.mff.cuni.cz/udpipe
</p>
<p>Mono Cross UDP-S UDP-P
</p>
<p>la
ng
</p>
<p>ua
ge
</p>
<p>s En-DT 89.5 62.4 55.6 57.5
Pt-DT 82.2 64.0 49.0 62.5
Es-DT 79.3 64.3 64.9 53.3
De-DT 85.1 76.6 69.7 68.7
Nl-DT - 82.6 80.2 76.6
</p>
<p>do
m
</p>
<p>ai
ns
</p>
<p>En-DT (news) 89.5 63.0 55.6 57.5
En-SFU-DT 85.5 81.5 70.2 66.1
En-Instr-DT 87.1 77.7 66.5 69.5
En-Gum-DT - 68.1 77.2 61.8
</p>
<p>Table 2: Results (F1), comparing cross-lingual
and cross-domain results with UDPipe.
</p>
<p>the languages and domains represented by enough
data (upper part of Table 1). The “cross” models
are trained using multi-task learning.
</p>
<p>Parameters The hyper-parameters are tuned on
the development set: number of iterations i ∈
{10, 20, 30}, Gaussian noise σ ∈ {0.1, 0.2}, and
number of dimensions d ∈ {50, 500}. We fix the
number n of stacked hidden layers to 2 and the size
of the hidden layers h to 100 after experimenting
on the En-DT.9 Our final models use σ = 0.2 and
d = 500.
</p>
<p>Representation We use tokens and POS tags as
input data.10 The aim is to build a representa-
tion considering the current word and its context,
i.e. its POS and the surrounding words/POS. We
use the pre-trained UDPipe models to postag the
documents for all languages. We experiment with
randomly initialized and pre-trained cross-lingual
word embeddings built on Europarl (Levy et al.,
2017), keeping either the full 500 dimensions, or
the first 50 ones.
</p>
<p>Results Our systems are evaluated using F1 over
the boundaries (B labels), disregarding the first
word of each document. Our scores are summa-
rized in Table 2.
</p>
<p>Our supervised, monolingual systems unsur-
prisingly give the best performance, with F1 above
80%. The results are generally linked to the size of
the corpora, the larger the better. Only exception
is the En-SFU-DT, which, however, include more
varied annotation (the authors stated that the anno-
tations “have not been checked for reliability”).
</p>
<p>The (semi-supervised) cross-domain setting al-
lows us to present the scores one can expect when
</p>
<p>9With n ∈ {1, 2, 3} and h ∈ {100, 200, 400}).
10A document is a sequence alternating words and POS.
</p>
<p>The tokens are labeled with a B or an I, the POS, always
labeled with an I, are inserted after each token they refer to.
</p>
<p>240</p>
<p />
</div>
<div class="page"><p />
<p>only 25 documents are annotated for a new domain
(i.e. the development set for the target domain),
and to give the first results on the En-Gum-DT,
but here, our model is actually outperformed by
the sentence-based baseline (UDP-S).
</p>
<p>The (unsupervised) cross-lingual models are
generally largely better than UDPipe. These are
scores that one can expect when doing cross-
lingual transfer to build a discourse segmenter for
a new language for which no annotated data are
available. The performance is still quite high,
demonstrating the coherence between the anno-
tation schemes, and the potential of cross-lingual
transfer. We acknowledge that this is a small set of
relatively similar Indo-European languages, how-
ever.
</p>
<p>Note that the sentence-based baseline has a high
precision (e.g. 96.6 on Es-DT against 59.8 for
the cross-lingual system), but a much lower recall,
since it mainly predicts the sentence boundaries.
On corpora that mostly contain sentential EDUs
(e.g. Nl-DT, see Table 1), this is a good strat-
egy. Using the punctuation (UDP-P) could be a
better approximation for corpora with more var-
ied EDUs, see the large gain for the Pt-DT and the
En-Instr-DT.
</p>
<p>Our scores are not directly comparable with
sentence-level state-of-the-art systems (see Sec-
tion 2). However, for En-DT, our best system
correctly identifies 950 sentence boundaries out of
991, but gets only 84.5% in F1 for intra-sentential
boundaries,11 thus lower than the state-of-the-art
(91.0%). This is because we consider much less
information, and because the system was not opti-
mized for this task. Interestingly, our simple sys-
tem beats HILDA (Hernault et al., 2010) (74.1% in
F1), is as good as the other neural network based
system (Subba and Di Eugenio, 2007), and is close
to SPADE (Soricut and Marcu, 2003) (85.2% in
F1) (Joty et al., 2015), while all of these systems
use parse tree information.
</p>
<p>Finally, looking at the errors of our system on
the En-DT, we found that most of them are on the
tokens “to” (30 out of 94 not predicted as ’B’) and
“and” (24 out of 103), as expected given the anno-
tation guidelines (see Section 3). These words are
highly ambiguous regarding discourse segmenta-
tion (e.g. in the test set, 42.3% of “and” indicates a
boundary). We also found errors with coordinated
</p>
<p>11This score ignores the sentences containing only one
EDU (Sporleder and Lapata, 2005).
</p>
<p>verb phrases – e.g. “[when rates are rising] [and
shift out at times]” – that should be split (Carl-
son et al., 2001), a distinction hard to make with-
out syntactic trees. Finally, since we use predicted
POS tags, our system learns from noisy data and
makes errors due to postagging and tokenisation
errors.
</p>
<p>8 Conclusion
</p>
<p>We proposed new discourse segmenters with good
performance for many languages and domains, at
the document level, within a fully predicted setting
and using only language independent tools.
</p>
<p>Acknowledgements
</p>
<p>We would like to thank the anonymous reviewers
for their comments. This research is funded by the
ERC Starting Grant LOWLANDS No. 313695.
</p>
<p>References
Paula C.F. Cardoso, Erick G. Maziero, Mara Luca Cas-
</p>
<p>tro Jorge, Eloize R.M. Seno, Ariani Di Felippo, Lu-
cia Helena Machado Rino, Maria das Gracas Volpe
Nunes, and Thiago A. S. Pardo. 2011. CSTNews
- a discourse-annotated corpus for single and multi-
document summarization of news texts in Brazilian
Portuguese. In Proceedings of the 3rd RST Brazilian
Meeting. pages 88–105.
</p>
<p>Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging reference manual. Technical report, University
of Southern California Information Sciences Insti-
tute.
</p>
<p>Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue.
</p>
<p>Rich Caruana. 1993. Multitask learning: a knowledge-
based source of inductive bias. In Proceedings of
ICML.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research
12:2493–2537.
</p>
<p>Sandra Collovini, Thiago I Carbonel, Juliana Thiesen
Fuchs, Jorge César Coelho, Lúcia Rino, and Renata
Vieira. 2007. Summ-it: Um corpus anotado com
informaçoes discursivas visandoa sumarizaçao au-
tomática. In Proceedings of TIL.
</p>
<p>Iria da Cunha, Eric SanJuan, Juan-Manuel Torres-
Moreno, Marina Lloberas, and Irene Castellón.
</p>
<p>241</p>
<p />
</div>
<div class="page"><p />
<p>2010. DiSeg: Un segmentador discursivo au-
tomático para el español. Procesamiento del
lenguaje natural 45:145–152.
</p>
<p>Iria da Cunha, Eric SanJuan, Juan-Manuel Torres-
Moreno, Marina Lloberes, and Irene Castellón.
2012. DiSeg 1.0: The first system for Span-
ish discourse segmentation. Expert Syst. Appl.
39(2):1671–1678.
</p>
<p>Iria da Cunha, Juan-Manuel Torres-Moreno, and Ger-
ardo Sierra. 2011. On the development of the RST
Spanish Treebank. In Proceedings of LAW.
</p>
<p>Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of ACL.
</p>
<p>Seeger Fisher and Brian Roark. 2007. The utility of
parse-derived features for automatic discourse seg-
mentation. In Proceedings ACL.
</p>
<p>Alex Graves and Jrgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works pages 5–6.
</p>
<p>Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification.
Dialogue and Discourse 1:1–33.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.
</p>
<p>Mikel Iruskieta, Marı́a J. Aranzabe, Arantza Diaz de
Ilarraza, Itziar Gonzalez-Dios, Mikel Lersundi, and
Oier Lopez de la Calle. 2013. The RST Basque
Treebank: an online search interface to check rhetor-
ical relations. In Proceedings of the 4th Workshop
RST and Discourse Studies.
</p>
<p>Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
Proceedings of ACL.
</p>
<p>Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng.
2015. Codra: A novel discriminative framework for
rhetorical analysis. Computational Linguistics 41:3.
</p>
<p>Shafiq R. Joty, Giuseppe Carenini, Raymond T. Ng,
and Yashar Mehdad. 2013. Combining intra- and
multi-sentential rhetorical parsing for document-
level discourse analysis. In Proceedings of ACL.
</p>
<p>Sigrid Klerke, Yoav Goldberg, and Anders Søgaard.
2016. Improving sentence compression by learning
to predict gaze. In Proceedings of NAACL.
</p>
<p>Omer Levy, Anders Søgaard, and Yoav Goldberg.
2017. A strong baseline for learning cross-lingual
word embeddings from sentence alignments. In
Proceedings of EACL.
</p>
<p>Jiwei Li, Rumeng Li, and Eduard H. Hovy. 2014. Re-
cursive deep models for discourse parsing. In Pro-
ceedings of EMNLP.
</p>
<p>William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text 8:243–281.
</p>
<p>Erick G. Maziero, Graeme Hirst, and Thiago A. S.
Pardo. 2015. Adaptation of discourse parsing mod-
els for Portuguese language. In Proceedings of
the Brazilian Conference on Intelligent Systems
(BRACIS).
</p>
<p>Joakim Nivre, Željko Agić, Lars Ahrenberg, Maria Je-
sus Aranzabe, Masayuki Asahara, Aitziber Atutxa,
Miguel Ballesteros, John Bauer, Kepa Bengoetxea,
Yevgeni Berzak, Riyaz Ahmad Bhat, Cristina
Bosco, Gosse Bouma, Sam Bowman, Gülşen Ce-
birolu Eryiit, Giuseppe G. A. Celano, Çar Çöltekin,
Miriam Connor, Marie-Catherine de Marneffe,
Arantza Diaz de Ilarraza, Kaja Dobrovoljc, Timo-
thy Dozat, Kira Droganova, Tomaž Erjavec, Richárd
Farkas, Jennifer Foster, Daniel Galbraith, Sebas-
tian Garza, Filip Ginter, Iakes Goenaga, Koldo Go-
jenola, Memduh Gokirmak, Yoav Goldberg, Xavier
Gómez Guinovart, Berta Gonzáles Saavedra, Nor-
munds Grūzītis, Bruno Guillaume, Jan Hajič, Dag
Haug, Barbora Hladká, Radu Ion, Elena Irimia, An-
ders Johannsen, Hüner Kaşkara, Hiroshi Kanayama,
Jenna Kanerva, Boris Katz, Jessica Kenney, Si-
mon Krek, Veronika Laippala, Lucia Lam, Alessan-
dro Lenci, Nikola Ljubešić, Olga Lyashevskaya,
Teresa Lynn, Aibek Makazhanov, Christopher Man-
ning, Cătălina Mărănduc, David Mareček, Héctor
Martı́nez Alonso, Jan Mašek, Yuji Matsumoto,
Ryan McDonald, Anna Missilä, Verginica Mititelu,
Yusuke Miyao, Simonetta Montemagni, Keiko So-
phie Mori, Shunsuke Mori, Kadri Muischnek, Nina
Mustafina, Kaili Müürisep, Vitaly Nikolaev, Hanna
Nurmi, Petya Osenova, Lilja Øvrelid, Elena Pas-
cual, Marco Passarotti, Cenel-Augusto Perez, Slav
Petrov, Jussi Piitulainen, Barbara Plank, Martin
Popel, Lauma Pretkalnia, Prokopis Prokopidis, Ti-
ina Puolakainen, Sampo Pyysalo, Loganathan Ra-
masamy, Laura Rituma, Rudolf Rosa, Shadi Saleh,
Baiba Saulīte, Sebastian Schuster, Wolfgang Seeker,
Mojgan Seraji, Lena Shakurova, Mo Shen, Na-
talia Silveira, Maria Simi, Radu Simionescu, Katalin
Simkó, Kiril Simov, Aaron Smith, Carolyn Spadine,
Alane Suhr, Umut Sulubacak, Zsolt Szántó, Takaaki
Tanaka, Reut Tsarfaty, Francis Tyers, Sumire Ue-
matsu, Larraitz Uria, Gertjan van Noord, Vik-
tor Varga, Veronika Vincze, Jing Xian Wang,
Jonathan North Washington, Zdeněk Žabokrtský,
Daniel Zeman, and Hanzhi Zhu. 2016. Universal de-
pendencies 1.3. LINDAT/CLARIN digital library at
Institute of Formal and Applied Linguistics, Charles
University in Prague. http://hdl.handle.net/11234/1-
1699.
</p>
<p>Thiago A. S. Pardo and Maria das Graças Volpe
Nunes. 2003. A construção de um corpus de textos
cientı́ficos em Português do Brasil e sua marcação
</p>
<p>242</p>
<p />
</div>
<div class="page"><p />
<p>retórica. Technical report, Universidade de São
Paulo.
</p>
<p>Thiago A. S. Pardo and Maria das Graças Volpe Nunes.
2004. Relações retóricas e seus marcadores superfi-
ciais: Análise de um corpus de textos cientı́ficos em
Português do Brasil. Relatório Técnico NILC .
</p>
<p>Thiago A. S. Pardo and Maria das Graças Volpe Nunes.
2008. On the development and evaluation of a
Brazilian Portuguese discourse parser. Revista de
Informática Teórica e Aplicada 15(2):43–64.
</p>
<p>Thiago A. S. Pardo and Eloize R. M. Seno. 2005.
Rhetalho: Um corpus de referłncia anotado retori-
camente. In Proceedings of Encontro de Corpora.
</p>
<p>Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In Proceedings of ACL.
</p>
<p>Gisela Redeker, Ildik Berzlnovich, Nynke van der
Vliet, Gosse Bouma, and Markus Egg. 2012. Multi-
layer discourse annotation of a dutch text corpus. In
Proceedings of LREC.
</p>
<p>Uladzimir Sidarenka, Andreas Peldszus, and Manfred
Stede. 2015. Discourse segmentation of german
texts. Journal of Language Technology and Com-
putational Linguistics 30(1):71–98.
</p>
<p>Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of NAACL.
</p>
<p>Caroline Sporleder and Mirella Lapata. 2005. Dis-
course chunking and its application to sentence com-
pression. In Proceedings of HLT/EMNLP.
</p>
<p>Manfred Stede. 2004. The potsdam commentary cor-
pus. In Proceedings of the ACL Workshop on Dis-
course Annotation.
</p>
<p>Manfred Stede and Arne Neumann. 2014. Potsdam
commentary corpus 2.0: Annotation for discourse
research. In Proceedings of LREC.
</p>
<p>Milan Straka, Jan Hajič, and Straková. 2016. UDPipe:
Trainable Pipeline for Processing CoNLL-U Files
Performing Tokenization, Morphological Analysis,
POS Tagging and Parsing. In Proceedings of LREC.
</p>
<p>C N Subalalitha and Ranjani Parthasarathi. 2012. An
approach to discourse parsing using sangati and
Rhetorical Structure Theory. In Proceedings of the
Workshop on Machine Translation and Parsing in
Indian Languages (MTPIL-2012).
</p>
<p>Rajen Subba and Barbara Di Eugenio. 2007. Au-
tomatic discourse segmentation using neural net-
works. In Workshop on the Semantics and Pragmat-
ics of Dialogue.
</p>
<p>Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of ACL-HLT .
</p>
<p>Nynke van der Vliet. 2010. Syntax-based discourse
segmentation of Dutch text. In 15th Student Session,
ESSLLI.
</p>
<p>Nynke Van Der Vliet, Ildikó Berzlnovich, Gosse
Bouma, Markus Egg, and Gisela Redeker. 2011.
Building a discourse-annotated Dutch text corpus.
In S. Dipper and H. Zinsmeister (Eds.), Beyond Se-
mantics, Bochumer Linguistische Arbeitsberichte 3.
pages 157–171.
</p>
<p>Yunfang Wu, Fuqiang Wan, Yifeng Xu, and Xueqiang
Lü. 2016. A new ranking method for Chinese dis-
course tree building. Acta Scientiarum Naturalium
Universitatis Pekinensis 52(1):65–74.
</p>
<p>Ngo Xuan Bach, Nguyen Le Minh, and Akira Shimazu.
2012. A reranking model for discourse segmenta-
tion using subtree features. In Proceedings of Sig-
dial.
</p>
<p>243</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244–249
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2038
</p>
<p>Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?
</p>
<p>Beata Beigman Klebanov, Binod Gyawali, Yi Song
Educational Testing Service
</p>
<p>660 Rosedale Road
Princeton, NJ, USA
</p>
<p>bbeigmanklebanov,bgyawali,ysong@ets.org
</p>
<p>Abstract
</p>
<p>Automatic identification of good argu-
ments on a controversial topic has appli-
cations in civics and education, to name a
few. While in the civics context it might
be acceptable to create separate models for
each topic, in the context of scoring of stu-
dents’ writing there is a preference for a
single model that applies to all responses.
Given that good arguments for one topic
are likely to be irrelevant for another, is
a single model for detecting good argu-
ments a contradiction in terms? We inves-
tigate the extent to which it is possible to
close the performance gap between topic-
specific and across-topics models for iden-
tification of good arguments.
</p>
<p>1 Introduction &amp; Related Work
</p>
<p>Argumentation is an important skill in higher edu-
cation and the workplace; students are expected to
show sound reasoning and use relevant evidence
(Council of Chief State School Officers &amp; Na-
tional Governors Association, 2010). The increase
in argumentative writing tasks, in both instruc-
tional and assessment contexts, results in a high
demand for automated feedback on and scoring of
arguments.
</p>
<p>Automated analysis of argumentative writing
has mostly concentrated on argument structure –
namely, presence of claims and premises, and
relationships between them (Ghosh et al., 2016;
Nguyen and Litman, 2016; Persing and Ng, 2016;
Ong et al., 2014; Stab and Gurevych, 2014). Ad-
dressing the content of arguments in on-line de-
bates, Habernal and Gurevych (2016) ranked argu-
ments on the same topic by convincingness; they
showed that convincingness can be automatically
predicted, to an extent, in a cross-topics fashion, as
</p>
<p>they trained their systems on 31 debates and tested
on a new one. Swanson et al. (2015) reported
that annotation of argument quality is challeng-
ing, with inter-annotator agreement (ICC) around
0.40. They also showed that automated across-
topics prediction is very hard; for some topics, no
effective prediction was achieved.
</p>
<p>Song et al. (2014) developed an annotation pro-
tocol for analyzing argument critiques in students’
essays, drawing on the theory of argumentation
schemes (Walton et al., 2008; Walton, 1996). Ac-
cording to this theory, different types of arguments
invite specific types of critiques. For example, an
argument from authority made in the prompt –
According to X, Y is the case – avails critiques
along the lines of whether X has the necessary
knowledge and is an unbiased source of informa-
tion about Y. Analyzing prompts used in an assess-
ment of argument critique skills, Song et al. (2014)
identified a number of common schemes, such
as arguments from policy, sample, example, and
used the argumentation schemes theory to specify
what critiques would count as “good” for argu-
ments from the given scheme. Once a prompt
is associated with a specific set of argumentation
schemes, it follows that those critiques that count
as good under one of the schemes used in the
prompt would be considered as good critiques in
essays responding to that prompt. The goal of the
annotation was to identify all sentences in an essay
that participate in making a good critique, accord-
ing to the above definition. Every sentence in an
essay is annotated with the label of the critique that
it raises, or “generic” if none. In the current paper,
we build upon this earlier work.
</p>
<p>In practical large-scale automated scoring con-
texts, new essay prompts are often introduced
without rebuilding the scoring system, which
is typically subject to a periodic release sched-
ule. Therefore, the assumption that the system
</p>
<p>244</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2038">https://doi.org/10.18653/v1/P17-2038</a></div>
</div>
<div class="page"><p />
<p>will have seen essays responding to each of the
prompts it could encounter at deployment time
is often unwarranted. Further, not only should a
system be able to handle responses to an unseen
prompt, it must do it gracefully, since a large dis-
parity in the system’s performance across different
prompts might raise fairness concerns.
</p>
<p>Our practical goal is thus a development of a ro-
bust argument critique analysis system for essays.
Our theoretical goal is the investigation of the ex-
tent that it is at all possible to capture aspects of
argument content in a fashion that would genera-
lize across various essay topics.
</p>
<p>2 Annotation
</p>
<p>We used Song et al. (2014) annotation protocol,
adapting as needed to cover additional argumen-
tation schemes. Song et al. (2017) provides a
detailed exposition of the argumentation-scheme-
based analysis of a number of prompts and of the
annotation process. For the current study, we used
a simplified version of the annotation where sen-
tences are labeled as non-generic (namely, con-
taining a good critique according to some argu-
mentation scheme), or generic (all the rest of
the sentences in the essay). The average inter-
annotator agreement on the “generic” vs “non-
generic” sentence-level classification is k=0.67.
</p>
<p>The “non-generic” category covers all sen-
tences that raise a good critique; everything else
is “generic”. The latter category thus includes, for
example, sentences that rehash the argument in the
prompt, provide critical but vague statements that
cannot be clearly identified as a specific critique
from our list (such as “The author should provide
more information”), provide specific critical state-
ments that aren’t valid arguments. For example,
in response to the prompt that states that the new
policy led to a 10% decrease in unemployment in
four years, one writer argued that “People who
were unemployed could have died within the last
four years and that is why there is a decrease.”
While trying to provide an alternative explanation
to the putative effect of the policy is a reasonable
move, this is not a valid argument, because it is
exceedingly unlikely that unemployed people died
in such disproportionate numbers to have such a
big impact on the unemployment statistics.
</p>
<p>3 Data
</p>
<p>For this study, we use a same-topic and an across-
topics sets of college-level argument critique es-
says. The first is used to set the bar for the per-
formance in the context where the training and the
testing essays respond to the same prompt. The
second is the main dataset focused on generaliza-
tion across prompts.
</p>
<p>3.1 Same-topic
</p>
<p>A total of 900 essays were annotated, 300 essays
for each of 3 prompts. For each prompt, we train
a model on 260 responses and test on 40. The
training sets per prompt contain on average 2,700
sentences, of which 38% are classified as con-
taining good argument critiques. Two of the three
same-topic sets were used previously in Song et al.
(2014).
</p>
<p>3.2 Across-topics
</p>
<p>A total of 500 essays were annotated, 50 essays
for each of 10 prompts. We perform 10-fold cross
validation, training on 9 prompts and testing on the
10th, modeling a scenario of generalization to an
unknown topic. There are, on average, 5,492 sen-
tences available for training, of which 3,917 (42%)
are classified as containing good critiques.
</p>
<p>4 How far do we get with pure content?
</p>
<p>Given that making a good critique is presumably
mostly about saying the right things, we ex-
pect lexical models to perform well in same-
topic context and badly in the across-topics one.
We evaluated 1-3grams, 1-4grams, and 1-5grams
models learned using a logistic regression clas-
sifier. Differences in performance tended to be
in the third or fourth decimal digit; we there-
fore report results for 1-3grams only. Classifica-
tion accuracies are shown in row 2 of Table 1,
following the majority baseline (row 1), in both
same-topic and across-topics scenarios. We show
average performance (Av) as well as the worst per-
formance (Min) on 3 prompts (same-topic) and
on 10 prompts (across topics). We also evaluated
models built using chi-square based feature selec-
tion (f.s.), eliminating all features with p value
above 0.05 (row 3 in Table 1).
</p>
<p>For the same-topic context, lexical features per-
form at .738. As expected, lexical features are
much less effective across topics, with average
</p>
<p>245</p>
<p />
</div>
<div class="page"><p />
<p>performance of only .645. We observe substan-
tial gaps of 9 (.738 vs .645) and 8 (.679 vs .604)
accuracy points, for average and worst case, re-
spectively, between same-topic and across-topics
scenarios for ngram models. Feature selection is
ineffective in both scenarios (compare rows 2 and
3 in Table 1).
</p>
<p>5 How far do we get with pure structure?
</p>
<p>An approach that is perhaps better aligned with the
across-topics setting is to notice that in detailing
one’s arguments, one tends to utilize a specially
structured discourse, and that discourse role could
provide a clue to the argumentative function of a
sentence, without reliance on what the sentence
is actually saying (beyond discourse connectives
that are used to help identify the discourse role).
In particular, argumentative essays often have a
fairly standard structure, where a general claim
(or stance, or thesis) on the issue is introduced
in the beginning of the essay, followed by a se-
quence of main points, each elaborated using sup-
porting statements, and finally followed by a con-
clusion that often re-states the thesis and provides
a high-level summary of the argument. We expect
the “meat” of the argument to occur mostly in the
supporting statements that provide detailed expo-
sition of the author’s arguments. We use a dis-
course parser for argumentative essays (Burstein
et al., 2003) to classify sentences into the follow-
ing discourse units: Thesis, Background, Main-
Point, Support, Conclusion, and Other. Row 4 (dr)
in Table 1 shows the performance of this set of
6 binary features. Of the 6 features, Support and
MainPoint have a positive weight (predict “non-
generic”), the rest predict “generic”.
</p>
<p>We further hypothesize that the position of a
sentence inside a discourse segment might also
provide some information: A sentence surrounded
by Support sentences is likely to be in the middle
of exposition of an argument, as opposed to the
last Supporting sentence before the next Main
Point that could be summary-like, leading up to
a shift to a new topic. We therefore built two
sets of transition features, one for all pairs of
&lt;previous sentence role,current sentence role&gt;
(such as &lt;Thesis,Main Point&gt; for a sentence
that is classified as Main Point and follows a
Thesis sentence), and the other – for all pairs of
&lt;current sentence role,next sentence role&gt;. We
also added BeginningOfEssay and EndOfEssay
</p>
<p>Model Same Topic Across Topics
Av. Min. Av. Min.
</p>
<p>1 Majority .660 .612 .580 .3991
</p>
<p>2 1-3gr .738 .679 .645 .604
3 1-3gr f.s. .697 .635 .633 .580
4 dr .668 .619 .678 .634
5 dr pn .677 .631 .687 .649
6 dr pn+1-3gr .741 .690 .700 .674
7 1-3gr ppos .728 .687 .654 .616
8 dr pn+1-3gr ppos .745 .701 .706 .686
9 SongEtAl2014 .756 .702 .678 .642
</p>
<p>Table 1: Classification accuracies for generic vs
non-generic sentences. Our best results for same-
topic and across-topics scenarios are boldfaced.
</p>
<p>discourse tags to handle the first and the last
sentences of the essay. Table 2 shows the weights
for some of the features.
</p>
<p>Discourse Transition Feature Weight
Previous Current Next
</p>
<p>1 Support Support 0.760
2 MainPoint Support 0.238
3 Thesis Support -0.028
4 Support Support 0.716
5 Support MainPoint 0.220
6 Support Concl. 0.047
7 Concl. Concl. 0.063
8 Concl. EndOfEssay -0.680
</p>
<p>Table 2: Weights of the transition features.
</p>
<p>We observe that the likelihood of the current
Support sentence to carry argumentative content is
higher if it follows another Support sentence (row
1) than if it follows a Main Point (row 2); if the
Support sentence follows Thesis, it is actually not
likely to contain argumentative content (perhaps it
is more like a Main Point sentence than like a typi-
cal Support). Likewise, being followed by another
Support sentence is a good sign (row 4), but being
the last Support sentence before transitioning to a
new Main Point has a much lower positive weight
(row 5), and being the last Support before Conclu-
sion has a still lower positive weight (row 6). Inter-
estingly, while being the last Conclusion sentence
in the essay strongly predicts “generic” (row 8),
if the next sentence is still within the Conclusion
segment, the prediction is actually slightly posi-
tive (row 7), suggesting that some authors rehash
their arguments in substantial detail in concluding
remarks, warranting a “non-generic” designation.
</p>
<p>1The majority baseline for one prompt is below 50% be-
cause for that prompt the majority class is actually sentences
that raise appropriate arguments, differently from the other 9
prompts.
</p>
<p>246</p>
<p />
</div>
<div class="page"><p />
<p>Table 1 shows the performance of the discourse
role features (dr), the transition pairs using pre-
vious and next discourse roles (dr pn),2 and the
combination of content and discourse (row 6).
</p>
<p>We observe that transforming the discourse
role features into transitional features is effective.
Second, the discourse role features are inferior to
the content features for same-topic, while the op-
posite is true for the across-topics scenario.
</p>
<p>Discourse structure information does in fact get
us quite far in the across-topics scenario, further
than the lexical information on its own. Com-
bining the two types of information further im-
proves performance in across-topics scenario, and
reduces the gap between across-topics and same-
topic contexts to 4 points on average (.741 vs .700)
and 1.5 points in worst case (.690 vs .674), for a
combined discourse structure and content model.
</p>
<p>6 Can we do better?
</p>
<p>In an attempt to further improve across-topics per-
formance, we generalized ngrams representations
and adapted feature selection to reflect the across-
topics dynamic more directly.
</p>
<p>6.1 Generalized ngrams
</p>
<p>Suppose the prompt is arguing that some entity N
should do some action V. While N and V might
differ across prompts, critical sentences to the end
that N should not do V are likely to occur across
different prompts. In the current ngrams repre-
sentation, N and V differ across prompts, and
are unknown for a prompt that is unseen during
training. We represent all content words (nouns,
verbs, adjectives, adverbs, and cardinal numbers)
in the prompt as their part-of-speech labels; we
should be able to capture features such as “should
not VB”. Rows 7 and 8 in Table 1 show the 1-
3gr ppos model; it improves over 1-3gr in both
the average and the worst cases, on its own and
on top of the dr pn features, in the across-topics
scenario. The improvement over dr pn+1-3gr is
marginally significant (p&lt;0.1, Wilcoxon Signed-
Ranks 2-tailed test, n=10, W=33).
</p>
<p>The single strongest lexical predictor of a
generic sentence is the first person singular pro-
noun I; such sentences are likely to express stance
</p>
<p>2Since argument critiques often span more than one sen-
tence, we experimented with sequence labeling using Condi-
tional Random Fields, but performance was not better than
with logistic regression.
</p>
<p>(I think this is a good plan), or contain discourse-
management expressions such as I will show that
the author’s arguments are flawed. Words such
as assumptions, evidence, information, argument,
statistics, idea, reasons all have negative weight,
suggesting that they typically belong to generic
sentences such as The author’s argument lacks evi-
dence that does not raise a specific critique. Lexi-
cal features for the positive class include moda-
lity as in might, perhaps, could, possible that, po-
tential, necessarily, if a; negation (not, will not),
as well as more specific lexica that point out,
for example, outcomes of a policy (expensive, in-
crease in, affected the, fails to). Positive features
with prompt elements include NNS does not, NN
do not, many NNS, NN NNS are, NNS who VBD,
could have VBN, will not VB.
</p>
<p>6.2 Feature Selection
We experimented with three feature selection
methods. (1) We selected features with p&lt;0.05
using χ2 test (p0.05). (2) We selected features
with p&lt;0.05 for at least two out of the 9 training
prompts, to find features that are likely to genera-
lize across prompts (p0.05 2pr). (3) We selected
features based on their mutual information with
the label conditioned on values of the dr pn fea-
tures, to encourage selection of features that aug-
ment, rather than repeat, the discourse informa-
tion. We calculated for each training prompt, and
took the 2nd highest of the 9 values.3 We selected
features in the top 5% of this metric (mi5% 2pr).
</p>
<p>Table 3 shows the results. The p0.05 mecha-
nism is ineffective; 0.05 2pr selection is better.
The mi5% 2pr mechanism performs within .002
of the original, while reducing the number of fea-
tures by two orders of magnitude.
</p>
<p>F.s. #Features Av. Min.
No f.s. ∼ 200,000 .706 .686
p0.05 ∼ 3,500 .687 .656
p0.05 2pr &lt; 500 .702 .678
mi5% 2pr ∼ 1,000 .704 .684
</p>
<p>Table 3: Performance of feature selection, for the
dr pn+1-3gr ppos model, across topics.
</p>
<p>7 Benchmark
</p>
<p>We also compared our best across-topics system
(dr pn+1-3gr ppos) to the system described in
</p>
<p>3Thus, the feature has at least that much informaton be-
yond dr pn for at least two different prompts.
</p>
<p>247</p>
<p />
</div>
<div class="page"><p />
<p>Song et al. (2014). The Song et al. (2014) system
uses the following features: length of the sentence,
parts of speech, overlap of words in the sentence
with the prompt, relative position of the sentence
in the essay, 1-3gr, and 1-3gr in previous and next
sentences. The performance is shown in row 9 in
Table 1. Our improvement over Song et al. (2014)
is statistically significant in the across-topics sce-
nario (p&lt;0.05, Wilcoxon Signed-Ranks test, 2-
tailed, n=10, W=51).
</p>
<p>8 Conclusion
</p>
<p>We presented experiments on classifying essay
sentences as containing good argument critiques
or not. While a good argument critique is a mat-
ter of content, we show that it is possible to build
classifiers that are not prompt-specific, using dis-
course structure features and generalized lexical
features that take into account reference to the text
of the prompt to which the author is responding.
Starting from a ngrams baseline where the per-
formance gap between same-prompt and across-
topics scenarios is 9 accuracy points on average
(.738 vs .645) and 8 points in worst case (.679
vs .604), we close half the gap in average perfor-
mance (.745 vs .706) and are down to only 1.5
point difference in worst case performance (.701
vs .686). This performance is preserved with only
about 0.5% of the features, using a conditional
mutual information criterion. The improvement in
worst case performance is important for ensuring
that the system does not exhibit large performance
differences across different essay prompts used on
the same test. We also show that our best system
significantly improves over the state-of-art system
for argument critique detection task on compara-
ble essay data for the across-topics scenario.
</p>
<p>References
Jill Burstein, Daniel Marcu, and Kevin Knight.
</p>
<p>2003. Finding the write stuff: Automatic iden-
tification of discourse structure in student es-
says. IEEE Intelligent Systems 18(1):32–39.
https://doi.org/10.1109/MIS.2003.1179191.
</p>
<p>Debanjan Ghosh, Aquila Khanam, Yubo Han, and
Smaranda Muresan. 2016. Coarse-grained argu-
mentation features for scoring persuasive essays.
In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 549–554.
http://anthology.aclweb.org/P16-2089.
</p>
<p>Ivan Habernal and Iryna Gurevych. 2016. Which ar-
gument is more convincing? Analyzing and predict-
ing convincingness of Web arguments using bidirec-
tional LSTM. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1589–1599. http://www.aclweb.org/anthology/P16-
1150.
</p>
<p>Huy Nguyen and Diane Litman. 2016. Context-
aware argumentative relation mining. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1127–1137.
http://www.aclweb.org/anthology/P16-1107.
</p>
<p>Nathan Ong, Diane Litman, and Alexandra
Brusilovsky. 2014. Ontology-based argument
mining and automatic essay scoring. In Pro-
ceedings of the First Workshop on Argumentation
Mining. Association for Computational Lin-
guistics, Baltimore, Maryland, pages 24–28.
http://www.aclweb.org/anthology/W14-2104.
</p>
<p>Isaac Persing and Vincent Ng. 2016. End-to-end ar-
gumentation mining in student essays. In Pro-
ceedings of the 2016 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies. Association for Computational Linguis-
tics, San Diego, California, pages 1384–1394.
http://www.aclweb.org/anthology/N16-1164.
</p>
<p>Yi Song, Paul Deane, and Beata Beigman Kle-
banov. 2017. Toward the automated scoring of
written arguments: Developing an innovative ap-
proach for annotation. ETS Research Report Series
https://doi.org/10.1002/ets2.12138.
</p>
<p>Yi Song, Michael Heilman, Beata Beigman Kle-
banov, and Paul Deane. 2014. Applying ar-
gumentation schemes for essay scoring. In
Proceedings of the First Workshop on Argu-
mentation Mining. Association for Computational
Linguistics, Baltimore, Maryland, pages 69–78.
http://www.aclweb.org/anthology/W14-2110.
</p>
<p>Christian Stab and Iryna Gurevych. 2014. Identify-
ing argumentative discourse structures in persua-
sive essays. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 46–56.
http://www.aclweb.org/anthology/D14-1006.
</p>
<p>Reid Swanson, Brian Ecker, and Marilyn Walker. 2015.
Argument mining: Extracting arguments from on-
line dialogue. In Proceedings of the 16th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 217–226.
http://aclweb.org/anthology/W15-4631.
</p>
<p>248</p>
<p />
</div>
<div class="page"><p />
<p>Douglas N. Walton. 1996. Argumentation schemes
for presumptive reasoning. Mahwah, NJ: Lawrence
Erlbaum.
</p>
<p>Douglas N. Walton, Chris Reed, and Fabrizio
Macagno. 2008. Argumentation schemes. New
York, NY: Cambridge University Press.
</p>
<p>249</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 250–255
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2039
</p>
<p>Argumentation Quality Assessment: Theory vs. Practice
</p>
<p>Henning Wachsmuth ∗ Nona Naderi ∗∗ Ivan Habernal ∗∗∗ Yufang Hou ∗∗∗∗
Graeme Hirst ∗∗ Iryna Gurevych ∗∗∗ Benno Stein ∗
</p>
<p>∗ Bauhaus-Universität Weimar, Weimar, Germany, www.webis.de
∗∗ University of Toronto, Toronto, Canada, www.cs.toronto.edu/compling
</p>
<p>∗∗∗ Technische Universität Darmstadt, Darmstadt, Germany, www.ukp.tu-darmstadt.de
∗∗∗∗ IBM Research, Dublin, Ireland, ie.ibm.com
</p>
<p>Abstract
Argumentation quality is viewed different-
ly in argumentation theory and in practical
assessment approaches. This paper studies
to what extent the views match empirically.
We find that most observations on quality
phrased spontaneously are in fact adequa-
tely represented by theory. Even more, rel-
ative comparisons of arguments in prac-
tice correlate with absolute quality ratings
based on theory. Our results clarify how
the two views can learn from each other.
</p>
<p>1 Introduction
The assessment of argumentation quality is critical
for any application built upon argument mining,
such as debating technologies (Rinott et al., 2015).
However, research still disagrees on whether qual-
ity should be assessed from a theoretical or from a
practical viewpoint (Allwood, 2016).
</p>
<p>Theory states, among other things, that a cogent
argument has acceptable premises that are relevant
to its conclusion and sufficient to draw the conclu-
sion (Johnson and Blair, 2006). Practitioners ob-
ject that such quality dimensions are hard to assess
for real-life arguments (Habernal and Gurevych,
2016b). Moreover, the normative nature of theory
suggests absolute quality ratings, but in practice it
seems much easier to state which argument is more
convincing—a relative assessment. Consider two
debate-portal arguments for “advancing the com-
mon good is better than personal pursuit”, taken
from the corpora analyzed later in this paper:
Argument A “While striving to make advance-
ments for the common good you can change the
world forever. Allot of people have succeded in
doing so. Our founding fathers, Thomas Edison,
George Washington, Martin Luther King jr, and
many more. These people made huge advances for
the common good and they are honored for it.”
</p>
<p>Argument B “I think the common good is a better
endeavor, because it’s better to give then to receive.
It’s better to give other people you’re hand out in
help then you holding your own hand.”
</p>
<p>In the study of Habernal and Gurevych (2016b),
annotators assessed Argument A as more convinc-
ing than B. When giving reasons for their assess-
ment, though, they saw A as more credible and well
thought through; that does not seem to be too far
from the theoretical notion of cogency.
</p>
<p>This paper gives empirical answers to the ques-
tion of how different the theoretical and practical
views of argumentation quality actually are. Sec-
tion 2 briefly reviews existing theories and practical
approaches. Section 3 then empirically analyzes
correlations in two recent argument corpora, one
annotated for 15 well-defined quality dimensions
taken from theory (Wachsmuth et al., 2017a) and
one with 17 reasons for quality differences phrased
spontaneously in practice (Habernal and Gurevych,
2016a). In a crowdsourcing study, we test whether
lay annotators achieve agreement on the theoretical
quality dimensions (Section 4).
</p>
<p>We find that assessments of overall argumenta-
tion quality largely match in theory and practice.
Nearly all phrased reasons are adequately repre-
sented in theory. However, some theoretical quality
dimensions seem hard to separate in practice. Most
importantly, we provide evidence that the observed
relative quality differences are reflected in abso-
lute quality ratings. Still, our study underpins the
fact that the theory-based argumentation quality
assessment remains complex. Our results do not
generally answer the question of what view of ar-
gumentation quality is preferable, but they clarify
where theory can learn from practice and vice versa.
In particular, practical approaches indicate what to
focus on to simplify theory, whereas theory seems
beneficial to guide quality assessment in practice.
</p>
<p>250</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2039">https://doi.org/10.18653/v1/P17-2039</a></div>
</div>
<div class="page"><p />
<p>Quality Dimension Short Description of Dimension
</p>
<p>Cogency Argument has (locally) acceptable,
relevant, and sufficient premises.
</p>
<p>Local acceptability Premises worthy of being believed.
Local relevance Premises support/attack conclusion.
Local sufficiency Premises enough to draw conclusion.
Effectiveness Argument persuades audience.
Credibility Makes author worthy of credence.
Emotional appeal Makes audience open to arguments.
Clarity Avoids deviation from the issue, uses
</p>
<p>correct and unambiguous language.
Appropriateness Language proportional to the issue,
</p>
<p>supports credibility and emotions.
Arrangement Argues in the right order.
Reasonableness Argument is (globally) acceptable,
</p>
<p>relevant, and sufficient.
Global acceptability Audience accepts use of argument.
Global relevance Argument helps arrive at agreement.
Global sufficiency Enough rebuttal of counterarguments.
Overall quality Argumentation quality in total.
</p>
<p>Table 1: The 15 theory-based quality dimensions
rated in the corpus of Wachsmuth et al. (2017a).
</p>
<p>2 Theory versus Practice
This section outlines major theories and practical
approaches to argumentation quality assessment,
including those we compare in the present paper.
</p>
<p>2.1 Theoretical Views of Quality Assessment
Argumentation theory discusses logical, rhetorical,
and dialectical quality. As few real-life arguments
are logically sound, requiring true premises that de-
ductively entail a conclusion, cogency (as defined
in Section 1) is largely seen as the main logical qual-
ity (Johnson and Blair, 2006; Damer, 2009; Govier,
2010). Toulmin (1958) models the general struc-
ture of logical arguments, and Walton et al. (2008)
analyze schemes of fallacies and strong arguments.
A fallacy is a kind of error that undermines reason-
ing (Tindale, 2007). Strength may mean cogency
but also rhetorical effectiveness (Perelman and
Olbrechts-Tyteca, 1969). Rhetoric has been studied
since Aristotle (2007) who developed the notion of
the means of persuasion (logos, ethos, pathos) and
their linguistic delivery in terms of arrangement
and style. Dialectical quality dimensions resemble
those of cogency, but arguments are judged specifi-
cally by their reasonableness for achieving agree-
ment (van Eemeren and Grootendorst, 2004).
</p>
<p>Wachsmuth et al. (2017a) point out that dialecti-
cal builds on rhetorical, and rhetorical builds on log-
ical quality. They derive a unifying taxonomy from
the major theories, decomposing quality hierarchi-
cally into cogency, effectiveness, reasonableness,
and subdimensions. Table 1 lists all 15 dimensions
</p>
<p>Polarity Label Short Description of Reason
</p>
<p>Negative
properties of
Argument B
</p>
<p>5-1 B is attacking / abusive.
5-2 B has language/grammar issues, or
</p>
<p>uses humour or sarcasm.
5-3 B is unclear / hard to follow.
6-1 B has no credible evidence / no facts.
6-2 B has less or insufficient reasoning.
6-3 B uses irrelevant reasons.
7-1 B is only an opinion / a rant.
7-2 B is non-sense / confusing.
7-3 B does not address the topic.
7-4 B is generally weak / vague.
</p>
<p>Positive
properties of
Argument A
</p>
<p>8-1 A has more details/facts/examples,
has better reasoning / is deeper.
</p>
<p>8-4 A is objective / discusses other views.
8-5 A is more credible / confident.
9-1 A is clear / crisp / well-written.
9-2 A sticks to the topic.
9-3 A makes you think.
9-4 A is well thought through / smart.
</p>
<p>Overall Conv A is more convincing than B.
</p>
<p>Table 2: The 17+1 practical reason labels given in
the corpus of Habernal and Gurevych (2016a).
</p>
<p>covered. In Section 3, we use their absolute quality
ratings from 1 (low) to 3 (high) annotated by three
experts for each dimension of 304 arguments taken
from the UKPConvArg1 corpus detailed below.
</p>
<p>2.2 Practical Views of Quality Assessment
There is an application area where absolute quality
ratings of argumentative text are common practice:
essay scoring (Beigman Klebanov et al., 2016).
Persing and Ng (2015) annotated the argumentative
strength of essays composing multiple arguments
with notable agreement. For single arguments, how-
ever, all existing approaches that we are aware of
assess quality in relative terms, e.g., Cabrio and Vil-
lata (2012) find accepted arguments based on attack
relations, Wei et al. (2016) rank arguments by their
persuasiveness, and Wachsmuth et al. (2017b) rank
them by their relevance. Boudry et al. (2015) ar-
gue that normative concepts such as fallacies rarely
apply to real-life arguments and that they are too
sophisticated for operationalization.
</p>
<p>Based on the idea that relative assessment is eas-
ier, Habernal and Gurevych (2016b) crowdsourced
the UKPConvArg1 corpus. Argument pairs (A, B)
from a debate portal were classified as to which
argument is more convincing. Without giving any
guidelines, the authors also asked for reasons as to
why A is more convincing than B. In a follow-up
study (Habernal and Gurevych, 2016a), these rea-
sons were used to derive a hierarchical annotation
scheme. 9111 argument pairs were then labeled
with one or more of the 17 reason labels in Table 2
</p>
<p>251</p>
<p />
</div>
<div class="page"><p />
<p>Negative Properties of Argument B Positive Properties of Argument A
</p>
<p>Quality Dimension 5-1 5-2 5-3 6-1 6-2 6-3 7-1 7-2 7-3 7-4 8-1 8-4 8-5 9-1 9-2 9-3 9-4 Conv
</p>
<p>Cog Cogency .86 .74 .67 .66 .85 .43 .81 .83 .84 .75 .59 .58 .62 .70 .67 .64 .75 .59
LA Local acceptability .92 .77 .86 .49 .90 .80 .86 .89 .89 .74 .58 .43 .73 .64 .67 .56 .73 .58
LR Local relevance .87 .77 .86 .70 .95 .45 .84 .92 .95 .73 .61 .56 .68 .69 .65 .70 .66 .62
LS Local sufficiency .79 .69 .67 .68 .74 .38 .85 .92 .84 .79 .63 .67 .54 .64 .52 .78 .70 .61
Eff Effectiveness .84 .71 .67 .66 .85 .62 .87 .92 .84 .71 .59 .57 .65 .66 .58 .78 .72 .59
Cre Credibility .78 .69 .71 .52 .95 .80 .66 .81 .67 .57 .51 .44 .66 .60 .71 .39 .62 .50
Emo Emotional appeal .80 .50 .59 .55 .70 .80 .70 .80 .67 .60 .36 .35 .41 .30 .42 .73 .50 .38
Cla Clarity .61 .70 .91 .41 .95 .58 .61 .87 .67 .60 .41 .40 .41 .68 .71 .56 .58 .44
App Appropriateness .94 .86 .91 .50 .95 .45 .87 .74 .36 .79 .57 .59 .69 .72 .79 .53 .57 .59
Arr Arrangement .81 .75 .86 .67 .85 .40 .78 .77 .67 .68 .60 .73 .64 .73 .73 .78 .72 .62
Rea Reasonableness .92 .86 .67 .73 .90 .49 .85 .94 .84 .73 .64 .56 .70 .69 .65 .78 .64 .63
GA Global acceptability 1.00 .80 .82 .65 .76 .62 .87 .86 .95 .71 .63 .62 .75 .59 .67 .72 .68 .63
GR Global relevance .97 .86 .82 .63 .82 .71 .86 .82 .95 .75 .61 .51 .49 .66 .46 .72 .57 .61
GS Global sufficiency .77 .57 .59 .62 .85 .47 .75 .72 .71 .64 .59 .69 .46 .53 .39 .71 .61 .56
OQ Overall quality .94 .85 .79 .71 .90 .53 .85 .92 .84 .72 .65 .58 .69 .72 .61 .73 .73 .64
</p>
<p># Pairs with label x-y 34 55 18 115 11 16 64 37 10 50 536 79 68 86 34 26 39 736
</p>
<p>Table 3: Kendall’s τ rank correlation of each of the 15 quality dimensions of all argument pairs annotated
by Wachsmuth et al. (2017a) given for each of the 17+1 reason labels of Habernal and Gurevych (2016a).
Bold/gray: Highest/lowest value in each column. Bottom row: The number of labels for each dimension.
</p>
<p>by crowd workers (UKPConvArg2). These pairs
represent the practical view in our experiments.
</p>
<p>3 Matching Theory and Practice
We now report on experiments that we performed
to examine to what extent the theory and practice
of argumentation quality assessment match.1
</p>
<p>3.1 Corpus-based Comparison of the Views
Several dimensions and reasons in Tables 1 and 2
seem to refer to the same or opposite property, e.g.,
clarity and 5-3 (unclear). This raises the question
of how absolute ratings of arguments based on the-
ory relate to relative comparisons of argument pairs
in practice. We informally state three hypotheses:
</p>
<p>Hypothesis 1 The reasons for quality differences
in practice are adequately represented in theory.
</p>
<p>Hypothesis 2 The perception of overall argumen-
tation quality is the same in theory and practice.
</p>
<p>Hypothesis 3 Relative quality differences are re-
flected by differences in absolute quality ratings.
</p>
<p>As both corpora described in Section 2 are based
on the UKPConvArg1 corpus and thus share many
arguments, we can test the hypotheses empirically.
</p>
<p>3.2 Correlations of Dimensions and Reasons
For Hypotheses 1 and 2, we consider all 736 pairs
of arguments from Habernal and Gurevych (2016a)
where both have been annotated by Wachsmuth
et al. (2017a). For each pair (A, B) with A being
</p>
<p>1Source code and annotated data: http://www.arguana.com
</p>
<p>more convincing than B, we check whether the rat-
ings of A and B for each dimension (averaged over
all annotators) show a concordant difference (i.e.,
a higher rating for A), a disconcordant difference
(lower), or a tie. This way, we can correlate each di-
mension with all reason labels in Table 2 including
Conv. In particular, we compute Kendall’s τ based
on all argument pairs given for each label.2
</p>
<p>Table 3 presents all τ -values. The phrasing of a
reason can be assumed to indicate a clear quality
difference—this is underlined by the generally high
correlations. Analyzing the single values, we find
much evidence for Hypothesis 1: Most notably, la-
bel 5-1 perfectly correlates with global acceptabil-
ity, fitting the intuition that abuse is not acceptable.
The high τ ’s of 8-5 (more credible) for local accept-
ability (.73) and of 9-4 (well thought through) for
cogency (.75) confirm the match assumed in Sec-
tion 1. Also, the values of 5-3 (unclear) for clarity
(.91) and of 7-2 (non-sense) for reasonableness
(.94) as well as the weaker correlation of 8-4 (ob-
jective) for emotional appeal (.35) makes sense.
</p>
<p>Only the comparably low τ of 6-1 (no credible
evidence) for local acceptability (.49) and credibil-
ity (.52) seem really unexpected. Besides, the de-
scriptions of 6-2 and 6-3 sound like local but cor-
</p>
<p>2Lacking better options, we ignore pairs where a label is
not given: It is indistinguishable whether the associated reason
does not hold, has not been given, or is just not included in
the corpus. Thus, τ is more “boosted” the fewer pairs exist
for a label and, thus, its values are not fully comparable across
labels. Notice, though, that Conv exists for all pairs. So, the
values of Conv suggest the magnitude of τ without boosting.
</p>
<p>252</p>
<p />
</div>
<div class="page"><p />
<p>Polarity Label Cog LA LR LS Eff Cre Emo Cla App Arr Rea GA GR GS OQ
</p>
<p>Negative
properties of
Argument B
</p>
<p>5-1 1.30 1.44 1.77 1.29 1.26 1.46 1.64 1.84 1.62 1.55 1.34 1.45 1.65 1.19 1.29
5-2 1.51 1.73 1.97 1.39 1.41 1.66 1.82 1.96 1.89 1.72 1.55 1.72 1.74 1.21 1.48
5-3 1.46 1.78 2.06 1.43 1.39 1.63 1.96 1.87 2.04 1.65 1.63 1.85 1.76 1.28 1.52
6-1 1.54 1.87 2.22 1.43 1.44 1.72 1.85 2.15 2.12 1.79 1.62 1.89 1.89 1.27 1.55
6-2 1.30 1.52 1.88 1.27 1.21 1.52 1.85 1.94 1.88 1.67 1.36 1.61 1.55 1.15 1.33
6-3 1.60 1.85 2.23 1.52 1.52 1.65 1.79 2.00 2.15 1.92 1.63 1.85 2.00 1.40 1.60
7-1 1.43 1.74 1.97 1.33 1.34 1.60 1.82 1.95 1.89 1.72 1.48 1.71 1.68 1.22 1.43
7-2 1.45 1.68 1.97 1.41 1.39 1.53 1.86 1.84 1.95 1.67 1.53 1.68 1.70 1.25 1.48
7-3 1.20 1.47 1.60 1.10 1.17 1.47 1.60 1.70 1.80 1.40 1.20 1.40 1.30 1.07 1.13
7-4 1.43 1.71 2.02 1.37 1.34 1.71 1.79 1.95 1.97 1.65 1.55 1.75 1.75 1.23 1.46
</p>
<p>Positive
properties of
Argument A
</p>
<p>8-1 1.56 1.89 2.20 1.46 1.48 1.71 1.88 2.05 2.07 1.79 1.65 1.88 1.92 1.30 1.57
8-4 1.65 1.97 2.27 1.53 1.61 1.73 1.86 2.12 2.14 1.89 1.73 1.92 1.96 1.37 1.64
8-5 1.69 2.07 2.39 1.58 1.60 1.81 1.98 2.19 2.25 1.99 1.82 2.04 2.11 1.38 1.75
9-1 1.54 1.86 2.22 1.49 1.43 1.67 1.84 2.09 2.03 1.74 1.63 1.85 1.92 1.30 1.54
9-2 1.56 1.76 2.22 1.45 1.49 1.58 1.98 2.02 2.00 1.74 1.62 1.81 1.84 1.28 1.51
9-3 1.55 1.78 2.31 1.42 1.49 1.68 2.01 2.18 2.10 1.79 1.63 1.83 1.97 1.27 1.50
9-4 1.78 1.99 2.32 1.64 1.68 1.81 1.99 2.17 2.19 1.93 1.86 2.05 2.09 1.44 1.79
</p>
<p>min(Pos.)−min(Neg.) 0.34 0.32 0.60 0.32 0.26 0.12 0.24 0.32 0.38 0.34 0.42 0.41 0.54 0.20 0.37
max(Pos.)−max(Neg.) 0.18 0.20 0.16 0.12 0.16 0.09 0.05 0.04 0.10 0.07 0.23 0.16 0.11 0.04 0.19
</p>
<p>Table 4: The mean rating for each quality dimension of those arguments from Wachsmuth et al. (2017a)
given for each reason label (Habernal and Gurevych, 2016a). The bottom rows show that the minimum
maximum mean ratings are consistently higher for the positive properties than for the negative properties.
</p>
<p>relate more with global relevance and sufficiency
respectively. Similarly, 7-3 (off-topic) correlates
strongly with local and global relevance (both .95).
So, these dimensions seem hard to separate.
</p>
<p>In line with Hypothesis 2, the highest correlation
of Conv is indeed given for overall quality (.64).
Thus, argumentation quality assessment seems to
match in theory and practice to a broad extent.
</p>
<p>3.3 Absolute Ratings for Relative Differences
The correlations found imply that the relative qual-
ity differences captured are reflected in absolute dif-
ferences. For explicitness, we computed the mean
rating for each quality dimension of all arguments
from Wachsmuth et al. (2017a) with a particular
reason label from Habernal and Gurevych (2016a).
As each reason refers to one argument of a pair, this
reveals whether the labels, although meant to signal
relative differences, indicate absolute ratings.
</p>
<p>Table 4 compares the mean ratings of “negative
labels” (5-1 to 7-4) and “positive” ones (8-1 to 9-4).
For all dimensions, the maximum and minimum
value are higher for the positive than for the nega-
tive labels—a clear support of Hypothesis 3.3 Also,
Table 4 reveals which reasons predict absolute dif-
ferences most: The mean ratings of 7-3 (off-topic)
are very low, indicating a strong negative impact,
while 6-3 (irrelevant reasons) still shows rather
</p>
<p>3While the differences seem not very large, this is expec-
ted, as in many argument pairs from Habernal and Gurevych
(2016a) both arguments are strong or weak respectively.
</p>
<p>high values. Vice versa, especially 8-5 (more credi-
ble) and 9-4 (well thought through) are reflected in
high ratings, whereas 9-2 (sticks to topic) does not
have much positive impact.
</p>
<p>4 Annotating Theory in Practice
The results of Section 3 suggest that theory may
guide the assessment of argumentation quality in
practice. In this section, we evaluate the reliability
of a crowd-based annotation process.
</p>
<p>4.1 Absolute Quality Ratings by the Crowd
We emulated the expert annotation process carried
out by Wachsmuth et al. (2017a) on CrowdFlower
in order to evaluate whether lay annotators suffice
for a theory-based quality assessment. In particular,
we asked the crowd to rate the same 304 arguments
as the experts for all 15 given quality dimensions
with scores from 1 to 3 (or choose “cannot judge”).
Each argument was rated 10 times at an offered
price of $0.10 for each rating (102 annotators in
total). Given the crowd ratings, we then performed
two comparisons as detailed in the following.
</p>
<p>4.2 Agreement of the Crowd with Experts
First, we checked to what extent lay annotators and
experts agree in terms of Krippendorff’s α. On one
hand, we compared the mean of all 10 crowd rat-
ings to the mean of the three ratings of Wachsmuth
et al. (2017a). On the other hand, we estimated a
reliable rating from the crowd ratings using MACE
(Hovy et al., 2013) and compared it to the experts.
</p>
<p>253</p>
<p />
</div>
<div class="page"><p />
<p>(a) Crowd / Expert (b) Crowd 1 / 2 / Expert (c) Crowd 1 / Expert (d) Crowd 2 / Expert
</p>
<p>Quality Dimension Mean MACE Mean MACE Mean MACE Mean MACE
</p>
<p>Cog Cogency .27 .38 .24 .29 .38 .37 .05 .27
LA Local acceptability .49 .35 .37 .27 .49 .33 .30 .25
LR Local relevance .42 .39 .33 .28 .41 .39 .26 .25
LS Local sufficiency .18 .31 .21 .21 .34 .27 –.04 .19
Eff Effectiveness .13 .31 .19 .20 .27 .28 –.06 .20
Cre Credibility .41 .27 .31 .20 .43 .23 .22 .19
Emo Emotional appeal .45 .23 .32 .13 .41 .20 .25 .10
Cla Clarity .42 .28 .33 .23 .39 .27 .29 .20
App Appropriateness .54 .26 .40 .20 .48 .24 .43 .17
Arr Arrangement .53 .30 .36 .24 .49 .27 .35 .24
Rea Reasonableness .33 .40 .27 .31 .42 .40 .09 .29
GA Global acceptability .54 .40 .36 .29 .53 .37 .33 .28
GR Global relevance .44 .31 .31 .20 .50 .29 .22 .18
GS Global sufficiency –.17 .19 .04 .11 .00 .16 –.27 .11
OQ Overall quality .43 .43 .38 .33 .43 .40 .28 .33
</p>
<p>Table 5: Mean and MACE Krippendorff’s α agreement between (a) the crowd and the experts, (b) two
independent crowd groups and the experts, (c) group 1 and the experts, and (d) group 2 and the experts.
</p>
<p>Table 5(a) presents the results. For the mean
ratings, most α-values are above .40. This is similar
to the study of Wachsmuth et al. (2017b), where
a range of .27 to .51 is reported, meaning that lay
annotators achieve similar agreement to experts.
Considering the minimum of mean and MACE, we
observe the highest agreement for overall quality
(.43)—analog to Wachsmuth et al. (2017b). Also,
global sufficiency has the lowest agreement in both
cases. In contrast, the experts hardly said “cannot
judge” at all, whereas the crowd chose it for about
4% of all ratings (most often for global sufficiency),
possibly due to a lack of training. Still, we conclude
that the crowd generally handles the theory-based
quality assessment almost as well as the experts.
</p>
<p>However, the complexity of the assessment is
underlined by the generally limited agreement, sug-
gesting that either simplification or stricter guide-
lines are needed. Regarding simplification, the
most common practical reasons of Habernal and
Gurevych (2016a) imply what to focus on.
</p>
<p>4.3 Reliability of the Crowd Annotations
In the second comparison, we checked how many
crowd annotators are needed to compete with the
experts. For this purpose, we split the crowd ratings
into two independent groups of 5 and treated the
mean and MACE of each group as a single rating.
We then computed the agreement of both groups
and each group individually against the experts.
</p>
<p>The α-values for both groups are listed in Ta-
ble 5(b). On average, they are a bit lower than those
of all 10 crowd annotators in Table 5(a). Hence,
five crowd ratings per argument seem not enough
</p>
<p>for sufficient reliability. Tables 5(c) and 5(d) reveal
the reason behind, namely, the results of crowd
group 1 and group 2 differ clearly. At the same
time, the values in Table 5(c) are close to those in
Table 5(a), so 10 ratings might suffice. Moreover,
we see that the most stable α-values in Table 5 are
given for overall quality, indicating that the theory
indeed helps assessing quality reliably.
</p>
<p>5 Conclusion
This paper demonstrates that the theory and prac-
tice of assessing argumentation quality can learn
from each other. Most reasons for quality differ-
ences phrased in practice seem well-represented
in the normative view of theory and correlate with
absolute quality ratings. In our study, lay annota-
tors had similar agreement on the ratings as experts.
Considering that some common reasons are quite
vague, the diverse and comprehensive theoretical
view of argumentation quality may guide a more
insightful assessment. On the other hand, some
quality dimensions remain hard to assess and/or to
separate in practice, resulting in limited agreement.
Simplifying theory along the most important rea-
sons will thus improve its practical applicability.
</p>
<p>Acknowledgments
We thank Vinodkumar Prabhakaran and Yonatan
Bilu for their ongoing participation in our research
on argumentation quality. Also, we acknowledge
financial support of the DFG (ArguAna, AIPHES),
the Natural Sciences and Engineering Research
Council of Canada, and the Volkswagen Founda-
tion (Lichtenberg-Professorship Program).
</p>
<p>254</p>
<p />
</div>
<div class="page"><p />
<p>References
Jens Allwood. 2016. Argumentation, activity and cul-
</p>
<p>ture. In 6th International Conference on Computa-
tional Models of Argument (COMMA 16). Potsdam,
Germany, page 3.
</p>
<p>Aristotle. 2007. On Rhetoric: A Theory of Civic Dis-
course (George A. Kennedy, translator). Clarendon
Aristotle series. Oxford University Press.
</p>
<p>Beata Beigman Klebanov, Christian Stab, Jill Burstein,
Yi Song, Binod Gyawali, and Iryna Gurevych. 2016.
Argumentation: Content, structure, and relationship
with essay quality. In Proceedings of the Third
Workshop on Argument Mining (ArgMining2016).
Association for Computational Linguistics, pages
70–75. https://doi.org/10.18653/v1/W16-2808.
</p>
<p>Maarten Boudry, Fabio Paglieri, and Massimo Pigli-
ucci. 2015. The fake, the flimsy, and the fallacious:
Demarcating arguments in real life. Argumentation
29(4):431–456.
</p>
<p>Elena Cabrio and Serena Villata. 2012. Combining tex-
tual entailment and argumentation theory for sup-
porting online debates interactions. In Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers). Association for Computational Linguistics,
pages 208–212. http://aclweb.org/anthology/P12-
2041.
</p>
<p>T. Edward Damer. 2009. Attacking Faulty Reason-
ing: A Practical Guide to Fallacy-Free Arguments.
Wadsworth, Cengage Learning, 6th edition.
</p>
<p>Trudy Govier. 2010. A Practical Study of Argument.
Wadsworth, Cengage Learning, 7th edition.
</p>
<p>Ivan Habernal and Iryna Gurevych. 2016a. What
makes a convincing argument? Empirical anal-
ysis and detecting attributes of convincingness
in web argumentation. In Proceedings of the
2016 Conference on Empirical Methods in
Natural Language Processing. Association for
Computational Linguistics, pages 1214–1223.
http://aclweb.org/anthology/D16-1129.
</p>
<p>Ivan Habernal and Iryna Gurevych. 2016b. Which
argument is more convincing? Analyzing and pre-
dicting convincingness of web arguments using bidi-
rectional lstm. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 1589–1599.
https://doi.org/10.18653/v1/P16-1150.
</p>
<p>Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to
trust with MACE. In Proceedings of the
2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics, pages 1120–1130.
http://aclweb.org/anthology/N13-1132.
</p>
<p>Ralph H. Johnson and J. Anthony Blair. 2006. Logical
Self-defense. Intern. Debate Education Association.
</p>
<p>Chaïm Perelman and Lucie Olbrechts-Tyteca. 1969.
The New Rhetoric: A Treatise on Argumentation
(John Wilkinson and Purcell Weaver, translator).
University of Notre Dame Press.
</p>
<p>Isaac Persing and Vincent Ng. 2015. Modeling ar-
gument strength in student essays. In Proceed-
ings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Associa-
tion for Computational Linguistics, pages 543–552.
https://doi.org/10.3115/v1/P15-1053.
</p>
<p>Ruty Rinott, Lena Dankin, Carlos Alzate Perez,
M. Mitesh Khapra, Ehud Aharoni, and Noam
Slonim. 2015. Show me your evidence — An au-
tomatic method for context dependent evidence de-
tection. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, pages
440–450. https://doi.org/10.18653/v1/D15-1050.
</p>
<p>Christopher W. Tindale. 2007. Fallacies and Argument
Appraisal. Critical Reasoning and Argumentation.
Cambridge University Press.
</p>
<p>Stephen E. Toulmin. 1958. The Uses of Argument.
Cambridge University Press.
</p>
<p>Frans H. van Eemeren and Rob Grootendorst. 2004. A
Systematic Theory of Argumentation: The Pragma-
Dialectical Approach. Cambridge University Press.
</p>
<p>Henning Wachsmuth, Nona Naderi, Yufang Hou,
Yonatan Bilu, Vinodkumar Prabhakaran, Alberd-
ingk Tim Thijm, Graeme Hirst, and Benno Stein.
2017a. Computational argumentation quality assess-
ment in natural language. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 1, Long
Papers. Association for Computational Linguistics,
pages 176–187. http://aclweb.org/anthology/E17-
1017.
</p>
<p>Henning Wachsmuth, Benno Stein, and Yamen Ajjour.
2017b. “PageRank” for argument relevance. In
Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 1, Long Papers. Association
for Computational Linguistics, pages 1117–1127.
http://aclweb.org/anthology/E17-1105.
</p>
<p>Douglas Walton, Christopher Reed, and Fabrizio
Macagno. 2008. Argumentation Schemes. Cam-
bridge University Press.
</p>
<p>Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is this
post persuasive? Ranking argumentative comments
in online forum. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 195–200.
https://doi.org/10.18653/v1/P16-2032.
</p>
<p>255</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 256–262
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2040
</p>
<p>A Recurrent Neural Model with Attention for
the Recognition of Chinese Implicit Discourse Relations
</p>
<p>Samuel Rönnqvist1,2,∗, Niko Schenk2,∗ and Christian Chiarcos2
1Turku Centre for Computer Science – TUCS, Åbo Akademi University, Turku, Finland
</p>
<p>2Applied Computational Linguistics Lab, Goethe University, Frankfurt am Main, Germany
sronnqvi@abo.fi
</p>
<p>{schenk,chiarcos}@informatik.uni-frankfurt.de
</p>
<p>Abstract
</p>
<p>We introduce an attention-based Bi-LSTM
for Chinese implicit discourse relations
and demonstrate that modeling argument
pairs as a joint sequence can outperform
word order-agnostic approaches. Our
model benefits from a partial sampling
scheme and is conceptually simple, yet
achieves state-of-the-art performance on
the Chinese Discourse Treebank. We also
visualize its attention activity to illustrate
the model’s ability to selectively focus on
the relevant parts of an input sequence.
</p>
<p>1 Introduction
</p>
<p>True text understanding is one of the key goals in
Natural Language Processing and requires capa-
bilities beyond the lexical semantics of individual
words or phrases. Natural language descriptions
are typically driven by an inter-sentential coher-
ent structure, exhibiting specific discourse proper-
ties, which in turn contribute significantly to the
global meaning of a text. Automatically detecting
how meaning units are organized benefits practi-
cal downstream applications, such as question an-
swering (Sun and Chai, 2007), recognizing tex-
tual entailment (Hickl, 2008), sentiment analysis
(Trivedi and Eisenstein, 2013), or text summariza-
tion (Hirao et al., 2013).
</p>
<p>Various formalisms in terms of semantic co-
herence frameworks have been proposed to ac-
count for these contextual assumptions (Mann
and Thompson, 1988; Lascarides and Asher,
1993; Webber, 2004). The annotation schemata
of the Penn Discourse Treebank (Prasad et al.,
2008, PDTB) and the Chinese Discourse Treebank
(Zhou and Xue, 2012, CDTB), for instance, define
</p>
<p>∗Both first authors contributed equally to this work.
</p>
<p>discourse units as syntactically motivated charac-
ter spans in the text, augmented with relations
pointing from the second argument (Arg2, proto-
typically, a discourse unit associated with an ex-
plicit discourse marker) to its antecedent, i.e., the
discourse unit Arg1. Relations are labeled with
a relation type (its sense) and the associated dis-
course marker. Both, PDTB and CDTB, distin-
guish explicit from implicit relations depending on
the presence of such a marker (e.g., because/因).1
</p>
<p>Sense classification for implicit relations is by far
more challenging because the argument pairs lack
the marker as an important feature. Consider, for
instance, the following example from the CDTB
as implicit CONJUNCTION:
</p>
<p>Arg1: 会谈就一些原则和具体问题进行了
深入讨论，达成了一些谅解 In the talks, they
discussed some principles and specific questions
in depth, and reached some understandings
</p>
<p>Arg2: 双方一致认为会谈具有积极成果
Both sides agree that the talks have positive re-
sults
</p>
<p>Motivation: Previous work on implicit sense la-
beling is heavily feature-rich and requires domain-
specific, semantic lexicons (Pitler et al., 2009;
Feng and Hirst, 2012; Huang and Chen, 2011).
Only recently, resource-lean architectures have
been proposed. These promising neural meth-
ods attempt to infer latent representations appro-
priate for implicit relation classification (Zhang
et al., 2015; Ji et al., 2016; Chen et al., 2016). So
far, unfortunately, these models have been eval-
uated only on four top-level senses—sometimes
even with inconsistent evaluation setups.2 Further-
more, most systems have initially been designed
for the English PDTB and involve complex, task-
</p>
<p>1The set of relation types and senses is completed by alter-
native lexicalizations (ALTLEX/discourse marker rephrased),
and entity relations (ENTREL/anaphoric coherence).
</p>
<p>2E.g., four binary classifiers vs. four-way classification.
</p>
<p>256</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2040">https://doi.org/10.18653/v1/P17-2040</a></div>
</div>
<div class="page"><p />
<p>specific architectures (Liu and Li, 2016), while
discourse modeling techniques for Chinese have
received very little attention in the literature and
are still seriously underrepresented in terms of
publicly available systems. What is more, over
80% of all words in Chinese discourse relations
are implicit—compared to only 52% in English
(Zhou and Xue, 2012).
</p>
<p>Recently, in the context of the CoNLL 2016
shared task (Xue et al., 2016), a first independent
evaluation platform beyond class level has been
established. Surprisingly, the best performing neu-
ral architectures to date are standard feedforward
networks, cf. Wang and Lan (2016); Schenk et al.
(2016); Qin et al. (2016). Even though these spe-
cific models completely ignore word order within
arguments, such feedforward architectures have
been claimed by Rutherford et al. (2016) to gen-
erally outperform any thoroughly-tuned recurrent
architecture.
</p>
<p>Our Contribution: In this work, we release the
first attention-based recurrent neural sense clas-
sifier, specifically developed for Chinese implicit
discourse relations. Inspired by Zhou et al. (2016),
our system is a practical adaptation of the recent
advances in relation modeling extended by a novel
sampling scheme.
</p>
<p>Contrary to previous assertions by Rutherford
et al. (2016), our model demonstrates superior
performance over traditional bag-of-words ap-
proaches with feedfoward networks by treating
discourse arguments as a joint sequence. We eval-
uate our method within an independent frame-
work and show that it performs very well beyond
standard class-level predictions, achieving state-
of-the-art accuracy on the CDTB test set.
</p>
<p>We illustrate how our model’s attention mech-
anism provides means to highlight those parts of
an input sequence that are relevant for the classi-
fication decision, and thus, it may enable a bet-
ter understanding of the implicit discourse pars-
ing problem. Our proposed network architecture
is flexible and largely language-independent as it
operates only on word embeddings. It stands out
due to its structural simplicity and builds a solid
ground for further development towards other tex-
tual domains.
</p>
<p>2 Approach
</p>
<p>We propose the use of an attention-based bidi-
rectional Long Short-Term Memory (Hochreiter
</p>
<p>. . .
. . .
</p>
<p>A'1 A'2 . . .
</p>
<p>A''1 A''2
</p>
<p>h2
</p>
<p>e2
</p>
<p>A'k
A''k
</p>
<p>hk
</p>
<p>ek
</p>
<p>h1
</p>
<p>e1
</p>
<p>hk
</p>
<p>r
</p>
<p>. . .t2 tkt1
Inp
</p>
<p>ut 
lay
</p>
<p>er
</p>
<p>y
</p>
<p>Sense labels
</p>
<p>(&lt;ARG1&gt;, 会 谈, ..., &lt;/ARG1&gt;, &lt;ARG2&gt;, 双 方, ..., &lt;/ARG2&gt;)
Token input sequence
</p>
<p>Em
be
</p>
<p>dd
ing
</p>
<p>lay
er
</p>
<p>Re
cu
</p>
<p>rre
nt
</p>
<p>lay
ers
</p>
<p>At
ten
</p>
<p>tio
n
</p>
<p>lay
er
</p>
<p>Ou
tpu
</p>
<p>t la
ye
</p>
<p>r
</p>
<p>h'1 h''1
⊕ ⊕ ⊕
</p>
<p>h'2 h''2 h'k h''kh''2
</p>
<p>· α
</p>
<p>Figure 1: The attention-based bidirectional LSTM
network for the task of modeling argument pairs
for Chinese implicit discourse relations.
</p>
<p>and Schmidhuber, 1997, LSTM) network to pre-
dict senses of discourse relations. The model
draws upon previous work on LSTM, in particu-
lar its bidirectional mode of operation (Graves and
Schmidhuber, 2005), attention mechanisms for re-
current models (Bahdanau et al., 2014; Hermann
et al., 2015), and the combined use of these tech-
niques for entity relation recognition in annotated
sequences (Zhou et al., 2016). More specifically,
our model is a flexible recurrent neural network
with capabilities to sequentially inspect tokens and
to highlight which parts of the input sequence are
most informative for the discourse relation recog-
nition task, using the weighting provided by the at-
tention mechanism. Furthermore, the model bene-
fits from a novel sampling scheme for arguments,
as elaborated below. The system is learned in an
end-to-end manner and consists of multiple layers,
which are illustrated in Figure 1.
</p>
<p>First, token sequences are taken as input and
special markers (&lt;ARG1&gt;, &lt;/ARG1&gt;, etc.) are
inserted into the corresponding positions to inform
the model on the start and end points of argument
spans. This way, we can ensure a general flexi-
bility in modeling discourse units and could eas-
ily extend them with additional context, for in-
stance. In our experiments on implicit arguments,
</p>
<p>257</p>
<p />
</div>
<div class="page"><p />
<p>only the tokens in the respective spans are con-
sidered. Note that, unlike previous works, our ap-
proach models Arg1-Arg2 pairs as a joint sequence
and does not first compute intermediate represen-
tations of arguments separately.
</p>
<p>Second, an input layer encodes tokens using
one-hot vector representations (ti for tokens at po-
sitions i ∈ [1, k]), and a subsequent embedding
layer provides a dense representation (ei) to serve
as input for the recurrent layers. The embedding
layer is initialized using pre-trained word vectors,
in our case 300-dimensional Chinese Gigaword
vectors (Graff and Chen, 2005).3 These embed-
dings are further tuned as the network is trained
towards the prediction task. Embeddings for un-
known tokens, e.g., markers, are trained by back-
propagation only. Note that, tokens, markers and
the pre-trained vectors represent the only source of
information for the prediction task.
</p>
<p>For the recurrent setup, we use a layer of LSTM
networks in a bidirectional manner, in order to bet-
ter capture dependencies between parts of the in-
put sequence by inspection of both left and right-
hand-side contexts at each time step. The LSTM
holds a state representation as a continuous vector
passed to the subsequent time step, and it is capa-
ble of modeling long-range dependencies due to
its gated memory. The forward (A′) and backward
(A′′) LSTMs traverse the sequence ei, producing
sequences of vectors h′i and h
</p>
<p>′′
i respectively, which
</p>
<p>are then summed together (indicated by ⊕ in Fig-
ure 1).
</p>
<p>The resulting sequence of vectors hi is reduced
into a single vector and fed to the final softmax
output layer in order to classify the sense label y
of the discourse relation. This vector may be ob-
tained either as the final vector h produced by an
LSTM, or through pooling of all hi, or by using
attention, i.e., as a weighted sum over hi. While
the model may be somewhat more difficult to opti-
mize using attention, it provides the added benefit
of interpretability, as the weights highlight to what
extent the classifier considers the LSTM state vec-
tors at each token during modeling. This is par-
ticularly interesting for discourse parsing, as most
previous approaches have provided little support
for pinpointing the driving features in each argu-
ment span.
</p>
<p>Finally, the attention layer contains the trainable
</p>
<p>3http://www.cs.brandeis.edu/˜clp/
conll16st/dataset.html
</p>
<p>vector w (of the same dimensionality as vectors
hi) which is used to dynamically produce a weight
vector α over time steps i by:
</p>
<p>α = softmax(wT tanh(H))
</p>
<p>where H is a matrix consisting of vectors hi. The
output layer r is the weighted sum of vectors inH:
</p>
<p>r = HαT
</p>
<p>Partial Argument Sampling: For the purpose
of enlarging the instance space of training items
in the CDTB, and thus, in order to improve the
predictive performance of the model, we propose
a novel partial sampling scheme of arguments,
whereby the model is trained and validated on se-
quences containing both arguments, as well as sin-
gle arguments. A data point (a1, a2, y), with ai be-
ing the token sequence of argument i, is expanded
into {(a1, a2, y), (a1, a2, y), (a1, y), (a2, y)}. We
duplicate bi-argument samples (a1, a2, y) (in
training and development data only) to balance
their frequencies against single-argument samples.
</p>
<p>Two lines of motivation support the inclusion
of single argument training examples, grounded
in linguistics and machine learning, respectively.
First, it has been shown that single arguments in
isolation can evoke a strong expectation towards
a certain implicit discourse relation, cf. Asr and
Demberg (2015) and, in particular, Rohde and
Horton (2010) in their psycholinguistic study on
implicit causality verbs. Second, the procedure
may encourage the model to learn better represen-
tations of individual argument spans in support of
modeling of arguments in composition, cf. LeCun
et al. (2015). Due to these aspects, we believe this
data augmentation technique to be effective in re-
inforcing the overall robustness of our model.
</p>
<p>Implementational Details: We train the model
using fixed-length sequences of 256 tokens with
zero padding at the beginning of shorter sequences
and truncate longer ones. Each LSTM has a vector
dimensionality of 300, matching the embedding
size. The model is regularized by 0.5 dropout rate
between the layers and weight decay (2.5e−6) on
the LSTM inputs. We employ Adam optimization
(Kingma and Ba, 2014) using the cross-entropy
loss function with mini batch size of 80.4
</p>
<p>4The model is implemented in Keras https://
keras.io/.
</p>
<p>258</p>
<p />
</div>
<div class="page"><p />
<p>CDTB Development Set CDTB Test Set
Rank System % accuracy Rank System % accuracy
</p>
<p>1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.42
2 Qin et al. (2016) 71.57 2 Schenk et al. (2016) 71.87
3 Schenk et al. (2016) 70.59 3 Rutherford and Xue (2016) 70.47
4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41
5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.07
6 Weiss and Bajec (2016) 61.44 6 Weiss and Bajec (2016) 63.51
7 Jian et al. (2016) 21.90 7 Jian et al. (2016) 21.73
</p>
<p>This Paper: 93.52∗ This Paper: 73.01
</p>
<p>Table 1: Non-explicit parser scores on the official CoNLL 2016 CDTB development and test sets.
(∗Scores on development set are obtained through partial sampling and are not directly comparable.)
</p>
<p>Sense Label Training Dev’t Test
CONJUNCTION 5,174 189 228
majority class (66.3%) (62.8%) (64.8%)
</p>
<p>EXPANSION 1,188 48 40
ENTREL 1,099 50 71
CAUSATION 187 10 8
CONTRAST 66 3 1
PURPOSE 56 1 3
CONDITIONAL 26 0 1
TEMPORAL 26 0 0
PROGRESSION 7 0 0
# impl. rels 7,804 301 352
</p>
<p>Table 2: Implicit sense labels in the CDTB.
</p>
<p>3 Evaluation
</p>
<p>We evaluate our recurrent model on the CoNLL
2016 shared task data5 which include the official
training, development and test sets of the CDTB;
cf. Table 2 for an overview of the implicit sense
distribution.6
</p>
<p>In accordance with previous setups (Rutherford
et al., 2016), we treat entity relations (ENTREL)
as implicit and exclude ALTLEX relations. In the
evaluation, we focus on the sense-only track, the
subtask for which gold arguments are provided
and a system is supposed to label a given argu-
ment pair with the correct sense. The results are
shown in Table 1.
</p>
<p>With our proposed architecture it is possible to
correctly label 257/352 (73.01%) of implicit rela-
</p>
<p>5http://www.cs.brandeis.edu/˜clp/
conll16st/
</p>
<p>6Note that, in the CDTB, implicit relations appear almost
three times more often than explicit relations. Out of these,
65% appear within the same sentence. Finally, 25 relations in
the training set have two labels.
</p>
<p>tions on the test set, outperforming the best feed-
forward system of Wang and Lan (2016) and all
other word order-agnostic approaches. Develop-
ment and test set performances suggest the robust-
ness of our approach and its ability to generalize
to unseen data.
</p>
<p>Ablation Study: We perform an ablation study to
quantitatively assess the contribution of two of the
characteristic aspects of our model. First, we com-
pare the use of the attention mechanism against
the simpler alternative of feeding the final LSTM
hidden vectors (h′k and h
</p>
<p>′′
1) directly to the output
</p>
<p>layer. When attention is turned off, this yields
an absolute decrease in performance of 2.70% on
the test set, which is substantial and significant ac-
cording to a Welch two-sample t-test (p &lt; .001).
Second, we independently compare the use of the
partial sampling scheme against training on the
standard argument pairs in the CDTB. Here, the
absence of the partial sampling scheme yields an
absolute decrease in accuracy of 5.74% (p &lt; .001),
which demonstrates its importance for achieving
competitive performance on the task.
</p>
<p>Performance on the PDTB: As a side experi-
ment, we investigate the model’s language inde-
pendence by applying it to the implicit argument
pairs of the English PDTB. Due to computational
time constraints we do not optimize hyperparam-
eters, but instead train the model using identical
settings as for Chinese, which is expected to lead
to suboptimal performance on the evaluation data.
Nevertheless, we measure 27.09% accuracy on the
PDTB test set (surpassing the majority class base-
line of 22.01%), which shows that the model has
potential to generalize across implicit discourse
relations in a different language.
</p>
<p>259</p>
<p />
</div>
<div class="page"><p />
<p>CONJUNCTION:
</p>
<p>&lt;Arg1&gt; 会会会谈谈谈 就就就 ⼀⼀⼀些些些 原原原则则则 和和和 具具具体体体 问问问题题题 进进进⾏⾏⾏ 了了了 深深深⼊⼊⼊ 讨讨讨论论论 ，，， 达达达成成成 了了了 ⼀⼀⼀些些些 谅谅谅解解解 &lt;/Arg1&gt;
</p>
<p>&lt;Arg2&gt; 双双双⽅⽅⽅ ⼀⼀⼀致致致 认认认为为为 会会会谈谈谈 具具具有有有 积积积极极极 成成成果果果 &lt;/Arg2&gt;
</p>
<p>ENTREL:
</p>
<p>&lt;Arg1&gt; 他他他 说说说 ：：： 我我我们们们 希希希望望望 澳澳澳门门门 政政政府府府 对对对于于于 这这这 三三三 个个个 问问问题题题 继继继续续续 给给给予予予 关关关注注注 ，，，
</p>
<p>以以以 求求求得得得 最最最后后后 的的的 妥妥妥善善善 解解解决决决 &lt;/Arg1&gt;
</p>
<p>&lt;Arg2&gt; 李李李鹏鹏鹏 说说说 ，，， ⻙⻙⻙奇奇奇⽴⽴⽴ 总总总督督督 为为为 澳澳澳门门门 问问问题题题 的的的 顺顺顺利利利 解解解决决决 做做做 了了了 许许许多多多 有有有益益益 的的的 ⼯⼯⼯作作作 ，，，
</p>
<p>对对对 此此此 我我我们们们 表表表⽰⽰⽰ 赞赞赞赏赏赏 &lt;/Arg2&gt;
</p>
<p>In the talks, they discussed some principles and specific questions in depth, and reached some understandings
</p>
<p>Both sides agree that the talks have positive results
</p>
<p>He said: We hope that the Macao government will continue to pay attention to these three issues, 
</p>
<p>in order to find a final proper solution
</p>
<p>Peng Li said, Governor Liqi Wei has done a lot of useful work for the smooth settlement of the Macao question, 
</p>
<p>we appreciate that
</p>
<p>Figure 2: Visualization of attention weights for Chinese characters with high (dark blue) and low (light
blue) intensities. The underlined English phrases are semantically structure-shared by the two arguments.
</p>
<p>Visualizing Attention Weights: Finally, in Fig-
ure 2, we illustrate the learned attention weights
which pinpoint important subcomponents within
a given implicit discourse relation. For the im-
plicit CONJUNCTION relation the weights indicate
a peak on the transition between the argument
boundary, establishing a connection between the
semantically related terms understandings–agree.
Most ENTRELs show an opposite trend: here sec-
ond arguments exhibit larger intensities than Arg1,
as most entity relations follow the characteristic
writing style of newspapers by adding additional
information by reference to the same entity.
</p>
<p>4 Summary &amp; Outlook
</p>
<p>In this work, we have presented the first attention-
based recurrent neural sense labeler specifically
developed for Chinese implicit discourse relations.
Its ability to model discourse units sequentially
and jointly has been shown to be highly benefi-
cial, both in terms of state-of-the-art performance
on the CDTB (outperforming word order-agnostic
feedforward approaches), and also in terms of
insightful observations into the inner workings
of the model through its attention mechanism.
The architecture is structurally simple, benefits
from partial argument sampling, and can be eas-
</p>
<p>ily adapted to similar relation recognition tasks. In
future work, we intend to extend our approach to
different languages and domains, e.g., to the recent
data sets on narrative story understanding or ques-
tion answering (Mostafazadeh et al., 2016; Feng
et al., 2015). We believe that recurrent modeling
of implicit discourse information can be a driving
force in successfully handling such complex se-
mantic processing tasks.7
</p>
<p>Acknowledgments
</p>
<p>The authors would like to thank Ayah Zirikly,
Philip Schulz and Wei Ding for their very help-
ful suggestions on an early draft version of the pa-
per, and also thank the anonymous reviewers for
their valuable feedback and insightful comments.
We are grateful to Farrokh Mehryary for techni-
cal support with the attention layer implementa-
tion. Computational resources were provided by
CSC – IT Centre for Science, Finland, and Arcada
University of Applied Sciences, Helsinki, Finland.
Our research at Goethe University Frankfurt was
supported by the project ‘Linked Open Dictionar-
ies (LiODi, 2015-2020)’, funded by the German
Ministry for Education and Research (BMBF).
</p>
<p>7The code involved in this study is publicly
available at http://www.acoli.informatik.
uni-frankfurt.de/resources/.
</p>
<p>260</p>
<p />
</div>
<div class="page"><p />
<p>References
Fatemeh Torabi Asr and Vera Demberg. 2015. Uni-
</p>
<p>form Information Density at the Level of Dis-
course Relations: Negation Markers and Dis-
course Connective Omission. In 11th International
Conference on Computational Semantics (IWCS).
page 118. http://www.coli.uni-saarland.de/ fate-
meh/iwcs2015.pdf.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural Machine Translation by
Jointly Learning to Align and Translate. CoRR
abs/1409.0473. http://arxiv.org/abs/1409.0473.
</p>
<p>Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu,
and Xuanjing Huang. 2016. Implicit Discourse
Relation Detection via a Deep Architecture with
Gated Relevance Network. In Proceedings of the
54th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers.
http://aclweb.org/anthology/P/P16/P16-1163.pdf.
</p>
<p>Minwei Feng, Bing Xiang, Michael R. Glass,
Lidan Wang, and Bowen Zhou. 2015. Ap-
plying deep learning to answer selection: A
study and an open task. In 2015 IEEE Work-
shop on Automatic Speech Recognition and
Understanding, ASRU 2015, Scottsdale, AZ,
USA, December 13-17, 2015. pages 813–820.
https://doi.org/10.1109/ASRU.2015.7404872.
</p>
<p>Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistic Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, ACL ’12, pages 60–68.
http://www.aclweb.org/anthology/P12-1007.
</p>
<p>David Graff and Ke Chen. 2005. Chinese Gigaword.
LDC Catalog No.: LDC2003T09, ISBN, 1:58563–
58230.
</p>
<p>Alex Graves and Jürgen Schmidhuber. 2005.
Framewise phoneme classification with bidi-
rectional LSTM and other neural network ar-
chitectures. Neural Networks 18(5-6):602–610.
https://doi.org/10.1016/j.neunet.2005.06.042.
</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.
</p>
<p>Andrew Hickl. 2008. Using Discourse Commit-
ments to Recognize Textual Entailment. In
Proceedings of the 22nd International Confer-
ence on Computational Linguistics - Volume 1.
Association for Computational Linguistics, Strouds-
burg, PA, USA, COLING ’08, pages 337–344.
http://dl.acm.org/citation.cfm?id=1599081.1599124.
</p>
<p>Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-Document Summarization as a Tree Knap-
sack Problem. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2013, 18-21 Octo-
ber 2013, Grand Hyatt Seattle, Seattle, Wash-
ington, USA, A meeting of SIGDAT, a Special
Interest Group of the ACL. pages 1515–1520.
http://aclweb.org/anthology/D/D13/D13-1158.pdf.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
Short-Term Memory. Neural Comput. 9(8):1735–
1780. https://doi.org/10.1162/neco.1997.9.8.1735.
</p>
<p>Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese
Discourse Relation Recognition. In Proceedings
of 5th International Joint Conference on Natural
Language Processing. Asian Federation of Natural
Language Processing, Chiang Mai, Thailand, pages
1442–1446. http://www.aclweb.org/anthology/I11-
1170.
</p>
<p>Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen-
stein. 2016. A Latent Variable Recurrent Neu-
ral Network for Discourse-Driven Language Mod-
els. In Proceedings of the 2016 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies. Association for Computational Lin-
guistics, San Diego, California, pages 332–342.
http://www.aclweb.org/anthology/N16-1037.
</p>
<p>Ping Jian, Xiaohan She, Chenwei Zhang, Pengcheng
Zhang, and Jian Feng. 2016. Discourse
Relation Sense Classification Systems for
CoNLL-2016 Shared Task. In Proceedings
of the CoNLL-16 shared task. Association for
Computational Linguistics, pages 158–163.
https://doi.org/10.18653/v1/K16-2022.
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.
</p>
<p>Alex Lascarides and Nicholas Asher. 1993. Tempo-
ral Interpretation, Discourse Relations and Com-
monsense entailment. Linguistics and Philosophy
16(5):437–493.
</p>
<p>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature 521(7553):436–444.
</p>
<p>Yang Liu and Sujian Li. 2016. Recognizing Im-
plicit Discourse Relations via Repeated Read-
ing: Neural Networks with Multi-Level Atten-
tion. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016. pages 1224–1233.
http://aclweb.org/anthology/D/D16/D16-1130.pdf.
</p>
<p>William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text 8(3):243–281.
</p>
<p>261</p>
<p />
</div>
<div class="page"><p />
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A Cor-
pus and Cloze Evaluation for Deeper Understand-
ing of Commonsense Stories. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Com-
putational Linguistics, San Diego, California, pages
839–849. http://www.aclweb.org/anthology/N16-
1098.
</p>
<p>Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 2 -
Volume 2. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, ACL 2009, pages 683–
691. http://www.aclweb.org/anthology/P/P09/P09-
1077.pdf.
</p>
<p>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse TreeBank
2.0. In Proceedings, 6th International Conference
on Language Resources and Evaluation. Marrakech,
Morocco, pages 2961–2968.
</p>
<p>Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016.
Shallow Discourse Parsing Using Convolutional
Neural Network. In Proceedings of the CoNLL-16
shared task. Association for Computational Linguis-
tics, pages 70–77. https://doi.org/10.18653/v1/K16-
2010.
</p>
<p>Hannah Rohde and William Horton. 2010. Why or
what next? Eye movements reveal expectations
about discourse direction. Talk at the 23rd Annual
CUNY Conference on Human Sentence Processing.
New York, NY.
</p>
<p>Attapol Rutherford and Nianwen Xue. 2016. Ro-
bust Non-Explicit Neural Discourse Parser in En-
glish and Chinese. In Proceedings of the CoNLL-16
shared task. Association for Computational Linguis-
tics, pages 55–59. https://doi.org/10.18653/v1/K16-
2007.
</p>
<p>Attapol T. Rutherford, Vera Demberg, and Nian-
wen Xue. 2016. Neural Network Models for Im-
plicit Discourse Relation Classification in English
and Chinese without Surface Features. CoRR
abs/1606.01990. http://arxiv.org/abs/1606.01990.
</p>
<p>Niko Schenk, Christian Chiarcos, Kathrin Donandt,
Samuel Rönnqvist, Evgeny Stepanov, and Giuseppe
Riccardi. 2016. Do We Really Need All Those
Rich Linguistic Features? A Neural Network-Based
Approach to Implicit Sense Labeling. In Pro-
ceedings of the CoNLL-16 shared task. Associa-
tion for Computational Linguistics, pages 41–49.
https://doi.org/10.18653/v1/K16-2005.
</p>
<p>Mingyu Sun and Joyce Y Chai. 2007. Discourse pro-
cessing for context question answering based on
linguistic knowledge. Knowledge-Based Systems
20(6):511–526.
</p>
<p>Rakshit S. Trivedi and Jacob Eisenstein. 2013. Dis-
course connectors for latent subjectivity in senti-
ment analysis. In Human Language Technolo-
gies: Conference of the North American Chap-
ter of the Association of Computational Linguis-
tics, Proceedings, June 9-14, 2013, Westin Peachtree
Plaza Hotel, Atlanta, Georgia, USA. pages 808–813.
http://aclweb.org/anthology/N/N13/N13-1100.pdf.
</p>
<p>Jianxiang Wang and Man Lan. 2016. Two End-
to-end Shallow Discourse Parsers for English and
Chinese in CoNLL-2016 Shared Task. In Pro-
ceedings of the CoNLL-16 shared task. Associa-
tion for Computational Linguistics, pages 33–40.
https://doi.org/10.18653/v1/K16-2004.
</p>
<p>Bonnie L. Webber. 2004. D-LTAG: extend-
ing lexicalized TAG to discourse. Cogni-
tive Science 28(5):751–779. http://dblp.uni-
trier.de/db/journals/cogsci/cogsci28.html.
</p>
<p>Gregor Weiss and Marko Bajec. 2016. Discourse Sense
Classification from Scratch using Focused RNNs. In
Proceedings of the CoNLL-16 shared task. Associ-
ation for Computational Linguistics, pages 50–54.
https://doi.org/10.18653/v1/K16-2006.
</p>
<p>Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Bon-
nie Webber, Attapol Rutherford, Chuan Wang, and
Hongmin Wang. 2016. The CoNLL-2016 Shared
Task on Shallow Discourse Parsing. In Proceedings
of the Twentieth Conference on Computational Nat-
ural Language Learning - Shared Task. Association
for Computational Linguistics, Berlin, Germany.
</p>
<p>Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, Hong
Duan, and Junfeng Yao. 2015. Shallow Convolu-
tional Neural Network for Implicit Discourse Re-
lation Recognition. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015. pages 2230–2235.
http://aclweb.org/anthology/D/D15/D15-1266.pdf.
</p>
<p>Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
Based Bidirectional Long Short-Term Memory Net-
works for Relation Classification. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 2: Short Papers.
http://aclweb.org/anthology/P/P16/P16-2034.pdf.
</p>
<p>Yuping Zhou and Nianwen Xue. 2012. PDTB-
style Discourse Annotation of Chinese Text.
In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for
Computational Linguistics, Jeju Island, Korea,
pages 69–77. http://www.aclweb.org/anthology-
new/P/P12/P12-1008.bib.
</p>
<p>262</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 263–268
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2041
</p>
<p>Discourse Annotation of Non-native Spontaneous Spoken Responses
Using the Rhetorical Structure Theory Framework
</p>
<p>Xinhao Wang1, James V. Bruno2, Hillary R. Molloy1, Keelan Evanini2, Klaus Zechner2
Educational Testing Service
</p>
<p>190 New Montgomery St #1500, San Francisco, CA 94105, USA
2660 Rosedale Road, Princeton, NJ 08541, USA
</p>
<p>xwang002, jbruno, hmolloy, kevanini, kzechner@ets.org
</p>
<p>Abstract
</p>
<p>The availability of the Rhetorical Struc-
ture Theory (RST) Discourse Treebank
has spurred substantial research into dis-
course analysis of written texts; how-
ever, limited research has been conducted
to date on RST annotation and parsing
of spoken language, in particular, non-
native spontaneous speech. Considering
that the measurement of discourse coher-
ence is typically a key metric in human
scoring rubrics for assessments of spo-
ken language, we initiated a research ef-
fort to obtain RST annotations of a large
number of non-native spoken responses
from a standardized assessment of aca-
demic English proficiency. The result-
ing inter-annotator κ agreements on the
three different levels of Span, Nuclear-
ity, and Relation are 0.848, 0.766, and
0.653, respectively. Furthermore, a set
of features was explored to evaluate the
discourse structure of non-native sponta-
neous speech based on these annotations;
the highest performing feature showed a
correlation of 0.612 with scores of dis-
course coherence provided by expert hu-
man raters.
</p>
<p>1 Introduction
</p>
<p>The spread of English as the global language of
education and commerce is continuing, and there
is a strong interest in developing assessment sys-
tems that can automatically score spontaneous
speech from non-native speakers with the goals
of reducing the burden on human raters, improv-
ing reliability, and generating feedback that can
be used by language learners. Discourse coher-
ence, which refers to the conceptual relations be-
</p>
<p>tween different units within a response, is an im-
portant aspect of communicative competence, as
is reflected in human scoring rubrics for assess-
ments of non-native English (ETS, 2012). How-
ever, discourse-level features have rarely been in-
vestigated in the context of automated speech scor-
ing. This study aims to construct a discourse-
level annotation of non-native spontaneous speech
in the framework of Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988), which can
then be used in automated discourse analysis and
coherence measurement for non-native spoken re-
sponses, thereby improving the validity of the au-
tomated scoring systems.
</p>
<p>RST is a descriptive framework that has been
widely used in the analysis of discourse organiza-
tion of written texts (Taboada and Mann, 2006b),
and has been applied to various natural lan-
guage processing tasks, including language gen-
eration, text summarization, and machine trans-
lation (Taboada and Mann, 2006a). In particu-
lar, the availability of RST annotations on a se-
lection of 385 Wall Street Journal articles from
the Penn Treebank1 (Carlson et al., 2001) has
facilitated RST-based discourse analysis of writ-
ten texts, since it provides a standard benchmark
for comparing the performance of different tech-
niques for document-level discourse parsing (Joty
et al., 2013; Feng and Hirst, 2014).
</p>
<p>Another important application of RST closely
related to our research is the automated evaluation
of discourse in student essays. For example, one
study used features for each sentence in an essay
to reflect the status of its parent node as well as its
rhetorical relation based on automatically parsed
RST trees with the goal of providing feedback to
students about the discourse structure in the essay
(Burstein et al., 2003). Another study compared
</p>
<p>1https://catalog.ldc.upenn.edu/
LDC2002T07
</p>
<p>263</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2041">https://doi.org/10.18653/v1/P17-2041</a></div>
</div>
<div class="page"><p />
<p>features derived from deep hierarchical discourse
relations based on RST parsing and features de-
rived from shallow discourse relations based on
Penn Discourse Treebank (PDTB) (Prasad et al.,
2008) parsing in the task of essay scoring and
demonstrated the effectiveness of deep discourse
structure in better differentiation of text coherence
(Feng et al., 2014).
</p>
<p>Related work has also been conducted to anno-
tate discourse relations in spoken language, which
is produced and processed differently from writ-
ten texts (Rehbein et al., 2016), and often lacks
explicit discourse connectives that are more fre-
quent in written language. Instead of the rooted-
tree structure that is employed in RST, the annota-
tion scheme with shallow discourse structure and
relations from the PDTB (Prasad et al., 2008) has
been generally used for spoken language (Demi-
rahin and Zeyrek, 2014; Stoyanchev and Banga-
lore, 2015). For example, Tonelli et al. adapted the
PDTB annotation scheme to annotate discourse
relations in spontaneous conversations in Italian
(Tonelli et al., 2010) and Rehbein et al. compared
two frameworks, PDTB and CCR (Cognitive ap-
proach to Coherence Relations) (Sanders et al.,
1992), for the annotation of discourse relations in
spoken language (Rehbein et al., 2016).
</p>
<p>In contrast to these previous studies, this study
focuses on monologic spoken responses produced
by non-native speakers within the context of a lan-
guage proficiency assessment. A discourse an-
notation scheme based on the RST framework
was selected due to the fact that it can effectively
demonstrate the deep hierarchical discourse struc-
ture across an entire response, rather than focusing
on the local coherence of adjacent units.
</p>
<p>2 Data
</p>
<p>This study obtained manual RST annotations on
a corpus of 600 spoken responses drawn from a
large-scale, high-stakes standardized assessment
of English for non-native speakers, the TOEFL R©
</p>
<p>Internet-based Test (TOEFL R© iBT), which as-
sesses English communication skills for academic
purposes (ETS, 2012). The speaking section of the
TOEFL iBT assessment contains six tasks, each
of which requires the test taker to provide an un-
scripted spoken response 45 or 60 seconds in du-
ration. The corpus used in this study includes
100 responses from each of six different test ques-
tions that comprise two different speaking tasks:
</p>
<p>1) Independent questions: providing an opinion
based on personal experience (N = 200 responses)
and 2) Integrated questions: summarizing or dis-
cussing material provided in a reading and/or lis-
tening passage (N = 400 responses). The spo-
ken responses were all manually transcribed using
standard punctuation and capitalization. The av-
erage number of words per response is 104.4 (st.
dev. = 34.4) and the average number of sentences
is 5.5 (st. dev. = 2.1).
</p>
<p>The spoken responses were all provided with
holistic English proficiency scores on a scale of 1
to 4 by expert human raters in the context of opera-
tional, high-stakes scoring for the spoken language
assessment. The scoring rubrics address the fol-
lowing three main aspects of speaking proficiency:
delivery (pronunciation, fluency, prosody), lan-
guage use (grammar and lexical choice), and topic
development (content and coherence). In order to
ensure a sufficient quantity of responses from each
proficiency level, 25 responses were selected ran-
domly from each of the 4 score points for each of
the 6 test questions.
</p>
<p>The current study builds on a previous study
that investigated approaches for modeling dis-
course coherence in non-native spontaneous
speech (but which did not consider the hierarchical
rhetorical structure of speech) (Wang et al., 2013).
In that study, each spoken response in the same
corpus that was used for the current study was
provided with global discourse coherence scores.
Two expert annotators (not drawn from the pool
of expert human raters who provided the holistic
scores) provided each response with a score on
a scale of 1 to 3 based on the orthographic tran-
scriptions of the spoken response. The three score
points were defined as follows: 3 = highly co-
herent (contains no instances of confusing argu-
ments or examples), 2 = somewhat coherent (con-
tains some awkward points in which the speaker’s
line of argument is unclear), 1 = barely coherent
(the entire response was confusing and hard to
follow). In addition, the annotators were specif-
ically required to ignore disfluencies and gram-
matical errors as much as possible. The inter-
annotator agreement for these coherence scores
was κ = 0.68. These discourse coherence scores
are reused in the current study (along with the
holistic profiency scores presented above) to eval-
uate the performance of features measuring dis-
course coherence based on the RST annotations.
</p>
<p>264</p>
<p />
</div>
<div class="page"><p />
<p>3 Annotation
</p>
<p>3.1 Guidelines
We used a modified version of the tagging ref-
erence manual for the RST Discourse Treebank
(Carlson and Marcu, 2001) for this study. Ac-
cording to these guidelines, annotators segment
a transcribed spoken response into Elementary
Discourse Unit (EDU) spans of text (correspond-
ing to clauses or clause-like units), and indi-
cate rhetorical relations between non-overlapping
spans which typically consist of a nucleus (the
most essential information in the rhetorical re-
lation) and a satellite (supporting or background
information). In contrast to well-formed writ-
ten text, non-native spontaneous speech frequently
contains ungrammatical sentences, disfluencies,
fillers, hesitations, false starts, and unfinished ut-
terances. In some cases, these spoken responses
do not constitute coherent, well-formed discourse.
On the other hand, spoken responses are relatively
shorter and comprise simpler discourse structures
with fewer relations, which simplifies the RST an-
notation task in comparison to written text. In
order to account for these differences, we cre-
ated an addendum to the RST Discourse Treebank
manual introducing the following additional rela-
tions: Disfluency relations (in which the disflu-
ent span is the satellite and the corresponding flu-
ent span is the nucleus), Awkward relations (cor-
responding to portions of the response where the
speaker’s discourse structure is infelicitous; awk-
ward relations are based on pre-existing relations,
such as awkward-Reason, if the intended relation
is clear but is expressed incoherently or are labeled
as awkward-Other if there is no clear relation
between the awkward EDU and the surrounding
discourse), Unfinished Utterance relations (repre-
senting EDUs at the end of a response that are in-
complete because the test taker ran out of time in
which the incomplete span is the satellite and the
root node of the discourse tree is the nucleus), and
Discourse Particle relations (such as you know and
right, which are satellites of adjacent spans).
</p>
<p>The discourse annotation tool used in the RST
Discourse Treebank2 was also adopted for this
study. Using this tool, annotators incrementally
build hierarchical discourse trees in which the
leaves are the EDUs and the internal nodes cor-
respond to contiguous spans of text. When the an-
</p>
<p>2Downloaded from http://www.isi.edu/
licensed-sw/RSTTool/index.html
</p>
<p>notators assign the rhetorical relation for a node of
the tree, they provide the relation’s label (drawn
from the pre-defined set of relations in the anno-
tation guidelines) and also indicate whether the
spans that comprise the relation are nuclei or satel-
lites. Figure 1 shows an example of an annotated
RST tree for a response with a proficiency score
of 1. This response includes three disfluencies
(EDUs 3, 6, and 9), which are satellites of the
corresponding repair nuclei. In addition, the re-
sponse also includes an awkward Comment-Topic
relation between EDU 2 and the node combin-
ing EDUs 3-11, indicated by awkward-Comment-
Topic-2; in this multinuclear relation, the annota-
tor judged that the second branch of the relation
was awkward, which is indicated by the 2 that was
appended to the relation label.
</p>
<p>3.2 Pilot Annotation
</p>
<p>The manual annotations were provided by two ex-
perts with prior experience in various types of data
annotation on both text and speech. First, a pi-
lot annotation was conducted to train and cali-
brate the annotators based on 48 training samples
drawn from the TOEFL R© Practice Online (TPO)
product3, which offers practice tests simulating
the TOEFL iBT testing experience with authentic
test questions. The training samples were selected
from a TPO test form with 6 test questions and
were balanced according to test question and pro-
ficiency score, i.e., 2 responses from each score
level for each question.
</p>
<p>Human annotators were trained in a two-step
process: 1) after a comprehensive study of the an-
notation guidelines described in Section 3.1, the
two annotators were initially trained with 16 TPO
responses (8 responses from an Independent ques-
tion and 8 responses from an Integrated question)
by first performing independent annotation and
then resolving all disagreements through a discus-
sion of the guidelines; 2) another round of training
was conducted on an additional set of 32 TPO re-
sponses (8 responses from an Independent ques-
tion and 24 responses from an Integrated ques-
tion). Each annotator first annotated this set of 32
responses independently; the two annotators sub-
sequently conducted a thorough joint review and
discussion of each other’s annotations in order to
resolve all disagreements on this set.
</p>
<p>In order to measure the human agreement on
</p>
<p>3https://toeflpractice.ets.org/
</p>
<p>265</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Example of an annotated RST tree on a response with a proficiency score of 1.
</p>
<p>the EDU segmentation task, we first converted the
segmentation sequences into 0/1 sequences: for
each word in a response, 1 is assigned if a seg-
ment boundary exists after the word; otherwise,
0 is assigned. The inter-annotator agreement rate
on the EDU segmentations of the 32 pilot samples
(from stage 2) was κ = 0.876. On the hierarchi-
cal tree building task, inter-annotator agreement
was evaluated on the levels of Span (assignment
of discourse segment), Nuclearity (assignment of
nucleus vs. satellite), and Relation (assignment of
rhetorical relation) using κ, as described in (Marcu
et al., 1999); on the 32 samples, the κ values are
0.861, 0.769, and 0.631 for the three levels, re-
spectively.
</p>
<p>3.3 Formal Annotation
</p>
<p>For the formal annotation on the full set of 600
TOEFL spoken responses, 120 responses from 6
test questions (5 responses from each score level
from each question) were selected for double an-
notation, and the remaining 480 responses re-
ceived a single annotation.
</p>
<p>The 120 double-annotated responses were split
into two batches of equal size and the two annota-
tors each performed EDU segmentation indepen-
dently on one of the batches. Subsequently, the
annotators reviewed each other’s EDU segmen-
tations and adjudicated all disagreements to ob-
tain gold-standard EDU segmentation for the 120
responses in the double-annotation set. The av-
erage number of EDUs per response in the two
batches in this set of 120 responses were 15.1 and
14.1. The annotators subsequently performed the
remaining steps of RST annotation (assigning the
relations, nuclearity, and hierarchical structure) in-
dependently on all 120 responses using the adjudi-
cated EDU segmentations. Table 1 shows that the
κ agreements on the three levels of Span, Nucle-
</p>
<p>Table 1: Human agreement on RST annotations in
terms of κ and F1-Measure.
</p>
<p>Span Nuclearity Relation
κ 0.848 0.766 0.653
</p>
<p>F1-Measure 0.872 0.724 0.522
</p>
<p>Table 2: The average number of awkward relations
appearing in responses from each of the four pro-
ficiency score levels.
</p>
<p>1 2 3 4
Annotator 1 3.2 1.1 1.1 0.3
Annotator 2 2.1 1.2 0.7 0.3
</p>
<p>arity, and Relation are 0.848, 0.766, and 0.653, re-
spectively. Besides the κ evaluation, the standard
ways of F1-Measure on three levels of Span, Nu-
clearity, and Relation (Marcu, 2000), commonly
used to evaluate the performance of RST parsers,
are also reported in Table 1. The F1-measures
were calculated according to each pair of trees
from two annotators on the same sample and then
averaged across all samples, i.e., a macroaveraged
F1-measure.
</p>
<p>The human agreement results also indicate that
two annotators tend to agree better on responses
from speakers with higher speaking proficiency
levels. This is demonstrated by positive correla-
tions between the F1 agreement scores and the hu-
man proficiency ratings: 0.197 for Span annota-
tions, 0.210 for Nuclearity, and 0.188 for Relation.
</p>
<p>In addition, we also examined the distribution
of the manually identified awkward relations. As
shown in Table 2, awkward points occur with
higher frequency in responses with lower profi-
ciency scores.
</p>
<p>266</p>
<p />
</div>
<div class="page"><p />
<p>4 Discourse Features
</p>
<p>The ultimate aim of this line of research is to
use an RST-annotated corpus to investigate fea-
tures for automatically assessing discourse struc-
ture in spontaneous non-native speech. Using
the annotated discourse trees, we extracted sev-
eral different features based on the distribution
of relations and the structure of the trees, includ-
ing the number of EDUs (n edu), the number
of relations (n rel), the number of awkward re-
lations (n awk rel), the number of rhetorical re-
lations, i.e., relations that were neither classified
as awkward nor as a disfluency (n rhe rel), the
number of different types of rhetorical relations
(n rhe relTypes), the percentage of rhetorical rela-
tions (perc rhe rel) out of all relations, the depth of
the RST trees (tree depth), and the ratio between
n edu and tree depth (ratio nedu depth). Table 3
lists the Pearson correlation coefficients of these
features with both the holistic proficiency scores
and the discourse coherence scores and demon-
strates the effectiveness of these features. The
n rhe rel feature achieves the highest correlation
of 0.691 with the holistic proficiency scores, and
the normalized feature perc rhe rel achieves the
highest correlation of 0.612 with the discourse co-
herence scores. It is interesting to note that RST-
based discourse features generally have higher
correlations with the holistic speaking proficiency
scores than with the more specific discourse co-
herence scores. This result is somewhat unex-
pected, since the holistic proficiency scores are
based only partially on discourse coherence and
also cover other aspects of speaking proficiency,
such as pronunciation, fluency, grammar, and vo-
cabulary. One potential explanation for the higher
correlations could be the difference in score range
(1-4 for the holistic proficiency scores and 1-3 for
the discourse scores). In addition, as described in
Section 2, the data set used in this study was cre-
ated using a stratified random sample with an even
distribution of holistic scores (which may increase
the features’ correlations with holistic scores), but
this constraint does not apply to the discourse co-
herence scores.
</p>
<p>5 Conclusion and Future Work
</p>
<p>In this study, we obtained discourse coherence an-
notations based on Rhetorical Structure Theory
for a corpus of 600 non-native spontaneous spo-
ken responses drawn from a standardized assess-
</p>
<p>Table 3: Pearson correlation coefficients (r) of dis-
course features with both the holistic proficiency
scores as well as the discourse coherence scores.
For the 120 double-annotated responses, the aver-
aged feature values were used.
</p>
<p>Features Proficiency Coherence
n edu 0.58 0.397
n rel 0.584 0.396
n awk rel -0.396 -0.509
n rhe rel 0.691 0.541
n rhe relTypes 0.64 0.557
perc rhe rel 0.589 0.612
tree depth 0.365 0.25
ratio nedu depth 0.529 0.367
</p>
<p>ment of non-native English. The RST annotation
results show that the annotators achieved similar
inter-annotator agreement rates as have been re-
ported in previous studies that investigated well-
formed written text (Marcu et al., 1999). In ad-
dition, we demonstrate the potential of using fea-
tures derived from these RST annotations for as-
sessing non-native spoken English through moder-
ately high correlations with both holistic speaking
proficiency scores and discourse coherence scores;
the highest performing feature when evaluated on
the discourse coherence scores provided by expert
raters was the percentage of rhetorical relations in
the entire spoken response (perc rhe rel), with a
correlation of 0.612.
</p>
<p>In the future, we will continue this work by ad-
dressing the following main research questions: a)
how can we develop additional effective features
from the discourse trees; b) how well can an auto-
matic discourse parser trained on the obtained an-
notations perform; c) how well will the proposed
features perform when extracted using an auto-
matic RST parser; d) how well will the features
perform when using an automated speech recog-
nizer (rather than human transcribers) to obtain the
textual transcriptions of a spoken response.
</p>
<p>References
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
</p>
<p>Finding the write stuff: Automatic identification of
discourse structure in student essays. IEEE Intelli-
gent Systems 18(1):32–39.
</p>
<p>Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging reference manual. Technical Report ISI-TR-
545, ISI Technical Report.
</p>
<p>267</p>
<p />
</div>
<div class="page"><p />
<p>Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurows. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In 2nd SIGDIAL Workshop on Discourse and Dia-
logue. Aalborg, Denmark, pages 1–10.
</p>
<p>In Demirahin and Deniz Zeyrek. 2014. Annotating
discourse connectives in spoken Turkish. In The
8th Liguistic Annotation Workshop. Dublin, Ireland,
pages 105–109.
</p>
<p>ETS. 2012. The official guide to the TOEFL R© test.
Fourth Edition, McGraw-Hill .
</p>
<p>Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics. Baltimore, Maryland, pages 511–521.
</p>
<p>Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst.
2014. The impact of deep hierarchical discourse
structures in the evaluation of text coherence. In
Proceedings of COLING 2014, the 25th Interna-
tional Conference on Computational Linguistics:
Technical Papers. Dublin, Ireland, pages 940–949.
</p>
<p>Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics. Sofia, Bulgaria, pages 486–496.
</p>
<p>William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text - Interdisciplinary
Journal for the Study of Discourse (Text) 8(3):243–
281.
</p>
<p>Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.
</p>
<p>Daniel Marcu, Estibaliz Amorrortu, and Magdalena
Romera. 1999. Experiments in constructing a cor-
pus of discourse trees. In ACL Workshop on Stan-
dards and Tools for Discourse Tagging. College
Park, Maryland, pages 48–57.
</p>
<p>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, and Livio Robaldo. 2008. The Penn Dis-
course TreeBank 2.0. In The 6th International
Conference on Language Resources and Evaluation
(LREC). Marrakech, Morocco, pages 2961–2968.
</p>
<p>Ines Rehbein, Merel Scholman, and Vera Demberg.
2016. Annotating discourse relations in spoken lan-
guage: A comparison of the PDTB and CCR frame-
works. In The Tenth International Conference on
Language Resources and Evaluation (LREC 2016).
Portorož, Slovenia, pages 1039–1046.
</p>
<p>Ted J. M. Sanders, Wilbert P. M. Spooren, and Leo
G. M. Noordman. 1992. Toward a taxonomy of co-
herence relations. Discourse Processes 15(1):1–35.
</p>
<p>Svetlana Stoyanchev and Srinivas Bangalore. 2015.
Discourse in customer care dialogues. Poster pre-
sented at the Workshop of Identification and Anno-
tation of Discourse Relations in Spoken Language.
Saarbrücken, Germany.
</p>
<p>Maite Taboada and William C. Mann. 2006a. Appli-
cations of Rhetorical Structure Theory. Discourse
Studies 8(4):567–588.
</p>
<p>Maite Taboada and William C. Mann. 2006b. Rhetor-
ical Structure Theory: Looking back and moving
ahead. Discourse Studies 8(3):423–459.
</p>
<p>Sara Tonelli, Giuseppe Riccardi, Rashmi Prasad, and
Aravind Joshi. 2010. Annotation of discourse re-
lations for conversational spoken dialogs. In The
Seventh International Conference on Language Re-
sources and Evaluation (LREC’10). Valetta, Malta,
pages 2084–2090.
</p>
<p>Xinhao Wang, Keelan Evanini, and Klaus Zechner.
2013. Coherence modeling for the automated as-
sessment of spontaneous spoken responses. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. At-
lanta, Georgia, pages 814–819.
</p>
<p>268</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 269–274
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2042
</p>
<p>Improving Implicit Discourse Relation Recognition with
Discourse-specific Word Embeddings
</p>
<p>Changxing Wu1,2, Xiaodong Shi1,2∗, Yidong Chen1,2, Jinsong Su3, Boli Wang1,2
Fujian Key Lab of the Brain-like Intelligent Systems, Xiamen University, China1
</p>
<p>School of Information Science and Technology, Xiamen University, China2
</p>
<p>Xiamen University, China3
</p>
<p>wcxnlp@163.com boliwang@stu.xmu.edu.cn
{mandel, ydchen, jssu}@xmu.edu.cn
</p>
<p>Abstract
</p>
<p>We introduce a simple and effective
method to learn discourse-specific word
embeddings (DSWE) for implicit dis-
course relation recognition. Specifically,
DSWE is learned by performing connec-
tive classification on massive explicit dis-
course data, and capable of capturing dis-
course relationships between words. On
the PDTB data set, using DSWE as fea-
tures achieves significant improvements
over baselines.
</p>
<p>1 Introduction
</p>
<p>Recognizing discourse relations (e.g., Contrast,
Conjunction) between two sentences is a crucial
subtask of discourse structure analysis. These re-
lations can benefit many downstream NLP tasks,
including question answering, machine translation
and so on. A discourse relation instance is usually
defined as a discourse connective (e.g., but, and)
taking two arguments (e.g., clause, sentence). For
explicit discourse relation recognition, using only
connectives as features achieves more than 93%
in accuracy (Pitler and Nenkova, 2009). Without
obvious clues like connectives, implicit discourse
relation recognition is still challenging.
</p>
<p>The earlier researches usually develop linguisti-
cally informed features and use supervised learn-
ing method to perform the task (Pitler et al., 2009;
Lin et al., 2009; Louis et al., 2010; Rutherford and
Xue, 2014; Braud and Denis, 2015). Among these
features, word pairs occurring in argument pairs
are considered as important features, since they
can partially catch discourse relationships between
two arguments. For example, synonym word pairs
like (good, great) may indicate a Conjunction re-
lation, while antonym word pairs like (good, bad)
</p>
<p>∗Corresponding author.
</p>
<p>may mean a Contrast relation. However, classi-
fiers based on word pairs in previous work do not
work well because of the data sparsity problem. To
address this problem, recent researches use word
embeddings (aka distributed representations) in-
stead of words as input features, and design vari-
ous neural networks to capture discourse relation-
ships between arguments (Zhang et al., 2015; Ji
and Eisenstein, 2015; Qin et al., 2016; Chen et al.,
2016; Liu and Li, 2016). While these researches
achieve promising results, they are all based on
pre-trained word embeddings ignoring discourse
information (e.g., good, great, and bad are of-
ten mapped into close vectors). Intuitively, using
word embeddings sensitive to discourse relations
would further boost the performance.
</p>
<p>In this paper, we propose to learn discourse-
specific word embeddings (DSWE) from explicit
data for implicit discourse relation recognition.
Our method is inspired by the observation that
synonym (antonym) word pairs tend to appear
around the discourse connective and (but). Other
connectives can also provide some discourse
clues. We expect to encode these discourse clues
into the distributed representations of words, to
capture discourse relationships between them. To
this end, we use a simple neural network to per-
form connective classification on massive explicit
data. Explicit data can be considered to be au-
tomatically labeled by connectives. While they
cannot be directly used as training data for im-
plicit discourse relation recognition and contain
some noise, they are effective enough to pro-
vide weakly supervised signals for training the
discourse-specific word embeddings.
</p>
<p>We apply DSWE as features in a supervised
neural network for implicit discourse relations
recognition. On the PDTB (Prasad et al., 2008),
using DSWE yields significantly better perfor-
mance than using off-the-shelf word embeddings,
</p>
<p>269</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2042">https://doi.org/10.18653/v1/P17-2042</a></div>
</div>
<div class="page"><p />
<p>or recent systems incorporating explicit data. We
detail our method in Section 2 and evaluate it in
Section 3. Conclusions are given in Section 4. Our
learned DSWE is publicly available at here.
</p>
<p>2 Discourse-specific Word Embeddings
</p>
<p>In this section, we first introduce the neural net-
work model for learning discourse-specific word
embeddings (DSWE), and then the way of collect-
ing explicit discourse data for training. Finally, we
highlight the differences between our work and the
related researches.
</p>
<p>avg
</p>
<p>cw
1
</p>
<p>hw
2
</p>
<p>hw
c
o
n
n
</p>
<p>1
</p>
<p>1warg
</p>
<p>1
wmarg
</p>
<p>2
</p>
<p>1warg
</p>
<p>2
wnarg
</p>
<p>…
</p>
<p>…
…
</p>
<p>…
</p>
<p>Figure 1: Neural network model for learn-
ing DSWE. An explicit instance is denoted as
(arg1, arg2, conn). w1arg1 , ..., w
</p>
<p>m
arg1 mean the
</p>
<p>words in arg1. Two arguments are concatenated
as input and the number of hidden layers is not
limited to two.
</p>
<p>We induce DSWE based on explicit data by per-
forming connective classification. The connective
classification task predicts which discourse con-
nective is suitable for combining two given argu-
ments. It is essentially similar to implicit rela-
tion recognition, just with different output labels.
Therefore, any existing neural network model for
implicit relation recognition can be easily used for
connective classification. We adapt the model in
(Wu et al., 2016) for connective classification be-
cause it is simple enough to enable us to train on
massive data. As illustrated in Figure 1, an ar-
gument is first represented as the average of dis-
tributed representations of words in it. On the con-
catenation of two arguments, multiple non-linear
hidden layers are then used to capture the inter-
actions between them. Finally, a softmax layer is
stacked for classification. We combine the cross-
entropy error and regularization error multiplied
</p>
<p>by the coefficient λ as the objective function. Dur-
ing training, we initialize distributed representa-
tions of all words randomly and tune them to min-
imize the objective function. The finally obtained
distributed representations of all words are our
discourse-specific word embeddings.
</p>
<p>Collecting explicit discourse data includes two
steps: 1) distinguish whether a connective occur-
ring reflects a discourse relation. For example, the
connective and can either function as a discourse
connective to join two Conjunction arguments, or
be just used to link two nouns in a phrase. 2) iden-
tify the positions of two arguments. According to
(Prasad et al., 2008), arg2 is defined as the argu-
ment following a connective, however, arg1 can
be located within the same sentence as the connec-
tive, in some previous or following sentence. Lin
et al. (2014) show that the accuracy of distinguish-
ing connectives is more than 97%, while identify-
ing arguments is below than 80%. Therefore, we
use the existing toolkit1 to find discourse connec-
tives, and just collect explicit instances using pat-
terns like [arg1 because arg2], where two argu-
ments are in the same sentence, to decrease noise.
We believe these simple patterns are enough when
using a very large corpus. Note that there are 100
discourse connectives in the PDTB, we ignore four
parallel connectives (e.g., if...then) for simplicity.
The way of collecting explicit data can be easily
generalized to other languages, one just need to
train a classifier to find discourse connectives fol-
lowing (Lin et al., 2014).
</p>
<p>Some aspects of this work are similar to (Bi-
ran and McKeown, 2013; Braud and Denis, 2016).
Based on massive explicit instances, they first
build a word-connective co-occurrence frequency
matrix2, and then weight these raw frequencies. In
this way, they represent words in the space of con-
nectives to directly encode their discourse func-
tion. The major limitation of their approach is that
the dimension of the word representations must
be less than or equal to the number of connec-
tives. By comparison, we learn DSWE by predict-
ing connectives conditioning on arguments, which
yields better performance and has no such dimen-
sion limitation. Some researchers use explicit data
as additional training data via multi-task learning
(Lan et al., 2013; Liu et al., 2016) or data selec-
tion (Rutherford and Xue, 2015; Wu et al., 2016).
</p>
<p>1https://github.com/linziheng/pdtb-parser.
2Biran and McKeown (2013) calculate co-occurrences be-
</p>
<p>tween word pairs and connectives.
</p>
<p>270</p>
<p />
</div>
<div class="page"><p />
<p>In both cases, explicit data are directly used to esti-
mate the parameters of implicit relation classifiers.
As a result, it is hard for them to incorporate mas-
sive explicit data because of the noise problem.
By contrast, we leverage massive explicit data by
learning word embeddings from them.
</p>
<p>3 Experiments
</p>
<p>3.1 Data and Settings
We collect explicit data from the Xin and Ltw parts
of the English Gigaword Corpus (3rd edition), and
get about 4.92M explicit instances. We randomly
sample 20,000 instances as the development set
and the others as the training set for DSWE. Af-
ter discarding words occurring less than 5 times,
the size of the vocabulary is 185,048. For the con-
nective classification task, we obtain an accuracy
of about 53% on the development set.
</p>
<p>We adapt the neural network model described in
Figure 1 as the classifier for implicit discourse re-
lation recognition (CDRR). Specifically, we con-
catenate some surface features with the last hid-
den layer as the input of the softmax layer to pre-
dict discourse relations. We choose 500 Produc-
tion rule (Lin et al., 2009) and 500 Brown Cluster
Pair (Rutherford and Xue, 2014) features based on
mutual information using the toolkit provided by
Peng et al. (2005). Our learned DSWE is used as
the pre-trained word embeddings for CDRR, and
fixed during training.
</p>
<p>Hyper-parameters for training DSWE and
CDRR are selected based on their corresponding
development set, and listed in Table 1.
</p>
<p>Hyper-parameter DSWE CDRR
wdim 300 300
hsizes [200] [200, 50]
lr 1.0 0.005
λ 0.0001 0.0001
</p>
<p>update SGD AdaGrad
f ReLU ReLU
</p>
<p>Table 1: Hyper-parameters for training DSWE and
CDRR. wdim means the dimension of word em-
beddings, hsizes the sizes of hidden layers, lr
the learning rate, λ the regularization coefficient,
update the parameter update strategy and f the
nonlinear function. Note that [200, 50] means that
CDRR uses two layers with the sizes of 200 and
50, respectively. And the learning rate for training
DSWE is decayed by a factor of 0.8 per epoch.
</p>
<p>Following Liu et al. (2016), we perform a 4-
way classification on the four top-level relations
in the PDTB: Temporal (Temp), Comparison
(Comp), Contingency (Cont) and Expansion
(Expa). The PDTB is split into the training set
(Sections 2-20), development set (Sections 0-1)
and test set (Sections 21-22). Table 2 lists the
statistics of these data sets. Due to the small and
uneven test data set, we run our method 10 times
with different random seeds (therefore different
initial parameters), and report the results (of a run)
which are closest to the average results. Finally,
we use both Accuracy and Macro F1 (macro-
averaged F1) to evaluate our method.
</p>
<p>Relation Train Dev Test
Temp 582 48 55
Comp 1855 189 145
Cont 3235 281 273
Expa 6673 638 538
</p>
<p>Table 2: Statistics of data sets on the PDTB.
</p>
<p>3.2 Results
</p>
<p>We compare our learned discourse-specific word
embeddings (DSWE) with two publicly available
embeddings3:
</p>
<p>1) GloVe4: trained on 6B words from Wikipedia
2014 and Gigaword 5 using the count based model
in (Pennington et al., 2014), with a vocabulary of
400K and a dimensionality of 300.
</p>
<p>2) word2vec5: trained on 100B words from
Google News using the CBOW model in (Mikolov
et al., 2013), with a vocabulary of 3M and a dimen-
sionality of 300.
</p>
<p>Results in Table 3 show that using DSWE gains
significant improvements (one-tailed t-test with
p&lt;0.05) over using GloVe or word2vec, on both
Accuracy and Macro F1. Furthermore, using
DSWE achieves better performance across all re-
lations on the F1 score, especially for minority re-
lations (Temp, Comp and Cont). Overall, our
DSWE can effectively incorporate discourse infor-
</p>
<p>3The reasons for using those publicly available word em-
beddings are: 1) They are both trained on massive data. 2)
It will be convenient for other people to reproduce our ex-
periments. 3) Using GloVe or word2vec word embeddings
trained on the same corpus as DSWE achieves worse perfor-
mance than using these two public ones.
</p>
<p>4http://nlp.stanford.edu/projects/glove/glove.6B.zip
5https://code.google.com/archive/p/word2vec/GoogleNews-
</p>
<p>vectors-negative300.bin.gz
</p>
<p>271</p>
<p />
</div>
<div class="page"><p />
<p>CDRR +GloVe +word2vec +DSWE
Temp P 36.00 27.03 31.58
</p>
<p>R 16.36 18.18 21.82
F1 22.50 21.74 25.81
</p>
<p>Comp P 53.97 50.00 43.00
R 23.45 20.00 29.66
F1 32.69 28.57 35.10
</p>
<p>Cont P 44.90 51.81 55.29
R 40.29 36.63 42.12
F1 42.47 42.92 47.82
</p>
<p>Expa P 60.47 60.72 63.91
R 76.21 81.60 79.00
F1 67.43 69.63 70.66
</p>
<p>Accuracy 55.68 57.17 58.85
Macro F1 41.27 40.71 44.84
</p>
<p>Table 3: Results of using different word embed-
dings. We also list the Precision, Recall and F1
score for each relation.
</p>
<p>mation in explicit data, and thus benefits implicit
discourse relation recognition.
</p>
<p>We also compare our method with three recent
systems which also use explicit data to boost the
performance:
</p>
<p>1) R&amp;X2015: Rutherford and Xue (2015) con-
struct weakly labeled data from explicit data based
on the chosen connectives, to enlarge the training
data directly.
</p>
<p>2) B&amp;D2016: Braud and Denis (2016) learn
connective-based word representations and build
a logistic regression model based on them6.
</p>
<p>3) Liu2016: Liu et al. (2016) use a multi-task
neural network to incorporate several discourse-
related data, including explicit data and the RST-
DT corpus (William and Thompson, 1988).
</p>
<p>System Accuracy Macro F1
R&amp;X2015 57.10 40.50
B&amp;D2016 52.81 42.27
Liu2016 57.27 44.98
</p>
<p>CDRR+DSWE 58.85 44.84
</p>
<p>Table 4: Comparison with recent systems.
</p>
<p>Results in Table 4 show the superiority of our
method. Although Liu2016 performs slightly bet-
ter on Macro F1, it uses the additional labeled
RST-DT corpus. For R&amp;X2015 and Liu2016, they
</p>
<p>6We carefully reproduce their model since they adopt a
different setting in preprocessing the PDTB.
</p>
<p>both incorporate relatively small explicit data be-
cause of the noise problem, for example, 20,000
and 40,000 instances respectively. By contrast,
our method benefits from about 4.9M explicit in-
stances. While B&amp;D2016 uses massive explicit
data, it is limited by the fact that the maximum
dimension of word representations is restricted to
the number of connectives, for example 96 in their
work. Overall, our method can effectively utilize
massive explicit data, and thus is more powerful
than baselines.
</p>
<p>not good
word2vec DSWE word2vec DSWE
</p>
<p>do no great great
did n’t bad lot
</p>
<p>anymore never terrific very
necessarily nothing decent better
anything neither nice success
anyway none excellent well
</p>
<p>does difficult fantastic happy
never nor better certainly
want refused solid respect
</p>
<p>neither impossible lousy fine
if limited wonderful import
</p>
<p>know declined terrible positive
anybody nobody Good help
</p>
<p>yet little tough useful
either denied best welcome
</p>
<p>Table 5: Top 15 closest words of not and good in
both word2vec and DSWE.
</p>
<p>To give an intuition of what information is en-
coded into the learned DSWE, we list in Table 5
the top 15 closest words of not and good, accord-
ing to the cosine similarity. We can find that, in
DSWE, words similar to not to some extent have
negative meanings. And since declined is similar
to not, a classifier may easily identify the implicit
instance [A network spokesman would not com-
ment. ABC Sports officials declined to be inter-
viewed.] as the Conjunction relation. For good in
DSWE, the similar words no longer include words
like bad. Furthermore, the similar score between
good and great is 0.54 while the score between
good and bad is just 0.33, which may make a clas-
sifier easier to distinguish word pairs (good, great)
and (good, bad), and thus is helpful for predicting
the Conjunction relation. This qualitative analysis
demonstrates the ability of our DSWE to capture
the discourse relationships between words.
</p>
<p>272</p>
<p />
</div>
<div class="page"><p />
<p>10 20 30 60 all
</p>
<p>Number of connectives
</p>
<p>0.42
</p>
<p>0.43
</p>
<p>0.44
</p>
<p>0.45
</p>
<p>0.46
M
a
cr
o
F
</p>
<p>1
</p>
<p>Figure 2: Impact of connectives used in training
DSWE.
</p>
<p>Finally, we conduct experiments to investigate
the impact of connectives used in training DSWE
on our results. Specifically, we use the explicit
discourse instances with the top 10, 20, 30, 60
most frequent or all connectives to learn DSWE,
accounting for 78.9%, 91.9%, 95.8%, 99.4% or
100% of total instances, respectively. The top
10 most frequent connectives are: and, but, also,
while, as, when, after, if, however and because,
which cover all four top-level relations defined in
the PDTB. As illustrated in Figure 2, with only the
top 10 connectives, the learned DSWE achieves
better performance than the common word em-
beddings. We observe a significant improvement
when using top 20 connectives, almost the best
performance with top 30 connectives, and no fur-
ther substantial improvement with more connec-
tives. These results indicate that we can use only
top n most frequent connectives to collect explicit
discourse data for DSWE, which is very conve-
nient for most languages.
</p>
<p>4 Conclusion
</p>
<p>In this paper, we learn discourse-specific word em-
beddings from massive explicit data for implicit
discourse relation recognition. Experiments on the
PDTB show that using the learned word embed-
dings as features can significantly boost the per-
formance. We also show that our method can
use explicit data more effectively than previous
work. Since most of neural network models for
implicit discourse relation recognition use pre-
trained word embeddings as input, we hope that
our learned word embeddings would benefit them.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank all the reviewers for their
constructive and helpful suggestions on this pa-
per. This work is partially supported by the Nat-
ural Science Foundation of China (Grant Nos.
61573294, 61672440, 61075058), the Ph.D. Pro-
grams Foundation of Ministry of Education of
China (Grant No. 20130121110040), the Foun-
dation of the State Language Commission of
China (Grant No. WT135-10), the Natural Sci-
ence Foundation of Fujian Province (Grant No.
2016J05161).
</p>
<p>References
Or Biran and Kathleen McKeown. 2013. Aggregated
</p>
<p>Word Pair Features for Implicit Discourse Relation
Disambiguation. In Proceedings of ACL. Sofia, Bul-
garia, pages 69–73.
</p>
<p>Chloé Braud and Pascal Denis. 2015. Comparing
Word Representations for Implicit Discourse Rela-
tion Classification. In Proceedings of EMNLP. Lis-
bon, Portugal, pages 2201–2211.
</p>
<p>Chloé Braud and Pascal Denis. 2016. Learning
Connective-based Word Representations for Implicit
Discourse Relation Identification. In Proceedings of
EMNLP. Austin, Texas, pages 203–213.
</p>
<p>Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, and
Xuanjing Huang. 2016. Implicit Discourse Relation
Detection via a Deep Architecture with Gated Rele-
vance Network. In Proceedings of ACL. Berlin, Ger-
many, pages 1726–1735.
</p>
<p>Yangfeng Ji and Jacob Eisenstein. 2015. One Vector is
Not Enough: Entity-Augmented Distributed Seman-
tics for Discourse Relations. Transactions of the As-
sociation for Computational Linguistics 3:329–344.
</p>
<p>Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging
Synthetic Discourse Data via Multi-task Learning
for Implicit Discourse Relation Recognition. In Pro-
ceedings of ACL. Sofia, Bulgaria, pages 476–485.
</p>
<p>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing Implicit Discourse Relations in the
Penn Discourse Treebank. In Proceedings of
EMNLP. PA, USA, pages 343–351.
</p>
<p>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled End-to-end Discourse Parser. Natural
Language Engineering 20(02):151–184.
</p>
<p>Yang Liu and Sujian Li. 2016. Recognizing Implicit
Discourse Relations via Repeated Reading: Neural
Networks with Multi-Level Attention. In Proceed-
ings of EMNLP. Austin, Texas, pages 1224–1233.
</p>
<p>273</p>
<p />
</div>
<div class="page"><p />
<p>Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang
Sui. 2016. Implicit Discourse Relation Classifica-
tion via Multi-Task Neural Networks. In Proceed-
ings of AAAI. Arizona, USA, pages 2750–2756.
</p>
<p>Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using Entity Features to Classify
Implicit Discourse Relations. In Proceedings of
SIGDIAL. PA, USA, pages 59–62.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. arXiv:1301.3781 [cs] .
</p>
<p>H. Peng, Fulmi Long, and C. Ding. 2005. Fea-
ture Selection Based on Mutual Information Cri-
teria of Max-dependency, Max-relevance, and
Min-redundancy. IEEE Transactions on PAMI
27(8):1226–1238.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global Vectors for Word
Representation. In Proceedings of EMNLP. Doha,
Qatar, pages 1532–1543.
</p>
<p>Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In Proceedings of ACL-IJCNLP.
PA, USA, pages 683–691.
</p>
<p>Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In Proceedings of ACL-IJCNLP. PA, USA,
pages 13–16.
</p>
<p>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of LREC. volume 24, pages 2961–
2968.
</p>
<p>Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016. A
Stacking Gated Neural Architecture for Implicit Dis-
course Relation Classification. In Proceedings of
EMNLP. Austin, Texas, pages 2263–2270.
</p>
<p>Attapol Rutherford and Nianwen Xue. 2014. Discov-
ering Implicit Discourse Relations Through Brown
Cluster Pair Representation and Coreference Pat-
terns. In Proceedings of EACL. Gothenburg, Swe-
den, pages 645–654.
</p>
<p>Attapol Rutherford and Nianwen Xue. 2015. Improv-
ing the Inference of Implicit Discourse Relations via
Classifying Explicit Discourse Connectives. In Pro-
ceedings of NAACL. Denver, Colorado, pages 799–
808.
</p>
<p>Mann William and Sandra Thompson. 1988. Rhetori-
cal Structure Theory: Towards a Functional Theory
of Text Organization. Text 8(3):243–281.
</p>
<p>Changxing Wu, Xiaodong Shi, Yidong Chen, Yanzhou
Huang, and Jinsong Su. 2016. Bilingually-
constrained Synthetic Data for Implicit Discourse
Relation Recognition. In Proceedings of EMNLP.
Austin, Texas, pages 2306–2312.
</p>
<p>Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, Hong
Duan, and Junfeng Yao. 2015. Shallow Convolu-
tional Neural Network for Implicit Discourse Rela-
tion Recognition. In Proceedings of EMNLP. Lis-
bon, Portugal, pages 2230–2235.
</p>
<p>274</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 275–280
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2043
</p>
<p>Oracle Summaries of Compressive Summarization
</p>
<p>Tsutomu Hirao and Masaaki Nishino and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
</p>
<p>{hirao.tsutomu,nishino.masaaki,nagata.masaaki}@lab.ntt.co.jp
</p>
<p>Abstract
</p>
<p>This paper derives an Integer Linear Pro-
gramming (ILP) formulation to obtain an
oracle summary of the compressive sum-
marization paradigm in terms of ROUGE.
The oracle summary is essential to re-
veal the upper bound performance of the
paradigm. Experimental results on the
DUC dataset showed that ROUGE scores
of compressive oracles are significantly
higher than those of extractive oracles
and state-of-the-art summarization sys-
tems. These results reveal that com-
pressive summarization is a promising
paradigm and encourage us to continue
with the research to produce informative
summaries.
</p>
<p>1 Introduction
</p>
<p>Compressive summarization, a joint model inte-
grating sentence extraction and sentence compres-
sion within a unified framework, has been attract-
ing attention in recent years (Martins and Smith,
2009; Berg-Kirkpatrick et al., 2011; Almeida and
Martins, 2013; Qian and Liu, 2013; Kikuchi et al.,
2014; Yao et al., 2015). Since compressive sum-
marization methods can use a sub-sentence as an
atomic unit, they can pack more information into
summaries than extractive methods, which em-
ploy sentences as atomic units. Thus, compres-
sive summarization is essential when we want to
produce summaries under tight length constraints.
There are two approaches to compress entire doc-
ument(s) to be grammatical; one is trimming the
phrase structure trees (Berg-Kirkpatrick et al.,
2011) and the other is trimming the dependency
trees obtained from the document(s) (Martins and
Smith, 2009; Almeida and Martins, 2013; Qian
and Liu, 2013; Kikuchi et al., 2014; Yao et al.,
</p>
<p>2015). This paper focuses on the latter approach
because recently it has been receiving much atten-
tion.
</p>
<p>To measure the performance of compressive
summarization methods, ROUGE (Lin, 2004),
an automatic evaluation metric, is widely used.
ROUGE evaluates a system summary by exploit-
ing a set of human-made reference summaries to
give a score in the range [0,1]. When n-gram
occurrences of the system summary agree with
those in a set of reference summaries, the value
is 1. However, system summaries cannot achieve
ROUGE=1 since summarization systems cannot
reproduce reference summaries in most cases. In
other words, the maximum ROUGE score that can
be achieved by compressive summarization is un-
clear. As a result, researchers cannot know how
much room for further improvement is left. Thus,
it is beneficial to reveal the upper bound summary
that achieves the maximum ROUGE score and can
be produced by the systems. The upper bound
summary is known as the oracle summary. To
obtain the oracle summary on extractive summa-
rization paradigms, several approaches have been
proposed. Sipos et al. (2012) utilized a greedy al-
gorithm, and Kubina et al. (2013) utilized exhaus-
tive search based on heuristics. However, their
oracle summaries do not always retain the opti-
mal (maximum) ROUGE score. Recently, Hirao
et al. (2017) derived an Integer Linear Program-
ming (ILP) formulation to obtain the optimal or-
acle summary. Their oracle summary can help
researchers to comprehend the strict limitation of
the extractive summarization paradigm. However,
their method cannot be applied to obtain compres-
sive oracle summaries.
</p>
<p>To reveal the ultimate limitation of the compres-
sive summarization paradigm, we propose an ILP
formulation to obtain a compressive oracle sum-
mary that maximizes the ROUGE score. We con-
</p>
<p>275</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2043">https://doi.org/10.18653/v1/P17-2043</a></div>
</div>
<div class="page"><p />
<p>ducted experimental evaluation on the Document
Understanding Conference (DUC) 2004 dataset.
The result demonstrated that ROUGE scores of
compressive oracle summaries completely outper-
formed those of extractive oracle summaries and
those of state-of-the-art summarization methods.
This indicates that compressive summarization is
a promising paradigm for leveraging research re-
sources.
</p>
<p>2 Definition of Compressive Oracle
Summaries
</p>
<p>Before defining compressive oracle summary, we
briefly describe ROUGEn. Given K reference
summaries R={R1, . . . , RK} and a system sum-
mary S. Let G={gn1 , . . . , gnM} be the set of all
n-grams appearing in reference summaries. Let
|G|=M . ROUGEn is defined as follows:
</p>
<p>ROUGEn(R, S) =
∑K
</p>
<p>k=1
</p>
<p>∑M
j=1min{N(gnj , Rk), N(gnj , S)}∑K
k=1
</p>
<p>∑M
j=1N(g
</p>
<p>n
j , Rk)
</p>
<p>(1)
</p>
<p>gnj represents the j-th n-gram appearing in refer-
ence summaries. N(gnj , Rk) and N(g
</p>
<p>n
j , S) are the
</p>
<p>number of occurrences of n-gram gnj in Rk and S,
respectively. Thus, compressive oracle summaries
are defined as follows:
</p>
<p>O =argmax
S⊆T
</p>
<p>ROUGEn(R, S)
</p>
<p>s.t. `(S) ≤ Lmax.
(2)
</p>
<p>T is the set of all valid word subsequences1 ob-
tained from sentences contained in the input docu-
ment(s), and Lmax is the length limitation of the
oracle summary. `(S) indicates the number of
words in the summary. Neither approximation nor
exact algorithms are known for solving this prob-
lem.
</p>
<p>3 ILP Formulation to Obtain the
Compressive Oracle Summary
</p>
<p>3.1 Dependency Structure of a Sentence
</p>
<p>In this paper, we follow the dependency tree trim-
ming approach proposed by Filippova et al. (2008;
2013). They proposed rules that transform a
tree that represents dependency relation between
</p>
<p>1Word subsequences can be regarded as grammatical sen-
tences. We regard rooted subtrees of dependency trees as
valid word subsequences. For details, see Section 3.1.
</p>
<p>words into a tree that represents dependency re-
lation between chunks (consisting of a word or
word sequence). Since we can trim their depen-
dency trees without loss of grammatical consis-
tency, Thus, we employ the trees in our compres-
sive summarization framework. Figure 1 shows
examples.
</p>
<p>3.2 ILP Formulation
</p>
<p>maximize
K∑
</p>
<p>k=1
</p>
<p>M∑
</p>
<p>j=1
</p>
<p>zk,j (3)
</p>
<p>s.t.
|D|∑
</p>
<p>i=1
</p>
<p>Ei∑
</p>
<p>u=1
</p>
<p>`i,ubi,u ≤ Lmax (4)
</p>
<p>∀j :
∑
</p>
<p>τ∈T (gnj )
mτ ≥ zk,j (5)
</p>
<p>∀j : N(gnj , Rk) ≥ zk,j (6)
∀i, u : bi,parent(i,u) ≥ bi,u (7)
</p>
<p>∀i, v, q ∈ Vi(wi,v) : bi,q ≥ mi,v (8)
∀i, v, p ∈ Ui(wi,v) : bi,p ≤ 1−mi,v (9)
</p>
<p>∀i, u : mi,v ∈ {0, 1} (10)
∀i, v : bi,u ∈ {0, 1} (11)
∀k, j : zk,j ∈ Z+. (12)
</p>
<p>Since the denominator of equation (1) is constant
for a given set of reference summaries, we can find
an oracle summary by maximizing the numerator
of equation (1). Equation (3) is the objective func-
tion that corresponds to maximization of the nu-
merator of equation (1). zk,j is the count of the
j-th n-gram that is contained in both the k-th refer-
ence summary and the oracle summary. Equation
(4) ensures that the length of the oracle summary
is less than Lmax. bi,u is a binary decision variable
indicating whether u-th chunk in i-th sentence is
contained in an oracle summary or not. `i,u indi-
cates the number of the words in u-th chunk in the
i-th sentence. D is a set of sentences and Ei is
the number of chunks in the i-th sentence. Equa-
tions (5) and (6) represent min operation in equa-
tion (1). wi,v is the v-th possible word sequence
whose length is n and that is contained in the i-
th sentence, and mi,v is a binary decision vari-
able indicating whether wi,v is contained in the
oracle summary or not. T (gnj ) is a set of tuples
consisting of indices (i, v) whose word sequence
corresponds to gnj , i.e., T (gnj )={(i, v)|wi,v=gnj }.
Thus, zk,j=min{N(gnj , Rk), N(gnj , S)}. Equa-
tion (7) ensures that an oracle summary consists
</p>
<p>276</p>
<p />
</div>
<div class="page"><p />
<p>w1,1: Most_dolphins, w1,2: Most_live, w1,3: Most_in, w1,4: dolphins_live, w1,5: dolphins_in, w1,6: live_in, 
w1,7: in_ervery, w1,8:every_ocean.
</p>
<p>w2,1: Some_dolphins, w2,2: dolphins_live, w2,3: dolphins_in, w2,4: dolphins_in, w2,5: live_in, w2,6: live_in, 
w2,7: rivers_in, w2,8: in_some, w2,9: some_regions.
</p>
<p>[Most]c1,1 [dolphins]c1,2 [live]c1,3 [in every ocean.]c1,4
</p>
<p>ROOT	
</p>
<p>S1:	
</p>
<p>[Some dolphins]c2,1 [live]c2,2 [in rivers]c2,3 [in some regions.]c2,4	
	
</p>
<p>ROOT	
</p>
<p>S2:	
</p>
<p>[Dolphins]c3,1 [usually]c3,2 [live]c3,3 [20-40 years]c3,4 [in the wild.]c3,5 	S3:	
</p>
<p>ROOT	
</p>
<p>w3,1: Dolphins_usually, w3,2: Dolphins_live, w3,3: Dolphins_20-40, w3,4: Dolphins_in, w3,5: usually_live 
w3,6:usually_20-40, w3,7: usually_in, w3,8: live_20-40, w3,9: live_in, w3,10: years_in, w3,11: in_the, w3,12: the_wild.
</p>
<p>Figure 1: Examples of trees that represent dependency relations between chunks, and word sequences
(whose length is 2). Chunks are enclosed in square brackets. Note that we disregard word sequences
that are generated by destroying the structure of chunks such as “live every” in S1, “dolphins in” in S2,
“live wild” in S3.
</p>
<p>of a set of rooted subtrees of the sentences in the
entire document(s). Function parent (i, u) returns
the index of the parent chunk of the u-th chunk in
the dependency tree obtained from the i-th sen-
tence. Equations (8) and (9) represent the de-
pendency relation between n-grams and chunks.
When we include wi,v in the oracle summary, we
have to include all chunks that contain the words
in wi,v. In addition, when the above chunks have
gap(s), we have to drop chunk(s) within the gap(s).
Here, Vi(wi,v) is a set of indices of chunks that
includes words in wi,v, and Ui(wi,v) is a set of
indices of chunks within the gap(s), defined as
{h|min(Vi(wi,v)) &lt; h &lt; max(Vi(wi,v))} and
h /∈ Vi(wi,v).
</p>
<p>We give an example to show how chunks
and word sequences are related. When we
pack a bigram “live in” in an oracle summary,
there are four candidates in the source document
(Fig. 1). Word subsequences, w1,6,w2,5,w2,6
and w3.9 match “live in”. Thus, T (live in) =
{(1, 6), (2, 5), (2, 6), (3, 9)}. Here, when we want
to pack w2,6 into the oracle summary, we have to
pack both chunks c2,2 and c2,4 (b2,2 = b2,4 = 1)
because U2(w2,6) = {2, 4}. Then, we have to
drop chunk c2,3(b2,3 = 0) because c2,3 is within
the gap between chunks c2,2 and c2,4 (V2(w2,6) =
3). Similarly, when we pack w3,9 into an oracle
summary, we have to pack both chunks c3,3 and
c3,5 and drop chunk c3,4. However, this compres-
</p>
<p>sion is not allowed since there is no dependency
relationship between c3,3 and c3,5.
</p>
<p>After solving the ILP problem, we can ob-
tain compressive oracle summaries by collecting
chunks according to bi,u=1.
</p>
<p>4 Experiments
</p>
<p>To investigate the potential limitation of the com-
pressive summarization paradigm, we compare
ROUGE scores of compressive oracle summaries
with those of extractive oracle summaries and
those obtained from state-of-the-art summariza-
tion systems. Extractive oracle summaries are ob-
tained by solving the ILP formulation proposed
by (Hirao et al., 2017). System summaries are
extracted from a public repository2 (Hong et al.,
2014).
</p>
<p>4.1 Settings
</p>
<p>We conducted experimental evaluation on the
DUC-2004 dataset for multiple document sum-
marization evaluation, a widely used benchmark
test set for generic multiple document summariza-
tion tasks. The dataset consists of 50 topics, each
of which contains 10 newspaper articles. To ob-
tain oracle summaries based on the ILP formula-
tion described in section 3.2, first, we applied the
Stanford parser (de Marneffe et al., 2006) to all
</p>
<p>2http://www.cis.upenn.edu/˜nlp/
corpora/sumrepo.html
</p>
<p>277</p>
<p />
</div>
<div class="page"><p />
<p>Method
Metric
</p>
<p>n=1 n=2 n=1+2 Sent.
n=1 42.6 13.1 24.1 5.34
</p>
<p>Ext. n=2 36.6 16.9 24.3 5.06
n=1+2 40.9 16.1 25.4 5.24
n=1 50.9 13.8 27.7 10.6
</p>
<p>Comp. n=2 40.9 21.3 28.6 7.82
n=1+2 47.9 19.7 30.3 8.48
</p>
<p>RegSum 33.1 10.2 18.8 4.9
ICSISumm 31.0 10.3 18.0 4.2
</p>
<p>Table 1: ROUGE scores and the number of sen-
tences of extractive and compressive oracle sum-
maries and those obtained from state-of-the-art
summarization systems, RegSum and ICSISumm.
n= 1 corresponds to ROUGE1, n=2 corresponds
to ROUGE2, n=1+2 corresponds to ROUGE-SU0.
“Sent.” indicates the average number of sentences
in the summaries.
</p>
<p>sentences in the dataset to obtain dependency re-
lations between words, and then we transformed
them into trees that represent the dependency re-
lations between chunks by applying Filippova’s
rules (Filippova and Strube, 2008; Filippova and
Altun, 2013). To solve the ILP problem, we uti-
lized CPLEX version 12.5.1.0.
</p>
<p>We obtained and evaluated oracle summaries
based on three variants of ROUGE, ROUGE1,
ROUGE2 and ROUGE-SU0, with the following
conditions3: (1) ROUGE1, utilizing unigrams ex-
cluding stopwords (2) ROUGE2, utilizing bigrams
with stopwords, and (3) ROUGE-SU0, which is an
extension of ROUGEn, utilizing unigram and bi-
gram (excluding skip-bigram) statistics.
</p>
<p>4.2 Results and Discussion
</p>
<p>Table 1 shows ROUGE scores of compressive and
extractive oracle summaries and those of RegSum
(Hong and Nenkova, 2014) that achieved the best
ROUGE1 and ICSISumm (Gillick and Favre, 2009;
Gillick et al., 2009) that achieved the best ROUGE2
on the DUC-2004 dataset, respectively.
</p>
<p>We compare ROUGE scores of compressive or-
acle summaries with extractive oracle summaries.
The best scores are obtained when we use the same
ROUGE variant for both computation and evalu-
ation (see bolded scores in Table 1). There are
large differences between the best scores of ex-
</p>
<p>3With stop words: options “-n 2 -s -m -x” are used. With-
out stop words: options “-n 2 -m -x” are used.
</p>
<p>Method Score
</p>
<p>Ext.
n=1 4.55
n=2 4.58
</p>
<p>Comp.
n=1 3.88
n=2 4.07
</p>
<p>Table 2: Readability evaluation by human subjects
</p>
<p>tractive method and compressive method. The dif-
ferences are 8.3 points, 4.4 points and 4.9 points
for ROUGE1, ROUGE2, ROUGE-SU0, respectively.
As one of the reasons for the above results, com-
pressive oracle summaries have a much larger
number of (sub-)sentences than extractive oracle
summaries for the same length limitation. This is
an advantage of compressive summarization over
extractive summarization.
</p>
<p>However, we have to note that compressive or-
acle summaries optimized to ROUGE1 may not
be desirable since they are produced by com-
pressing sentences by ignoring contexts. In fact,
they obtained remarkable gain for ROUGE1 score
(8.3 points), while they obtained modest gains in
ROUGE2 and ROUGE-SU0 (0.7 and 3.6 points, re-
spectively). This may suggest that the resultant
summaries overfit to the unigrams in the reference
summaries.
</p>
<p>We compare ROUGE scores of compressive or-
acle summaries with those of system summaries,
ROUGE scores of compressive oracle summaries
completely outperformed those of state-of-the-art
systems. The differences are in a range from 11 to
17 points.
</p>
<p>The results demonstrated that compressive sum-
marization is a promising approach to produce
more informative summaries, and room still exists
for further improvement. Thus, compressive sum-
marization is important research topic to leverage
our resources.
</p>
<p>4.3 Readability evaluation
</p>
<p>We conducted human evaluation to compare read-
ability of extractive oracle summaries to that of
compressive oracle summaries. We presented the
oracle summaries to five human subjects and asked
them to rate the summaries using an integer scale
from 1 (very poor) to 5 (very good). Table 2 shows
the results. Extractive oracle summaries achieved
near perfect scores. Although the scores of com-
pressive oracle summaries are inferior to those of
extractive oracle summaries, they achieved good
</p>
<p>278</p>
<p />
</div>
<div class="page"><p />
<p>Reference:
The Wye River accord has not been implemented. As the Israeli cabinet was considering the agreement, Islamic Jihad
militants exploded a car bomb in nearby Mahane Yehuda market. The cabinet suspended ratification of the agreement,
demanding the Palestinian Authority take steps against terrorism. Further, after the bombing, Israeli Prime Minister
Netanyahu announced the resumption of construction of a new settlement, Har Homa, in a traditionally Arab area east of
Jerusalem. Israel also demands that Arafat outlaw the military wings of Islamic Jihad and Hamas. The attack injured 24
Israelis, but only the two assailants, Sughayer and Tahayneh, were killed.
</p>
<p>Extractive oracle summary n = 1:
The procedure is part of the Wye River agreement negotiated last month. The radical group Islamic Jihad claimed
responsibility Saturday for the market bombing and vowed more attacks to try to block the new peace accord. Most recently,
Israel’s Cabinet put off a vote to ratify the accord after a suicide bombing Friday in Jerusalem that killed the two assailants and
injured 21 Israelis. David Bar-Illan, a top aide to Israeli Prime Minister Benjamin Netanyahu, said Sunday that Israel expects
Palestinian leader Yasser Arafat to formally outlaw the military wings of Islamic Jihad and the larger militant group Hamas.
</p>
<p>Compressive oracle summary n = 1:
The Israeli cabinet suspended ratification of the Wye agreement. A Prime Minister Benjamin Netanyahu said that Israel would
continue to build Jewish neighborhoods throughout Jerusalem including at a site in the Arab sector of the city. Netanyahu’s
Cabinet delayed action on the peace accord. The radical group Islamic Jihad claimed responsibility for the bombing and
vowed attacks. Implementation of the Israeli-Palestinian land-for-security accord was to have begun. David Bar-Illan said that
Israel expects Palestinian Yasser Arafat to outlaw the military wings of Islamic Jihad and the Hamas. Their car-bomb blew in a
Jerusalem market killing men and wounding 24 people.
</p>
<p>Extractive oracle summary n = 2:
In response to the attack, the Israeli cabinet suspended ratification of the Wye agreement until there “ is verification that the
Palestinian authority is indeed fighting terrorism.” The radical group Islamic Jihad claimed responsibility Saturday for the
market bombing and vowed more attacks to try to block the new peace accord. Most recently, Israel’s Cabinet put off a vote to
ratify the accord after a suicide bombing Friday in Jerusalem that killed the two assailants and injured 21 Israelis. Their
car-bomb blew apart two hours later in a Jerusalem market, killing both men and wounding 24 people. I’m going to Paradise. ”
</p>
<p>Compressive oracle summary n = 2:
The Israeli cabinet suspended ratification of the agreement. Hassan Asfour said the Palestinian Authority condemned the
attack. Two people were killed. The procedure is part of the Wye River agreement. The radical group Islamic Jihad claimed
responsibility for the bombing and vowed more attacks. Israel is demanding that the military wings of two radical Islamic
groups be outlawed. Implementation of the land-for-security accord was to have begun. Israel’s Cabinet put off a vote to ratify
the accord after a bombing in Jerusalem that killed the two assailants and injured 21 Israelis. Their car-bomb blew in a
Jerusalem market killing men.
</p>
<p>Figure 2: Summaries obtained from topic:D30010
</p>
<p>enough score, around 4. The results support that
our trimming approach based on chunk is effec-
tive. We show examples of oracle summaries in
Figure 2.
</p>
<p>5 Conclusion
</p>
<p>To reveal the ultimate limitations of the compres-
sive summarization paradigm, this paper proposed
an Integer Linear Programming (ILP) formula-
tion to obtain compressive oracle summaries in
terms of ROUGE. Evaluation results obtained from
the DUC 2004 dataset demonstrated that ROUGE
scores of compressive summaries are significantly
superior to those of extractive oracle summaries
and those of the state-of-the-art systems. These
results imply that the compressive summarization
paradigm is a promising direction to produce in-
formative summaries and encourage leveraging of
further resources for the research.
</p>
<p>References
Miguel B. Almeida and André F.T. Martins. 2013. Fast
</p>
<p>and robust compressive summarization with dual de-
composition and multi-task learning. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL). pages 196–206.
</p>
<p>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL). pages 481–
490.
</p>
<p>Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In In Proceedings of International Conference on
Language Resources and Evaluation (LREC). pages
449–454.
</p>
<p>Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compres-
sion. In Proc. of the 2013 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). pages 1481–1491.
</p>
<p>Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proc. of
</p>
<p>279</p>
<p />
</div>
<div class="page"><p />
<p>the 5th International Natural Language Generation
Conference (INLG). pages 25–32.
</p>
<p>Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proc. of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing. pages 10–18.
</p>
<p>Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
ICSI/UTD summarization system at TAC 2009. In
Proc. of the Text Analysis Conference (TAC).
</p>
<p>Tsutomu Hirao, Masaaki Nishino, Jun Suzuki, and
Masaaki Nagata. 2017. Enumeration of extractive
oracle summaries. In Proc. of the 15th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL). pages 386–396.
</p>
<p>Kai Hong, John Conroy, Benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A repository
of state of the art and competitive baseline sum-
maries for generic news summarization. In Proc.
of the Ninth International Conference on Language
Resources and Evaluation (LREC’14). pages 1608–
1616.
</p>
<p>Kai Hong and Ani Nenkova. 2014. Improving the
estimation of word importance for news multi-
document summarization. In Proc. of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL). pages 712–
721.
</p>
<p>Yuta Kikuchi, Tsutomu Hirao, Hiroya Takamura, Man-
abu Okumura, and Masaaki Nagata. 2014. Single
document summarization based on nested tree struc-
ture. In Proc. of the 52nd Annual Meeting of the
Association for Computational Linguistics (ACL).
pages 315–320.
</p>
<p>Jeff Kubina, John Conroy, and Judith Schlesinger.
2013. ACL 2013 multiling pilot overview. In Proc.
of the MultiLing 2013 Workshop on Multilingual
Multi-document Summarization. pages 29–38.
</p>
<p>Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proc. of Work-
shop on Text Summarization Branches Out. pages
74–81.
</p>
<p>Andre Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction
and compression. In Proc. of the Workshop on Inte-
ger Linear Programming for Natural Language Pro-
cessing. pages 1–9.
</p>
<p>Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proc. of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP). pages 1492–1502.
</p>
<p>Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin learning of submod-
ular summarization models. In Proc. of the 13th
</p>
<p>Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL). pages
224–233.
</p>
<p>Jin-ge Yao, Xiaojun Wan, and Jianguo Xiao. 2015.
Compressive document summarization via sparse
optimization. In Proc. of the 24th International
Joint Conference on Artificial Intelligence (IJCAI
2015). pages 1376–1382.
</p>
<p>280</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281–286
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2044
</p>
<p>Japanese Sentence Compression with a Large Training Dataset
</p>
<p>Shun Hasegawa
Tokyo Institute of Technology, Japan
</p>
<p>hasegawa.s@lr.pi.titech.ac.jp
</p>
<p>Hiroya Takamura
Tokyo Institute of Technology, Japan
takamura@pi.titech.ac.jp
</p>
<p>Yuta Kikuchi
Preferred Networks, Inc., Japan
kikuchi@preferred.jp
</p>
<p>Manabu Okumura
Tokyo Institute of Technology, Japan
</p>
<p>oku@pi.titech.ac.jp
</p>
<p>Abstract
</p>
<p>In English, high-quality sentence com-
pression models by deleting words have
been trained on automatically created
large training datasets. We work on
Japanese sentence compression by a sim-
ilar approach. To create a large Japanese
training dataset, a method of creating En-
glish training dataset is modified based
on the characteristics of the Japanese lan-
guage. The created dataset is used to
train Japanese sentence compression mod-
els based on the recurrent neural network.
</p>
<p>1 Introduction
</p>
<p>Sentence compression is the task of shorten-
ing a sentence while preserving its important
information and grammaticality. Robust sen-
tence compression systems are useful by them-
selves and also as a module in an extractive sum-
marization system (Berg-Kirkpatrick et al., 2011;
Thadani and McKeown, 2013). In this paper, we
work on Japanese sentence compression by delet-
ing words. One advantage of compression by
deleting words as opposed to abstractive compres-
sion lies in the small search space. Another one is
that the compressed sentence is more likely to be
free from incorrect information not mentioned in
the source sentence.
</p>
<p>There are many sentence compression mod-
els for Japanese (Harashima and Kurohashi,
2012; Hirao et al., 2009; Hori and Furui, 2004)
and for English (Knight and Marcu, 2000;
Turner and Charniak, 2005; Clarke and Lapata,
2006). In recent years, a high-quality En-
glish sentence compression model by deleting
words was trained on a large training dataset
(Filippova and Altun, 2013; Filippova et al.,
2015). While it is impractical to create a large
</p>
<p>training dataset by hand, one can be created auto-
matically from news articles (Filippova and Altun,
2013). The procedure is as follows (where S, H,
and C respectively denote the first sentence of an
article, the headline, and the created compressed
sentence of S). Firstly, to restrict the training
data to grammatical and informative sentences,
only news articles satisfying certain conditions
are used. Then, nouns, verbs, adjectives, and
adverbs (i.e., content words) shared by S and H
are identified by matching word lemmas, and a
rooted dependency subtree that contains all the
shared content words is regarded as C.
</p>
<p>However, their method is designed for English,
and cannot be applied to Japanese as it is. Thus,
in this study, their method is modified based on
the following three characteristics of the Japanese
language: (a) Abbreviation of nouns and nominal-
ization of verbs frequently occur in Japanese. (b)
Words that are not verbs can also be the root node
especially in headlines. (c) Subjects and objects
that can be easily estimated from the context are
often omitted.
</p>
<p>The created training dataset is used to train three
models. The first model is the original Filippova et
al.’s model, an encoder-decoder model with a long
short-term memory (LSTM), which we extend in
this paper to make the other two models that can
control the output length (Kikuchi et al., 2016),
because controlling the output length makes a
compressed sentence more informative under the
desired length.
</p>
<p>2 Creating training dataset for Japanese
</p>
<p>Filippova et al.’s method of creating training data
consists of the conditions imposed on news arti-
cles and the following three steps: (1) identifica-
tion of shared content words, (2) transformation of
a dependency tree, and (3) extraction of the min-
</p>
<p>281</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2044">https://doi.org/10.18653/v1/P17-2044</a></div>
</div>
<div class="page"><p />
<p>Figure 1: Dependency tree of S1. Squares and arrows respectively indicate chunks and dependency
relations. Bold arrows mean that chunks are merged into a single node. Bold squares are extracted as C.
</p>
<p>Figure 2: A sequence of chunks of H1
</p>
<p>imum rooted subtree. We modified their method
based on the characteristics of the Japanese lan-
guage as follows. To explain our method, a de-
pendency tree of S and a sequence of bunsetsu
chunks of H in Japanese are shown in Figures 1
and 2. Note that nodes of dependency trees in
Japanese are bunsetsu chunks each consisting of
content words followed by function words.
</p>
<p>2.1 Identification of shared content words
</p>
<p>Content words shared by S and H are identified by
matching lemmas and pronominal anaphora reso-
lution in Filippova et al.’s method. Abbreviation of
nouns and nominalization of verbs frequently oc-
cur in Japanese (characteristic (a) in Introduction),
and it is difficult to identify these transformations
simply by matching lemmas. Thus, after the iden-
tification by matching lemmas, two identification
methods (described below) using character-level
information are applied. Note that pronominal
anaphora resolution is not used, because pronouns
are often omitted in Japanese.
Abbreviation of nouns: There are two types of
abbreviations of nouns in Japanese. One is the
abbreviation of a proper noun, which shortens the
form by deleting characters (e.g., the pair with “+”
in Figures 1 and 2). The other is the abbreviation
of consecutive nouns, which deletes nouns that be-
have as adjectives (e.g., the pair with “-”). To deal
with such cases, if the character sequence of the
noun sequence in a chunk in H is identical to a
subsequence of characters composing the noun se-
quence in a chunk in S, the noun sequences in H
and S are regarded as shared.
Nominalization of verbs: Many verbs in
Japanese have corresponding nouns with similar
meanings (e.g., the pair with # in Figures 1 and
</p>
<p>1 Words in red are regarded as shared through lemma
matching, while words in blue (also underlined) are through
other ways.
</p>
<p>2). Such pairs often share the same Chinese
character, kanji. Kanji is an ideogram and is
more informative than the other types of Japanese
letters. Thus, if a noun in H and a verb in S2 share
one kanji, the noun and the verb are regarded as
shared.
</p>
<p>2.2 Transformation of a dependency tree
</p>
<p>Some edges in dependency trees cannot be cut
without changing the meaning or losing the gram-
maticality of the sentence. In Filippova et al.’s
method, the nodes linked by such an edge are
merged into a single node before extraction of the
subtree. The method is adapted to Japanese as fol-
lows. If the function word of a chunk is one of
the specific particles3, which often make obliga-
tory cases, the chunk and its modifiee are merged
into a single node. In Figure 1, the chunks at the
start and end of a bold arrow are merged into a
single node.
</p>
<p>2.3 Extraction of the minimum rooted
subtree
</p>
<p>In Filippova et al.’s method, the minimum rooted
subtree that contains all the shared content words
is extracted from the transformed tree. We modify
their method to take into account the characteris-
tics (a) and (b).
Deleting the global root: In English, only verbs
can be the root node of a subtree. However, in
Japanese, words with other parts-of-speech can
also be the root node in headlines (characteristics
(b)). Therefore, the global root, which is the root
node of S, and the chunks including a word that
can be located at the end of a sentence4, are the
candidates for the root node of a subtree. Then,
if the root node is not the global root, words suc-
ceeding the word that can be located at the end are
removed from the root node. In Figure 1, among
the two words with “*” that can be located at the
</p>
<p>2Candidates are restricted to verbs consisting of one kanji
and some following hiragana.
</p>
<p>3“を (wo)”, “に (ni)”, “が (ga)” and “は (ha)”
4An auxiliary verb, a noun, or a verb of which next word
</p>
<p>is not a noun, an auxiliary verb, or “の (no)”
</p>
<p>282</p>
<p />
</div>
<div class="page"><p />
<p>end, the latter is extracted as the root, and the suc-
ceeding word is removed from the chunk.
Reflecting abbreviated forms: Abbreviation of
nouns frequently occurs in Japanese (characteris-
tic (a)). Thus, in C, original forms are replaced
by their abbreviated forms obtained as explained
in Section 2.1 (e.g., the pair with “-” in Figures
1 and 2). However, we do not allow the head of
a chunk to be deleted to keep the grammaticality.
We also restrict here ourselves to word-level dele-
tion and do not allow character-level deletion, be-
cause our purpose is to construct a training dataset
for compression by word deletion. In the example
of Figure 1, chunks in bold squares are extracted
as C.
</p>
<p>2.4 Conditions imposed on news articles
</p>
<p>Filippova et al.’s method imposed eight conditions
on news articles to restrict the training data to
grammatical and informative sentences. In our
method, these conditions are modified to adapt to
Japanese. Firstly, the condition “S should include
content words of H in the same order” is removed,
because word order in Japanese is relatively free.
Secondly, the condition “S should include all con-
tent words of H” is relaxed to “the ratio of shared
content words to content words in H is larger than
threshold θ” because in Japanese, subjects and ob-
jects that can be easily estimated from the context
are often omitted (characteristics (c)). In addition,
two conditions “H should have a verb” and “H
must not begin with a verb” are removed, leaving
four conditions5.
</p>
<p>3 Sentence compression with LSTM
</p>
<p>Three models are used for sentence compres-
sion. Each model predicts a label sequence,
where the label for each word is either “retain”
or “delete” (Filippova et al., 2015). The first
is an encoder-decoder model with LSTM (lstm)
(Sutskever et al., 2014). Given an input sequence
x = (x1, x2, . . . , xn), yt in label sequence y =
(y1, y2, . . . , yn) is computed as follows:
</p>
<p>s0 = encoder(x)
</p>
<p>(st,mt) = decoder(st−1, mt−1, e1(xt)⊕el(yt−1))
yt = softmax(W ∗ st + b),
</p>
<p>5“H is not a question”, “both H and S are not less than
four words.”, “S is more than 1.5 times longer than H”, and
“S is more than 1.5 times longer than C”.
</p>
<p>threshold θ 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
size(10k) 202 189 172 154 134 106 81 59 35 27
vocab(k) 105 102 98 94 90 81 72 62 48 42
</p>
<p>ROUGE-2 54.6 54.4 54.0 54.3 55.3 54.0 55.1 54.4 53.0 53.3
</p>
<p>Table 1: Created training data with each θ.
ROUGE-2 is the score of compressed sentence
generated by the model trained on each training
data to target.
</p>
<p>where ⊕ indicates concatenation, st and mt are re-
spectively a hidden state and a memory cell at time
t, and e1(word) is embedding of word. If label is
“retain”, el(label) is (1; 0); otherwise, (0; 1). m0
is a zero vector.
</p>
<p>As the second and the third models, we
extend the first model to control the output
length (Kikuchi et al., 2016). The second model,
lstm+leninit, initializes the memory cell of the de-
coder as follows: m0 = tarlen∗blen where tarlen
is the desired output length, and blen is a train-
able parameter. The third model, lstm+lenemb,
uses the embedding of the potential desired length
e2(length) as an additional input. In this case,
e1(xt) ⊕ el(yt−1) ⊕ e2(lt) is used as the input of
the decoder where lt is the potential desired length
at time t.
</p>
<p>4 Experiments
</p>
<p>The created training datasets were used to train
three models for sentence compression. To see the
effect of the modified subtree extraction method
(Section 2.3), two training datasets were tested:
rooted and multi-root+. rooted includes only dele-
tion of the leafs in a dependency tree. In contrast,
multi-root+ includes deleting the global root and
reflecting abbreviated forms besides it.
Setting: Training datasets were created from
seven million, 35-years’ worth of news articles
from the Mainichi, Nikkei, and Yomiuri news-
papers, from which duplicate sentences and sen-
tences in test data were filtered out. Gold-standard
data were composed of the first sentences of the
1,000 news articles from Mainichi, each of which
has 5 compressed sentences separately created
by five human annotators. 100 sentences of the
gold-standard data were used as development data,
while the other sentences were used as test data.
The three models were trained on datasets created
with each value of threshold θ6, which is the pa-
rameter used in the condition (introduced in Sec-
</p>
<p>6In Table 2, the thresholds for our models are 0.7, 0.5, 0.3,
0.6, 0.4, and 0.2, respectively, from the top.
</p>
<p>283</p>
<p />
</div>
<div class="page"><p />
<p>dataset model R-1 ROUGE-2 R-L CRR(R-2) P F
prop-w-dpnd 73.8 56.5 51.5 53.7 32.7 60.5
tree-base 68.6 52.6 50.4 51.4 34.4 59.1
</p>
<p>rooted lstm 66.3 51.5 59.3 54.6 36.1 51.8
rooted lstm+leninit 70.8 55.2 57.1 55.9 36.0 56.8
rooted lstm+lenemb 71.0 55.2 56.7 55.7 35.9 57.8
multi-root+ lstm 60.8 50.6 60.5 54.1 33.7 47.9
multi-root+ lstm+leninit 71.7 56.0 57.8 56.7 36.3 57.8
multi-root+ lstm+lenemb 72.6 56.2 56.4 56.1 35.8 59.4
</p>
<p>Table 2: Automatic evaluation. R-1, R-2, and R-L are ROUGE-1, ROUGE-2, and ROUGE-L score. R,
P and F are recall, precision and F-measure.
</p>
<p>tion 2.4). The properties of the dataset in relation
to θ are shown in Table 1. θ is tuned for ROUGE-
2 score on the development data. In Table 1, we
show the tendency of the ROUGE-2 scores when
lstm+leninit was trained on created rooted with
each θ. From Table 1, we chose 0.5 as θ. All
models are three stacked LSTM layers7. Words
with frequency lower than five are regarded as un-
known words. ADAM8 was used as the optimiza-
tion method. The desired length was set to the
bytes of a compressed sentence randomly chosen
from the five human-generated sentences. In the
test step, beam-search was used (beam size: 20)
and candidates exceeding the desired length were
truncated.
tree-base and prop-w-dpnd: Existing methods
for Japanese sentence compression are not based
on the training on a large dataset. Therefore, the
proposed method is compared with two methods,
tree-base, (Filippova and Strube, 2008) and prop-
w-dpnd (Harashima and Kurohashi, 2012), which
are not based on supervised learning. tree-base
is implemented as an integer linear programming
problem that finds a subtree of a dependency tree.
prop-w-dpnd is also implemented as an integer lin-
ear programming problem, but modified based on
characteristics of the Japanese language. prop-w-
dpnd allows the deletion inside the chunks.
</p>
<p>4.1 Automatic evaluation
</p>
<p>Table 2 shows the ROUGE score of each model.
We used ROUGE-1, ROUGE-2, and ROUGE-
L (R-1, R-2, and R-L) for evaluation. In addi-
tion, we also show the precision and F-measure
of ROUGE-2 because the output length of each
model is not same. Compression ratio (CR) is
the ratio of the bytes of the compressed sentence
to the bytes of the source sentence. Average
</p>
<p>7Dimension of word embedding and hidden layer:256,
dropout:20%
</p>
<p>8α:0.001, β1:0.9, β2:0.999, eps:1e-8, batchsize:180
</p>
<p>CR of the test data is approximately 61.0%. We
think we should focus on F-measure of ROUGE-
2 because of the different CR of the models.
The models trained on a large training dataset
achieved higher F-measure than the unsupervised
models. Moreover, F-measure of lstm+leninit and
lstm+lenemb, which were trained on either multi-
root+ or rooted, is significantly better than prop-
w-dpnd (p &lt; 0.001). lstm achieved lower R-1 and
R-2 than the other models trained on a large train-
ing dataset, probably because it tends to gener-
ate too short sentences, as indicated by low CR.
It is also noteworthy that CRs of lstm+leninit
and lstm+lenemb are mostly closer to the aver-
age CR of the test data than lstm. Furthermore,
lstm+leninit and lstm+lenemb trained on multi-
root+ instead of rooted worked better in terms
of F-measure. We consider it is because vari-
ous types of deletion make the compression model
more flexible, as indicated by closer CR of the
model trained on multi-root+ instead of rooted to
the average CR of the test data.
</p>
<p>4.2 Human evaluation
The difference between lstm+leninit and
lstm+lenemb trained on multi-root+ was in-
vestigated first. With lstm+leninit, 2 out of 100
sentences, chosen randomly, ended with a word
that cannot be located at the end of a sentence. In
contrast, with lstm+lenemb, 24 sentences ended
with such words and therefore are ungrammatical,
although lenemb has shown to be effective in ab-
stractive sentence summarization (Kikuchi et al.,
2016). This result suggests that lstm+lenemb is
excessively affected by the desired length because
lenemb receives the potential desired length at
each time of decoding. In fact, 21 out of the 24
sentences are as long as the desired length.
</p>
<p>Then, lstm+leninit trained on multi-root+ was
evaluated by crowdsourcing in comparison with
the gold-standard and tree-base. Each crowd-
</p>
<p>284</p>
<p />
</div>
<div class="page"><p />
<p>model read info
gold-standard 4.22 3.76
lstm+leninit 3.99 3.09
prop-w-dpnd 3.55 2.81
</p>
<p>Table 3: Human absolute evaluation
</p>
<p>model read info
lstm+leninit 78 108
lstm 47 47
tie 125 95
</p>
<p>training data read info
multi-root+ 55 85
rooted 57 33
tie 138 132
</p>
<p>Table 4: Human relative evaluation
</p>
<p>sourcing worker reads each source sentence and
a set of the compressed sentences, reordered ran-
domly, and gives a score from 1 to 5 (where 5
is best) to each compressed sentence in terms of
informativeness (info) and readability (read). As
shown in Table 3, in terms of both read and info,
lstm+leninit archived higher scores than prop-w-
dpnd, and the difference is significant (p &lt; 0.001).
</p>
<p>Next, lstm+leninit was compared with lstm
(both are trained on multi-root+), and multi-root+
was compared with rooted (lstm+leninit was used
as the model), by human relative evaluation. In
this evaluation, each worker votes for one of
lstm+leninit, lstm, or tie, and for one of multi-
root+, rooted, or tie. 250 votes in total were re-
ceived (50 sentences × 5 votes). The results are
shown in Table 4. lstm+leninit is better than lstm
in terms of read, which is not directly related to
the output length, as well as of info. It is also clear
that multi-root+ achieves much higher info with a
negligible reduction in read than rooted.
</p>
<p>5 Conclusion
</p>
<p>Filippova et al.’s method of creating training data
for English sentence compression was modified
to create a training dataset for Japanese sentence
compression. The effectiveness of the created
Japanese training dataset was verified by auto-
matic and human evaluation. Our method can re-
fine the created dataset by giving more flexibil-
ity in compression and achieved better informa-
tiveness with negligible reduction in readability.
Furthermore, it has been shown that controlling
the output length improves the performance of the
sentence compression models.
</p>
<p>Acknowledgment
</p>
<p>This work was supported by Google Re-
search Award, JSPS KAKENHI Grant Number
JP26280080, and JPMJPR1655.
</p>
<p>References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
</p>
<p>2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1. pages 481–490.
</p>
<p>James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computa-
tional Linguistics. pages 377–384.
</p>
<p>Katja Filippova, Enrique Alfonseca, Carlos A. Col-
menares, Lukasz Kaiser, and Oriol Vinyals. 2015.
Sentence compression by deletion with lstms. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. pages
360–368.
</p>
<p>Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1481–1491.
</p>
<p>Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference. pages 25–32.
</p>
<p>Jun Harashima and Sadao Kurohashi. 2012. Flexible
Japanese sentence compression by relaxing unit con-
straints. In Proceedings of COLING 2012. pages
1097–1112.
</p>
<p>Tsutomu Hirao, Jun Suzuki, and Hideki Isozaki. 2009.
A syntax-free approach to japanese sentence com-
pression. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP. pages 826–833.
</p>
<p>Chiori Hori and Sadaoki Furui. 2004. Speech summa-
rization : An approach through word extraction and
a method for evaluation (¡special section¿the 2002
ieice excellent paper award). IEICE transactions on
information and systems pages 15–25.
</p>
<p>Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hi-
roya Takamura, and Manabu Okumura. 2016. Con-
trolling output length in neural encoder-decoders.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1328–1338.
</p>
<p>Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence and Twelfth
Conference on Innovative Applications of Artificial
Intelligence. pages 703–710.
</p>
<p>285</p>
<p />
</div>
<div class="page"><p />
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems. pages 3104–3112.
</p>
<p>Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of the Seventeenth Conference on Com-
putational Natural Language Learning. pages 65–
74.
</p>
<p>Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
pages 290–297.
</p>
<p>286</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 287–292
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2045
</p>
<p>A Neural Architecture for Generating Natural Language Descriptions
from Source Code Changes
</p>
<p>Pablo Loyola, Edison Marrese-Taylor and Yutaka Matsuo
Graduate School of Engineering
</p>
<p>The University of Tokyo
Tokyo, Japan
</p>
<p>{pablo,emarrese,matsuo}@weblab.t.u-tokyo.ac.jp
</p>
<p>Abstract
</p>
<p>We propose a model to automatically de-
scribe changes introduced in the source
code of a program using natural language.
Our method receives as input a set of code
commits, which contains both the modifi-
cations and message introduced by an user.
These two modalities are used to train an
encoder-decoder architecture. We evalu-
ated our approach on twelve real world
open source projects from four different
programming languages. Quantitative and
qualitative results showed that the pro-
posed approach can generate feasible and
semantically sound descriptions not only
in standard in-project settings, but also in
a cross-project setting.
</p>
<p>1 Introduction
</p>
<p>Source code, while conceived as a set of structured
and sequential instructions, inherently reflects hu-
man intent: it encodes the way we command a ma-
chine to perform a task. In that sense, it is expected
that it follows to some extent the same distribu-
tional regularities that a proper natural language
manifests (Hindle et al., 2012). Moreover, the un-
ambiguous nature of source code, comprised in
plain and human-readable format, allows an indi-
rect way of communication between developers, a
phenomenon boosted in recent years given the cur-
rent software development paradigm, where bil-
lions of lines code are written in a distributed and
asynchronous way (Gousios et al., 2014).
</p>
<p>The scale and complexity of software systems
these days has naturally led to explore automated
ways to support developers’ code comprehension
(Letovsky, 1987) from a linguistic perspective.
One of these attempts is automatic summarization,
which aims to generate a compact representation
</p>
<p>of the source code in a portion of natural language
(Haiduc et al., 2010).
</p>
<p>While existing code summarization methods are
able to provide relevant insights about the pur-
pose and functional features of the code, their
scope is inherently static. In contrast, software
development can be seen as a sequence of incre-
mental changes, intended to either generate a new
functionality or to repair an existing one. Source
code changes are critical for understanding pro-
gram evolution, which motivated us to explore if
it is possible to extend the notion of summariza-
tion to encode code changes into natural language
representations, i.e., develop a model able to ex-
plain a source code level modification. With this,
we envision a tool for developers that is able to
i) ease the comprehension of the dynamics of the
system, which could be useful for debugging and
repairing purposes and ii) automate the documen-
tation of source code changes.
</p>
<p>To this end, we rely on the concept of code com-
mit, the standard contribution procedure imple-
mented in modern subversion systems (Gousios
et al., 2014), which provides both the actual
change and a short explanatory paragraph. Our
model consists of an encoder-decoder architec-
ture which is trained on a set of triples conformed
by the version of a system before and after the
change, along with the comment. Given the high
heterogeneity of the modalities involved, we rely
on an attention mechanism to efficiently learn the
parts of the sequences that are more expressive and
have more explanatory power.
</p>
<p>We performed an empirical study on twelve
real world software systems, from which we ob-
tained the commit activity to evaluate our model.
Our experiments explored in-project and cross-
project scenarios, and our results showed that the
proposed model is able to generate semantically
sound descriptions.
</p>
<p>287</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2045">https://doi.org/10.18653/v1/P17-2045</a></div>
</div>
<div class="page"><p />
<p>2 Related Work
</p>
<p>The use natural language processing to support
software engineering tasks has increased consis-
tently over the years, mainly in terms of source
code search, traceability and program feature lo-
cation (Panichella et al., 2013; Asuncion et al.,
2010).
</p>
<p>The emergence of unifying paradigms that ex-
plicitly relate programming and natural languages
in distributional terms (Hindle et al., 2012) and
the availability of large corpus mainly from open
source software opened the door for the use of lan-
guage modeling for several tasks (Raychev et al.,
2015). Examples of this are approaches for learn-
ing program representations (Mou et al., 2016),
bug localization (Huo et al.), API suggestion (Gu
et al., 2016) and code completion (Raychev et al.,
2014).
</p>
<p>Source code summarization has received spe-
cial attention, ranging from the use of information
retrieval techniques to the addition of physiologi-
cal features such as eye tracking (Rodeghero et al.,
2014). In recent years several representation learn-
ing approaches have been proposed, such as (Al-
lamanis et al., 2016), where the authors employ a
convolutional architecture embedded inside an at-
tention mechanism to learn an efficient mapping
between source code tokens and natural language
keywords.
</p>
<p>More recently, (Iyer et al., 2016) proposed a
encoder-decoder model that learns to summarize
from Stackoverflow data, which contains snippet
of code along with descriptions. Both approaches
share the use of attention mechanisms (Bahdanau
et al., 2014) to overcome the natural disparity be-
tween the modalities when finding relevant token
alignments. Although we also use an attention
mechanism, we differ from them in the sense we
are targeting the changes in the code rather than
the description of a file.
</p>
<p>In terms of specifically working on code change
summarization, Cortés-Coy et al. (2014); Linares-
Vásquez et al. (2015) propose a method based on
a set of rules that considers the type and impact of
the changes, and (Buse and Weimer, 2010) com-
bines summarization with symbolic execution. To
the best of our knowledge, our approach represents
the first attempt to generate natural language de-
scriptions from code changes without the use of
hand-crafted features, a desirable setting given the
heterogeneity of the data involved.
</p>
<p>3 Proposed Model
</p>
<p>Our model assumes the existence of T versions of
a given project {v1, . . . , vT }. Given a pair of con-
secutive versions (vt−1, vt), we define the tuple
(Ct, Nt), where Ct = ∆tt−1(v) represents a code
snippet associated to changes over v in time t and
Nt represents its corresponding natural language
(NL) description. Let C be the set of all source
code snippets and N be the set of all descriptions
in NL. We consider a training corpus with T code
snippets and summary pairs (Ct, Nt), 1 ≤ t ≤ T ,
Ct ∈ C , Nt ∈ N. Then, for a given code snippet
Ck ∈ C, the goal of our model is to produce the
most likely NL description N?.
</p>
<p>Concretely, similarly to (Iyer et al., 2016), we
use an attention-augmented encoder-decoder ar-
chitecture. The encoder can be seen as a lookup
layer, which simply reads through the source input
sequence and returns the embedded tokens. The
decoder is a RNN that reads this representation
and generates NL words one at a time based on
its current hidden state and guided by a global at-
tention model (Luong et al., 2015). We model the
probability of a description as a product of the con-
ditional next-word probabilities. More formally,
for each NL token ni ∈ Nt we define,
</p>
<p>hi = f(ni−1E, hi−1) (1)
</p>
<p>p(ni|n1, ..., ni−1) ∝W tanh(W1hi +W2ai) (2)
</p>
<p>where E is the embedding matrix for NL to-
kens, ∝ denotes a softmax operation, hi repre-
sents the hidden state and ai is the contribution
from the attention model on the source code. W ,
W1 and W2 are trainable combination matrices.
The decoder repeats the recurrence until a fixed
number of words or a special END token is gen-
erated. The attention contribution ai is defined as
ai =
</p>
<p>∑k
j=1 αi,j · cjF , where cj ∈ Ct is a source
</p>
<p>code token, F is the source code token embedding
matrix and αi,j is:
</p>
<p>αi,j =
exp (h&gt;i cjF )∑
</p>
<p>cj∈Ct exp (h
&gt;
i cjF )
</p>
<p>(3)
</p>
<p>We use a dropout-regularized LSTM cell for
the decoder (Zaremba et al., 2015) and also add
dropout at the NL embeddings and at the output
softmax layer, to prevent over-fitting. We added
special START and END tokens to our training se-
quences and replaced all tokens and output words
occurring less than 2 and 3 times, respectively,
</p>
<p>288</p>
<p />
</div>
<div class="page"><p />
<p>with a special UNK token. We set the maximum
code and NL length to be 100 tokens. For decod-
ing, we approximate N? by performing a beam
search on the space of all possible summaries us-
ing the model output, with a beam size of 10 and a
maximum summary length of 20 words.
</p>
<p>To evaluate the quality of our generated descrip-
tions we use both METEOR (Lavie and Agarwal,
2007) and sentence level BLEU-4 (Papineni et al.,
2002). Since the training objective does not di-
rectly optimize for these scores, we compute ME-
TEOR on our validation set after every epoch and
save the intermediate model that gives the maxi-
mum score as the final model. For evaluation on
our test set we used the BLEU-4 score.
</p>
<p>4 Empirical Study
</p>
<p>Data and pre-processing: We captured histori-
cal data from twelve open source projects hosted
on Github based on their popularity and maturity,
selecting 3 projects for each of the following lan-
guages: python, java, javascript and c++. For
each project, we downloaded diff files and meta-
data of the full commit history. Diff files encode
per-line differences between two files or sets of
files in a standard format, allowing us to recover
source code changes in each commit at the line
level. On the other hand, medatada allows us to
recover information such as the author and mes-
sage of each commit.
</p>
<p>The extracted commit messages were processed
using the Penn Treebank tokenizer (Marcus et al.,
1993), which nicely deals with punctuation and
other text marks. To obtain a source code repre-
sentation of each commit, we parsed the diff files
and used a lexer (Brandl, 2016) to tokenize their
contents in a per-line fashion allowing us to max-
imize the amount of source code recovered from
the diff files. Data and source code available1.
</p>
<p>Experimental Setup: Given the flat structure
of the diff file, source code in contiguous lines
might not necessarily correspond to originally
neighboring code lines. Moreover, they might
come from different files in the project. To deal
with this issue, we first worked only with those
commits that modify a single file in the project;
we call this the atomicity assumption. By using
only atomic commits we reduced our training data
by an average of roughly 50%, but in exchange we
made sure all the extracted code lines came from
</p>
<p>1http://github.com/epochx/commitgen
</p>
<p>Language Project Full Atomic Added Rem.
</p>
<p>python
Theano 24,200 65.40% 11.43% 2,83%
keras 2,855 66.02% 11.07% 3,01%
</p>
<p>youtube-dl 13,968 74.49% 11.52% 2,59%
</p>
<p>javascript
node 15,811 53.17% 11.87% 3,21%
</p>
<p>angular 6,204 32.90% 5.59% 1,72%
react 7,806 53.29% 12.67% 2,72%
</p>
<p>c++
opencv 20,480 50.08% 8.83% 1,66%
CNTK 10,792 38.36% 6.00% 2,23%
bitcoin 12,596 48.11% 9.84% 2,56%
</p>
<p>java
CoreNLP 9,149 42.77% 7.84% 1,98%
</p>
<p>elasticsearch 25,764 43.77% 9.02% 2,61%
guava 3,821 38.63% 8.90% 2,64%
</p>
<p>Average 12,787 50.58% 9.55% 2,48%
</p>
<p>Table 1: Summary of our collected data.
</p>
<p>the same file. At the same time, we expect to max-
imize the likelihood of observing a direct relation
between the commit message and the lines altered.
</p>
<p>We then relaxed our atomicity assumption and
experimented with the full commit history. Given
our maximum sequence length constrain of 100 to-
kens, we only observed an average of 1,97% extra
data on each project. Since source code lines may
come from different files, we added a delimiting
token NEW FILE when corresponding.
</p>
<p>We were also interested in studying the per-
formance of the model in a cross-project setting.
Given the additional challenges that this involves,
we designed a more controlled experiment. Start-
ing from the atomic dataset, we selected commits
that only add or only remove code lines, conform-
ing a derived dataset that we call uni-action. We
chose the python language to maximize the avail-
able data. See Table 1.
</p>
<p>Results and Discussion: We begin by training
our model on the atomic dataset. As baseline we
used MOSES (Koehn et al., 2007) which although
is designed as a phrase-based machine translation
system, was previously used by Iyer et al. (2016)
to generate text from source code. Concretely, we
treated the tokenized code snippet as the source
language and the NL description as the target. We
trained a 3-gram language model using KenLM
(Heafield et al., 2013) and used mGiza to obtain
alignments. For validation, we use minimum error
rate training (Bertoldi et al., 2009; Och, 2003) in
our validation set.
</p>
<p>As Table 3 shows, our model trained on atomic
data outperforms the baseline in all but one project
with an average gain of 5 BLEU points. In par-
ticular, we observe bigger gains for java projects
such as CoreNLP and guava. We hypothesize this
is because program differences in Java tend to be
longer than the rest. While this impacts on train-
ing time, at the same time it allows the model to
</p>
<p>289</p>
<p />
</div>
<div class="page"><p />
<p>work with a larger vocabulary space. On the other
hand, our model performs similarly to MOSES for
the node and slightly worse for the youtube-dl. A
detailed inspection of the NL messages for node
showed that many of them exhibit a fixed pattern
in their structure. We believe this rigidity restrains
the generation capabilities of the decoder, making
it more prone to memorization.
</p>
<p>Table 2 shows examples of generated descrip-
tions for real changes and their references. Re-
sults suggest that our model is able to generate
semantically sound descriptions for the changes.
We can also visualize the summarizing power of
the model, as seen in the Theano and bitcoin ex-
amples. We observe a tendency to choose more
general terms over too specific ones meanwhile
also avoiding irrelevant words such as numbers
or names. Results also suggest the emergence of
rephrasing capabilities, specifically in the second
example from Theano. Finally, our generated de-
scriptions are, in most cases, semantically well
correlated to the reference descriptions. We also
report not so successful results, such as case of
youtube-dl, where we can see signs of memoriza-
tion on the generated descriptions.
</p>
<p>Regarding the cross-project setting experiments
on python, we obtained BLEU scores of 14.6 and
18.9 for only-adding and only-removing instances
in the uni-action dataset, respectively. We also ob-
tained validation accuracies up to 43.94%, sug-
gesting feasibility in this more challenging sce-
nario. Moreover, as the generated descriptions
from the keras project in Table 2 show, the model
is still able to generate semantically sound descrip-
tions.
</p>
<p>Figure 1: Heatmaps of attention weights αi,j .
</p>
<p>Despite the small data increase, we also trained
our model on full datasets as a way to confirm
the generative power of our model. In particular,
we wanted to test the model is able leverage on
atomic data to also capture and compress multi-
file changes. As shown in Table 3, results in terms
of BLEU and validation accuracy manifest reason-
able consistency, despite the higher disparity be-
</p>
<p>Reference Generated
</p>
<p>ke
ra
</p>
<p>s Fix image resizing in
preprocessing/image Fixed image preprocessing .
</p>
<p>Fix test flakes Fix flaky test
</p>
<p>T
he
</p>
<p>an
o fix crash in the new warning
</p>
<p>message . Better warning message .
</p>
<p>remove var not used . remove not used code .
Better error msg better error message .
</p>
<p>bi
tc
</p>
<p>oi
n
</p>
<p>Merge pull request 4486
45abeb2 Update Debian
</p>
<p>packaging description for new
bitcoin-cli ( Johnathan Corgan )
</p>
<p>Update Debian packaging
description for new
</p>
<p>bitcoin-cli
</p>
<p>Add two unittest-related files to
.gitignore
</p>
<p>Add : Minor files to
.gitignore
</p>
<p>C
or
</p>
<p>eN
L
</p>
<p>P
</p>
<p>Add a bunch of verbs which are
more likely to be xcomp than
</p>
<p>vmod
</p>
<p>Add a bunch of verbs which
are more to be xcomp than
</p>
<p>vmod
Add a brief test for optional
</p>
<p>nodes make this test do something
</p>
<p>yo
ut
</p>
<p>ub
e-
</p>
<p>dl [ crunchyroll ] Fix uploader andupload date extraction
[ crunchyroll ] Fix uploader
</p>
<p>extraction
[ extractor/common ] Improve
</p>
<p>base url construction
[ extractor/common ]
Improve extraction
</p>
<p>[ mixcloud ] Use
unicode literals
</p>
<p>[ common ] Use
unicode literals
</p>
<p>op
en
</p>
<p>cv fixed gcc compilation fixed compile under linux
remove unused variables in
</p>
<p>OCL PERF TEST P ( )
remove unused variable in
</p>
<p>the module
</p>
<p>Table 2: Examples of generated natural language
passages v/s original ones taken from the test set.
</p>
<p>tween source code and natural language on this
dataset, which means the model was able to learn
representations with more compressive power.
</p>
<p>Soft alignments derived from Figure 1, which
shows examples of attention heatmaps, illustrate
how the model effectively associates source code
tokens with meaningful words.
</p>
<p>Dataset atomic fullVal. acc BLEU Moses Val. acc BLEU
Theano 36.81% 9.5 7.1 39.88% 10.9
keras 45.76% 13.7 7.8 59.30% 8.8
</p>
<p>youtube-dl 50.84% 16.4 17.5 53.65% 17.7
node 52.46% 7.8 7.7 53.70% 7.2
</p>
<p>angular 44.39% 13.9 11.7 45.06% 15.3
react 49.44% 11.4 10.7 48.61% 12.1
</p>
<p>opencv 50.77% 11.2 9.0 49.00% 8.4
CNTK 48.88% 17.9 11.8 44.85% 9.3
bitcoin 50.04% 17.9 13.0 55.03% 15.1
</p>
<p>CoreNLP 63.20% 28.5 10.1 62.25% 26.7
elasticsearch 36.53% 11.8 5.2 35.98% 6.4
</p>
<p>guava 65.52% 29.8 19.5 67.15% 34.3
</p>
<p>Table 3: Results on the atomic and full datasets.
</p>
<p>5 Conclusion and Future work
</p>
<p>We proposed an encoder-decoder model for au-
tomatically generating natural descriptions from
source code changes. We believe our current re-
sults suggest that the idea is feasible and, if im-
proved, could represent a contribution for the un-
derstanding of software evolution from a linguis-
tic perspective. As future work, we will consider
improving the model by allowing feature learning
from richer inputs, such as abstract syntax trees
and also functional data, such as execution traces.
</p>
<p>290</p>
<p />
</div>
<div class="page"><p />
<p>References
Miltiadis Allamanis, Hao Peng, and Charles Sutton.
</p>
<p>2016. A convolutional attention network for ex-
treme summarization of source code. arXiv preprint
arXiv:1602.03001 .
</p>
<p>Hazeline U. Asuncion, Arthur U. Asuncion, and
Richard N. Taylor. 2010. Software traceabil-
ity with topic modeling. In Proceedings of
the 32Nd ACM/IEEE International Conference
on Software Engineering - Volume 1. ACM,
New York, NY, USA, ICSE ’10, pages 95–104.
https://doi.org/10.1145/1806799.1806817.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .
</p>
<p>Nicola Bertoldi, Haddow Barry, and Jean-Baptiste
Fouet. 2009. Improved minimum error rate train-
ing in moses. The Prague Bulletin of Mathematical
Linguistics pages 1–11.
</p>
<p>Georg Brandl. 2016. Pygments: Python syntax high-
lighter. http://pygments.org.
</p>
<p>Raymond P.L. Buse and Westley R. Weimer. 2010.
Automatically documenting program changes. In
Proceedings of the IEEE/ACM International Con-
ference on Automated Software Engineering. ACM,
New York, NY, USA, ASE ’10, pages 33–42.
https://doi.org/10.1145/1858996.1859005.
</p>
<p>Luis Fernando Cortés-Coy, Mario Linares Vásquez,
Jairo Aponte, and Denys Poshyvanyk. 2014. On
automatically generating commit messages via sum-
marization of source code changes. In SCAM. vol-
ume 14, pages 275–284.
</p>
<p>Georgios Gousios, Martin Pinzger, and Arie van
Deursen. 2014. An exploratory study of the pull-
based software development model. In Proceedings
of the 36th International Conference on Software
Engineering. ACM, pages 345–355.
</p>
<p>Xiaodong Gu, Hongyu Zhang, Dongmei Zhang,
and Sunghun Kim. 2016. Deep api learn-
ing. In Proceedings of the 2016 24th ACM
SIGSOFT International Symposium on Foun-
dations of Software Engineering. ACM, New
York, NY, USA, FSE 2016, pages 631–642.
https://doi.org/10.1145/2950290.2950334.
</p>
<p>Sonia Haiduc, Jairo Aponte, and Andrian Marcus.
2010. Supporting program comprehension with
source code summarization. In Proceedings of the
32nd ACM/IEEE International Conference on Soft-
ware Engineering-Volume 2. ACM, pages 223–226.
</p>
<p>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics. Sofia, Bulgaria,
pages 690–696.
</p>
<p>Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. 2012. On the natural-
ness of software. In 2012 34th International Confer-
ence on Software Engineering (ICSE). IEEE, pages
837–847.
</p>
<p>Xuan Huo, Ming Li, and Zhi-Hua Zhou. ???? Learn-
ing unified features from natural and programming
languages for locating buggy source code .
</p>
<p>Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source
code using a neural attention model. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 2073–2083.
http://www.aclweb.org/anthology/P16-1195.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 177–180.
http://www.aclweb.org/anthology/P07-2045.
</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. Meteor:
An automatic metric for mt evaluation with high
levels of correlation with human judgments. In
Proceedings of the Second Workshop on Statis-
tical Machine Translation. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
StatMT ’07, pages 228–231. http://dl.acm.org/cita-
tion.cfm?id=1626355.1626389.
</p>
<p>Stanley Letovsky. 1987. Cognitive processes in pro-
gram comprehension. Journal of Systems and soft-
ware 7(4):325–339.
</p>
<p>Mario Linares-Vásquez, Luis Fernando Cortés-Coy,
Jairo Aponte, and Denys Poshyvanyk. 2015.
Changescribe: A tool for automatically generating
commit messages. In Proceedings of the 37th In-
ternational Conference on Software Engineering-
Volume 2. IEEE Press, pages 709–712.
</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-
based neural machine translation. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics, Lisbon, Portugal, pages
1412–1421. http://aclweb.org/anthology/D15-1166.
</p>
<p>Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.
</p>
<p>291</p>
<p />
</div>
<div class="page"><p />
<p>Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.
2016. Convolutional neural networks over tree
structures for programming language processing. In
Proc. AAAI. AAAI Press, pages 1287–1293.
</p>
<p>Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Sapporo, Japan, pages 160–167.
https://doi.org/10.3115/1075096.1075117.
</p>
<p>Annibale Panichella, Bogdan Dit, Rocco Oliveto, Mas-
similiano Di Penta, Denys Poshyvanyk, and An-
drea De Lucia. 2013. How to effectively use topic
models for software engineering tasks? an ap-
proach based on genetic algorithms. In Proceedings
of the 2013 International Conference on Software
Engineering. IEEE Press, Piscataway, NJ, USA,
ICSE ’13, pages 522–531. http://dl.acm.org/cita-
tion.cfm?id=2486788.2486857.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-Jing Zhu. 2002. Bleu: a method
for automatic evaluation of machine transla-
tion. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA, pages 311–318.
https://doi.org/10.3115/1073083.1073135.
</p>
<p>Veselin Raychev, Martin Vechev, and Andreas Krause.
2015. Predicting program properties from big code.
In ACM SIGPLAN Notices. ACM, volume 50, pages
111–124.
</p>
<p>Veselin Raychev, Martin Vechev, and Eran Yahav.
2014. Code completion with statistical language
models. In ACM SIGPLAN Notices. ACM, vol-
ume 49, pages 419–428.
</p>
<p>Paige Rodeghero, Collin McMillan, Paul W McBurney,
Nigel Bosch, and Sidney D’Mello. 2014. Improving
automated source code summarization via an eye-
tracking study of programmers. In Proceedings of
the 36th International Conference on Software En-
gineering. ACM, pages 390–401.
</p>
<p>Wojciech Zaremba, Ilya Sutskever, and Vinyals Oriol.
2015. Recurrent neural network regularization. In
Proceedings of the 3rd International Conference on
Learning Representations.
</p>
<p>292</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 293–298
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2046
</p>
<p>English Event Detection With Translated Language Features
</p>
<p>Sam Wei
School of IT
</p>
<p>University of Sydney
Sydney, Australia
</p>
<p>swei4829@uni.sydney.edu.au
</p>
<p>Igor Korostil
TEG Analytics
</p>
<p>Sydney, Australia
</p>
<p>Joel Nothman
Sydney Informatics Hub
</p>
<p>University of Sydney
Sydney, Australia
</p>
<p>{eeghor,joel.nothman,ben.hachey}@gmail.com
</p>
<p>Ben Hachey
School of IT
</p>
<p>University of Sydney
Sydney, Australia
</p>
<p>Abstract
</p>
<p>We propose novel radical features from
automatic translation for event extraction.
Event detection is a complex language
processing task for which it is expensive
to collect training data, making generali-
sation challenging. We derive meaningful
subword features from automatic transla-
tions into target language. Results suggest
this method is particularly useful when us-
ing languages with writing systems that fa-
cilitate easy decomposition into subword
features, e.g., logograms and Cangjie. The
best result combines logogram features
from Chinese and Japanese with syllable
features from Korean, providing an addi-
tional 3.0 points f-score when added to
state-of-the-art generalisation features on
the TAC KBP 2015 Event Nugget task.
</p>
<p>1 Introduction
</p>
<p>Event trigger detection is the task of identifying
the mention that predicates the occurrence of an
event and assigning it an event type (e.g., attack).
Typical training data for event trigger detection in-
cludes fewer than 200 annotated documents (Ellis
et al., 2015). Yet systems attempt to identify many
event types (e.g., 38 for the data used here), mak-
ing data sparsity a particular challenge (Ji, 2009;
Zhu et al., 2014).
</p>
<p>Existing approaches use two main strategies for
handling data sparsity. One strategy is to use lex-
ical databases. Lexical databases have become
a standard feature set for event detection. They
make it easy to include synonyms and word-class
information through hypernym relations. How-
ever, they require substantial human effort to build
and can have low coverage. Another approach is
to induce word-class information through cluster-
</p>
<p>ing. Here cluster co-membership can be used to
find synonyms and cluster identifiers provide ab-
stracted word-class information.
</p>
<p>We propose novel semantic features for English
event detection derived from automatic transla-
tions into thirteen languages. In particular, we
explore the use of Cangjie1 radicals in Chinese
and Japanese. Where characters represent con-
cepts, they have often been composed of smaller
pictographic units, called radicals. For example:
明(bright) is composed of two radicals日,月(sun,
moon) with corresponding Latin letter sequence
”AB”. While this composition is often not produc-
tive, we hypothesise that the recurrence of some
radicals among related concepts’ logograms may
be exploited to identify semantic affinity.
</p>
<p>Results suggest that (1) translated language fea-
tures are especially useful if the target language
has a writing system facilitating easy decomposi-
tion into useful subword features; (2) logograms
(e.g., Chinese, Japanese), radicals (e.g., Chinese,
Japanese) and syllables (e.g., Japanese, Korean)
prove beneficial and complementary; and (3) Chi-
nese characters are particularly useful, compara-
ble to WordNet. Adding the best translated lan-
guage features to the final system improves F1 by
3.0 points over a state-of-the-art feature set on the
TAC KBP 2015 nugget type detection task.
</p>
<p>2 Background
</p>
<p>Multilingual resources have been successfully ap-
plied to various NLP tasks such as named entity
recognition (Klementiev and Roth, 2006), para-
phrasing (Bannard and Callison-Burch, 2005),
sentiment analysis (Wan, 2008), and word sense
disambiguation (Lefever and Hoste, 2010).
</p>
<p>1https://en.wikipedia.org/wiki/
Cangjie_input_method
</p>
<p>293</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2046">https://doi.org/10.18653/v1/P17-2046</a></div>
</div>
<div class="page"><p />
<p>Ji (2009) reports significantly improved event
trigger extraction via cross-lingual clusters of En-
glish translations to Chinese trigger words over
large corpora. At runtime, these are used to
replace low-confidence event triggers with other
high-confidence predicates from the same cluster.
We describe an approach leveraging cross-lingual
information not only from words, but also at the
level of characters and radicals. Like Zhu et al.
(2014), we use Google Translate and build bilin-
gual feature vectors from the translations as well
as original English sentences. While they address
event trigger type classification only, we address
both trigger detection and classification. We use
new translated language features and evaluate with
a range of languages.
</p>
<p>Li et al. (2012) show that monolingual Chinese
event trigger extraction benefits from using com-
positional semantics inferred from Chinese char-
acters. We use similar Chinese character informa-
tion as features for English event trigger detection
also using maximum entropy modelling. Further-
more, we introduce new radical features that take
advantage of semantic compositionality of Chi-
nese characters.
</p>
<p>2.1 Task
</p>
<p>We address the event nugget detection task from
the Text Analysis Conference Knowledge Base
Population (TAC KBP) 2015 shared task (Mita-
mura and Hovy, 2015), which includes trigger de-
tection and classification. An event trigger is the
smallest extent of text (usually a word or short
continuous phrase) that predicates the occurrence
of an event (LDC, 2015). The task defines 9 event
types and and 38 subtypes. Like most task par-
ticipants, we formulate event trigger detection as
a token-level classification task. We use a max-
imum entropy classifier here, with IOB encoding
(Sang and Veenstra, 1999) to represent multi-word
mentions.
</p>
<p>For comparison, we implement the baseline and
lexical generalisation features from Hong et al.
(2015). This was the best-performing system in
the TAC 2015 nugget type detection task, with
an F1 of 58.3. We do not replicate their semi-
supervised techniques here as we want to iso-
late the comparison of translated language features
to other generalisation features. Since translated
language features leverage off-the-shelf automatic
translation, we believe the results here will gener-
</p>
<p>alise to semi-supervised learning as well.
Baseline Features (BASE) Our baseline sys-
</p>
<p>tem uses standard surface features used for event
extraction. Features of the current token include
the full word token as it appears in the sentence,
its lemma, its part of speech (POS), its entity type,
and a feature that indicates whether the first char-
acter of the token is capitalised. Context features
are computed for a window of one token on ei-
ther side of the current token. They include lemma
bigrams, POS bigrams and entity type bigrams.
Finally, grammatical features are computed based
on a dependency parse of the sentence. These in-
clude dependency relation types for the governor
and any dependents, conjoined relation type and
lemma, conjoined relation type and POS, and con-
joined relation type and entity type.
</p>
<p>Lexical generalisation Features (LEX) We in-
clude three generalisation feature sets from the lit-
erature as a benchmark. The first lexical resource
we use is Nomlex (Macleod et al., 1998) – a dic-
tionary of nouns that are generated from another
verb class, usually verbs. We also use Brown
clusters trained on the Reuters corpus (Brown
et al., 1992; Turian et al., 2010). Brown clusters
group words into classes by performing a hier-
archical clustering over distributional representa-
tions of the contexts in which they appear. Fi-
nally, we use WordNet (Miller, 1995) – a lexical
database that includes synonym relations and se-
mantic type-of/hypernym relationships. These re-
lations have been used to extend feature sets be-
yond observed tokens which can help with identi-
fication of rare or unseen event triggers.
</p>
<p>3 Approach
</p>
<p>We use machine translation (MT) service to ob-
tain translated text. The translation is done at sen-
tence level. We cache the translation results on
files to ensure the experiments are repeatable. Be-
low are example sentences translated from English
into Chinese and Spanish.
</p>
<p>EN The attack by insurgents happened yesterday.
</p>
<p>ZH 叛亂分子的襲擊發生在昨天。
ES El ataque de los insurgentes pasó ayer.
</p>
<p>(1)
</p>
<p>3.1 Translated Language Features (TRANS)
We generate three types of logogram features and
use stem features for non-logogram languages.
</p>
<p>Word features (word) Different words in En-
glish can be translated into the same word in an-
other language. For example there are 201 unique
</p>
<p>294</p>
<p />
</div>
<div class="page"><p />
<p>Chinese
Character
</p>
<p>Radical
Symbol
</p>
<p>Latin
Radical
</p>
<p>English
Word
</p>
<p>打 手一弓 QMN hit
擊 十水手 JEQ strike
投 手竹弓水 QHNE throw
擲 手廿大中 QTKL throw
折(磨) 手竹一中 QHML torture
拆 手竹一卜 QHMY demolish
拷(打) 手十大尸 QJKS torture
割 十口中弓 JRLN cut
刺 木月中弓 DBLN stab
</p>
<p>Table 1: Attack event triggers. The radical “手” (Q, hand)
frequently appears in the attack event triggers. Radicals “中
弓(刀)” (LN, knife) appear frequently when events are asso-
ciated with actions that are performed with a knife
</p>
<p>English trigger words for attack events and only
160 unique words in their Chinese translations.
Therefore if an English trigger word is not in the
training data, the model might still recognise the
trigger if it has seen the Chinese translation before.
</p>
<p>Logogram character features (char) Chi-
nese and Japanese logograms are compositions of
one or more characters defining their meanings.
Therefore, different words representing the same
event often contain similar characters. There are
195 unique Chinese characters for the attack event
triggers in the corpus. The most frequently ap-
pearing characters are “擊” (strike, attack), “戰”
(war, fight), “殺” (kill), “爭” (fight, dispute), and
“炸” (bomb, explode).
</p>
<p>Logogram Cangjie features (Cangjie) Chi-
nese and Japanese characters can be further de-
composed to smaller components called radicals.
Certain radicals are more commonly found for a
particular event type (Table 1). Cangjie is one of
the methods to decompose Chinese characters. It
was designed to use on computers with QWERTY
keyboards so the radicals can be easily stored, in-
dexed and searched by most computer systems. In
addition to word and character features, we com-
pute Cangjie features for logographic languages.
</p>
<p>Stem features (stem) For many languages
character and radical features cannot be generated.
We generate stem features in addition to the word
features where available. We use the NLTK Snow-
ball stemmer for German, Spanish, Finnish, Hun-
garian, Dutch and Russian; and the NLTK ISRI
stemmer for Arabic. By including a range of lan-
guages, we hope to separate the effect of syllabic
from semantic components of logograms.
</p>
<p>3.2 Translation Alignment
Translated language features require each English
word to be aligned to one in the translated sen-
tence. We use the translation service obtain all
possible translations of a given English word, e.g.:
</p>
<p>EN attack
</p>
<p>ZH 進攻,砰擊,發作,攻擊,攻打,掊擊,抨擊, ...
ES acometida, ataque contra, agresión, ...
</p>
<p>(2)
</p>
<p>If one of these is in the translated sentence, then
an alignment is made. If not, then we use the most
likely word translation (underlined above).
</p>
<p>4 Experiments
</p>
<p>We use the TAC KBP 2015 English event
nugget data (Ellis et al., 2015) for the experi-
ments. Development experiments use the train-
ing data (LDC2015E73) and the evaluation data
(LDC2015R26) is held out for final results. The
development corpus contains a total of 158 docu-
ments from two genres: 81 newswire documents
and 77 discussion forum documents. We split this
into 80% for training and 20% for development
testing. We use Google Translate to obtain sen-
tence and word translations into target languages
and derive translated language features to help
with the English task. Evaluation uses the offi-
cial scorer from the shared task, where a trigger is
counted as correct if both the trigger span and its
event subtype are correctly identified.
</p>
<p>Comparing languages First, we explore how
translated language features perform across the
thirteen languages. Figure 1 shows how much
each target language improves BASE on devel-
opment data. We include all word, stem, charac-
ter and Cangjie features as available for each lan-
guage. Chinese, Japanese and Korean stand out,
with improvements as high as 19.17 points f-score
due mostly to large increases in recall. These re-
sults suggest that languages with writing systems
that facilitate easy decomposition into meaningful
subword features are particularly useful.
</p>
<p>Combining languages Next, we test whether
system performance can be further improved us-
ing TRANS features from multiple languages. We
add target languages one at a time in order of indi-
vidual performance, and find that Traditional Chi-
nese, Japanese and Korean to Simplified Chinese
together improve F1 by 2.5 points. This combined
feature set is used in the remaining analysis and
experimental results.
</p>
<p>295</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Effect of individual languages on development data, showing the difference in precision, recall and F1 compared
to the BASE scores of 55.16, 20.62 and 30.02. AR:Arabic, DE:German, ES:Spanish, FI:Finnish, HI:Hindi, HU:Hungarian,
JA:Japanese, KO:Korean, NL:Dutch, RU:Russian, TR:Turkish, ZH:Chinese (Simplified), ZHCN:Chinese (Traditional).
</p>
<p>Error analysis We explore characteristic er-
rors for BASE+LEX versus BASE+TRANS for
the attack event on evaluation data. We ran-
domly sample twenty instances where one is cor-
rect and the other is incorrect. Of six LEX FN er-
rors, two are triggers not seen in the training data,
e.g., ‘wages’ (Transfer-Money), and ‘resignation’
(End-Position). In other cases, there seem to be
too few training instances, e.g., ‘pardoning’ (Par-
don) only appears once in the training data. The
TRANS FN error is due to a bad translation in
which ‘strike’ (Attack) is a translated to the ‘work
stoppage’ sense instead of the ‘forceful hit’ sense.
</p>
<p>For both systems, most FP errors correspond to
cases with challenging ambiguity. For instance,
both systems label ‘appeal’ as Justice.Appeal
event in two sentences where the word ‘appeal’
means ‘ask for aid’, instead of ‘taking a court case
to a higher court’. The translation was incorrect
in this case. Similarly, ‘report’ appears six times
in the training data as three different event types
(Broadcast, Correspondence, Move-Person).
</p>
<p>Long-tail generalisation Table 2 shows type-
level results for BASE+LEX and BASE+TRANS
compared to BASE alone. The generalisation fea-
ture sets outperform the baseline for all but three
of the 38 event types. For Pardon, BASE ob-
tains 97 F1 so there is little room for improve-
ment. For Execute, LEX features have no ef-
fect while TRANS doubles BASE F1. Contact
is the only type where generalisation features are
harmful. Ignoring ties, BASE+TRANS performs
best on more types (13) than BASE+LEX (11).
TRANS appears to help more with long-tail en-
tity types that have fewer training instances (e.g.,
Bankruptcy, Appeal, Born). Encouragingly, this
</p>
<p>Type Trn Tst BA LX TR
Attack 547 253 29 60 58
Move-Person 390 127 15 37 33
Transfer-Money 366 185 18 35 49
Die 357 157 45 63 66
Broadcast 305 112 14 20 16
Contact 260 77 29 24 23
Transfer-Ownership 234 46 9 20 34
Meet 221 23 15 44 38
Pardon 221 18 97 97 95
Arrest-Jail 208 79 54 70 71
Convict 173 49 71 74 81
End-Position 130 79 18 51 55
Extradite 62 1 0 0 100
Execute 51 15 12 12 24
Release-Parole 45 28 0 87 95
Bankruptcy 30 3 0 50 89
Appeal 25 12 0 57 92
Born 13 6 0 22 40
</p>
<p>Table 2: Comparing instance count in training (Trn) and test
(Tst) to F1 for BASE (BA), LEX (LX) and TRANS (TR).
</p>
<p>analysis also suggests that LEX and TRANS can
be complementary, with LEX doing particularly
well on some types (e.g., Trial-Hearing, Corre-
spond) and TRANS doing particularly well on oth-
ers (e.g., Transfer-Money, Release-Parole).
</p>
<p>5 Final Results and Discussion
</p>
<p>Table 3 contains final results on the held-out eval-
uation data. The final translated language fea-
ture set (TRANS) comprises word, character and
Cangjie features from Traditional Chinese, Sim-
plified Chinese, Japanese and Korean. TRANS
features provide a large F1 improvement of 17.4
over the baseline (BASE), similar to the bench-
mark lexical generalisation features (LEX). They
differ in precision-recall tradeoff, with higher re-
call but lower precision from TRANS. LEX and
TRANS are complementary, giving F1 of 55.0.
</p>
<p>296</p>
<p />
</div>
<div class="page"><p />
<p>System P R F
BASE 60.4 24.1 34.4
BASE+LEX 66.8 42.6 52.0
BASE+TRANS 59.6 45.8 51.8
BASE+LEX+TRANS 67.9 46.2 55.0
TAC 2015 medians 61.7 40.7 48.8
TAC 2015 #1 75.2 47.7 58.4
</p>
<p>Table 3: Final results comparing translated language fea-
tures (TRANS) to benchmark lexical generalisation features
(LEX). BASE+LEX is our implementation of the core Hong
et al. classifier. TAC KPB 2015 #1 corresponds to reported re-
sults for Hong et al. including semi-supervised learning. TAC
KPB 2015 shared task has 38 runs submitted from 14 teams.
</p>
<p>This is 20.6 points higher than the baseline fea-
tures alone, and improves both the precision of
LEX and the recall of TRANS.
</p>
<p>The main appeal of the approach here is that
translated character and radical features are easy
to obtain using off-the-shelf tools. This provides a
simple technique to capture semantic information
and leverage the word sense disambiguation en-
coded in translation models trained over very large
datasets. Given the positive results here, we plan
to explore translation and alignment strategies to
improve precision. We also plan to quantify the
effect of different translation systems and system
change over time.
</p>
<p>6 Conclusion
</p>
<p>We described an event detection system leverag-
ing features from off-the-shelf automatic transla-
tion to improve generalisation to new data. Chi-
nese, Japanese and Korean prove especially useful
as they provide natural decomposition into infor-
mative subword features, i.e., characters (Chinese
and Japanese), radicals (Chinese and Japanese)
and syllables (Korean). None of the nine other lan-
guages explored provide similar levels of natural
decomposition and none provided additional ben-
efit. The best system includes Chinese, Japanese
and Korean character features. These translated
language features improve f-score by 3 points
on top of the English-only generalisation features
from WordNet, Nomlex and Brown clusters.
</p>
<p>Acknowledgments
</p>
<p>We wish to thank Will Radford and the anonymous
reviewers for their helpful feedback. This research
is funded by the Capital Markets Co-operative Re-
search Centre. Ben Hachey is the recipient of an
Australian Research Council Discovery Early Ca-
reer Researcher Award (DE120102900).
</p>
<p>References
Colin Bannard and Chris Callison-Burch. 2005.
</p>
<p>Paraphrasing with bilingual parallel corpora.
In Annual Meeting of the Association for
Computational Linguistics. pages 597–604.
https://doi.org/10.3115/1219840.1219914.
</p>
<p>Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural
language. Computational Linguistics 18(4):467–
479. http://www.aclweb.org/anthology/J/J92/J92-
4003.pdf.
</p>
<p>Joe Ellis, Jeremy Getman, Dana Fore, Neil Kuster,
Zhiyi Song, Ann Bies, and Stephanie Strassel. 2015.
Overview of linguistic resources for the TAC KBP
2015 evaluations: Methodologies and results. In
Text Analysis Conference.
</p>
<p>Yu Hong, Di Lu, Dian Yu, Xiaoman Pan, Xiaobin
Wang, Yadong Chen, Lifu Huang, and Heng Ji.
2015. RPI BLENDER TAC-KBP2015 system de-
scription. In Text Analysis Conference.
</p>
<p>Heng Ji. 2009. Cross-lingual predicate cluster
acquisition to improve bilingual event extrac-
tion by inductive learning. In NAACL Work-
shop on Unsupervised and Minimally Supervised
Learning of Lexical Semantics. pages 27–35.
http://www.aclweb.org/anthology/W09-1704.
</p>
<p>Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In
International Conference on Computational Lin-
guistics and Annual Meeting of the Association
for Computational Linguistics. pages 817–824.
https://doi.org/10.3115/1220175.1220278.
</p>
<p>LDC. 2015. Rich ERE Annotation Guidelines
Overview. Linguistic Data Consortium. Version
4.1. Accessed 14 November 2015 from http:
//cairo.lti.cs.cmu.edu/kbp/2015/
event/summary_rich_ere_v4.1.pdf.
</p>
<p>Els Lefever and Veronique Hoste. 2010. SemEval-
2010 task 3: Cross-lingual word sense
disambiguation. In International Work-
shop on Semantic Evaluation. pages 15–20.
http://www.aclweb.org/anthology/S10-1003.
</p>
<p>Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-
bin Hou. 2012. Employing compositional seman-
tics and discourse consistency in chinese event ex-
traction. In Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning. pages 1006–
1016. http://www.aclweb.org/anthology/D12-1092.
</p>
<p>Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Euralex International
Congress. pages 187–193.
</p>
<p>297</p>
<p />
</div>
<div class="page"><p />
<p>George A. Miller. 1995. Wordnet: A lexical
database for english. Commun. ACM 38(11):39–41.
https://doi.org/10.1145/219717.219748.
</p>
<p>Teruko Mitamura and Eduard Hovy. 2015.
TAC KBP Event Detection and Corefer-
ence Tasks for English. Version 1.0. Ac-
cessed 14 November 2015 from http:
//cairo.lti.cs.cmu.edu/kbp/2015/
event/Event_Mention_Detection_and_
Coreference-2015-v1.1.pdf.
</p>
<p>Erik F. Tjong Kim Sang and Jorn Veenstra. 1999.
Representing text chunks. In Conference of
the European Chapter of the Association for
Computational Linguistics. pages 173–179.
https://doi.org/10.3115/977035.977059.
</p>
<p>Joseph Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word representations: A simple
and general method for semi-supervised learn-
ing. In Annual Meeting of the Association
for Computational Linguistics. pages 384–394.
http://www.aclweb.org/anthology/P10-1040.
</p>
<p>Xiaojun Wan. 2008. Using bilingual knowl-
edge and ensemble techniques for unsupervised
Chinese sentiment analysis. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing. pages 553–561.
http://www.aclweb.org/anthology/D08-1058.
</p>
<p>Zhu Zhu, Shoushan Li, Guodong Zhou, and Rui Xia.
2014. Bilingual event extraction: a case study on
trigger type determination. In Annual Meeting of the
Association for Computational Linguistics. pages
842–847. http://www.aclweb.org/anthology/P14-
2136.
</p>
<p>298</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 299–304
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2047
</p>
<p>EviNets: Neural Networks for Combining Evidence Signals for Factoid
Question Answering
</p>
<p>Denis Savenkov
Emory University
</p>
<p>denis.savenkov@emory.edu
</p>
<p>Eugene Agichtein
Emory University
</p>
<p>eugene.agichtein@emory.edu
</p>
<p>Abstract
</p>
<p>A critical task for question answering is
the final answer selection stage, which
has to combine multiple signals available
about each answer candidate. This paper
proposes EviNets: a novel neural network
architecture for factoid question answer-
ing. EviNets scores candidate answer enti-
ties by combining the available supporting
evidence, e.g., structured knowledge bases
and unstructured text documents. EviNets
represents each piece of evidence with a
dense embeddings vector, scores their rel-
evance to the question, and aggregates the
support for each candidate to predict their
final scores. Each of the components is
generic and allows plugging in a variety
of models for semantic similarity scoring
and information aggregation. We demon-
strate the effectiveness of EviNets in ex-
periments on the existing TREC QA and
WikiMovies benchmarks, and on the new
Yahoo! Answers dataset introduced in this
paper. EviNets can be extended to other
information types and could facilitate fu-
ture work on combining evidence signals
for joint reasoning in question answering.
</p>
<p>1 Introduction
</p>
<p>Most of the recent works in Question Answering
(QA) have focused on the problem of semantic
matching between a question and candidate an-
swer sentences (He and Lin, 2016; Rao et al.,
2016; Yang et al., 2016). The datasets used in
these works, such as Answer Sentence Selection
Dataset (Wang et al., 2007) and WikiQA (Yang
et al., 2015), typically contain a relatively small
set of sentences, and the task is to select those
that state the answer to the question. However, for
many questions, a single sentence does not pro-
</p>
<p>vide sufficient information, and it may not be reli-
able in isolation. At the same time, the redundancy
of information in large corpora, such as the Web,
has been shown useful to improve information re-
trieval approaches to QA (Clarke et al., 2001).
</p>
<p>This work focuses on factoid questions, which
can be answered with an entity, i.e., an object in a
Knowledge Base (KB) such as Freebase. Knowl-
edge Base Question Answering (KBQA) tech-
niques, such as Berant et al. (2013); Yih et al.
(2015); Bast and Haussmann (2015), can be used
to answer some of the user questions directly
from a KB. However, KBs are inherently incom-
plete (Dong et al., 2014), and do not have suf-
ficient information to answer many other ques-
tions (Fader et al., 2014).
</p>
<p>Previous, feature-engineering, approaches for
combining different data sources to improve an-
swer retrieval were shown to be quite effective for
QA (Sun et al., 2015; Xu et al., 2016; Savenkov
and Agichtein, 2016). Alternatively, Memory Net-
works (Sukhbaatar et al., 2015) and their exten-
sions (Miller et al., 2016) use embeddings to rep-
resent relevant data as memories, and summarize
them into a single vector, therefore losing infor-
mation about answers provenances.
</p>
<p>In this paper, we introduce EviNets, a novel neu-
ral network architecture for factoid question an-
swering, which provides a unified framework for
aggregating evidence, supporting answer candi-
dates. Given a question, EviNets retrieves a set
of relevant pieces of information, e.g., sentences
from a corpora or knowledge base triples, and ex-
tracts mentioned entities as candidate answers. All
the evidence signals are then embedded into the
same vector space, scored and aggregated using
multiple strategies for each answer candidate. Ex-
periments on the TREC QA, WikiMovies and new
Yahoo! Answers datasets demonstrate the effec-
tiveness of EviNets, and its ability to handle both
unstructured text and structured KB triples.
</p>
<p>299</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2047">https://doi.org/10.18653/v1/P17-2047</a></div>
</div>
<div class="page"><p />
<p>Figure 1: The EviNets neural network architecture for combining evidence in factoid question answering.
</p>
<p>2 EviNets Question Answering Model
</p>
<p>The high level architecture of EviNets is illus-
trated in Figure 1. For a given question, we ex-
tract potentially relevant information, e.g., sen-
tences from documents retrieved from text corpora
using a search system. Next, we can use an en-
tity linking system, such as TagMe (Ferragina and
Scaiella, 2010), to identify entities mentioned in
the extracted information, which become candi-
date answers. EviNets can further incorporate ad-
ditional supporting evidence, e.g., textual descrip-
tion of candidate answer entities, and potentially
useful KB triples, such as types (Sun et al., 2015).
Finally, question, answer candidates and support-
ing evidence are given as input to the EviNets neu-
ral network.
</p>
<p>Let us denote a question by q, and {qt ∈ R|V |},
as a one-hot encoding of its tokens from a fixed
vocabulary V . ai is a candidate answer from the
setA, and we will assume, that each answer is rep-
resented as a single entity. For each question, we
have a fixed set E = Etext ∪ EKB of evidence
statements e(i), i = 1..M , and their tokens e(i)t . A
boolean functionmention : A×E → {0, 1} pro-
vides the information about which answer candi-
dates are mentioned in which evidences. Individ-
ual tokens qt, ai, e
</p>
<p>(i)
t are translated into the embed-
</p>
<p>ding space using a matrixWD× |V |, whereD is the
dimension of the embeddings, i.e., qemb,t = Wqt,
</p>
<p>aemb,i = Wat and e
(i)
emb,t = We
</p>
<p>(i)
t . In our ex-
</p>
<p>periments, we use the same matrix for questions,
evidence, and answers. KB entities are considered
to be individual tokens, while predicates and type
names are tokenized into constituent words.
</p>
<p>2.1 Memory Matching Module
</p>
<p>Evidence matching is responsible for estimating
the relevance of each of the pieces of evidence to
the question, i.e., we = softmax(match(q, e)).
The function match(q, e) can be implemented us-
ing any of the recently proposed semantic simi-
larity estimation architectures1. One of the sim-
plest approaches is to average question and each
evidence token embeddings and score the similar-
ity using the dot product: qemb = 1Lq
</p>
<p>∑
t qemb,t
</p>
<p>and e(i)emb =
1
Le
</p>
<p>∑
t e
</p>
<p>(i)
emb,t and match(q, e
</p>
<p>(i)) =
</p>
<p>qTemb · e
(i)
emb.
</p>
<p>2.2 Evidence Aggregation Module
</p>
<p>After all the evidence signals have been scored,
EviNets aggregates the support for each answer
candidate. Table 1 summarizes the evidence sig-
nals used. With these features, EviNets captures
different aspects, i.e., how well individual sen-
tences match the question, how frequently the can-
didate is mentioned and how well a set of answer
</p>
<p>1https://goo.gl/6gWrgA
</p>
<p>300</p>
<p />
</div>
<div class="page"><p />
<p>Evidence Feature Description
Maximum evidence score mentioning the answer maxe{we|mention(a, e)}, e ∈ E,Etext or EKB
Average evidence score mentioning the answer avge{we|mention(a, e)}, e ∈ E,Etext or EKB
Sum of evidence scores mentioning the answer
</p>
<p>∑
e
{we|mention(a, e)}, e ∈ E,Etext or EKB
</p>
<p>Number of mentions
∑
</p>
<p>e
{1|mention(a, e)}, e ∈ Etext
</p>
<p>Weighted memory similarity to the question ( 1
M
</p>
<p>∑
i
wee
</p>
<p>(i)
emb) · qemb
</p>
<p>Weighted memory similarity to the answer (Sukhbaatar
et al., 2015)
</p>
<p>( 1
M
</p>
<p>∑
i
wee
</p>
<p>(i)
emb) · aemb or RT ( 1M
</p>
<p>∑
i
wee
</p>
<p>(i)
emb + qemb) ·
</p>
<p>aemb, where RD× D is a rotation matrix
Weighted memory answer mentions similarity to the an-
swer (Miller et al., 2016)
</p>
<p>( 1
M
</p>
<p>∑
e
we[
∑
</p>
<p>a
aemb |mention(e, a)]) · aemb
</p>
<p>Table 1: Signals we used to aggregate evidence in support for each of the answer candidates a.
</p>
<p>Dataset Example Questions
TREC QA Where is the highest point in Japan?
1236 train What is the coldest place on earth?
202 test Who was the first U.S. president to ap-
</p>
<p>pear on TV?
</p>
<p>WikiMovies what films did Ira Sachs write?
96185 train what films does Claude Akins appear
</p>
<p>in?
</p>
<p>10000 dev the movie Victim starred who?
9952 test what type of film is Midnight Run?
Y! Answers What is Elvis’s hairstyle called?
1898 train Who is this kid in Mars Attacks?
271 dev who invented denim jeans?
542 test who’s the woman on the progres-
</p>
<p>sive.com commercials?
</p>
<p>Table 2: Description of TREC QA, WikiMovies
and Yahoo! Answers factoid QA datasets.
</p>
<p>evidences covers the information requested in the
question.
</p>
<p>2.3 Answer Scoring Module
</p>
<p>Finally, EviNets uses the aggregated signals to pre-
dict the answer scores, to rank them, and to return
the best candidate as the final answer to the ques-
tion. For this purpose, we use two fully-connected
neural network layers with the ReLU activation
function, with 32 and 8 hidden units respectively.
The model was trained end-to-end by optimizing
the cross entropy loss function using the Adam al-
gorithm (Kingma and Ba, 2014).
</p>
<p>3 Experimental Evaluation
</p>
<p>To test our framework we used TREC QA (Sun
et al., 2015), WikiMovies (Miller et al., 2016)
benchmarks and the new Yahoo! Answers dataset2
</p>
<p>derived from factoid questions posted on the CQA
</p>
<p>2available for research purposes at
http://ir.mathcs.emory.edu/software-data/
</p>
<p>website (Table 2). In all experiments, embed-
dings were initialized with 300-dimensional vec-
tors pre-trained with Glove (Pennington et al.,
2014). Embeddings for multi-word entity names
were obtained by averaging the word vectors of
constituent words.
</p>
<p>3.1 Baselines
</p>
<p>As baselines for different experiments depending
on availability and specifics of a dataset we con-
sidered the following methods:
• IR-based QA systems: AskMSR (Brill et al.,
</p>
<p>2002) and AskMSR+ (Tsai et al., 2015),
which select the best answer based on the
frequency of entity mentions in retrieved text
snippets.
• KBQA systems: SemPre (Berant et al., 2013)
</p>
<p>and Aqqu (Bast and Haussmann, 2015),
which identify possible topic entities of the
question, and select the answer from the can-
didates in the neighborhood of these entities
in a KB.
• Hybrid system QuASE (Sun et al., 2015) de-
</p>
<p>tects mentions of knowledge base entities in
text passages, and uses the types and descrip-
tion information from the KB to support an-
swer selection.
• Hybrid system Text2KB (Savenkov and
</p>
<p>Agichtein, 2016), which uses textual re-
sources to improve different stages of the
KBQA pipeline.
• Memory Networks: MemN2N (Sukhbaatar
</p>
<p>et al., 2015) and KV MemN2N (Miller et al.,
2016) represent relevant information with
embeddings, and summarize the memories
into a single vector using the soft attention
mechanism. Additionally, KV MemN2N
splits memories into key-value pairs, where
keys are used for matching against the ques-
tion, and values are used to summarize the
memories.
</p>
<p>301</p>
<p />
</div>
<div class="page"><p />
<p>Method P R F1
SemPre 0.157 0.104 0.125
Text2KB 0.287 0.287 0.288
AskMSR+ 0.493 0.490 0.491
QuASE (text) 0.550 0.550 0.550
QuASE (text+kb) 0.579 0.579 0.579
MemN2N 0.333 0.328 0.330
KV MemN2N 0.517 0.500 0.508
EviNets (text) 0.580 0.560 0.569
EviNets (text+kb) 0.585 0.564 0.574
</p>
<p>Table 3: Precision, Recall and F1 of different
methods on TREC QA dataset. Improvements
over KV MemN2N are statistically significant.
</p>
<p>3.2 TREC QA dataset
</p>
<p>The TREC QA dataset is composed of factoid
questions, which can be answered with an en-
tity, and were used in TREC 8-12 question an-
swering tracks. Similarly to Sun et al. (2015) we
used web search (using the Microsoft Bing Web
Search API) to retrieve top 50 documents, parsed
them, extracted sentences and ranked them using
tf-idf similarity to the question. To compare our
results with the existing state-of-the-art, we used
the same set of candidate entities as used by the
QuASE model. We note that the extracted evi-
dence differs between the models, and we were
unable to match some of the candidates to our sen-
tences. For text+kb experiment, just as QuASE,
we used entity descriptions and types from Free-
base knowledge base. Table 3 summarizes the
results. EviNets achieves competitive results on
the dataset, beating KV MemN2N by 13% in F1
score, and, unlike QuASE, does not rely on ex-
pensive feature engineering and does not require
any external resources to train.
</p>
<p>3.3 WikiMovies dataset
</p>
<p>The WikiMovies dataset contains questions in the
movies domain along with relevant Wikipedia
passages and OMDb knowledge base. Since
KVMemN2N already achieves an almost perfect
result answering the questions using the KB, we
focus on using the provided movie articles from
Wikipedia. We followed the preprocessing pro-
cedures described in Miller et al. (2016). Unlike
TREC QA, where there are often multiple rel-
evant supporting pieces of evidence, answers in
the WikiMovies dataset usually have a single rel-
evant sentence, which, however, mentions multi-
</p>
<p>Method Accuracy
MemN2N (wiki windows) 0.699*
KV MemN2N (wiki windows) 0.762*
AskMSR (entities) 0.314
KV MemN2N (wiki sentences) 0.524
EviNets (wiki) 0.616
EviNets (wiki + entity types) 0.667
</p>
<p>Table 4: Accuracy of EviNets and baseline mod-
els on the WikiMovies dataset. The results marked
* are obtained using a different setup, i.e., they
use pre-processed entity window memories, and
the whole set of entities as candidates.
</p>
<p>ple entities. To help the model distinguish the
correct answer, and explore its abilities to en-
code structured and unstructured data, we gener-
ated additional entity type triples. For example,
if an entity E appears as an object of the predi-
cate directed by in OMDb, we added the [E,
type, director] triple. As baselines, we
used MemN2N and KV MemN2N models, and the
results are presented in Table 4. As we can see,
with the same setup using individual sentences as
evidence/memories EviNets significantly outper-
forms the KV MemN2N model by 27%. It is im-
portant to emphasize that the best-reported results
of memory networks were obtained using entity-
centered windows as memories, which requires
special pre-processing and increases the number
of memories. Additionally, these models used all
of the KB entities as candidate answers, whereas
EviNets relies only on the mentioned ones, which
is a more scalable scenario for open-domain ques-
tion answering, where it is not realistic to score
millions of candidate answers in real-time.
</p>
<p>3.4 Yahoo! Answers dataset
</p>
<p>Yahoo! recently released a dataset with search
queries, which lead to clicks on factoid Ya-
hoo! Answers questions, identified as questions
with the best answer containing less than 3 words
and a Wikipedia page as the specified source of
information3. This dataset contains 15K queries,
which correspond to 4725 unique Yahoo! An-
swers questions (Table 2). We took these ques-
tions, and mapped answers to KB entities using
the TagMe entity linking library (Ferragina and
Scaiella, 2010). We filtered out questions, for
</p>
<p>3L27 dataset https://webscope.sandbox.yahoo.com
</p>
<p>302</p>
<p />
</div>
<div class="page"><p />
<p>Method P R F1
Aqqu 0.116 0.117 0.116
Text2KB 0.170 0.170 0.170
AskMSR (entities) 0.175 0.319 0.226
MemN2N 0.072 0.131 0.092
KV MemN2N 0.126 0.228 0.162
EviNets (text) 0.210 0.383 0.271
EviNets (text+kb) 0.226 0.409 0.291
Oracle 0.622 1.0 0.767
</p>
<p>Table 5: Precision, Recall and F1 of different
methods on Yahoo! Answers factoid QA dataset.
The Oracle assumes candidate answers are ranked
perfectly and its performance is limited by the ini-
tial retrieval step.
</p>
<p>which no answer entities with a good confidence4
</p>
<p>were identified, e.g., date answers, and randomly
split the rest into training, development and test
sets, with 2711 questions in total. Similarly to
the TREC QA experiments, we extracted textual
evidence using Bing Web Search API, by retriev-
ing top 50 relevant documents, extracting the main
content blocks, and splitting them into sentences.
We applied the TagMe entity linker to the ex-
tracted sentences, and considered all entities of
mentions with the confidence score above the 0.2
threshold as candidate answers. For candidate en-
tities we also retrieved relevant KB triples, such as
entity types and descriptions, which extended the
original pool of evidences.
</p>
<p>Table 5 summarizes the results of EviNets and
some baseline methods on the created Yahoo! An-
swers dataset. As we can see, knowledge base data
is not enough to answer most of these questions,
and a state-of-the-art KBQA system Aqqu gets
only 0.116 precision. Adding textual data helps
significantly, and Text2KB improves the precision
to 0.17, which roughly matches the results of the
AskMSR system, that ranks candidate entities by
their popularity in the retrieved documents. Using
text along with KB evidence gave higher perfor-
mance metrics, boosting F1 from 0.271 to 0.291.
EviNets significantly improves over the baseline
approaches, beating AskMSR by 28% and KV
MemN2N by almost 80% in F1 score.
</p>
<p>4 Related Work
</p>
<p>The success of deep neural network architectures
in computer vision and NLP applications mo-
</p>
<p>4A minimum ρ score of 0.2 from TagMe was required.
</p>
<p>tivated researchers to investigate applying these
techniques for answer sentence selection, eval-
uated on TREC QA (Wang et al., 2007), Wik-
iQA (Yang et al., 2015) and other datasets. A num-
ber of models proposed in recent years explore dif-
ferent ways of matching questions and answer sen-
tences (He and Lin, 2016; Yang et al., 2016; Rao
et al., 2016). Our EviNets architecture allows to
easily plug these sentence matching networks into
the evidence matching module, and provides the
aggregation layer, which helps to make a decision
based on all available information.
</p>
<p>Our evidence representation module is based on
the ideas of memory networks (Sukhbaatar et al.,
2015; Kumar et al., 2015; Miller et al., 2016),
which also embed relevant information into a vec-
tor space. However, they use soft attention mecha-
nism to retrieve the memories, and do not use links
from memories to the corresponding answer can-
didates, which means that all relevant information
is squeezed into a fixed dimensional vector. This
limitation has been partially addressed in Wang
et al. (2016) and Henaff et al. (2016), which accu-
mulate evidence for each answer separately using
a recurrent neural network. In contrast, the evi-
dence aggregation in our EviNets model uses mul-
tiple different features, which is more flexible and
can be extended with other signals.
</p>
<p>5 Conclusions
</p>
<p>We presented EviNets, a neural network for ques-
tion answering, which encodes and aggregates
multiple evidence signals to select answers. Ex-
periments on TREC QA, WikiMovies and Ya-
hoo! Answers datasets demonstrate that EviNets
can be trained end-to-end to use both the available
textual and knowledge base information. EviNets
improves over the baselines, both in cases when
there are many or just a few relevant pieces of
evidence, by helping build an aggregate picture
and distinguish between candidates, mentioned to-
gether in a relevant memory, as is the case for
WikiMovies dataset. The results of our experi-
ments also demonstrate that EviNets can incor-
porate signals from different data sources, e.g.,
adding KB triples helps to improve the perfor-
mance over text-only setup. As a limitation of this
work and a direction for future research, EviNets
could be extended to support dynamic evidence
retrieval, which would allow retrieving additional
answer candidates and evidence as needed.
</p>
<p>303</p>
<p />
</div>
<div class="page"><p />
<p>References
Hannah Bast and Elmar Haussmann. 2015. More accu-
</p>
<p>rate question answering on freebase. In Proceedings
of the 24th ACM International on Conference on In-
formation and Knowledge Management.
</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing.
</p>
<p>Eric Brill, Susan Dumais, and Michele Banko. 2002.
An analysis of the askmsr question-answering sys-
tem. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing-
Volume 10.
</p>
<p>Charles LA Clarke, Gordon V Cormack, and Thomas R
Lynam. 2001. Exploiting redundancy in question
answering. In Proceedings of the 24th annual in-
ternational ACM SIGIR conference.
</p>
<p>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge
vault: A web-scale approach to probabilistic knowl-
edge fusion. In Proceedings of the 20th ACM
SIGKDD.
</p>
<p>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In Proceedings of the
20th ACM SIGKDD international conference on
Knowledge discovery and data mining.
</p>
<p>Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
on-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of the 19th ACM
ICKM.
</p>
<p>Hua He and Jimmy Lin. 2016. Pairwise word inter-
action modeling with deep neural networks for se-
mantic similarity measurement. In Proceedings of
NAACL-HLT .
</p>
<p>Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2016. Tracking the world
state with recurrent entity networks. arXiv preprint
arXiv:1612.03969 .
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.
</p>
<p>Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. 2015. Ask
me anything: Dynamic memory networks for natu-
ral language processing. CoRR, abs/1506.07285 .
</p>
<p>Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for
directly reading documents. arXiv preprint
arXiv:1606.03126 .
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vec-
tors for word representation. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
http://www.aclweb.org/anthology/D14-1162.
</p>
<p>Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-
contrastive estimation for answer selection with
deep neural networks. In Proceedings of the 25th
ACM International on Conference on Information
and Knowledge Management.
</p>
<p>Denis Savenkov and Eugene Agichtein. 2016. When
a knowledge base is not enough: Question answer-
ing over knowledge bases with external text data. In
Proceedings of the 39th ACM SIGIR conference.
</p>
<p>Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems. pages
2440–2448.
</p>
<p>Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai,
Jingjing Liu, and Ming-Wei Chang. 2015. Open do-
main question answering via semantic enrichment.
In Proceedings of the 24th International Conference
on World Wide Web.
</p>
<p>C Tsai, Wen-tau Yih, and C Burges. 2015. Web-based
question answering: Revisiting askmsr. Techni-
cal report, Technical Report MSR-TR-2015-20, Mi-
crosoft Research.
</p>
<p>Mengqiu Wang, Noah A Smith, and Teruko Mita-
mura. 2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP-CoNLL.
volume 7, pages 22–32.
</p>
<p>Xun Wang, Katsuhito Sudoh, Masaaki Nagata, To-
mohide Shibata, Kawahara Daisuke, and Kuro-
hashi Sadao. 2016. Reading comprehension us-
ing entity-based memory network. arXiv preprint
arXiv:1612.03551 .
</p>
<p>Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,
and Dongyan Zhao. 2016. Question answering on
freebase via relation extraction and textual evidence.
arXiv preprint arXiv:1603.00957 .
</p>
<p>Liu Yang, Qingyao Ai, Jiafeng Guo, and W Bruce
Croft. 2016. anmm: Ranking short answer texts
with attention-based neural matching model. In Pro-
ceedings of the 25th ACM International on Confer-
ence on Information and Knowledge Management.
</p>
<p>Yi Yang, Scott Wen-tau Yih, and Chris Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing.
</p>
<p>Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He,
and Jianfeng Gao. 2015. Semantic parsing via
staged query graph generation: Question answering
with knowledge base. In Proceedings of the 53rd
Annual Meeting of the ACL.
</p>
<p>304</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 305–310
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2048
</p>
<p>Pocket Knowledge Base Population
</p>
<p>Travis Wolfe Mark Dredze Benjamin Van Durme
Human Language Technology Center of Excellence
</p>
<p>Johns Hopkins University
</p>
<p>Abstract
</p>
<p>Existing Knowledge Base Population
methods extract relations from a closed
relational schema with limited coverage,
leading to sparse KBs. We propose Pocket
Knowledge Base Population (PKBP), the
task of dynamically constructing a KB
of entities related to a query and find-
ing the best characterization of relation-
ships between entities. We describe
novel Open Information Extraction meth-
ods which leverage the PKB to find infor-
mative trigger words. We evaluate using
existing KBP shared-task data as well as
new annotations collected for this work.
Our methods produce high quality KBs
from just text with many more entities and
relationships than existing KBP systems.
</p>
<p>1 Introduction
</p>
<p>Much of human knowledge is contained in text
in books, encyclopedias, the internet, and writ-
ten communications. Building knowledge bases
to store, search, and reason over this information
is an important problem in natural language un-
derstanding. A lot of work in knowledge base
population (KBP) has focused on the NIST Text
Analysis Conference track of the same name, and
specifically the slot filling task. Slot Filling (SF)
defines a relational schema similar to Wikipedia
infoboxes. SF KBP systems extract facts from text
corresponding to an entity called the query.
</p>
<p>This work addresses two issues concerning SF
KBP. First, the SF schema has strict semantics for
the relations which can be extracted, and thus no
SF relation can be extracted for most related enti-
ties, leading to sparse KBs. Second, because SF
has a small static schema, most research has fo-
cused on batch processing for a single schema,
</p>
<p>limiting downstream usefulness. This means KBs
built by slot filling have limited applicability in
some real world settings of interest.
</p>
<p>We address these issues by proposing Pocket
Knowledge Base Population. Pocket KBs (PKBs)
are dense entity-centric KBs dynamically con-
structed for a query. In both SF and pocket KBP,
a query is an entity of interest and a document
mentioning that entity. However, in PKB the pri-
mary goal is to populate the KB with nodes for
all entities related to the query, irrespective of any
prior beliefs about relations. PKB edges store rep-
resentations of mentions referring to the entities
connected by that edge, and thus may better serve
downstream tasks which don’t perfectly align to a
particular schema.
</p>
<p>We describe a PKBP system which builds KBs
from text corpora. This includes unsupervised
methods for finding related entities and mentions
of them and the query with accuracies of 89.5 and
93.1 respectively when evaluated on SF queries.
We also propose novel entity-centric Open IE
(Banko et al., 2007) methods for characterizing the
relationship between entities which perform twice
as well as a syntactically-informed baseline. Our
contributions also include a comparison between
pocket and SF KBs constructed on SF queries,
showing our KBs are multiple times larger while
remaining high quality. We make our system pub-
licly available.1
</p>
<p>2 Pocket Knowledge Base Population
</p>
<p>The defining characteristic of pocket KBs is they
are small, entity-centric, and dynamically gener-
ated according to a query. Most work in KBP is
centered around batch processing with a relation
extractor, whereas PKBP is based on entity men-
</p>
<p>1https://hub.docker.com/r/hltcoe/pocket-knowledge-base-
population
</p>
<p>305</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2048">https://doi.org/10.18653/v1/P17-2048</a></div>
</div>
<div class="page"><p />
<p>Figure 1: High level steps of the PKB construction process. Example PKBs can be found in Table 3. The
first two steps of initial search and related entities are described in §3.1, the third step of joint searching
in §3.2, and finally extracting triggers in §3.3.
</p>
<p>tion search and ad-hoc trigger extraction. PKBs
resemble a hub and spoke graph where the hub is
the query and nodes on the outside are related en-
tities. The spokes represent mentions proving that
an entity is related to the query. Traversing this
graph by crossing a spoke is akin to building a new
PKB with that related entity as the new hub.
</p>
<p>PKBs are aids for “knowledge-based” search
over a document collection. KBs are useful for
question answering (Yao, 2014) and PKBs serve
this goal because related entities are good answer
candidates for questions about a query. SF KBs
are specialized to the case where you know the
questions ahead of time, like “Where does Mary
work?” and “Who is ACME’s parent company?”.
PKBs offer a set of answers for queries with clues
as to their relationship, like Marc Bolland and
Stuart Rose are related because of events de-
scribed using the word replaced. KBs are also
useful information retrieval (IR) tools (Dietz and
Schuhmacher, 2015) for human guided corpus ex-
ploration. PKBs serve this goal by providing
ranked lists of related entities and over the men-
tions describing their relationship to the query.
</p>
<p>We describe a system which achieves the goals
of PKBP using low-resource and unsupervised
methods. We discuss how PKBs are built in §3
and evaluate their quality in §4 on SF queries.
</p>
<p>3 Construction
</p>
<p>There are three steps to PKB construction:
§3.1 discovering related entities, §3.2 finding men-
tions of the query and related entities, and §3.3 ex-
tracting trigger word explanations.
</p>
<p>3.1 Discovering Related Entities
Candidate related entities are collected by search-
ing for mentions of the query and taking other
mentions which appear in the results. This process
is similar to cross document entity coreference and
</p>
<p>we adopt the vector space model (Bagga and Bald-
win, 1998). First, triage features are used to lo-
cate sentences in an inverted index, including the
mention headword and all word unigrams and bi-
grams (case sensitive and insensitive).
</p>
<p>Next, we use context features: a word uni-
gram tf-idf vector. We implement a compromise
between Cucerzan (2007) (used sentences before,
after, and containing a mention) and Bagga and
Baldwin (1998) (used any sentence in a corefer-
ence chain). Instead of running a coreference re-
solver, we use the high-precision heuristic of link-
ing mentions with the same headword and NER
type. Terms are weighted as 21+d where d is the
distance in sentences to the nearest mention.
</p>
<p>Our attribute features are a generalization of
Mann and Yarowsky (2003). They train a few
ad-hoc relation extractors like birth year and
occupation from seed facts. Their extractions
provide high-precision signal for merging entity
mentions. We found extracting all NNP* or capi-
talized JJ* words within 4 edges in a dependency
tree was less sparse, requires no seeds, and pro-
duced similar quality attributes. We union these
attributes across mentions found by the headword
and NER type coreference heuristic to build a
fine-grain tf-idf vector. We use the same 21+d
re-weighting for attributes, except where d is the
distance in dependency edges to the entity men-
tion head. The closest attributes are descriptors
within a noun phrase like HEAD-nn-Dr.. We in-
clude the NER type of the headword to distinguish
between attributes like PERSON-nn-American
and ORGANIZATION-nn-American.
</p>
<p>Given the triage features t(m), context features
c(m), and attribute features a(m), we search for
mentions m which maximize
</p>
<p>(1 + αt cos θt)(1 + αc cos θc)(1 + αaθa)
</p>
<p>where cos θt is the cosine similarity between
</p>
<p>306</p>
<p />
</div>
<div class="page"><p />
<p>t(mquery) and t(m). We only consider the sub-
set of mentions that have cos θt &gt; 0, which can be
efficiently retrieved via an inverted index.
</p>
<p>Any mention with a score higher than τ is con-
sidered coreferent with the query. We extract men-
tions in the same sentences as the query as candi-
date related entities if they have an NER type of
PER, ORG, or LOC. We link candidate mentions
against entities in the PKB using the same coref-
erence score used to retrieve query mentions. If a
candidate’s best link has a score s &lt; τ , we pro-
mote it to an entity and add it to the PKB with
probability 1− sτ .2
</p>
<p>3.2 Joint Linking of Related Entities
</p>
<p>At this point there are on the order of 100 mentions
of the query and 20 to 50 related entities.3 For
each entity, we perform a joint search for it and the
query. These entity co-occurrences will form the
spokes in the PKB and be used to characterize the
relationship between and relatedness to the query.
</p>
<p>Joint entity searches are similar to single-
mention searches in §3.1 with two differences.
First, instead of having a single mention to com-
pute feature vectors from, there are multiple. Fea-
ture vectors for entities are built up from men-
tions, where the weight of a mention w(m) = ρb
</p>
<p>for ρ ∈ (0, 1) and b is how many mentions were
linked before m. Second, we are scoring mention
pairs (with both mentions in the same sentence) as
the geometric mean of the coreference scores of
both links. The coreference score function does
not need to change, but the triage step does: we
only consider sentences which have cos θt &gt; 0 for
both the query and the related entity and use the
same τ . Entity relatedness is a function of how of-
ten entities are mentioned together. We modeled it
as the sum of the joint entity linking probabilities,
where the probability of a link is logit−1( sτ ).
</p>
<p>3.3 Trigger Word Analysis
</p>
<p>At this stage we have found on the order of 2 to 20
sentences which mention the query and a related
entity which will be used to determine the rela-
tion between them. There is work on rule-based
(Banko et al., 2007; Fader et al., 2011; Angeli
et al., 2015), supervised (Mausam et al., 2012),
</p>
<p>2Mentions with a score near τ may be coreferent, so we
prefer low scoring mentions to avoid over-splitting entities.
</p>
<p>3These values depend on the query (which are more or
less rare in a corpus) and pruning thresholds (for our experi-
ments we stop at 100 query mentions)
</p>
<p>and distantly-supervised (Mintz et al., 2009) meth-
ods for characterizing relations in text. Our
method is similar to distant supervision, where a
KB of known facts is used to infer how relations
are expressed, but we use supervision from the KB
being constructed. We cast the problem of charac-
terizing a relation as a search for trigger words.
We state our priors on trigger words and condition
on the data to find likely triggers.
</p>
<p>Predicate (triggers) and arguments are syntac-
tically close together. Assuming the related en-
tity mention heads are arguments, we compute the
probability that these two random walks in a de-
pendency tree end up at the same token. This
serves as a weak syntactically informed prior.
</p>
<p>Information is conveyed as a surprisal under a
background distribution (codebook). We compute
a unigram distribution over words which are likely
under our syntactic prior for triggers (conditioned
on the NER type of the two arguments). We use
this marginal distribution as a codebook. We di-
vide out this codebook probability in every pair of
related entity mentions in the PKB giving a cost in
bits (log probability ratio) of each trigger word.
</p>
<p>Repetition indicates importance. We sum the
costs for each trigger across sentences. We
weaken this assumption by averaging the max and
the sum for each trigger for the final score.
</p>
<p>This process yields a score for every trigger
word, and we use the top k triggers to characterize
the relationship between entities. For each trigger
we keep, we also maintain provenance informa-
tion for mentions using a given trigger.
</p>
<p>4 Experiments
</p>
<p>We use the TAC SF13 query entities to evaluate
our methods; 50 person and 50 organization en-
tities are used as queries to construct 100 PKBs.
70 of the 100 query entities were NIL (26/60 PER
and 44/50 ORG), meaning that they do not appear
in the TAC KB, though our methods aren’t in prin-
ciple sensitive to this because they create entities
on the fly. We use annotated versions of Giga-
word 5 (Parker et al., 2011; Ferraro et al., 2014)
and English Wikipedia (February 24, 2016 dump)
to construct our PKBs.4 We use Amazon Mechan-
ical Turk workers as annotators. We generated our
PKBs with τ = 15, ρ = 0.5, αt = 40, αc = 20,
and αa = 10. These constants were tuned by hand
</p>
<p>4We do not use the coreference annotations provided by
Annotated Gigaword, only the features described in §3.1.
</p>
<p>307</p>
<p />
</div>
<div class="page"><p />
<p>and are not sensitive to small changes. We take a
subset of the PKB which covers the 15 most re-
lated entities and the one-best trigger for each. We
call these “explanations” where each is a sentence
with three labels: a) a mention of the query mq,
b) a mention of the a related entity mr, and c) a
trigger word t.
</p>
<p>Entity Linking and Relatedness For each ex-
planation, we ask: COREF: Does the query men-
tion refer to the same entity as mq? RELATED: Is
the query entity meaningfully related to the ref-
erent of mr? These annotations are not done by
the same annotators to avoid confirmation bias.
Worried annotators might be lulled into thinking
all COREF instances were true, we made the task
ternary by adding an intruder entity (randomly
drawn from SF13 queries). Annotators were
shown mq and could choose coreference with the
query, the intruder, or neither.5 We drop annota-
tions from annotators who chose an intruder6 be-
cause we know these to be incorrect, and compute
accuracy as proportion of the remaining annota-
tions which chose the query.
</p>
<p>RELATED was posed as a binary task of whether
mr is more related to the query or the intruder
(without highlighting mq). In positive cases, the
annotator should observe that sentence shown con-
tains a mention of the query entity and explains
why they are related. The results are in Table 1.
</p>
<p>Our system retrieves coreferent and related
mentions with high accuracy. For coreference,
mistakes usually happen when there is signifi-
cant lexical overlap but some distinguishing fea-
ture that proves too subtle for our system to doubt
the match, like Midwest High Speed Rail Asso-
ciation vs U.S. High Speed Rail Association or
[English] Nationwide Building Society vs Irish
Nationwide Building Society.
</p>
<p>For relatedness, the biggest source of errors are
news organizations listed as related entities be-
cause it is common to see sentences like “Mo-
hammed Sobeih, Moussa’s deputy, told The As-
sociated Press on Monday that...”. Future work
might address this problem by using normalized
measures of statistical relatedness like PMI rather
than raw co-occurrence counts.
</p>
<p>Trigger Words To evaluate the informativeness
of chosen triggers, we present annotators withmq,
</p>
<p>5The order of the intruder and the query were randomized.
6This affected 6.1% of COREF annotations.
</p>
<p>PER ORG All
COREF 94.6 91.5 93.1
RELATED 90.7 88.2 89.5
COREF and RELATED 86.6 80.9 83.9
</p>
<p>Table 1: PKB entity accuracy.
</p>
<p>System Intruder Neither
Person 29.4 12.4 58.2
Organization 29.1 17.3 53.7
All 29.2 14.7 56.1
</p>
<p>Table 2: Related entity trigger identification.
</p>
<p>mr, and two potential trigger words highlighted.
One trigger is chosen according to §3.3 and the
other is an NN*|VB*|JJ*|RB* word in the pro-
jection of the dependency node dominating both
entities.7 The annotator may choose either trigger
as a good characterization of the situation involv-
ing mq and mr, or label neither as sufficient. Note
that this baseline is strong: it shares the entity link-
ing (§3.2), trigger sentence selection (§3.3), and
dependency parse tree as our system. We report
the results in Table 2.
</p>
<p>Our method is chosen about twice as often as
a syntactically informed baseline, but fails to find
a high quality trigger word more than half of the
time. Some mistakes are caused by rare but oft-
repeated words like “50” in: “Bolland, 50, ... will
replace Briton Stuart Rose”. “50” has nothing
to do with the relationship between Bolland and
Rose, but it’s repeated in 4 sentences about both
of them, a stylistic coincidence our system can-
not ignore. In other cases there is no word in situ
which can explain entities’ relatedness, like “...
the day after Wimbledon concludes, Montcourt
must serve a five-week ban and ...”. The author
and the reader can likely infer that Montcourt com-
peted at Wimbledon, but this fact is not explicitly
committed to, limiting our systems ability to ex-
tract a trigger.
</p>
<p>Related Entities vs Slot Fillers There is no
fair way to evaluate systems without a com-
mon schema, but we offer some extraction statis-
tics. On SF13 queries our system generated 17.6
relevant entities/query,8 each having 4.6 trigger
words/pair, 2.1 mentions/trigger word, and 9.8
</p>
<p>7If no nodes match this, we walk up the tree until we find
a node which has at least one allowed descendant.
</p>
<p>8This is given a cap of 20 relevant entities per query to
avoid a skewed average and keep construction time down.
</p>
<p>308</p>
<p />
</div>
<div class="page"><p />
<p>Query Entity Related Entity Triggers
Marc Bolland PER Dalton Philips PER appointed, departure, following, move
Marc Bolland PER Stuart Rose PER replace, 50, Briton
Marc Bolland PER Marks &amp; Spencer ORG departure, CEO, become, following
Henry Olonga PER Givemore Makoni PER club, president, done, played
Henry Olonga PER England LOC cricketer, asylum, hiding, quit
Henry Olonga PER Harare LOC hiding, armbands, wore
Mohammad Oudeh PER Munich LOC massacre, briefed, defended
Mohammad Oudeh PER Fatah Revolutionary Council ORG faction, belonged, return
Mohammad Oudeh PER Gaza Strip LOC allows, asked, host
A123 Systems LLC ORG Fisker ORG supplier, struck, recall, owns
A123 Systems LLC ORG Watertown, Massachusetts LOC produces, batteries, company
A123 Systems LLC ORG Obama PER plant, opening, Granholm
United Steelworkers of America ORG Curt Brown PER spokesman, rejected, contracts
United Steelworkers of America ORG Wayne Fraser PER negotiator, spokesman, union
United Steelworkers of America ORG Jerry Fallos PER boss, broke, shut, local
BNSF ORG Santa Fe LOC asked, vote
BNSF ORG Chapman ORG venture, help, transition, joint
BNSF ORG Robert Krebs PER Burlington, chairman
</p>
<p>Table 3: Examples of slices of PKBs for the three most related entities for six queries and the best triggers
for each pair. Supporting sentences for related entities and trigger words are not shown.
</p>
<p>mentions/pair. In extractions from all systems in
the SF13 evaluation (pooling answers, filtering out
incorrect), they filled 6.0 slots/query with 14.2
fillers/query and 38.3 mentions/query as prove-
nance. Some slots have string-valued fillers, but
many could be related entities in the PKB sense.
In these cases, we found 2.2 entities/query over-
lapping, 1.7 fillers not in their corresponding PKB
and 10.8 related entities which weren’t fillers.
</p>
<p>5 Related Work
</p>
<p>Blanco and Zaragoza (2010) study the informa-
tion retrieval problem of finding support sentences
which explain the relationship between a query
and an entity, which is similar to this work. Our
work addresses two new aspects of this problem:
1) how to automatically find related entities, which
are assumed given in that work and 2) how to
find the salient parts of support sentences (trigger
words) by aggregating evidence across sentences.
</p>
<p>This work shares goals with Dalton and Di-
etz (2013) and Dietz and Schuhmacher (2015),
who create “knowledge sketches”: distributions
over documents, entities, and relations related to
a query. The primary difference is that our work
creates a KB instead of returning results from an
existing one. They use Freebase for relations and
Wikipedia for anchor text and links. Our approach
uses parsed and NER tagged text.
</p>
<p>Open vocabulary characterization of entities
was investigated by Raghavan et al. (2004). They
found intersecting entity language models yields
</p>
<p>common descriptors. Their notion of similarity
(e.g. Ronald Reagan and Richard Nixon are
both presidents) is different from our notion of re-
latedness (e.g. Alexander Haig and Princeton,
NJ are related via Meredith – Haig’s sister).
</p>
<p>Finally other work has used Open IE for SF
KBP. Soderland et al. (2013) and Finin et al.
(2015) manually created a mapping between the
Ollie (Mausam et al., 2012) and SF schemas. An-
geli et al. (2015) perform OpenIE and then map
between their schema and SF with PMI2.
</p>
<p>6 Conclusion
</p>
<p>We propose Pocket Knowledge Base Popula-
tion for dynamically building dense entity-centric
KBs. We evaluate our methods on SF queries and
find high accuracies of related entity discovery and
coreference. We propose novel Open Informa-
tion Extraction methods which leverage the PKB
to identify trigger words and show they are effec-
tive at explaining related entities. In future work
we hope to use PKBs for tasks like QA and IR.
</p>
<p>Acknowledgments
</p>
<p>This research was supported by the Human Lan-
guage Technology Center of Excellence (HLT-
COE) and Bloomberg L.P. The views and conclu-
sions contained in this publication are those of the
authors.
</p>
<p>309</p>
<p />
</div>
<div class="page"><p />
<p>References
Gabor Angeli, Melvin Jose Johnson Premkumar,
</p>
<p>and Christopher D. Manning. 2015. Leverag-
ing linguistic structure for open domain informa-
tion extraction. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 344–354.
http://www.aclweb.org/anthology/P15-1034.
</p>
<p>Amit Bagga and Breck Baldwin. 1998. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 36th
Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Con-
ference on Computational Linguistics - Volume
1. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’98, pages 79–85.
https://doi.org/10.3115/980845.980859.
</p>
<p>Michele Banko, Michael J. Cafarella, Stephen
Soderland, Matt Broadhead, and Oren Et-
zioni. 2007. Open information extraction from
the web. In Proceedings of the 20th Interna-
tional Joint Conference on Artifical Intelligence.
Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA, IJCAI’07, pages 2670–2676.
http://dl.acm.org/citation.cfm?id=1625275.1625705.
</p>
<p>Roi Blanco and Hugo Zaragoza. 2010. Finding sup-
port sentences for entities. In Proceedings of the
33rd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
ACM, New York, NY, USA, SIGIR ’10, pages 339–
346. https://doi.org/10.1145/1835449.1835507.
</p>
<p>Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL). Association for Computational
Linguistics, Prague, Czech Republic, pages 708–
716. http://www.aclweb.org/anthology/D/D07/D07-
1074.
</p>
<p>Jeffrey Dalton and Laura Dietz. 2013. Con-
structing query-specific knowledge bases. In
Proceedings of the 2013 Workshop on Auto-
mated Knowledge Base Construction. ACM, New
York, NY, USA, AKBC ’13, pages 55–60.
https://doi.org/10.1145/2509558.2509568.
</p>
<p>Laura Dietz and Michael Schuhmacher. 2015. An in-
terface sketch for queripidia: Query-driven knowl-
edge portfolios from the web. In Krisztian Ba-
log, Jeffrey Dalton, Antoine Doucet, and Yusra
Ibrahim, editors, Proceedings of the Eighth Work-
shop on Exploiting Semantic Annotations in Infor-
mation Retrieval, ESAIR 2015, Melbourne, Aus-
tralia, October 23, 2015. ACM, pages 43–46.
https://doi.org/10.1145/2810133.2810145.
</p>
<p>Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Strouds-
burg, PA, USA, EMNLP ’11, pages 1535–1545.
http://dl.acm.org/citation.cfm?id=2145432.2145596.
</p>
<p>Francis Ferraro, Max Thomas, Matthew R. Gorm-
ley, Travis Wolfe, Craig Harman, and Benjamin
Van Durme. 2014. Concretely annotated corpora.
In The NIPS 2014 AKBC Workshop.
</p>
<p>Tim Finin, Dawn Lawrie, Paul McNamee, James May-
field, Douglas Oard, Nanyun Peng, Ning Gao, Yiu-
Chang Lin, Josh MacLin, and Tim Dowd. 2015. Hlt-
coe participation in tac kbp 2015: Cold start and
tedl. In Text Analytics Conference (TAC).
</p>
<p>Gideon S. Mann and David Yarowsky. 2003. Unsu-
pervised personal name disambiguation. In Pro-
ceedings of the Seventh Conference on Natural
Language Learning at HLT-NAACL 2003 - Vol-
ume 4. Association for Computational Linguistics,
Stroudsburg, PA, USA, CONLL ’03, pages 33–40.
https://doi.org/10.3115/1119176.1119181.
</p>
<p>Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, EMNLP-CoNLL ’12, pages 523–534.
http://dl.acm.org/citation.cfm?id=2390948.2391009.
</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP:
Volume 2-Volume 2. Association for Computational
Linguistics, pages 1003–1011.
</p>
<p>Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edi-
tion, linguistic data consortium. Technical report,
Technical report, Technical Report. Linguistic Data
Consortium, Philadelphia.
</p>
<p>Hema Raghavan, James Allan, and Andrew McCallum.
2004. An exploration of entity models, collective
classification and relation description .
</p>
<p>Stephen Soderland, John Gilmer, Robert Bart, Oren Et-
zioni, and Daniel S Weld. 2013. Open information
extraction to kbp relations in 3 hours. In TAC.
</p>
<p>Xuchen Yao. 2014. Feature-driven Question Answer-
ing with Natural Language Alignment. Ph.D. thesis.
</p>
<p>310</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 311–316
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2049
</p>
<p>Answering Complex Questions Using Open Information Extraction
</p>
<p>Tushar Khot and Ashish Sabharwal and Peter Clark
Allen Institute for Artificial Intelligence, Seattle, WA, U.S.A.
</p>
<p>{tushark,ashishs,peterc}@allenai.org
</p>
<p>Abstract
</p>
<p>While there has been substantial progress
in factoid question-answering (QA), an-
swering complex questions remains chal-
lenging, typically requiring both a large
body of knowledge and inference tech-
niques. Open Information Extraction
(Open IE) provides a way to generate
semi-structured knowledge for QA, but to
date such knowledge has only been used
to answer simple questions with retrieval-
based methods. We overcome this limita-
tion by presenting a method for reasoning
with Open IE knowledge, allowing more
complex questions to be handled. Using a
recently proposed support graph optimiza-
tion framework for QA, we develop a new
inference model for Open IE, in particu-
lar one that can work effectively with mul-
tiple short facts, noise, and the relational
structure of tuples. Our model signifi-
cantly outperforms a state-of-the-art struc-
tured solver on complex questions of vary-
ing difficulty, while also removing the re-
liance on manually curated knowledge.
</p>
<p>1 Introduction
</p>
<p>Effective question answering (QA) systems have
been a long-standing quest of AI research. Struc-
tured curated KBs have been used successfully for
this task (Berant et al., 2013; Berant and Liang,
2014). However, these KBs are expensive to build
and typically domain-specific. Automatically con-
structed open vocabulary (subject; predicate; ob-
ject) style tuples have broader coverage, but have
only been used for simple questions where a single
tuple suffices (Fader et al., 2014; Yin et al., 2015).
</p>
<p>Our goal in this work is to develop a QA system
that can perform reasoning with Open IE (Banko
</p>
<p>et al., 2007) tuples for complex multiple-choice
questions that require tuples from multiple sen-
tences. Such a system can answer complex ques-
tions in resource-poor domains where curated
knowledge is unavailable. Elementary-level sci-
ence exams is one such domain, requiring com-
plex reasoning (Clark, 2015). Due to the lack of
a large-scale structured KB, state-of-the-art sys-
tems for this task either rely on shallow reasoning
with large text corpora (Clark et al., 2016; Cheng
et al., 2016) or deeper, structured reasoning with
a small amount of automatically acquired (Khot
et al., 2015) or manually curated (Khashabi et al.,
2016) knowledge.
</p>
<p>Consider the following question from an Alaska
state 4th grade science test:
</p>
<p>Which object in our solar system reflects
light and is a satellite that orbits around
one planet? (A) Earth (B) Mercury (C)
the Sun (D) the Moon
</p>
<p>This question is challenging for QA systems be-
cause of its complex structure and the need for
multi-fact reasoning. A natural way to answer it
is by combining facts such as (Moon; is; in the
solar system), (Moon; reflects; light), (Moon; is;
satellite), and (Moon; orbits; around one planet).
</p>
<p>A candidate system for such reasoning,
and which we draw inspiration from, is the
TABLEILP system of Khashabi et al. (2016).
TABLEILP treats QA as a search for an optimal
subgraph that connects terms in the question and
answer via rows in a set of curated tables, and
solves the optimization problem using Integer
Linear Programming (ILP). We similarly want
to search for an optimal subgraph. However, a
large, automatically extracted tuple KB makes
the reasoning context different on three fronts:
(a) unlike reasoning with tables, chaining tuples
is less important and reliable as join rules aren’t
</p>
<p>311</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2049">https://doi.org/10.18653/v1/P17-2049</a></div>
</div>
<div class="page"><p />
<p>available; (b) conjunctive evidence becomes
paramount, as, unlike a long table row, a single
tuple is less likely to cover the entire question;
and (c) again, unlike table rows, tuples are noisy,
making combining redundant evidence essen-
tial. Consequently, a table-knowledge centered
inference model isn’t the best fit for noisy tuples.
</p>
<p>To address this challenge, we present a new
ILP-based model of inference with tuples, im-
plemented in a reasoner called TUPLEINF. We
demonstrate that TUPLEINF significantly outper-
forms TABLEILP by 11.8% on a broad set of over
1,300 science questions, without requiring manu-
ally curated tables, using a substantially simpler
ILP formulation, and generalizing well to higher
grade levels. The gains persist even when both
solvers are provided identical knowledge. This
demonstrates for the first time how Open IE based
QA can be extended from simple lookup questions
to an effective system for complex questions.
</p>
<p>2 Related Work
</p>
<p>We discuss two classes of related work: retrieval-
based web question-answering (simple reason-
ing with large scale KB) and science question-
answering (complex reasoning with small KB).
</p>
<p>Web QA: There exist several systems for
retrieval-based Web QA problems (Ferrucci et al.,
2010; Brill et al., 2002). While structured KBs
such as Freebase have been used in many (Berant
et al., 2013; Berant and Liang, 2014; Kwiatkowski
et al., 2013), such approaches are limited by the
coverage of the data. QA systems using semi-
structured Open IE tuples (Fader et al., 2013,
2014; Yin et al., 2015) or automatically extracted
web tables (Sun et al., 2016; Pasupat and Liang,
2015) have broader coverage but are limited to
simple questions with a single query.
</p>
<p>Science QA: Elementary-level science QA tasks
require reasoning to handle complex questions.
Markov Logic Networks (Richardson and Domin-
gos, 2006) have been used to perform probabilistic
reasoning over a small set of logical rules (Khot
et al., 2015). Simple IR techniques have also
been proposed for science tests (Clark et al., 2016)
and Gaokao tests (equivalent to the SAT exam in
China) (Cheng et al., 2016).
</p>
<p>The work most related to TUPLEINF is the
aforementioned TABLEILP solver. This approach
focuses on building inference chains using man-
</p>
<p>ually defined join rules for a small set of curated
tables. While it can also use open vocabulary tu-
ples (as we assess in our experiments), its efficacy
is limited by the difficulty of defining reliable join
rules for such tuples. Further, each row in some
complex curated tables covers all relevant contex-
tual information (e.g., each row of the adaptation
table contains (animal, adaptation, challenge, ex-
planation)), whereas recovering such information
requires combining multiple Open IE tuples.
</p>
<p>3 Tuple Inference Solver
</p>
<p>We first describe the tuples used by our solver. We
define a tuple as (subject; predicate; objects) with
zero or more objects. We refer to the subject, pred-
icate, and objects as the fields of the tuple.
</p>
<p>3.1 Tuple KB
</p>
<p>We use the text corpora (S) from Clark
et al. (2016) to build our tuple KB. S contains
5 ⇥ 1010 tokens (280 GB of plain text) extracted
from Web pages as well as around 80,000 sen-
tences from various domain-targeted sources. For
each test set, we use the corresponding train-
ing questions Qtr to retrieve domain-relevant sen-
tences from S. Specifically, for each multiple-
choice question (q, A) 2 Qtr and each choice
a 2 A, we use all non-stopword stemmed tokens
in q and a as an ElasticSearch1 query against S. We
take the top 200 hits, run Open IE v4,2 and aggre-
gate the resulting tuples over all a 2 A and over
all questions in Qtr to create the tuple KB (T ).3
</p>
<p>3.2 Tuple Selection
</p>
<p>Given a multiple-choice question qa with question
text q and answer choices A={ai}, we select the
most relevant tuples from T and S as follows.
</p>
<p>Selecting from Tuple KB: We use an inverted
index to find the 1,000 tuples that have the most
overlapping tokens with question tokens tok(qa).4
</p>
<p>We also filter out any tuples that overlap only with
tok(q) as they do not support any answer. We
compute the normalized TF-IDF score by treating
the question, q, as a query and each tuple, t, as a
</p>
<p>1https://www.elastic.co/products/elasticsearch
2http://knowitall.github.io/openie
3Available at http://allenai.org/data.html
4All tokens are stemmed and stop-word filtered.
</p>
<p>312</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: An example support graph linking a
question (top), two tuples from the KB (colored)
and an answer option (nitrogen).
</p>
<p>document:
</p>
<p>tf(x, q) = 1 if x 2 q, 0 otherwise
idf(x) = log (1 + N/nx)
</p>
<p>tf-idf(t, q) =
X
</p>
<p>x2t\q
idf(x)
</p>
<p>where N is the total number of tuples in the KB
and nx is the number of tuples containing x. We
normalize the tf-idf score by the number of tokens
in t and q, and take the 50 top-scoring tuples Tqa .
</p>
<p>On-the-fly tuples from text: To handle ques-
tions from new domains not covered by the train-
ing set, we extract additional tuples on the fly from
S (similar to Sharma et al. (2015)). We perform
the same ElasticSearch query described earlier for
building T. We ignore sentences that cover none or
all answer choices as they are not discriminative.
We also ignore long sentences (&gt;300 characters)
and sentences with negation5 as they tend to lead
to noisy inference. We then run Open IE on these
sentences and re-score the resulting tuples using
the Jaccard score6 due to the lossy nature of Open
IE, and finally take the 50 top-scoring tuples T 0qa .
</p>
<p>3.3 Support Graph Search
Similar to TABLEILP, we view the QA task as
searching for a graph that best connects the terms
in the question (qterms) with an answer choice
via the knowledge; see Figure 1 for a simple il-
lustrative example. Unlike standard alignment
models used for tasks such as Recognizing Tex-
tual Entailment (RTE) (Dagan et al., 2010), how-
ever, we must score alignments between a set
Tqa [ T 0qa of structured tuples and a (potentially
multi-sentence) multiple-choice question qa.
</p>
<p>The qterms, answer choices, and tuples fields
form the set of possible vertices, V , of the support
graph. Edges connecting qterms to tuple fields and
tuple fields to answer choices form the set of pos-
sible edges, E . The support graph, G(V, E), is a
</p>
<p>5containing not, ’nt, or except
6| tok(t) \ tok(qa) | / | tok(t) [ tok(qa) |
</p>
<p>subgraph of G(V, E) where V and E denote “ac-
tive” nodes and edges, resp. We define an ILP
optimization model to search for the best support
graph (i.e., the active nodes and edges) as follows.
</p>
<p>Variables
The ILP has a binary variable for each qterm (xq),
tuple (xt), tuple field (xf ), and answer choice (xa),
indicating whether the corresponding graph node
is active. There is a binary activity variable (xe)
for each edge e 2 E . For efficiency, we only create
a qterm!field edge and a field!choice edge if
the corresponding coefficient is no smaller than a
certain threshold (0.1 and 0.2, resp.).
</p>
<p>Objective Function
The objective function coefficient ce of each edge
e(t, h) is determined by a word-overlap score.7
</p>
<p>While TABLEILP used WordNet (Miller, 1995)
paths to compute the edge weight, this measure re-
sults in unreliable scores when faced with longer
phrases found in Open IE tuples.
</p>
<p>Compared to a curated KB, it is easy to find
Open IE tuples that match irrelevant parts of the
questions. To mitigate this issue, we scale the co-
efficients cq of qterms in our ILP objective to focus
on important terms. Since the later terms in a ques-
tion tend to provide the most critical information,
we scale qterm coefficients based on their position
in the question. Also, qterms that appear in almost
all of the selected tuples tend not to be discrim-
inative as any tuple would support such a qterm.
Hence we scale qterm coefficients inversely by the
frequency with which they occur in the selected
tuples. Appendix A describes the coefficient for
qterm as well as other variables in detail.
</p>
<p>Constraints
Since Open IE tuples do not come with schema
and join rules, we can define a substantially sim-
pler model compared to TABLEILP. This reduces
the reasoning capability but also eliminates the re-
liance on hand-authored join rules and regular ex-
pressions used in TABLEILP. We discovered (see
empirical evaluation) that this simple model can
achieve the same score as TABLEILP on the Re-
gents test (target test set used by TABLEILP) and
generalizes better to different grade levels.
</p>
<p>We start with a few constraints defining what is
an active node or edge, shown as the first groups
of constraints in Table 1. To avoid positive edge
coefficients in the objective function resulting in
</p>
<p>7w(t, h) =| tok(t) \ tok(h) | / | tok(h) |
</p>
<p>313</p>
<p />
</div>
<div class="page"><p />
<p>Active variable must have an active edge
Active edge must have an active source node
Active edge must have an active target node
Exactly one answer choice must be active
Active field implies tuple must be active
Active field must have &lt; w1 connected edges
Active choice must have &lt; w2 edges
Active qterm must have &lt; w3 edges
Support graph must have &lt; w4 active tuples
Active tuple must have � w5 active fields
Active tuple must have an edge to some qterm
Active tuple must have an edge to some choice
Active tuple must have active subject
If a tuple predicate aligns to q, the subject (object) must
</p>
<p>align to a term preceding (following, resp.) q
</p>
<p>Table 1: High-level ILP constraints; we report re-
sults for ~w = (2, 4, 4, 4, 2); the model can be im-
proved with more careful parameter selection
</p>
<p>spurious edges in the support graph, we limit the
number of active edges from an active tuple, ques-
tion choice, tuple fields, and qterms (second group
of constraints in Table 1). Our model is also ca-
pable of using multiple tuples to support different
parts of the question as illustrated in Figure 1. To
avoid spurious tuples that only connect with the
question (or choice) or ignore the relation being
expressed in the tuple, we add constraints that re-
quire each tuple to connect a qterm with an answer
choice (third group of constraints in Table 1).
</p>
<p>We also define new constraints based on the
Open IE tuple structure. Since an Open IE tu-
ple expresses a fact about the tuple’s subject,
we require that the subject must be active. To
avoid issues such as (Planet; orbit; Sun) matching
the sample question in the introduction (“Which
object. . .orbits around a planet”), we also add an
ordering constraint (fourth group in Table 1).
</p>
<p>We note that TUPLEINF only combines parallel
evidence, i.e., each tuple must individually con-
nect words in the question to the answer choice.
For reliable multi-hop reasoning using OpenIE tu-
ples, one can add inter-tuple connections to the
support graph search, controlled by a small num-
ber of rules over Open IE predicates. Learning
such rules for the Science domain is an open prob-
lem and potential avenue for future work.
</p>
<p>4 Experiments
</p>
<p>Comparing our method with two state-of-the-art
systems for 4th and 8th grade science exams,
we demonstrate that (a) TUPLEINF with only au-
tomatically extracted tuples significantly outper-
forms TABLEILP with its original curated knowl-
</p>
<p>Solvers 4th Grade 8th Grade
</p>
<p>TABLEILP(C) 39.9 34.1
TUPLEINF(T+T’) 51.7 51.6
</p>
<p>TABLEILP(C+T) 42.1 37.9
TUPLEINF(C+T) 47.5 48.0
</p>
<p>Table 2: TUPLEINF is significantly better at struc-
tured reasoning than TABLEILP.9
</p>
<p>edge as well as with additional tuples, and (b) TU-
PLEINF’s complementary approach to IR leads to
an improved ensemble. Numbers in bold indicate
statistical significance based on the Binomial ex-
act test (Howell, 2012) at p = 0.05.
</p>
<p>We consider two question sets. (1) 4th Grade
set (1220 train, 1304 test) is a 10x larger superset
of the NY Regents questions (Clark et al., 2016),
and includes professionally written licensed ques-
tions. (2) 8th Grade set (293 train, 282 test) con-
tains 8th grade questions from various states.8
</p>
<p>We consider two knowledge sources:
(1) The Sentence corpus (S) consists of
</p>
<p>domain-targeted 80K sentences and 280 GB of
plain text extracted from web pages used by Clark
et al. (2016). This corpus is used as a collection of
sentences by the IR solver. It is also used to create
the tuple KB T (Sec. 3.1) and on-the-fly question-
specific tuples T 0qa (Sec. 3.2) for TUPLEINF.
</p>
<p>(2) TABLEILP uses ⇠70 Curated tables (C)
containing about 7,600 rows, designed for 4th
grade NY Regents exams.
</p>
<p>We compare TUPLEINF with two state-of-the-
art baselines. IR is a simple yet powerful
information-retrieval baseline (Clark et al., 2016)
that selects the answer option with the best match-
ing sentence in a corpus. TABLEILP is the state-
of-the-art structured inference baseline (Khashabi
et al., 2016) developed for science questions.
</p>
<p>4.1 Results
</p>
<p>Table 2 shows that TUPLEINF, with no curated
knowledge, outperforms TABLEILP on both ques-
tion sets by more than 11%. The lower half of the
table shows that even when both solvers are given
</p>
<p>8See the Middle School Without Diagrams set
from AI2 Science Questions v1 (Feb 2016) at
http://allenai.org/data/science-exam-questions.html for
the 8th Grade set. For future comparisons, we also report
our score on their smaller 4th Grade set: Elementary School
Without Diagrams (432 train, 339 test).
</p>
<p>9TUPLEINF(T+T’) achieves a score of 56.1% on the El-
ementary School Without Diagrams test set (cf. Footnote 8)
compared to TABLEILP(C)’s score of 46.7%.
</p>
<p>314</p>
<p />
</div>
<div class="page"><p />
<p>Solvers 4th Grade 8th Grade
</p>
<p>IR(S) 52.0 52.8
IR(S) + TABLEILP(C) 53.3 54.5
IR(S) + TUPLEINF(T+T’) 55.3 55.1
</p>
<p>Table 3: TUPLEINF is complementarity to IR, re-
sulting in a strong ensemble
</p>
<p>the same knowledge (C+T),10 the improved selec-
tion and simplified model of TUPLEINF11 results
in a statistically significant improvement. Our
simple model, TUPLEINF(C + T), also achieves
scores comparable to TABLEILP on the latter’s
target Regents questions (61.4% vs TABLEILP’s
reported 61.5%) without any specialized rules.
</p>
<p>Table 3 shows that while TUPLEINF achieves
similar scores as the IR solver, the approaches
are complementary (structured lossy knowledge
reasoning vs. lossless sentence retrieval). The
two solvers, in fact, differ on 47.3% of the train-
ing questions. To exploit this complementarity,
we train an ensemble system (Clark et al., 2016)
which, as shown in the table, provides a substan-
tial boost over the individual solvers. Further,
IR + TUPLEINF is consistently better than IR +
TABLEILP.
</p>
<p>Finally, in combination with IR and the statis-
tical association based PMI solver (which scores
54.1% by itself) of Clark et al. (2016), TUPLEINF
achieves a score of 58.2% on the 4th grade set.
This compares favorably to TABLEILP’s ensem-
ble score of 56.7%, again attesting to TUPLEINF’s
strength.12
</p>
<p>5 Error Analysis
</p>
<p>We describe four classes of failure of TUPLEINF,
and the future work they suggest.
</p>
<p>Missing Important Words: Which material
will spread out to completely fill a larger con-
tainer? (A) air (B) ice (C) sand (D) water
In this question, we have tuples that support that
water will spread out and fill a larger container, but
miss the critical word “completely”. A method for
detecting salient question words would help here.
</p>
<p>10See Appendix B for how tables (and tuples) are used by
TUPLEINF (and TABLEILP).
</p>
<p>11On average, TABLEILP (TUPLEINF) has 3,403 (1,628,
resp.) constraints and 982 (588, resp.) variables. TUPLE-
INF’s ILP can be solved in half the time taken by TABLEILP,
reducing the overall question answering time by 68.6%.
</p>
<p>12We observed no difference in scores on the 8th grade set.
</p>
<p>Lossy IE: Which action is the best method to
separate a mixture of salt and water? . . .
The IR solver correctly answers this question by
using the sentence: Separate the salt and wa-
ter mixture by evaporating the water. However,
TUPLEINF is not able to answer this question as
Open IE is unable to extract tuples from this im-
perative sentence. While the additional structure
from Open IE is generally helpful for more ro-
bust matching, the conversion to tuples sometimes
loses important bits of information.
</p>
<p>Bad Alignment: Which of the following gases is
necessary for humans to breathe in order to live?
(A) Oxygen (B) Carbon dioxide (C) Helium (D)
Water vapor
TUPLEINF returns “Carbon dioxide” as the answer
because of the tuple (humans; breathe out; carbon
dioxide). The chunk “to breathe” in the question
has a high alignment score to the “breathe out” re-
lation in the tuple, even though they have com-
pletely different meaning. An improved phrase
alignment module can mitigate this issue.
</p>
<p>Out of Scope: Deer live in forest for shelter. If
the forest was cut down, which situation would
most likely happen? . . .
Such questions require modeling a state presented
in the question and reasoning over this state, which
is out of scope of our solver.
</p>
<p>6 Conclusion
</p>
<p>We presented a new QA system, TUPLEINF, that
can reason over a large, potentially noisy knowl-
edge base of (subject, predicate, object) style tu-
ples, in order to answer complex questions. Our
results establish TUPLEINF as a new state-of-the-
art structured reasoning solver for elementary-
level science that does not rely on curated knowl-
edge and generalizes to higher grade levels. Our
error analysis points to lossy IE and textual mis-
alignments as two main causes of failure, suggest-
ing future work around incorporating tuple context
and distributional similarity measures.
</p>
<p>Acknowledgments
</p>
<p>The authors would like to thank Oren Etzioni for
valuable feedback on an early draft of this paper,
and Colin Arenz and Michal Guerquin for helping
us develop this system.
</p>
<p>315</p>
<p />
</div>
<div class="page"><p />
<p>References
Tobias Achterberg. 2009. SCIP: solving constraint in-
</p>
<p>teger programs. Math. Prog. Computation 1(1):1–
41.
</p>
<p>Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI.
</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Empirical Methods in
Natural Language Processing (EMNLP).
</p>
<p>Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In ACL.
</p>
<p>Eric Brill, Susan Dumais, and Michele Banko. 2002.
An analysis of the AskMSR question-answering sys-
tem. In Proceedings of EMNLP. pages 257–264.
</p>
<p>Gong Cheng, Weixi Zhu, Ziwei Wang, Jianghui Chen,
and Yuzhong Qu. 2016. Taking up the Gaokao Chal-
lenge: An information retrieval approach. In IJCAI.
</p>
<p>Peter Clark. 2015. Elementary school science and math
tests as a driver for AI: take the Aristo challenge! In
29th AAAI/IAAI. Austin, TX, pages 4019–4021.
</p>
<p>Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-
harwal, Oyvind Tafjord, Peter Turney, and Daniel
Khashabi. 2016. Combining retrieval, statistics, and
inference to answer elementary science questions.
In 30th AAAI.
</p>
<p>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ra-
tional, evaluation and approaches–erratum. Natural
Language Engineering 16(01):105–105.
</p>
<p>Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL.
</p>
<p>Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In KDD.
</p>
<p>David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building Watson: An overview
of the DeepQA project. AI Magazine 31(3):59–79.
</p>
<p>David Howell. 2012. Statistical methods for psychol-
ogy. Cengage Learning.
</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Pe-
ter Clark, Oren Etzioni, and Dan Roth. 2016. Ques-
tion answering via integer programming over semi-
structured knowledge. In IJCAI.
</p>
<p>Tushar Khot, Niranjan Balasubramanian, Eric
Gribkoff, Ashish Sabharwal, Peter Clark, and Oren
Etzioni. 2015. Exploring Markov logic networks
for question answering. In EMNLP.
</p>
<p>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling semantic
parsers with on-the-fly ontology matching. In
EMNLP.
</p>
<p>George Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM 38(11):39–
41.
</p>
<p>Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
ACL.
</p>
<p>Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine learning 62(1–
2):107–136.
</p>
<p>Arpit Sharma, Nguyen Ha Vo, Somak Aditya, and
Chitta Baral. 2015. Towards addressing the wino-
grad schema challenge - building and using a seman-
tic parser and a knowledge hunting module. In IJ-
CAI.
</p>
<p>Huan Sun, Hao Ma, Xiaodong He, Wen tau Yih, Yu Su,
and Xifeng Yan. 2016. Table cell search for question
answering. In WWW.
</p>
<p>Pengcheng Yin, Nan Duan, Ben Kao, Jun-Wei Bao, and
Ming Zhou. 2015. Answering questions with com-
plex semantic constraints on open knowledge bases.
In CIKM.
</p>
<p>316</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 317–323
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2050
</p>
<p>Bootstrapping for Numerical Open IE
</p>
<p>Swarnadeep Saha
Department of CSE
</p>
<p>I.I.T. Delhi
</p>
<p>writetoswarna@gmail.com
</p>
<p>Harinder Pal∗
Microsoft Corporation
</p>
<p>hapal@microsoft.com
</p>
<p>Mausam
Department of CSE
</p>
<p>I.I.T. Delhi
</p>
<p>mausam@cse.iitd.ac.in
</p>
<p>Abstract
</p>
<p>We design and release BONIE, the first
open numerical relation extractor, for ex-
tracting Open IE tuples where one of the
arguments is a number or a quantity-unit
phrase. BONIE uses bootstrapping to
learn the specific dependency patterns that
express numerical relations in a sentence.
BONIE’s novelty lies in task-specific cus-
tomizations, such as inferring implicit re-
lations, which are clear due to context such
as units (for e.g., ‘square kilometers’ sug-
gests area, even if the word ‘area’ is miss-
ing in the sentence). BONIE obtains 1.5x
yield and 15 point precision gain on nu-
merical facts over a state-of-the-art Open
IE system.
</p>
<p>1 Introduction
</p>
<p>Open Information Extraction (Open IE) systems
extract relational tuples from text, without re-
quiring a pre-specified vocabulary (Etzioni et al.,
2008; Mausam, 2016), by constructing the rela-
tion phrases and arguments from within the sen-
tences themselves. Early works on Open IE such
as REVERB (Etzioni et al., 2011) extract verb-
mediated relations via a handful of human-defined
patterns. OLLIE improves recall by learning de-
pendency patterns, using bootstrapping over RE-
VERB extractions (Mausam et al., 2012). Open
IE 4.2, a state-of-the-art open information extrac-
tor, is based on a combination of SRLIE, a verb-
mediated extractor over SRL frames (Christensen
et al., 2011), and RELNOUN 2.0, which performs
special linguistic processing for extraction from
complex noun phrases (Pal and Mausam, 2016).
1
</p>
<p>∗Most work was done when the author was a graduate
student at IIT Delhi.
</p>
<p>1https://github.com/knowitall/openie
</p>
<p>1. Sentence: Hong Kong’s labour force is 3.5 million.
Open IE 4.2: (Hong Kong’s labour force; is; 3.5 million)
BONIE: (Hong Kong; has labour force of; 3.5 million)
</p>
<p>2. Sentence: Microsoft has 100,000 employees.
Open IE 4.2: (Microsoft; has; 100,000 employees)
BONIE: (Microsoft; has number of employees; 100,000)
</p>
<p>3. Sentence: James Valley is nearly 600 miles long.
Open IE 4.2: (James Valley; is; nearly 600 miles long)
BONIE: (James Valley; has length of; nearly 600 miles)
</p>
<p>4. Sentence: Donald Trump is 70 years old.
Open IE 4.2: (Donald Trump; is; 70 years old)
BONIE: (Donald Trump; has age of; 70 years)
</p>
<p>5. Sentence: James Valley has 5 sq kms of fruit orchards.
Open IE 4.2: (James Valley; has; 5 sq kms of fruit orchards)
BONIE: (James Valley; has area of fruit orchards; 5 sq kms)
</p>
<p>Table 1: Comparison of Open IE 4.2 and BONIE
</p>
<p>In this work, we present and release2 the
first system for open numerical extraction, which
we name BONIE for Bootstrapping-based Open
Numerical Information Extractor. It is important
to note that existing Open IE systems, like Open
IE 4.2, may also extract numerical facts. How-
ever, they are oblivious to the presence of numbers
in arguments. Therefore, they may miss important
extractions and may not always output the best nu-
merical facts. Table 1 compares extractions gener-
ated by Open IE 4.2 and BONIE on some of the
sample sentences.
</p>
<p>At a high level BONIE follows OLLIE’s de-
sign of identifying seed facts, constructing training
data by bootstrapping sentences that may mention
a seed fact, pattern learning and ranking. Madaan
et al (2016) note that bootstrapping for numerical
IE is challenging; it can lead to high noise and
missed recall, since numbers can easily match out
of context, and numbers may not match due to ap-
proximations. In response, similar to most previ-
ous works (e.g., LUCHS (Hoffmann et al., 2010))
BONIE matches a number if it is within a percent-
age threshold. Additionally, BONIE uses a quan-
tity extractor (Roy et al., 2015), which provides
</p>
<p>2Available at https://github.com/Open-NRE
</p>
<p>317</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2050">https://doi.org/10.18653/v1/P17-2050</a></div>
</div>
<div class="page"><p />
<p>the units mentioned in the sentence – BONIE
bootstraps a sentence only when the units match.
</p>
<p>When compared to OLLIE, BONIE contributes
several numerical IE specific customizations. (1)
Since no open facts are available for this task, we
first manually define a set of high-precision seed
patterns, which are run over a large corpus to gen-
erate seed facts. (2) Not all seeds are fit for boot-
strapping – many don’t even have an entity as first
argument. We develop heuristics to identify an in-
formative subset from these. After bootstrapping
and pattern learning, we find that we are miss-
ing important tuples. E.g., sentence #3 in Table
1 above has no explicit relation word – the rela-
tion “has length of” is implicit via the adjective
‘long’. And, sentence #5 expresses the relation
‘area’ via the units. (3) BONIE identifies implicit
relations using additional processing of units and
adjectives. (4) Finally, BONIE can tag a quantity
as count and prepends “number of” in the relation
phrase (sentence #2).
</p>
<p>2 Related Work
One of the first Open IE systems to obtain sub-
stantial recall is OLLIE (Mausam et al., 2012),
which is a pattern learning approach based on a
bootstrapped training data using high precision
verb-based extractions. Other methods augment
the linguistic knowledge in the systems – Exem-
plar (de Sá Mesquita et al., 2013) adds new rules
over dependency parses, SRLIE develops extrac-
tion logic over SRL frames (Christensen et al.,
2011). Several works identify clauses and op-
erate over restructured sentences (Schmidek and
Barbosa, 2014; Corro and Gemulla, 2013; Bast
and Haussmann, 2013). Other approaches use tree
kernels (Xu et al., 2013), qualia-based patterns
(Xavier et al., 2015), and simple within-sentence
inference (Bast and Haussmann, 2014). However,
none of them handle numbers specifically, and
hence do not work for our problem.
</p>
<p>Numerical Relations: Numbers play an impor-
tant role in extracting information from text. Early
works have seen people working on understanding
numbers that express temporal information (Ling
and Weld, 2010). More recently, the focus has
been on numbers that express physical quantities
or measures, either mentioned in text (Chaganty
and Liang, 2016) or in the context of web tables
(Ibrahim et al., 2016; Neumaier et al., 2016), or
on numbers that represent cardinalities of relations
(Mirza et al., 2017).
</p>
<p>One of the prior works that applies to generic
numerical relations is LUCHS (Hoffmann et al.,
2010), where the system uses distant supervision
to create 5,000 relation extractors, which included
numerical relations as well. Researchers have also
specifically developed numerical relation extrac-
tors to extract those relations where one of the ar-
guments is a quantity (Vlachos and Riedel, 2015;
Intxaurrondo et al., 2015; Madaan et al., 2016).
However, all of them extract only an ontology rela-
tion, and hence are not directly applicable to Open
IE.
</p>
<p>Figure 1: BONIE flow diagram
</p>
<p>3 Open Numerical Relation Extraction
</p>
<p>The goal of Open Numerical Relation Extraction
is to process a sentence that has a quantity mention
in it, and extract any tuple of the form (Arg1, rela-
tion phrase, Arg2) where Arg2 (or Arg1) is a quan-
tity. As a first step, BONIE learns patterns where
Arg2 is a quantity, as most English sentences tend
to express numerical facts in active voice. Fig-
ure 1 outlines BONIE’s algorithm, which operates
in two phases: training and extraction. BONIE’s
training includes creation of seed facts, genera-
tion of training data via bootstrapping, and pat-
tern learning over dependency parses. In the ex-
traction phrase, BONIE performs pattern match-
ing and parse-based expansion to construct numer-
ical tuples. These numerical tuples are made more
coherent by a novel relation construction step.
</p>
<p>As an example, the sentence “India has a pop-
ulation of 1.2 billion” matches seed pattern #2
(from Figure 2) to create a seed fact (India; pop-
ulation; 1.2 billion; null). This ‘null’ represents
that the quantity needs no unit. While bootstrap-
ping, this seed fact may match a sentence “India
is the second most populous country in the world,
with a population of 1.25 billion.” in the corpus.
</p>
<p>318</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: Seed Dependency Patterns in BONIE
</p>
<p>This training example will help learn a new pat-
tern.3 This pattern, when applied to the sentence
“Microsoft Windows is the most popular operat-
ing system, with a customer base of 300 million
users”, will extract (Microsoft Windows; has cus-
tomer base of; 300 million users).
</p>
<p>While BONIE’s skeleton broadly resembles that
of OLLIE’s (Mausam et al., 2012), it brings in
customizations specific to the problem of numeri-
cal extraction such as a modified pattern language,
heuristics for generating high quality seed set and
training data, special processing for non-noun re-
lations, and a novel relation construction step. We
now describe BONIE’s algorithm in detail.
</p>
<p>3.1 Generation of Seed Facts
</p>
<p>Since open numerical facts are not readily avail-
able, we first write a handful of high-precision
dependency patterns (see Figure 2 for a list).
Each dependency pattern encodes the minimal
sub-tree of the dependency parse connecting the
relation, quantity and argument in that sen-
tence. BONIE encodes a node in a pattern
via ‘&lt;depLabel&gt;#&lt;word&gt;#&lt;POSTag&gt;’, where
‘depLabel’ is the edge connecting the node to its
parent, ‘word’ is the word at the node, ‘POSTag’
is its part of speech tag; ‘#’ is a delimiter separat-
ing them. {rel}, {arg} and {quantity} in the pat-
terns are placeholders for relation, argument and
quantity headwords, respectively. BONIE gener-
ates seed facts by parsing the corpus and match-
ing seed patterns with the parse. In case of a suc-
cessful match, a seed fact of the form (arg head-
word; relation headword; quantity; unit) is gen-
erated. Argument and relation headwords are ex-
tracted directly from the parse. For the other two,
it uses Illinois Quantifier (Roy et al., 2015), which
returns both the quantity and unit separately.
</p>
<p>Since seed facts form the basis of our training
task, they must be as clean as possible – BONIE
</p>
<p>3&lt;(#is#verb)&lt;(nsubj#{arg}#nnp|nn)(prep#with#in)&lt;
(pobj#{rel}#nnp|nn)&lt;(prep#of#in)&lt;(pobj#{quantity}#.+)
&gt;&gt;&gt;&gt;&gt;
</p>
<p>adds several filters to reduce noise. It considers a
seed fact as valid only when the quantity node in
the pattern is within some quantity span given by
Illinois Quantifier. It also rejects any fact whose
argument is not a proper noun.
</p>
<p>After these filters it gets high-precision extrac-
tions, but not necessarily good seeds – many seeds
are generic, which may easily match unrelated
sentences. E.g., (Michael; drove; 20; kms) isn’t
a good seed, since ‘Michael’ isn’t specific, and
could erroneously match sentences mentioning an-
other Michael with some unrelated reference of a
20 km drive. To improve the set, BONIE checks
for the presence of a seed fact in Yago KB (Hoffart
et al., 2013) and keeps only those that are com-
mon. Since Yago has many numerical facts for
height, area, latitude, GDP, etc., this gives BONIE
a diverse set of clean facts for further training.
</p>
<p>Finally, some numerical facts may be expressed
without using a nominal relation word. BONIE
uses WordNet (Miller, 1995) to generate new
seeds from such seed facts using the derivation-
ally related noun form of the relation headword.
For example, (Brown ; tall ; 13 ; inches) gets trans-
formed to (Brown; height; 13; inches), which gets
added as a seed fact.
</p>
<p>3.2 Bootstrapping
Similar to OLLIE, BONIE finds sentences that
contain all words in a seed fact and generates (sen-
tence, fact) pairs. But unlike OLLIE, BONIE has
quantities and units, and matching them as words
isn’t appropriate. Illinois Quantifier performs an
internal normalization for both, e.g, changes ‘dol-
lars’ and ‘$’ to ‘US$’, and ‘%’ to ‘percent’. Since
seed facts also have normalized units, we run Illi-
nois quantifier on candidate sentences and match
normalized units directly. Moreover, BONIE
maintains a percentage threshold δ to control the
amount of allowed difference between quantities
in the sentence and seed fact. Once all constituents
of a fact match with a sentence, BONIE generates
the (sentence, fact) pair.
</p>
<p>319</p>
<p />
</div>
<div class="page"><p />
<p>3.3 Open Pattern Learning
</p>
<p>For each (sentence, fact) pair, BONIE parses the
sentence, and replaces the argument and relation
words of the fact with ‘{arg}’ and ‘{rel}’ place-
holders. For quantity and unit words, BONIE
replaces the one at a higher level in the parse
with ‘{quantity}’. The minimal path containing
‘{arg}’, ‘{rel}’ and ‘{quantity}’ is learned as a
pattern. Since quantity and unit are typically ex-
pected to remain close to each other in a sentence,
BONIE rejects all such patterns where the distance
between them exceeds a certain threshold value.
</p>
<p>Some patterns are learned with specific
words such as ‘contains’ in example (partial)
&lt;(#contains#verb)&lt;(dobj#{quantity}#.+)&lt;...
We believe that this pattern should work with all
inflections and synonyms of ‘contain’, BONIE
uses WordNet to expand the pattern by including
all inflections and synset synonyms. Each pattern
is scored based on the number of times it is
learned from the data.
</p>
<p>3.4 Constructing Extractions
</p>
<p>After matching a pattern to a new sentence, sim-
ilar to OLLIE, arg/rel phrases are completed by
expanding the extracted headnouns on poss, det,
num, neg, amod, quantmod, nn, and pobj edges.
If one of the children of the argument headword
is a prep, rcmod or partmod edge, the whole sub-
tree under that is extracted. Quantity phrase is
extracted by Illinois quantifier, but if any sibling
node of the quantity node is connected by a prep
edge, with the word ‘of’, BONIE expands the en-
tire subtree below it. This allows “10 percent of
100 dollars” to be included in the quantity phrase.
</p>
<p>Relation Phrase Construction: Whenever the
relation headword is an adjective or an adverb,
BONIE uses WordNet to get its derivationally re-
lated noun form and that becomes the new rela-
tion. This transforms the tuple (Donald Trump ;
old ; about 70 years) to (Donald Trump ; has age
of ; about 70 years).
</p>
<p>Sometimes, sentences don’t use a numerical re-
lation word – it is obvious from the units. E.g.,
sentence #4 on page 1 expresses the ‘area’ relation
implicitly. BONIE infers these implicit relations
using the unit analysis in UnitTagger (Sarawagi
and Chakrabarti, 2014). Whenever BONIE sees
a unit (sq kms) getting mistreated as a relation it
uses UnitTagger to infer relations from units and
postprocesses the extraction accordingly. The ex-
</p>
<p>traction, as a result changes from (James Valley;
has sq kms of ; 5 of fruit orchards) to (James Val-
ley ; has area of fruit orchards ; 5 sq kms).
</p>
<p>Finally, in cases when a plural noun relation
word also appears as a unit in the quantifier,
BONIE hypothesizes that it is a count extraction,
and prepends ‘number of’ to the relation headword
and removes the unit from the quantity phrase.
E.g., (Microsoft; has employees; 100,000 employ-
ees) from sentence #2 becomes (Microsoft; has
number of employees; 100,000).
</p>
<p>4 Experiments
</p>
<p>We build BONIE over data from ClueWeb12,4 fil-
tered so as to keep only the sentences that contain
numbers. We further remove those where quantity
represents a date, time, or duration, and where the
quantity is accompanied by document words like
‘Section’, ‘Table’, or ‘Figure’. We use the depen-
dency parser from ClearNLP5. We generate about
21,000 seed facts from roughly 20 million numeri-
cal sentences. These are matched against 7 million
numerical sentences obtaining about 18,500 (sen-
tence, fact) pairs.
</p>
<p>We tried different values of δ (the matching
threshold) and found results to not be sensitive as
long as δ varies in the range of 2% to 5%. So we
set δ=2% during the final evaluation. The distance
threshold between the quantity and unit mentioned
in Section 2.3 is set to 3 and is based on our gen-
eral understanding of parse trees.
</p>
<p>BONIE learns around 7,000 new patterns. Since
pattern frequency is a good indicator of pattern
quality (Wu and Weld, 2010), we rank the patterns
on the basis of frequency and take the top 1,000
patterns for further analysis. We find that almost
all patterns beyond the top 1,000 are learned only
once or twice on our training set. Our decision to
ignore all patterns beyond the top 1,000 is so that
we have a support of at least three for each pattern.
</p>
<p>We sample a random testset of 2,000 numeri-
cal sentences from ClueWeb12 (not used in train-
ing). Two annotators with NLP experience anno-
tate each extraction for correctness. We obtain an
inter-annotator agreement of 97%, and report the
results on the subset where both annotators agree.
</p>
<p>Since there are no open numerical extractors
available, we compare BONIE against an Open IE
system and another closed numerical IE system.
</p>
<p>4http://www.lemurproject.org/clueweb12.php/
5https://github.com/clir/clearnlp
</p>
<p>320</p>
<p />
</div>
<div class="page"><p />
<p>Setting Precision Yield
NumberRule 50.00 6
Open IE 4.2 62.50 296
BONIE (seed patterns only) 85.71 72
+ learned patterns 13.88 362
+ fact filters 55.27 351
+ Yago + WordNet expansion on facts 72.69 418
+ Relation phrase construction 77.91 448
+ WordNet expansion on patterns 77.23 458
</p>
<p>Table 2: Precision and Yield (#correct numerical extrac-
tions) on a dataset of 2000 ClueWeb12 numerical sentences.
</p>
<p>Table 2 reports the precisions and yields of all sys-
tems. We first compare against numerical tuples
from Open IE 4.2,6 a publicly available state-of-
the-art Open IE system that combines SRLIE and
RELNOUN. BONIE outputs a much higher preci-
sion and yield on numerical facts, as compared to
Open IE 4.2. We also compare against Number-
Rule, a state-of-the-art closed numerical IE sys-
tem (Madaan et al., 2016). NumberRule7 can be
quickly re-targeted to any new semantic relation
by inputting keywords. We feed all of Yago nu-
merical relation words as keywords to Number-
Rule, but still find that it was able to generate only
12 extractions on our testset.
</p>
<p>Figure 3: Comparison of Precision-Yield curves for
BONIE and Open IE 4.2. BONIE achieves substantially
larger area under the curve than Open IE 4.2.
</p>
<p>We also perform additional ablation study to
evaluate the value of each component. Just
the seed patterns themselves have a significantly
higher precision but much smaller yield. This is
expected, since the seeds must be highly precise
for bootstrapping. If Yago matching and other
seed filtering heuristics are turned off, the preci-
sion of the system goes down drastically due to a
very noisy bootstrapped set. If the post-processing
of relation phrase construction is turned off, there
is a 5 point precision loss and about 7% yield
reduction due to some incorrect extracted tuples,
which are corrected by post-processing. Finally,
</p>
<p>6https://github.com/knowitall/openie
7https://github.com/NEO-IE
</p>
<p>Wordnet-based expansion has marginal increase in
yield and slight precision loss.
</p>
<p>Open IE 4.2 associates a confidence value with
each extraction - ranking against which generates
a precision-yield curve. For BONIE, we rank the
patterns in such a way that the seed patterns are
at the top, followed by the learned patterns. The
learned patterns are ordered based on their fre-
quencies. Figure 3 reports the curves for both the
systems and we find that BONIE has a larger area
under the curve as compared to Open IE 4.2.
</p>
<p>Estimating recall in Open IE is difficult since
it requires annotators to exhaustively tag all open
extractions in a sentence. To get an estimate, an
author manually tagged 100 sentences with all nu-
merical extractions. We find that BONIE’s recall
is about 48%. Two-thirds of missed recall is be-
cause of missing conjuncts. E.g., it misses the tu-
ple relating retirement age with 68 years in “The
retirement age for men is 65 years and 68 years for
women.” Other missed recall is due to complexity
of sentences or inaccuracy of parsers.
</p>
<p>5 Conclusions
We release BONIE 8, the first open numerical re-
lation extractor and other resources for further re-
search. BONIE is based on bootstrapping and pat-
tern learning and follows previous similar works
such as OLLIE. However, for effective bootstrap-
ping and training, it implements various cus-
tomizations specific to numerical relations in cu-
ration of seed fact set, matching of sentences, and
construction of relation phrase at the time of ex-
traction. BONIE significantly outperforms both
open non-numerical IE, and closed numerical IE
systems with 1.5x yield and 15 point precision
gain over a state-of-the-art Open IE system. We
find that better conjunction processing is an im-
portant future step for improving BONIE’s recall
even further.
</p>
<p>Acknowledgements
This work is supported by Google language un-
derstanding and knowledge discovery focused re-
search grants, a Bloomberg award, a Visvesvaraya
faculty award by Govt. of India and an IRD seed
grant at I.I.T, Delhi. We also thank Microsoft for
the Microsoft Azure sponsorship and a Microsoft
Travel grant in support of the work.
</p>
<p>8Available at https://github.com/Open-NRE
</p>
<p>321</p>
<p />
</div>
<div class="page"><p />
<p>References
Hannah Bast and Elmar Haussmann. 2013. Open in-
</p>
<p>formation extraction via contextual sentence decom-
position. In 2013 IEEE Seventh International Con-
ference on Semantic Computing, Irvine, CA, USA,
September 16-18, 2013. pages 154–159.
</p>
<p>Hannah Bast and Elmar Haussmann. 2014. More infor-
mative open information extraction via simple infer-
ence. In Advances in Information Retrieval - 36th
European Conference on IR Research, ECIR 2014,
Amsterdam, The Netherlands, April 13-16, 2014.
Proceedings. pages 585–590.
</p>
<p>Arun Tejasvi Chaganty and Percy Liang. 2016. How
much is 131 million dollars? putting numbers in
perspective with compositional descriptions. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2016, Au-
gust 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers.
</p>
<p>Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open infor-
mation extraction based on semantic role labeling.
In Proceedings of the 6th International Conference
on Knowledge Capture (K-CAP 2011), June 26-29,
2011, Banff, Alberta, Canada. pages 113–120.
</p>
<p>Luciano Del Corro and Rainer Gemulla. 2013. Clausie:
clause-based open information extraction. In 22nd
International World Wide Web Conference, WWW
’13, Rio de Janeiro, Brazil, May 13-17, 2013. pages
355–366.
</p>
<p>Filipe de Sá Mesquita, Jordan Schmidek, and Denil-
son Barbosa. 2013. Effectiveness and efficiency
of open relation extraction. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2013, 18-21 Octo-
ber 2013, Grand Hyatt Seattle, Seattle, Washington,
USA, A meeting of SIGDAT, a Special Interest Group
of the ACL. pages 447–457.
</p>
<p>Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM 51(12):68–74.
</p>
<p>Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam. 2011. Open infor-
mation extraction: The second generation. In IJCAI
2011, Proceedings of the 22nd International Joint
Conference on Artificial Intelligence, Barcelona,
Catalonia, Spain, July 16-22, 2011. pages 3–10.
</p>
<p>Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artif. Intell. 194:28–61.
</p>
<p>Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
ACL 2010, Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, July 11-16, 2010, Uppsala, Sweden. pages 286–
295.
</p>
<p>Yusra Ibrahim, Mirek Riedewald, and Gerhard
Weikum. 2016. Making sense of entities and quan-
tities in web tables. In Proceedings of the 25th ACM
International on Conference on Information and
Knowledge Management, CIKM 2016, Indianapolis,
IN, USA, October 24-28, 2016. pages 1703–1712.
</p>
<p>Ander Intxaurrondo, Eneko Agirre, Oier Lopez de La-
calle, and Mihai Surdeanu. 2015. Diamonds in the
rough: Event extraction from imperfect microblog
data. In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015. pages 641–650.
</p>
<p>Xiao Ling and Daniel S. Weld. 2010. Temporal infor-
mation extraction. In Proceedings of the Twenty-
Fourth AAAI Conference on Artificial Intelligence,
AAAI 2010, Atlanta, Georgia, USA, July 11-15,
2010.
</p>
<p>Aman Madaan, Ashish Mittal, Mausam, Ganesh Ra-
makrishnan, and Sunita Sarawagi. 2016. Numer-
ical relation extraction with minimal supervision.
In Proceedings of the Thirtieth AAAI Conference
on Artificial Intelligence, February 12-17, 2016,
Phoenix, Arizona, USA.. pages 2764–2771.
</p>
<p>Mausam. 2016. Open information extraction systems
and downstream applications. In Proceedings of the
Twenty-Fifth International Joint Conference on Arti-
ficial Intelligence, IJCAI 2016, New York, NY, USA,
9-15 July 2016. pages 4074–4077.
</p>
<p>Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL 2012,
July 12-14, 2012, Jeju Island, Korea. pages 523–
534.
</p>
<p>George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM 38(11):39–41.
</p>
<p>Paramita Mirza, Simon Razniewski, Fariz Darari, and
Gerhard Weikum. 2017. Cardinal virtues: Extract-
ing relation cardinalities from text.
</p>
<p>Sebastian Neumaier, Jürgen Umbrich, Josiane Xavier
Parreira, and Axel Polleres. 2016. Multi-level se-
mantic labelling of numerical values. In The Seman-
tic Web - ISWC 2016 - 15th International Semantic
Web Conference, Kobe, Japan, October 17-21, 2016,
Proceedings, Part I. pages 428–445.
</p>
<p>Harinder Pal and Mausam. 2016. Demonyms and
compound relational nouns in nominal open IE.
In Proceedings of the 5th Workshop on Automated
Knowledge Base Construction, AKBC@NAACL-
HLT 2016, San Diego, CA, USA, June 17, 2016.
pages 35–39.
</p>
<p>322</p>
<p />
</div>
<div class="page"><p />
<p>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Rea-
soning about quantities in natural language. TACL
3:1–13.
</p>
<p>Sunita Sarawagi and Soumen Chakrabarti. 2014.
Open-domain quantity queries on web tables: anno-
tation, response, and consensus models. In The 20th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’14, New
York, NY, USA - August 24 - 27, 2014. pages 711–
720.
</p>
<p>Jordan Schmidek and Denilson Barbosa. 2014. Im-
proving open relation extraction via sentence re-
structuring. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2014), Reykjavik, Iceland, May 26-
31, 2014.. pages 3720–3723.
</p>
<p>Andreas Vlachos and Sebastian Riedel. 2015. Iden-
tification and verification of simple claims about
statistical properties. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015. pages 2596–2601.
</p>
<p>Fei Wu and Daniel S. Weld. 2010. Open informa-
tion extraction using wikipedia. In ACL 2010, Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, July 11-16,
2010, Uppsala, Sweden. pages 118–127.
</p>
<p>Clarissa Castellã Xavier, Vera Lúcia Strube de Lima,
and Marlo Souza. 2015. Open information extrac-
tion based on lexical semantics. J. Braz. Comp. Soc.
21(1):4:1–4:14.
</p>
<p>Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Human Language
Technologies: Conference of the North American
Chapter of the Association of Computational Lin-
guistics, Proceedings, June 9-14, 2013, Westin
Peachtree Plaza Hotel, Atlanta, Georgia, USA.
pages 868–877.
</p>
<p>323</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 324–329
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2051
</p>
<p>Feature-Rich Networks for Knowledge Base Completion
</p>
<p>Alexandros Komninos
Department of Computer Science
</p>
<p>University of York
York, YO10 5GH
United Kingdom
</p>
<p>ak1153@york.ac.uk
</p>
<p>Suresh Manandhar
Department of Computer Science
</p>
<p>University of York2
York, YO10 5GH
United Kingdom
</p>
<p>suresh@cs.york.ac.uk
</p>
<p>Abstract
</p>
<p>We propose jointly modelling Knowledge
Bases and aligned text with Feature-Rich
Networks. Our models perform Knowl-
edge Base Completion by learning to rep-
resent and compose diverse feature types
from partially aligned and noisy resources.
We perform experiments on Freebase uti-
lizing additional entity type information
and syntactic textual relations. Our eval-
uation suggests that the proposed mod-
els can better incorporate side information
than previously proposed combinations of
bilinear models with convolutional neu-
ral networks, showing large improvements
when scoring the plausibility of unob-
served facts with associated textual men-
tions.
</p>
<p>1 Introduction
</p>
<p>Knowledge Bases (KB) are an important resource
for many applications such as question answer-
ing (Reddy et al., 2014), relation extraction (Mintz
et al., 2009) and named entity recognition (Ling
and Weld, 2012). While large collaborative KBs
like Freebase (Bollacker et al., 2008) and DBpe-
dia (Auer et al., 2007) contain facts about million
of entities, they are mostly incomplete and contain
errors. A large amount of research has been ded-
icated to automatically extend knowledge bases,
a task called Entity Linking or Knowledge Base
Completion (KBC). Proposed approaches to KBC
either reason about the internal structure of the
KB, or utilize external data sources that indicate
relations between the entities in the KB.
</p>
<p>A very successful approach to KBC is latent
feature models (Nickel et al., 2011; Bordes et al.,
2013; Socher et al., 2013; Nickel et al., 2016).
Such models embed the symbols of the KB into
</p>
<p>a low dimensional space and assign a score to
unseen triples as a function of the latent feature
representations. Most approaches define a scor-
ing function as a linear or bilinear operator. La-
tent feature models have shown good performance
when considering the internal structure of KBs and
are scalable to very large datasets.
</p>
<p>Utilizing textual data or other external resources
for KBC is a challenging task but has the poten-
tial of constantly updating KBs as new informa-
tion becomes available. A line of work uses the
KB as a means to obtain distant supervision to
train relation extraction systems that classify tex-
tual mentions into one of the KBs relations (Mintz
et al., 2009; Hoffmann et al., 2011; Surdeanu et al.,
2012). State-of-the-art approaches for KBC with
external textual data are obtained by latent feature
models that jointly embed the KB symbols and
text relations into the same space (Riedel et al.,
2013; Toutanova et al., 2015). The benefit of
such models over relation extraction systems is
that they can combine both the internal structure
of the KB and textual information to reason about
the plausibility of unobserved facts.
</p>
<p>A commonly used approach for augmenting a
KBC given an aligned text corpus is by adopting
a Universal Schema (Riedel et al., 2013), where
extracted textual relations between entities are di-
rectly added to the knowledge graph and treated
the same as KB relations. This allows appli-
cation of any latent variable model defined over
triples to jointly embed the KB and text relations
to the same space. An extension to the Univer-
sal Schema approach was proposed by (Toutanova
et al., 2015), where representations of text re-
lations are formed compositionally by Convolu-
tional Neural Networks (CNNs) and then com-
posed with entity vectors by a bilinear model to
score a fact. However, these models show only
moderate improvement when incorporating tex-
</p>
<p>324</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2051">https://doi.org/10.18653/v1/P17-2051</a></div>
</div>
<div class="page"><p />
<p>Subject 
</p>
<p>Entity 
</p>
<p>Object 
</p>
<p>Entity 
</p>
<p>Subject 
</p>
<p>Entity 
</p>
<p>Types 
</p>
<p>Object 
</p>
<p>Entity 
</p>
<p>Types 
</p>
<p>Relation Text 
</p>
<p>Features 
</p>
<p>Figure 1: Feature-Rich Network with all the aligned feature types associated with a fact.
</p>
<p>tual relations.
</p>
<p>A limitation of the Universal Schema approach
for joint embedding of KBs and text is that infor-
mation about the correspondence between KB and
text relations is only implicitly available through
their co-occurrence with entities. Text relations
can often be noisy and pairs of entities can co-
occur in the same sentence without sharing a se-
mantic relation. In addition, there is usually a mis-
match in the relations found in the KB and those
expressed in text. The model has to learn the align-
ment between KB and text relations without ex-
plicit evidence of co-occurrence between the two,
and then propagate that information through the
entity embeddings in order to score unseen KB
triples.
</p>
<p>We propose a different approach to combine KB
and textual evidence, where the textual relations
are not part of the same graph but are treated as
side evidence. In our setting, a fact does not nec-
essarily consist of a (sbj, rel, obj) triple, but as
an n-tuple where extra elements are formed by ex-
tracting additional information from the KB and
aligned side resources such as text. We score
the probability of the tuple being true by learn-
ing latent representations for each element of the
tuple, and then learning a composition and scor-
ing function parameterized by a Multilayer Per-
ceptron (MLP). We choose MLPs as they are a
generic method to model interactions between la-
tent features without having to specify the form of
a composition operator for tuples of different ar-
ity. When scoring the plausibility of unseen facts,
all the side evidence associated with that fact be-
comes explicit through the n-tuple.
</p>
<p>We evaluate the ability of the proposed Feature-
</p>
<p>Rich Networks (FRN) for KBC on the challenging
FB15k-237 (Toutanova et al., 2015). We compare
the performance of bilinear models to an MLP
when facts are represented as simple triples, and
the contribution of two additional types of aligned
information: entity types and textual relation men-
tions from a side corpus. We also evaluate the
contribution of initializing feature representations
from external models. Evaluation suggests that
while MLPs and bilinear models perform simi-
larly when treating facts as triples of KB symbols,
the proposed approach can better utilize additional
textual data than a combination of CNNs with bi-
linear models, showing large improvements in pre-
dicting unseen facts when they have linked rela-
tion mentions in text.
</p>
<p>2 Model Definition
</p>
<p>Knowledge Bases can be represented as a directed
graph where nodes are entities e ∈ E and edges
are typed relations r ∈ R. A fact in the KB is en-
coded as a triple (es, r, eo), where es is the subject
entity and eo is the object entity. Starting with an
existing KB consisting of a set of observed facts,
our goal is to reason about the plausibility of un-
observed facts, given some additional external re-
source. In our proposed model, we expand the
representation of a fact to an n-tuple by consider-
ing alignments of the additional resource with ele-
ments of the triple. Our most expressive model en-
codes a fact as X = (es, r, eo, ts, to, To,s), where
ts, to are associated representation of types of the
two entities, and To,s is the aligned textual ev-
idence associated with a pair of entities from a
side corpus. Representations of entities and entity
types are shared between subjects and objects.
</p>
<p>325</p>
<p />
</div>
<div class="page"><p />
<p>Francis Ford Coppola, director of  The Godfather, … 
</p>
<p>appos nmod:of 
</p>
<p>subject entity /m/02vyw
object entity /m/07g1sm
relation /film/director/film
subject entity types /people/person /film/director/ /award/award winner
object entity types /film/film /award/award winning work
text features appos−1 Esbj director appos director
</p>
<p>of nmod:of−1 director nmod:of Eobj
</p>
<p>Extracted features for a KB fact with a single associated textual relation mention.
</p>
<p>2.0.1 Feature Rich Networks
We model the probability of an n-tuple being true
with an MLP that learns to compose and score
the compatibility of the features associated with it.
Features for each individual element of the tuple
are assigned low dimensional embeddings which
are concatenated to form the input to the MLP. The
embeddings are jointly learned with the composi-
tion and scoring model through back-propagation.
The probability of a fact being true is given by:
</p>
<p>p(X = 1) = σ(w3 · g(W2 · g(W1 · x)) (1)
</p>
<p>x = v(es); v(r); v(eo); v(ts); v(to); v(Ts,o) (2)
</p>
<p>whereW1, W2, w3 are the weights of the network,
g(•) is a non-linear function applied element-
wise, σ(•) is the sigmoid function and v(•) are
latent feature representations of each element of
the tuple. We use Rectified Linear Units as non-
linearities (Nair and Hinton, 2010).
</p>
<p>2.1 Additional Features
</p>
<p>We create compositional representations for the
entity types and textual relation mentions with
simple aggregation functions of their feature
embeddings. Although not considered in this
work, the overall approach is highly modular
allowing for each component to be modelled by a
different kind of network.
</p>
<p>Freebase Entity Types
Entities in Freebase can have multiple types
assigned to them. While entity types are explicitly
provided in Freebase, we instead learn type
representations by considering observed relations
in the training set. Each relation in Freebase is
</p>
<p>encoded as a domain/type/property of the
subject entity. We extract the set of all triples
where an entity takes the subject position, and
keep the domain/type part as a type feature
of that entity. We aggregate embeddings of all
the observed discrete features using summation
followed by L2-normalization to create the final
representation of an entity’s type. We use a special
UNKNOWN symbol for entities with no observed
types in the training set (i.e., entities that do not
appear as subject of a triple). We create entity
type representations for both subject and object
entities and concatenate them to the input vector
of the network.
</p>
<p>Text Relations
We use a side corpus where pairs of entities are
linked to the KB and take the shortest dependency
path connecting them as a textual relation mention.
Since textual relations are tied to entity pairs, we
collect all mentions for a given entity pair and as-
sociate them with a fact. This results in a set of
phrases that act as textual evidence for relations of
an entity pair.
</p>
<p>We create a representation of the associated
text for each entity pair by using a Neural Bag of
Words model augmented with dependency fea-
tures. A dependency feature is a symbol for a word
having a specific dependency relation, such as
compound knowledge, compound−1 base
for the knowledge base noun compound.
Similar to the Entity Type representations, em-
beddings of words and dependency features
are aggregated by summation followed by L2-
normalization, and a special UNKNOWN symbol
is assigned to tuples whose pair of entities does
</p>
<p>326</p>
<p />
</div>
<div class="page"><p />
<p>All With Mentions Without Mentions
Model MRR H@10 MRR H@10 MRR H@10
</p>
<p>KB only
F 16.9 24.5 26.4 49.1 13.3 15.5
E 33.2 47.6 25.5 37.8 36.0 51.2
DistMult 35.7 52.3 26.0 39.0 39.3 57.2
E + DistMult 37.3 55.2 28.6 42.9 40.5 59.8
FRN trp 35.8 55.3 28.7 44.3 38.6 59.7
FRN trp + types 36.0 56.0 28.2 45.0 39.0 60.3
FRN trp + types + init 37.6 57.5 30.5 48.3 40.4 61.1
</p>
<p>KB and text
Conv-F 19.2 28.4 34.9 63.7 13.3 15.4
Conv-E 33.2 47.6 25.5 37.8 36.0 51.2
Conv-DistMult 36.6 53.5 28.3 43.4 39.7 57.2
Conv-E + Conv-DistMult 40.1 58.1 33.9 49.9 42.2 61.1
FRN trp + types + text 38.1 58.3 45.4 68.8 35.2 54.2
FRN trp + types + text + init 40.3 62.0 44.1 68.3 38.7 59.5
</p>
<p>Table 1: Evaluation results on the FB15k-237 dataset. Results for F,E,DistMult and their CNN versions
are reported from (Toutanova et al., 2015). With/Without Mentions indicates KB facts with/without
aligned textual relations for their entity pair.
</p>
<p>not have textual relation mentions. While our text
representation component is quite simple, similar
models have shown competitive performance on
modelling short text (Komninos and Manandhar,
2016).
</p>
<p>Initialization with Pre-trained Embeddings
We experiment with pre-trained embeddings to
initialize the entity vectors and text feature em-
beddings of our model. Text feature embeddings
are initialized from an available dependency based
skip-gram model trained on Wikipedia (Komninos
and Manandhar, 2016). Features that are not in-
cluded in the vocabulary of the pre-trained model
are initialized with a random vector from a nor-
mal distribution with zero mean and same variance
as the set of pre-trained embeddings. For entity
vectors, we retrieve the English name of the en-
tity from Freebase and construct a representation
by averaging the embeddings of the words appear-
ing in the name. Entities that do not have a name
property are initialized randomly.
</p>
<p>2.2 Training
</p>
<p>The network weights are optimized by mini-
mizing the binary cross-entropy loss over mini-
batches using the AdaM optimizer (Kingma and
Ba, 2014). To avoid the large computational cost
of training with all possible unobserved facts, we
</p>
<p>make use of negative sampling. The loss function
is:
</p>
<p>L(Θ) = −
∑
</p>
<p>|Xp|
log p(Xp)−
</p>
<p>∑
</p>
<p>|Xn|
log(1− p(Xn))
</p>
<p>(3)
where Θ are all the parameters of the network in-
cluding the feature embeddings, Xp are the ob-
served facts in the training set and Xn are ran-
domly drawn unobserved facts. We construct the
negative samples by fixing the subject entity and
relation, and uniformly sampling an object entity
with the restriction that the resulting triple is not
included in the training set. We then expand the
triple with entity type and text alignments. This
negative sampling schedule follows the evaluation
procedure, where the network has to rank triples
that only differ in the object entity position. Ex-
periments in the validation set indicated that for a
fixed number of negative samples, only consider-
ing negative samples that differ in the object po-
sition performs better than also including negative
samples for the subject position.
</p>
<p>3 Evaluation
</p>
<p>3.1 Dataset and Evaluation Protocol
The FB15k237 dataset consists of about 15k en-
tities and 237 relations derived from the FB15k
dataset (Toutanova et al., 2015). This sub-
</p>
<p>327</p>
<p />
</div>
<div class="page"><p />
<p>set of relations does not contain redundant re-
lations that can be easily inferred, resulting in
a more challenging task compared to the origi-
nal FB15k dataset. There are 310,116 triples in
the dataset split into 272,115/17,535/20,466 for
training/validation/testing. In addition to the KB,
the dataset includes dependency paths of approxi-
mately 2.7 million relation instances of linked en-
tity mentions extracted from the ClueWeb corpus
(Gabrilovich et al., 2013).
</p>
<p>Evaluation follows the procedure of (Toutanova
et al., 2015). Given a positive fact in the test set,
the subject entity and relation are fixed and models
have to rank all facts formed by the object entities
appearing in the training set. The reported met-
rics are mean reciprocal rank (MRR) and hits@10.
Hits@10 is the fraction of positive facts ranked in
the top 10 positions. Positive facts in the training,
validation and test set are removed before ranking.
</p>
<p>3.2 Implementation Details
Hyperparameters of the model were chosen by
maximizing MRR on the validation set. We use
two 300-dimensional hidden layers for the MLP,
and dimensions of feature embeddings are: 300
for entity and text features, 100 for relations and
20 for entity type features. The number of nega-
tive samples was set to 20 as increasing their num-
ber only resulted in minor gains, and the batch
size was set to 420. Best models were chosen
among 20 epochs of training by monitoring valida-
tion MRR. Models with embedding initializations
converged within the first 10 epochs. Initialization
in the text model includes initializing entity and re-
lation embeddings from a model without text men-
tions.
</p>
<p>3.3 Results
We compare our Feature-Rich Networks with the
bilinear models F and E (Riedel et al., 2013),
model DistMult (Yang et al., 2014) and their CNN
augmented versions (Toutanova et al., 2015). Re-
sults can be seen in Table 1.
</p>
<p>We first observe that when modelling just KB
triples, the MLP model outperforms individual bi-
linear formulations, performing similarly to the
best combination of DistMult + E. This shows that
an additive combination of bilinear models is a
strong baseline even though it does not use addi-
tional parameters other than embeddings to com-
pose and score triples. The addition of entity type
information has a positive but small contribution
</p>
<p>to performance. This is not surprising as entity
type information is extracted from observed re-
lations, and latent feature models can effectively
learn that during training. On the other hand, ini-
tializing entity embeddings with averaged word
embeddings of their names results in a substan-
tial performance gain of about 1.5 points in both
MRR and hits@10. In general, we observe that all
models perform worse on facts with textual rela-
tion mentions when they have not access to such
mentions.
</p>
<p>When textual relation mentions are added, we
observe that our proposed model increases its per-
formance score about 3 points in MRR and 4.5 in
hits@10 compared to the best model that does not
include text. Contrary to the conv-bilinear models,
the performance gain is much larger for facts with
textual mentions, reaching an additional 15/20 in
MRR/hits@10 respectively. We attribute this gain
to the explicitly represented textual relation align-
ments with the KB symbols as encoded by the ex-
panded tuple representations, and the non-linear
composition of its elements by the MLP. We also
notice that embedding initialization performs bet-
ter overall.
</p>
<p>4 Conclusion
</p>
<p>In this paper, we propose joint modelling of
Knowledge Bases and text with Feature-Rich Net-
works. Our models can learn to combine informa-
tion from different sources and better utilize noisy
information from text than bilinear models aug-
mented with convolutional neural networks. Be-
sides text, we experiment with entity types and
initialization with pre-trained embeddings, getting
positive gains in performance. An interesting di-
rection for future work is to combine our models
with additional aligned information, such as mul-
tiple KBs and to experiment with different compo-
nents such as CNNs or LSTMs for text encoding.
</p>
<p>Acknowledgements
</p>
<p>Alexandros Komninos was supported by EP-
SRC via an Engineering Doctorate in LSCITS.
Suresh Manandhar was supported by EPSRC
grant EP/I037512/1, A Unified Model of Compo-
sitional &amp; Distributional Semantics: Theory and
Application.
</p>
<p>328</p>
<p />
</div>
<div class="page"><p />
<p>References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
</p>
<p>Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, Springer, pages 722–735.
</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data. AcM, pages 1247–1250.
</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems. pages 2787–2795.
</p>
<p>Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. Facc1: Freebase annotation
of clueweb corpora, version 1 (release date 2013-
06-26, format version 1, correction level 0). Note:
http://lemurproject. org/clueweb09/FACC1/Cited by
5.
</p>
<p>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1. Association for Computational Linguis-
tics, pages 541–550.
</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
</p>
<p>Alexandros Komninos and Suresh Manandhar. 2016.
Dependency based embeddings for sentence classifi-
cation tasks. In Proceedings of NAACL:HLT . Asso-
ciation for Computational Linguistics, pages 1490–
1500.
</p>
<p>Xiao Ling and Daniel S Weld. 2012. Fine-grained en-
tity recognition. In AAAI. Citeseer.
</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP:
Volume 2-Volume 2. Association for Computational
Linguistics, pages 1003–1011.
</p>
<p>Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning (ICML-10). pages 807–814.
</p>
<p>Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2016. A review of relational
machine learning for knowledge graphs. Proceed-
ings of the IEEE 104(1):11–33.
</p>
<p>Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
the 28th international conference on machine learn-
ing (ICML-11). pages 809–816.
</p>
<p>Siva Reddy, Mirella Lapata, and Mark Steedman. 2014.
Large-scale semantic parsing without question-
answer pairs. Transactions of the Association for
Computational Linguistics 2:377–392.
</p>
<p>Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas .
</p>
<p>Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Advances in neural information processing systems.
pages 926–934.
</p>
<p>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 joint conference on empirical
methods in natural language processing and compu-
tational natural language learning. Association for
Computational Linguistics, pages 455–465.
</p>
<p>Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP. Citeseer, vol-
ume 15, pages 1499–1509.
</p>
<p>Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2014. Embedding entities and
relations for learning and inference in knowledge
bases. arXiv preprint arXiv:1412.6575 .
</p>
<p>329</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 330–334
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2052
</p>
<p>Fine-Grained Entity Typing with High-Multiplicity Assignments
</p>
<p>Maxim Rabinovich and Dan Klein
Computer Science Division
</p>
<p>University of California, Berkeley
{rabinovich,klein}@cs.berkeley.edu
</p>
<p>Abstract
</p>
<p>As entity type systems become richer and
more fine-grained, we expect the number
of types assigned to a given entity to in-
crease. However, most fine-grained typing
work has focused on datasets that exhibit
a low degree of type multiplicity. In this
paper, we consider the high-multiplicity
regime inherent in data sources such as
Wikipedia that have semi-open type sys-
tems. We introduce a set-prediction ap-
proach to this problem and show that our
model outperforms unstructured baselines
on a new Wikipedia-based fine-grained
typing corpus.
</p>
<p>1 Introduction
</p>
<p>Motivated by potential applications to information
retrieval, coreference resolution, question answer-
ing, and other downstream tasks, recent work on
entity typing has moved beyond coarse-grained
systems towards richer ontologies with much more
detailed information, and therefore correspond-
ingly more specific types (Ling and Weld, 2012;
Gillick et al., 2014; Yogatama et al., 2015).
</p>
<p>As types become more specific, entities will
tend to belong to more types (i.e. there will tend to
be higher type multiplicity). However, most data
used in previous work exhibits an extremely low
degree of multiplicity.
</p>
<p>In this paper, we focus on the high multiplic-
ity case, which we argue naturally arises in large-
scale knowledge resources. To illustrate this point,
we construct a corpus of entity mentions paired
with higher-multiplicity type assignments. Our
corpus is based on mentions and categories drawn
from Wikipedia, but we generalize and denoise the
raw Wikipedia categories to provide more coher-
ent supervision. Table 1 gives examples of type
</p>
<p>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
</p>
<p># Types
0.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>P
ro
</p>
<p>po
rt
</p>
<p>io
n
</p>
<p>Figer
Wikipedia
</p>
<p>Figure 1: Comparison of type set size CDFs for
the our Wikipedia corpus and the prior FIGER cor-
pus (Ling and Weld, 2012). The figure illustrates
that our corpus exhibits much greater type assign-
ment multiplicity.
</p>
<p>assignments from our dataset.
As type multiplicity grows, it is natural to con-
</p>
<p>sider type prediction as an inherently set-valued
problem and ask questions about how such sets
might be modeled. To this end, we develop a struc-
tured prediction approach in which the sets of as-
signed types are predicted as first-class objects, in-
cluding a preliminary consideration of how to ef-
ficiently search over them. The resulting model
captures type correlations and ultimately outper-
forms a strong unstructured baseline.
</p>
<p>Related work The fine-grained entity typing
problem was first investigated in detail by Ling
and Weld (2012). Subsequently, Gillick et al.
(2014) introduced a larger evaluation corpus for
this task and introduced methods for training pre-
dictors based on multiclass classification. Both
used the Freebase typing system, coarsened to
approximately 100 types, and subsequent work
</p>
<p>330</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2052">https://doi.org/10.18653/v1/P17-2052</a></div>
</div>
<div class="page"><p />
<p>David Foster Wallace novelist suicide sportswriter writer alumnus ...
Albert Einstein physicist agnostic emigrant people pacifist ...
</p>
<p>NATO organization treaty document organisation alliance ...
Federal Reserve agency authorities banks institution organization ...
</p>
<p>Industrial Revolution concept history evolution revolution past ...
Black Death concept epidemic pandemic disaster ...
</p>
<p>Table 1: With types from a large corpus like Wikipedia, large type set assignments become common.
</p>
<p>Entity Raw Type Projected Type
</p>
<p>David Foster Wallace
</p>
<p>Short story writers writer
Amherst alumni alumnus
</p>
<p>Illinois State faculty faculty
People from New York people
</p>
<p>Essayists essayist
</p>
<p>Table 2: Example of an entity and its types, before and after projection. The projection operation col-
lapses related types that would be very difficult to learn in their original, highly specific forms.
</p>
<p>has mostly followed this lead (Yaghoobzadeh and
Schütze, 2016; Yogatama et al., 2015), although
types based on WordNet have recently also been
investigated (Corro et al., 2015).
</p>
<p>Most prior work has focused on unstructured
predictors using some form of multiclass logistic
regression (Ling and Weld, 2012; Gillick et al.,
2014; Shimaoka et al., 2016; Yaghoobzadeh and
Schütze, 2016; Yogatama et al., 2015). Some
of these approaches implicitly incorporate struc-
ture during decoding by enforcing hierarchy con-
straints (Gillick et al., 2014), while neural ap-
proaches can encode correlations in a soft manner
via shared hidden layers (Shimaoka et al., 2016;
Yaghoobzadeh and Schütze, 2016).
</p>
<p>Our work differs from these lines of work in two
respects: its use of a corpus exhibiting high type
multiplicity with types derived from a semi-open
inventory and its use of a fully structured model
and decoding procedure, one that can in principle
be integrated with neural models if desired. Previ-
ously, most results focused on the low-multiplicity
Freebase-based FIGER corpus. The only work we
are aware of that uses a type system similar to ours
used a rule-based system and evaluated on their
own newswire- and Twitter-based evaluation cor-
pora (Corro et al., 2015).
</p>
<p>2 Model
</p>
<p>Our structured prediction framework is based on
modeling type assignments as sets. Each entity e
is assigned a set of types T ∗ drawn from the over-
</p>
<p>all set of types T . Our goal is thus to predict, given
an input sentence-entity pair, the set of types asso-
ciated with that entity.
</p>
<p>We take the commonly-used linear model ap-
proach to this structured prediction problem.
Given a featurizer ϕ that takes an input sentence
x and entity e, we seek to learn a weight vector w
such that
</p>
<p>f (x, e) = argmaxT w
&gt;ϕ (x, e, T ) (1)
</p>
<p>predicts T correctly with high accuracy.
Our approach stands in contrast to prior work,
</p>
<p>which deployed several techniques, of similar ef-
ficacy, to port single-type learning and inference
strategies to the multi-type setting (Gillick et al.,
2014). Provided type interactions can be ne-
glected, equation (1) can be simplified to
</p>
<p>fsingle (x, e) =
{
t ∈ T : w&gt;ϕ (x, e, t) ≥ r
</p>
<p>}
.
</p>
<p>This simplification corresponds to expanding each
multi-type example triple (x, e, T ∗) into a set
of single-type example triples
</p>
<p>{
(x, e, t∗)t∗∈T ∗
</p>
<p>}
.
</p>
<p>Learning can then be done using any technique
for multiclass logistic regression, and inference
can be carried out by specifying a threshold r and
predicting all types that score above that thresh-
old: In prior work, a simple r = 0 threshold was
used (Ling and Weld, 2012).
</p>
<p>In this paper, we focus on the more general
specification (1), though in Section 2.2, we explain
a simplification that can be used to speed up infer-
ence if desired.
</p>
<p>331</p>
<p />
</div>
<div class="page"><p />
<p>2.1 Features
Modeling type assignments as sets in principle
opens the door to non-decomposable set features
(a simple instance of which would be set size). For
reasons of tractability, we assume our features fac-
tor along type pairs:
</p>
<p>ϕ (x, e, T ) =
∑
</p>
<p>t∈T
ϕ (x, e, t) +
</p>
<p>∑
</p>
<p>t, t′∈T
ϕ
(
t, t′
</p>
<p>)
</p>
<p>(2)
Note that in addition to enforcing factorization
over type pairs, the specification (2) requires that
any features linking the type assignment to the ob-
served entity mention depend only on a single type
at a time. We investigated non-decomposable fea-
tures, but found they did not lead to improved per-
formance.
</p>
<p>We use entity mention features very similar to
those in previous work:
</p>
<p>1. Context unigrams and bigrams. Indicators
on all uni- and bigrams within a certain win-
dow of the entity mention.
</p>
<p>2. Dependency parse features. Indicators on
the lexical parent of the entity mention head,
as well as the corresponding dependency
type. Separately, indicators on the lexical
children of the entity mention head and their
dependency types.
</p>
<p>3. Entity head and non-head tokens. Indica-
tors on the syntactic head of the entity men-
tion and on its non-head tokens.
</p>
<p>4. Word shape features. Indicators on the
shape of each token in the entity mention.
</p>
<p>We combine these features with type-based fea-
tures to obtain the features our model actually
uses:
</p>
<p>1. Conjunction features. These are simple
conjunctions of mention features with indi-
cators on type membership in the predicted
set. Using only these features results in an
unstructured model.
</p>
<p>2. Type pair features. These are indicators on
pairs of types appearing in the predicted set.
</p>
<p>3. Graph-based features. As we discuss in
Section 3, the type system in our corpus
comes with a graph structure. We add indica-
tors on certain patterns occurring within the
</p>
<p>medical
</p>
<p>worker
scholar
</p>
<p>doctor researcher
</p>
<p>surgeon
medical 
</p>
<p>specialist
</p>
<p>heart
</p>
<p>surgeon
oncologist
</p>
<p>Figure 2: Fragment of the graph underlying our
type system.
</p>
<p>set–e.g. a parent-child type pair, sibling type
pairs, and so on, abstracting away the specific
types.
</p>
<p>2.2 Learning and Inference
</p>
<p>We train our system using structured max-
margin (Tsochantaridis et al., 2005). Optimization
is performed via AdaGrad on the primal (Kum-
merfeld et al., 2015). We use set-F1 as our loss
function.
</p>
<p>Inference, for both prediction and loss-
augmented decoding, poses a greater challenge, as
solving the maximization problem (1) exactly re-
quires iterating over all subsets of the type system.
</p>
<p>Fortunately, we find a simple greedy algorithm
is effective. Our decoder begins by choosing the
type that scores highest individually, taking only
single-type features into account. It then proceeds
by iteratively adding new types into the set until
doing so would decrease the score.
</p>
<p>At the cost of restricting the permissible type
sets slightly, we can speed up the greedy procedure
further. Specifically, we can require that the pre-
dicted type set T be connected in some constraint
graph over the types—either the co-occurrence
graph, the complete graph, or the graph underly-
ing the type system. If we denote by C the set of
all such connected sets, the corresponding predic-
tor would be
</p>
<p>fconn (x, e) = argmaxT∈C w
&gt;ϕ (x, e, T )
</p>
<p>332</p>
<p />
</div>
<div class="page"><p />
<p>Level Features P R F1
Entity Unstructured 50.0 67.2 52.9
</p>
<p>+ Pairs 53.3 64.1 54.3
+ Graph 53.9 63.9 54.5
</p>
<p>Sentence Unstructured 42.6 58.9 44.4
+ Pairs 46.5 54.1 45.6
+ Graph 47.0 53.6 45.6
</p>
<p>Table 3: Results on our corpus. All quantities are
macro-averaged.
</p>
<p>The greedy decoding procedure for this predictor
is faster because at each step, it need only consider
adding types that are adjacent to some type that
has already been included.
</p>
<p>3 Corpus
</p>
<p>Our corpus construction methodology involves
three key stages: mention identification, type sys-
tem construction, and type assignment.1 We ex-
plain each of these in turn.
</p>
<p>Mention identification. We follow prior work
on entity linking (Durrett and Klein, 2014) and
take all mentions that occur as anchor text. We
filter the resulting collection of mentions down to
those that pass a heuristic filter that removes men-
tions of common nouns, as well as spurious sen-
tences representing Wikipedia formatting.
</p>
<p>Type system construction. Prior work on fine-
grained entity typing has derived its type sys-
tem from Freebase (Ling and Weld, 2012; Gillick
et al., 2014). The resulting ontologies thus inherit
the coverage and specificity limitations of Free-
base, somewhat exacerbated by manual coarsen-
ing.
</p>
<p>Motivated by efforts to inject broader cover-
age, more complex knowledge resources into NLP
systems, we instead derive our types from the
Wikipedia category and WordNet graphs, in a
manner similar to that of Ponzetto and Strube
(2007).
</p>
<p>Our base type set consists of all Wikipedia cat-
egories. By following back-pointers in articles for
categories, we derive a base underlying directed
graph. To eliminate noise, we filter down to all
categories whose syntactic heads can be found in
WordNet and keep directed edges only when the
head of the parent is a WordNet ancestor of the
</p>
<p>1Our corpus will be released at http://people.
eecs.berkeley.edu/˜rabinovich/.
</p>
<p>102 103 104 105
</p>
<p># Type Occurrences
0.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>T
yp
</p>
<p>e
F
</p>
<p>1
</p>
<p>Figure 3: Per-type F1 scores plotted by type fre-
quency in the training corpus.
</p>
<p>head of the child. We conclude by projecting each
type down to its syntactic head.
</p>
<p>Type assignment. The type set for an entity
is obtained by taking its Wikipedia category as-
signments, augmenting these with their ancestors
in the category graph above, and then projecting
these down to their syntactic heads.
</p>
<p>4 Experiments
</p>
<p>We evaluate our method on the dataset described
in Section 3. For these experiments, we restrict
to the 100 most frequent types and downsam-
ple to 750K mentions. We use a baseline that
closely replicates the FIGER system (Ling and
Weld, 2012). Within our framework, this can be
thought of as a model that sets all type pair fea-
tures in (2) to zero.
</p>
<p>Table 3 summarizes our results. Starting with
the baseline, we incrementally add the type pair,
graph-based, and set size features discussed in 2.1.
Adding type pair features results in an appreciable
performance gain, while the graph features bring
little benefit—potentially because pairwise corre-
lations suffice to summarize the set structure when
the number of types is moderately low.
</p>
<p>A concern when studying multiclass problems
with large numbers of classes, whether predict-
ing sets or individual labels, is that performance
on instances associated with common classes will
dominate the performance metric. Figure 3 shows
micro-averaged F1 for the binary prediction task
associated with predicting the presence or absence
of each type, demonstrating that our performance
is strong even for many rare types.
</p>
<p>333</p>
<p />
</div>
<div class="page"><p />
<p>5 Conclusion
</p>
<p>We have highlighted the issue of multiplicity in
fine-grained entity typing. Whereas most prior
work has focused on corpora with low multiplicity
assignments, we denoised the Wikipedia type sys-
tem to construct a realistic corpus with high mul-
tiplicity type assignments. Using this corpus as
a testbed, we showed that an approach based on
structured prediction of sets can outperform un-
structured baselines when type assignments have
high multiplicity. Our approach may therefore be
preferable in such contexts.
</p>
<p>References
Luciano del Corro, Abdalghani Abujabal, Rainer
</p>
<p>Gemulla, and Gerhard Weikum. 2015. Finet:
Context-aware fine-grained named entity typing.
Assoc. for Computational Linguistics.
</p>
<p>Greg Durrett and Dan Klein. 2014. A joint model for
entity analysis: Coreference, typing, and linking.
Transactions of the Association for Computational
Linguistics 2:477–490.
</p>
<p>Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014. Context-
dependent fine-grained entity type tagging. arXiv
preprint arXiv:1412.1820 .
</p>
<p>Jonathan K Kummerfeld, Taylor Berg-Kirkpatrick, and
Dan Klein. 2015. An empirical analysis of optimiza-
tion for max-margin NLP. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. volume 273, page 279.
</p>
<p>Xiao Ling and Daniel S Weld. 2012. Fine-grained en-
tity recognition. In Twenty-Sixth AAAI Conference
on Artificial Intelligence.
</p>
<p>Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from wikipedia.
</p>
<p>Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2016. An attentive neural ar-
chitecture for fine-grained entity type classification.
arXiv preprint arXiv:1604.05525 .
</p>
<p>Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research
6(Sep):1453–1484.
</p>
<p>Yadollah Yaghoobzadeh and Hinrich Schütze.
2016. Corpus-level fine-grained entity typing
using contextual information. arXiv preprint
arXiv:1606.07901 .
</p>
<p>Dani Yogatama, Dan Gillick, and Nevena Lazic. 2015.
Embedding methods for fine grained entity type
classification. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Fed-
eration of Natural Language Processing, ACL.
</p>
<p>334</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 335–340
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2053
</p>
<p>Group Sparse CNNs for Question Classification with Answer Sets
</p>
<p>Mingbo Ma Liang Huang
School of EECS
</p>
<p>Oregon State University
Corvallis, OR 97331, USA
</p>
<p>{mam,liang.huang}@oregonstate.edu
</p>
<p>Bing Xiang Bowen Zhou
IBM Watson Group
</p>
<p>T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
</p>
<p>{bingxia,zhou}@us.ibm.com
</p>
<p>Abstract
</p>
<p>Question classification is an important
task with wide applications. However, tra-
ditional techniques treat questions as gen-
eral sentences, ignoring the corresponding
answer data. In order to consider answer
information into question modeling, we
first introduce novel group sparse autoen-
coders which refine question representa-
tion by utilizing group information in the
answer set. We then propose novel group
sparse CNNs which naturally learn ques-
tion representation with respect to their
answers by implanting group sparse au-
toencoders into traditional CNNs. The
proposed model significantly outperform
strong baselines on four datasets.
</p>
<p>1 Introduction
</p>
<p>Question classification has applications in many
domains ranging from question answering to di-
alog systems, and has been increasingly popular
in recent years. Several recent efforts (Kim, 2014;
Kalchbrenner et al., 2014; Ma et al., 2015) treat
questions as general sentences and employ Con-
volutional Neural Networks (CNNs) to achieve re-
markably strong performance in the TREC ques-
tion classification task.
</p>
<p>We argue, however, that those general sentence
modeling frameworks neglect two unique proper-
ties of question classification. First, different from
the flat and coarse categories in most sentence
classification tasks (i.e. sentimental classification),
question classes often have a hierarchical struc-
ture such as those from the New York State DMV
FAQ1 (see Fig. 1). Another unique aspect of ques-
tion classification is the well prepared answers for
each question or question category. These answer
</p>
<p>1Crawled from http://nysdmv.custhelp.com/app/home.
This data and our code will be at http://github.com/cosmmb.
</p>
<p>1: Driver License/Permit/Non-Driver ID
a: Apply for original (49 questions)
b: Renew or replace (24 questions)
...
2: Vehicle Registrations and Insurance
a: Buy, sell, or transfer a vehicle (22 questions)
b: Reg. and title requirements (42 questions)
...
3: Driving Record / Tickets / Points
...
</p>
<p>Figure 1: Examples from NYDMV FAQs. There
are 8 top-level categories, 47 sub-categories, and
537 questions (among them 388 are unique; many
questions fall into multiple categories).
</p>
<p>sets generally cover a larger vocabulary (than the
questions themselves) and provide richer informa-
tion for each class. We believe there is a great po-
tential to enhance question representation with ex-
tra information from corresponding answer sets.
</p>
<p>To exploit the hierarchical and overlapping
structures in question categories and extra infor-
mation from answer sets, we consider dictionary
learning (Candès and Wakin, 2008; Rubinstein
et al., 2010) which is a common approach for rep-
resenting samples from many correlated groups
with external information. This learning pro-
cedure first builds a dictionary with a series of
grouped bases. These bases can be initialized ran-
domly or from external data (from the answer set
in our case) and optimized during training through
Sparse Group Lasso (SGL) (Simon et al., 2013).
</p>
<p>To apply dictionary learning to CNN, we first
develop a neural version of SGL, Group Sparse
Autoencoders (GSAs), which to the best of our
knowledge, is the first full neural model with
group sparse constraints. The encoding matrix
of GSA (like the dictionary in SGL) is grouped
into different categories. The bases in different
groups can be either initialized randomly or by
</p>
<p>335</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2053">https://doi.org/10.18653/v1/P17-2053</a></div>
</div>
<div class="page"><p />
<p>the sentences in corresponding answer categories.
Each question sentence will be reconstructed by
a few bases within a few groups. GSA can use
either linear or nonlinear encoding or decoding
while SGL is restricted to be linear. Eventually,
to model questions with sparsity, we further pro-
pose novel Group Sparse Convolutional Neural
Networks (GSCNNs) by implanting the GSA onto
CNNs, essentially enforcing group sparsity be-
tween the convolutional and classification layers.
This framework is a jointly trained neural model
to learn question representation with group sparse
constraints from both question and answer sets.
</p>
<p>2 Group Sparse Autoencoders
</p>
<p>2.1 Sparse Autoencoders
</p>
<p>Autoencoder (Bengio et al., 2007) is an unsuper-
vised neural network which learns the hidden rep-
resentations from data. When the number of hid-
den units is large (e.g., bigger than input dimen-
sion), we can still discover the underlying struc-
ture by imposing sparsity constraints, using sparse
autoencoders (SAE) (Ng, 2011):
</p>
<p>Jsparse(ρ) = J + α
</p>
<p>s∑
</p>
<p>j=1
</p>
<p>KL(ρ‖ρ̂j) (1)
</p>
<p>where J is the autoencoder reconstruction loss, ρ
is the desired sparsity level which is small, and
thus Jsparse(ρ) is the sparsity-constrained version
of loss J . Here α is the weight of the sparsity
penalty term defined below:
</p>
<p>KL(ρ‖ρ̂j) = ρ log
ρ
</p>
<p>ρ̂j
+ (1− ρ) log 1− ρ
</p>
<p>1− ρ̂j
(2)
</p>
<p>where
</p>
<p>ρ̂j =
1
</p>
<p>m
</p>
<p>m∑
</p>
<p>i=1
</p>
<p>hij
</p>
<p>represents the average activation of hidden unit j
over m examples (SAE assumes the input features
are correlated).
</p>
<p>As described above, SAE has a similar objec-
tive to traditional sparse coding which tries to find
sparse representations for input samples. Besides
applying simple sparse constraints to the network,
group sparse constraints is also desired when the
class categories are structured and overlapped. In-
spired by group sparse lasso (Yuan and Lin, 2006)
and sparse group lasso (Simon et al., 2013), we
propose a novel architecture below.
</p>
<p>2.2 Group Sparse Autoencoders
Group Sparse Autoencoder (GSA), unlike SAE,
categorizes the weight matrix into different
groups. For a given input, GSA reconstructs the
input signal with the activations from only a few
groups. Similar to the average activation ρ̂j for
sparse autoencoders, GSA defines each grouped
average activation for the hidden layer as follows:
</p>
<p>η̂p =
1
</p>
<p>mg
</p>
<p>m∑
</p>
<p>i=1
</p>
<p>g∑
</p>
<p>l=1
</p>
<p>‖hip,l‖2 (3)
</p>
<p>where g represents the size of each group, and η̂j
first sums up all the activations within pth group,
then computes the average pth group respond
across different samples’ hidden activations.
</p>
<p>Similar to Eq. 2, we also use KL divergence
to measure the difference between estimated intra-
group activation and global group sparsity:
</p>
<p>KL(η‖η̂p) = η log
η
</p>
<p>η̂p
+ (1− η) log 1− η
</p>
<p>1− η̂p
(4)
</p>
<p>where G is the number of groups. Then the objec-
tive function of GSA is:
</p>
<p>Jgroupsparse(ρ, η) = J + α
</p>
<p>s∑
</p>
<p>j=1
</p>
<p>KL(ρ‖ρ̂j)
</p>
<p>+ β
G∑
</p>
<p>p=1
</p>
<p>KL(η‖η̂p)
(5)
</p>
<p>where ρ and η are constant scalars which are
our target sparsity and group-sparsity levels, resp.
When α is set to zero, GSA only considers the
structure between difference groups. When β is
set to zero, GSA is reduced to SAE.
</p>
<p>2.3 Visualizing Group Sparse Autoencoders
In order to have a better understanding of GSA, we
use the MNIST dataset to visualize GSA’s internal
parameters. Fig. 2 and Fig. 3 illustrate the pro-
jection matrix and the corresponding hidden acti-
vations. We use 10,000 training samples. We set
the size of the hidden layer to 500 with 10 groups.
Fig. 2(a) visualizes the input image for hand writ-
ten digit 0.
</p>
<p>In Fig. 2(b), we find similar patterns within each
group. For example, group 8 has different forms
of digit 0, and group 9 includes different forms of
digit 7. However, it is difficult to see any mean-
ingful patterns from the projection matrix of basic
autoencoders in Fig. 2(c).
</p>
<p>336</p>
<p />
</div>
<div class="page"><p />
<p>(a)
</p>
<p>1

2

3

4

5

6

7

8

9

10
</p>
<p>(b) (c)
</p>
<p>Figure 2: The input figure with hand written digit 0 is shown in (a). Figure (b) is the visualization of
trained projection matrix W on MNIST dataset. Different rows represent different groups of W in Eq. 5.
For each group, we only show the first 15 (out of 50) bases. The red numbers on the left side are the
indices of 10 different groups. Figure (c) is the projection matrix from basic autoencoders.
</p>
<p>1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10
(a) (b)
</p>
<p>Figure 3: (a): the hidden activations h for the input image in Fig. 2(a). The red numbers corresponds to
the index in Fig. 2(b). (b): the hidden activations h for the same input image from basic autoencoders.
</p>
<p>Fig. 3(a) shows the hidden activations with re-
spect to the input image of digit 0. The patterns of
the 10th row in Fig. 2(b) are very similar to digit
1 which is very different from digit 0 in shape.
Therefore, there is no activation in group 10 in
Fig. 3(a). The majority of hidden layer activations
are in groups 1, 2, 6 and 8, with group 8 being the
most significant. When compared to the projection
matrix visualization in Fig. 2(b), these results are
reasonable since the 8th row has the most similar
patterns of digit 0. However, we could not find any
meaningful pattern from the hidden activations of
basic autoencoder as shown in Fig. 3(b).
</p>
<p>GSA could be directly applied to small image
data (e.g. MINIST dataset) for pre-training. How-
ever, in tasks which prefer dense semantic rep-
resentations (e.g. sentence classification), we still
need CNNs to learn the sentence representation
automatically. In order to combine advantages
from GSA and CNNs, we propose Group Sparse
</p>
<p>Convolutional Neural Networks below.
</p>
<p>3 Group Sparse CNNs
</p>
<p>CNNs were first proposed by (LeCun et al., 1995)
in computer vision and adapted to NLP by (Col-
lobert et al., 2011). Recently, many CNN-based
techniques have achieved great successes in sen-
tence modeling and classification (Kim, 2014;
Kalchbrenner et al., 2014).
</p>
<p>Following sequential CNNs, one dimensional
convolutions operate the convolution kernel in se-
quential order xi,j = xi ⊕ xi+1 ⊕ · · · ⊕ xi+j ,
where xi ∈ Re represents the e dimensional word
representation for the i-th word in the sentence,
and ⊕ is the concatenation operator. Therefore
xi,j refers to concatenated word vector from the
i-th word to the (i+ j)-th word in sentence.
</p>
<p>A convolution operates a filter w ∈ Rn×e to
a window of n words xi,i+n with bias term b′ by
ai = σ(w · xi,i+n + b′) with non-linear activation
</p>
<p>337</p>
<p />
</div>
<div class="page"><p />
<p>Any  interesting   places   to   visit   in   Lisbon
</p>
<p>… … … … … …
</p>
<p>N
 fi
</p>
<p>lte
rs
</p>
<p>(
</p>
<p>Pooling Feed
 into 
NN
</p>
<p>Group Sparse Auto-Encoder
</p>
<p>Convolutional Layer
</p>
<p>WTz h
</p>
<p>z h�W,b(·)�W,b(·)�W,b(·)
Figure 4: Group Sparse CNN. We add an extra dictionary learning layer between sentence representation
z and the final classification layer. W is the projection matrix (functions as a dictionary) that converts z
to the group sparse representation h (Eq. 5). Different colors in the projection matrix represent different
groups. We show Wᵀ instead of W for presentation purposes. Darker colors in h mean larger values
and white means zero.
</p>
<p>function σ to produce a new feature. The filter w
is applied to each word in the sentence, generating
the feature map a = [a1, a2, · · · , aL] where L is
the sentence length. We then use â = max{a} to
represent the entire feature map after max-pooling.
</p>
<p>In order to capture different aspects of patterns,
CNNs usually randomly initialize a set of filters
with different sizes and values. Each filter will
generate a feature as described above. To take all
the features generated by N different filters into
count, we use z = [â1, · · · , âN ] as the final rep-
resentation. In conventional CNNs, this z will be
directly fed into classifiers after the sentence rep-
resentation is obtained, e.g. fully connected neural
networks (Kim, 2014). There is no easy way for
CNNs to explore the possible hidden representa-
tions with underlaying structures.
</p>
<p>In order to exploit these structures, we pro-
pose Group Sparse Convolutional Neural Net-
works (GSCNNs) by placing one extra layer be-
tween the convolutional and the classification lay-
ers. This extra layer mimics the functionality of
GSA from Section 2. Shown in Fig. 4, after the
conventional convolutional layer, we get the fea-
ture map z for each sentence. In stead of directly
feeding it into a fully connected neural network
for classification, we enforce the group sparse con-
straint on z in a way similar to the group sparse
constraints on hidden layer in GSA from Sec. 2.
Then, we use the sparse hidden representation h
in Eq. 5 as the new sentence representation, which
is then fed into a fully connected neural network
for classification. The parameters W in Eq. 5 will
</p>
<p>also be fine tunned during the last step.
Different ways of initializing the projection ma-
</p>
<p>trix in Eq. 5 can be summarized below:
</p>
<p>• Random Initialization: When there is no an-
swer corpus available, we first randomly ini-
tializeN vectors to represent the group infor-
mation from the answer set. Then we clus-
ter these N vectors into G categories with g
centroids for each category. These centroids
from different categories will be the initial-
ized bases for projection matrix W which
will be learned during training.
</p>
<p>• Initialization from Questions: Instead of
using random initialized vectors, we can also
use question sentences for initializing the
projection matrix when the answer set is not
available. We need to pre-train the sentences
with CNNs to get the sentence representa-
tion. We then select G largest categories in
terms of number of question sentences. Then
we get g centroids from each category by k-
means. We concatenate these G × g vectors
to form the projection matrix.
</p>
<p>• Initialization from Answers: This is the
most ideal case. We follow the same proce-
dure as above, with the only difference being
using the answer sentences in place of ques-
tion sentences to pre-train the CNNs.
</p>
<p>4 Experiments
</p>
<p>Since there is little effort to use answer sets in
question classification, we did not find any suit-
</p>
<p>338</p>
<p />
</div>
<div class="page"><p />
<p>Datasets Ct Cs Ndata Ntest Nans Multi-label
TREC 6 50 5952 500 - No
INSURANCE - 319 1580 303 2176 Yes
DMV 8 47 388 50 2859 Yes
YAHOO Ans 27 678 8871 3027 10365 No
</p>
<p>Table 1: Summary of datasets. Ct and Cs are
the numbers of top-level and sub- categories, resp.
Ndata, Ntest, Nans are the sizes of data set, test
set, and answer set, resp. Multilabel means each
question can belong to multiple categories.
</p>
<p>able datasets which are publicly available. We
collected two datasets ourselves and also used
two other well-known ones. These datasets are
summarized in Table 1. INSURANCE is a pri-
vate dataset we collected from a car insurance
company’s website. Each question is classified
into 319 classes with corresponding answer data.
All questions which belong to the same category
share the same answers. The DMV dataset is col-
lected from New York State the DMV’s FAQ web-
site. The YAHOO Ans dataset is only a subset
of the original publicly available YAHOO Answers
dataset (Fleming et al., 2012; Shah and Pomerantz,
2010). Though not very suitable for our frame-
work, we still included the frequently used TREC
dataset (factoid question type classification) for
comparison.
</p>
<p>We only compare our model’s performance with
CNNs for two following reasons: we consider our
“group sparsity” as a modification to the general
CNNs for grouped feature selection. This idea is
orthogonal to any other CNN-based models and
can be easily applied to them; in addition, as dis-
cussed in Sec. 1, we did not find any other model
in comparison with solving question classification
tasks with answer sets.
</p>
<p>There is crucial difference between the INSUR-
ANCE and DMV datasets on one hand and the YA-
HOO set on the other. In INSURANCE and DMV,
all questions in the same (sub)category share the
same answers, whereas YAHOO provides individ-
ual answers to each question.
</p>
<p>For multi-label classification (INSURANCE and
DMV), we replace the softmax layer in CNNs
with a sigmoid layer which predicts each category
independently while softmax is not.
</p>
<p>All experimental results are summarized in Ta-
ble 2. The improvements are substantial for IN-
SURANCE and DMV, but not as significant for
YAHOO and TREC. One reason for this is the
</p>
<p>TREC INSUR. DMV
YAHOO dataset
</p>
<p>sub top unseen
CNN† 93.6 51.2 60 20.8 53.9 47
</p>
<p>+sparsity‡ 93.2 51.4 62 20.2 54.2 46
WR 93.8 53.5 62 21.8 54.5 48
WQ 94.2 53.8 64 22.1 54.1 48
WA - 55.4 66 22.2 55.8 53
</p>
<p>Table 2: Experimental results. Baselines:
†sequential CNNs (α = β = 0 in Eq. 5), ‡CNNs
with global sparsity (β = 0). WR: randomly
initialized projection matrix. WQ: question-
initialized projection matrix. WA: answer set-
initialized projection matrix. There are three dif-
ferent classification settings for YAHOO: subcate-
gory, top-level category, and top-level accuracies
on unseen sub-labels.
</p>
<p>questions in YAHOO/TREC are shorter, which
makes the group information harder to encode.
Another reason is that each question in YA-
HOO/TREC has a single label, and thus can not
fully benefit from group sparse properties.
</p>
<p>Besides the conventional classification tasks,
we also test our proposed model on an unseen-
label case. In these experiments, there are a few
sub-category labels that are not included in the
training data. However, we still hope that our
model could still return the correct parent cate-
gory for these unseen subcategories at test time.
In the testing set of YAHOO dataset, we randomly
add 100 questions whose subcategory labels are
unseen in training set. The classification results of
YAHOO-unseen in Table 2 are obtained by map-
ping the predicted subcategories back to top-level
categories. The improvements are substantial due
to the group information encoding.
</p>
<p>5 Conclusions
</p>
<p>In order to better represent question sentences with
answer sets and group structure, we first presented
a novel GSA framework, a neural version of dic-
tionary learning. We then proposed group sparse
convolutional neural networks by embedding GSA
into CNNs, which result in significantly better
question classification over strong baselines.
</p>
<p>Acknowledgment
</p>
<p>We thank the anonymous reviewers for their sug-
gestions. This work is supported in part by
NSF IIS-1656051, DARPA FA8750-13-2-0041
(DEFT), DARPA XAI, a Google Faculty Research
Award, and an HP Gift.
</p>
<p>339</p>
<p />
</div>
<div class="page"><p />
<p>References
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
</p>
<p>Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems 19.
</p>
<p>Emmanuel J. Candès and Michael B. Wakin. 2008.
An Introduction To Compressive Sampling. In
Signal Processing Magazine, IEEE. volume 25.
http://dx.doi.org/10.1109/msp.2007.914731.
</p>
<p>R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. In Journal
of Machine Learning Research. volume 12, pages
2493–2537.
</p>
<p>Simon Fleming, Dan Chalmers, and Ian Wakeman.
2012. A deniable and efficient question and answer
service over ad hoc social networks. In Information
Retrieval.
</p>
<p>Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics.
</p>
<p>Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 1746–
1751. http://www.aclweb.org/anthology/D14-1181.
</p>
<p>Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,
J. Denker, H. Drucker, I. Guyon, U. Mller,
E. Sckinger, P. Simard, and V. Vapnik. 1995. Com-
parison of learning algorithms for handwritten digit
recognition. In International Conference on Artifi-
cial Neural Networks. pages 53–60.
</p>
<p>Mingbo Ma, Liang Huang, Bing Xiang, and Bowen
Zhou. 2015. Dependency-based convolutional neu-
ral networks for sentence embedding. In Proceed-
ings of ACL 2015.
</p>
<p>Andrew Ng. 2011. Sparse autoencoder. In CS294A
Lecture notes. Stanford University, page 72.
</p>
<p>R. Rubinstein, A. M. Bruckstein, and M. Elad. 2010.
Dictionaries for sparse representation modeling. In
Neural Computation.
</p>
<p>Chirag Shah and Jefferey Pomerantz. 2010. Evaluating
and predicting answer quality in community qa. In
Proceedings of the 33rd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval. ACM, New York, NY, USA.
</p>
<p>Noah Simon, Jerome Friedman, Trevor Hastie, and
Rob Tibshirani. 2013. A sparse-group lasso. In
Journal of Computational and Graphical Statistics.
</p>
<p>Ming Yuan and Yi Lin. 2006. Model selection and es-
timation in regression with grouped variables. In
Journal of the Royal Statistical Society. volume 68,
pages 49–67.
</p>
<p>340</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 341–346
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2054
</p>
<p>Multi-Task Learning of Keyphrase Boundary Classification
</p>
<p>Isabelle Augenstein ⇤
Department of Computer Science
</p>
<p>University College London
i.augenstein@ucl.ac.uk
</p>
<p>Anders Søgaard ⇤
Department of Computer Science
</p>
<p>University of Copenhagen
soegaard@di.ku.dk
</p>
<p>Abstract
</p>
<p>Keyphrase boundary classification (KBC)
is the task of detecting keyphrases in sci-
entific articles and labelling them with re-
spect to predefined types. Although im-
portant in practice, this task is so far un-
derexplored, partly due to the lack of la-
belled data. To overcome this, we explore
several auxiliary tasks, including semantic
super-sense tagging and identification of
multi-word expressions, and cast the task
as a multi-task learning problem with deep
recurrent neural networks. Our multi-task
models perform significantly better than
previous state of the art approaches on two
scientific KBC datasets, particularly for
long keyphrases.
</p>
<p>1 Introduction
</p>
<p>The scientific keyphrase boundary classification
(KBC) task consists of a) determining keyphrase
boundaries, and b) labelling keyphrases with their
types according to a predefined schema. KBC is
motivated by the need to efficiently search scien-
tific literature, which can be summarised by their
keyphrases. Several companies are working on
keyphrase-based recommender systems for scien-
tific literature or search interfaces where scien-
tific articles decorate graphs, in which nodes are
keyphrases. Such keyphrases must be dynamically
retrieved from the articles, because important sci-
entific concepts emerge on a daily basis, and the
most recent concepts are typically the ones of in-
terest to scientists.
</p>
<p>KBC is not a common task in NLP, and there
are only few small annotated datasets for inducing
supervised KBC models, made available recently
</p>
<p>?Both authors contributed equally
</p>
<p>(QasemiZadeh and Schumann, 2016; Augenstein
et al., 2017). Typical KBC approaches therefore
rely on hand-crafted gazetteers (Hasan and Ng,
2014) or reduce the task to extracting a list of
keyphrases for each document (Kim et al., 2010)
instead of identifying mentions of keyphrases in
sentences. For related more common NLP tasks
such as named entity recognition and identifica-
tion of multi-word expressions, neural sequence
labelling methods have been shown to be useful
(Lample et al., 2016). In order to overcome the
small data problem, we study using more widely
available data for tasks related to KBC and exploit
their synergies in a deep multi-task learning setup.
</p>
<p>Multi-task learning has become popular within
natural language processing and machine learn-
ing over the last few years; in particular, hard
parameter sharing of hidden layers in deep learn-
ing models. This approach to multi-task learning
has three advantages: a) It significantly reduces
Rademacher complexity (Baxter, 2000; Maurer,
2007), i.e., the risk of over-fitting, b) it is space-
efficient, reducing the number of parameters, and
c) it is easy to implement.
</p>
<p>This paper shows how hard parameter sharing
can be used to improve gazetteer-free keyphrase
boundary classification models, by exploiting dif-
ferent syntactically and semantically annotated
corpora, as well as more readily available data
such as hyperlinks.
</p>
<p>Contributions We study the so far widely un-
derexplored, though in practice important task of
scientific keyphrase boundary classification, for
which only a small amount of training data is
available. We overcome this by identifying good
auxiliary tasks and cast it as a multi-task learn-
ing problem. We evaluate our models across two
new, manually annotated corpora of scientific arti-
cles and outperform single-task approaches by up
</p>
<p>341</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2054">https://doi.org/10.18653/v1/P17-2054</a></div>
</div>
<div class="page"><p />
<p>to 9.64% F1, mostly due to better performance for
long keyphrases.
</p>
<p>2 Keyphrase Boundary Classification
</p>
<p>Consider the following sentence from a scientific
paper:
</p>
<p>(1) We find that simple interpolation methods,
like log-linear and linear interpolation, im-
prove the performance but fall short of the
performance of an oracle.
</p>
<p>This sentence occurs in the ACL RD-TEC 2.0
corpus. Here, interpolation methods and log-
linear and linear interpolation are annotated as
technical keyphrases, performance as a keyphrase
related to measurements, and oracle is a keyphrase
labelled as miscellaneous. Below, we are inter-
ested in predicting the boundaries and the types of
all keyphrases.
</p>
<p>3 Multi-Task Learning
</p>
<p>Multi-task learning is an approach to learning, in
which generalisation is improved by taking advan-
tage of the inductive bias in training signals of re-
lated tasks. When abundant labelled data is avail-
able for an auxiliary task, but little data for the
target task, multi-task learning can act as a form
of semi-supervised learning combined with a dis-
tant supervision signal. Inducing a model from
only the sparse target task data may lead to over-
fitting to random noise in the data, but relying on
auxiliary data helps the model generalise, making
it easier to abstract away from noise, as well as
leveraging the marginal distribution of auxiliary
input data. From a representation learning per-
spective, auxiliary tasks can be used to induce rep-
resentations that may be beneficial for the target
task. Caruana (1993) also suggests that the auxil-
iary task can help focus attention in the induction
of the target task model. Finally, multi-task learn-
ing can be cast as a regulariser as studies show re-
ductions in Rademacher complexity in multi-task
architectures over single-task architectures (Bax-
ter, 2000; Maurer, 2007).
</p>
<p>Here, we follow the probably most common ap-
proach to multi-task learning, known as hard pa-
rameter sharing. This was introduced in Caruana
(1993) in the context of deep neural networks, in
which hidden layers can be shared among tasks.
We assume T different training set, D1, · · · , DT ,
</p>
<p>where each Dt contains pairs of input-output se-
quences (w1:n, yt1:n), wi 2 V , yti 2 Lt. The input
vocabulary V is shared across tasks, but the out-
put vocabularies (tagset) Lt are task dependent.
At each step in the training process we choose a
random task t, followed by a random training in-
stance (w1:n, yt1:n) 2 Dt. We use the tagger to
predict the labels ŷti , suffer a loss with respect to
the true labels yti and update the model parame-
ters. The parameters are trained jointly for a sen-
tence, i.e. cross-entropy loss over each sentence is
employed. Each task is associated with an inde-
pendent classification function, but all tasks share
the hidden layers. Note that for our experiments,
we only consider one auxiliary task at a time.
</p>
<p>4 Experiments
</p>
<p>Experimental Setup We perform experiments
for both keyphrase boundary identification (un-
labelled), and keyphrase boundary identification
and classification (labelled). Metrics measured
are token-level precision, recall and F1, which
are micro-average results across keyphrase types.
Types are defined by the two datasets studied.
</p>
<p>Auxiliary tasks We experiment with five aux-
iliary tasks: (1) syntactic chunking using anno-
tations extracted from the English Penn Tree-
bank, following Søgaard and Goldberg (2016); (2)
frame target annotations from FrameNet 1.5 (cor-
responding to the target identification and clas-
sification tasks in Das et al. (2014)); (3) hyper-
link prediction using the dataset from Spitkovsky
et al. (2010), (4) identification of multi-word ex-
pressions using the Streusle corpus (Schneider
and Smith, 2015); and (5) semantic super-sense
tagging using the Semcor dataset, following Jo-
hannsen et al. (2014). We train our models on the
main task with one auxiliary task at a time. Note
that the datasets for the auxiliary tasks are not an-
notated with keyphrase boundary identification or
classification labels.
</p>
<p>Datasets We evaluate on the SemEval 2017
Task 10 dataset (Augenstein et al., 2017) and the
the ACL RD-TEC 2.0 dataset (QasemiZadeh and
Schumann, 2016). The SemEval 2017 dataset is
annotated with three keyphrase types, the ACL
RD-TEC dataset with seven. For the former, we
test on the development portion of the dataset, as
the test set is not released yet. We randomly split
ACL RD-TEC into a training and test set, reserv-
</p>
<p>342</p>
<p />
</div>
<div class="page"><p />
<p>SemEval 2017 Task 10 ACL RD-TEC
Labels Material, Process, Task Technology and Method,
</p>
<p>Tool and Library,
Language Resource,
Language Resource Product,
Measures and Measurements,
Models, Other
</p>
<p>Topics Computer Science, Physics, Natural Language Processing
Material Science
</p>
<p>Number all keyphrases 5730 2939
Proportion singleton keyphrases 31% 83%
Proportion single-word mentions 18% 23%
Proportion mentions with word length &gt;= 2 82% 77%
Proportion mentions with word length &gt;= 3 51% 33%
Proportion mentions with word length &gt;= 5 22% 8%
</p>
<p>Table 1: Characteristics of SemEval 2017 Task 10 and ACL-RD-TEC corpora, statistics of training sets
</p>
<p>ing 1/3 for testing. Key dataset characteristics are
summarised in Table 1. One important observa-
tion is that the SemEval 2017 dataset contains a
significantly higher proportion of long keyphrases
than the ACL dataset.
</p>
<p>Models Our single- and multi-task networks are
three-layer, bi-directional LSTMs (Graves and
Schmidhuber, 2005) with pre-trained SENNA em-
beddings.1 For the multi-task networks, we follow
the training procedure outlined in Section 3. The
dimensionality of the embeddings is 50, and we
follow Søgaard and Goldberg (2016) in using the
same dimensionality for the hidden layers. We add
a dropout of 0.1 to the input and train these archi-
tectures with momentum SGD with initial learning
rate of 0.001 and momentum of 0.9 for 10 epochs.
</p>
<p>Baselines Our baselines are Finkel et al. (2005)2
and Lample et al. (2016)3, in order to compare to
a lexicalised and a state-of-the-art neural method.
We use the implementations released by the au-
thors and re-train models on our data.
</p>
<p>5 Results and Analysis
</p>
<p>Results for SemEval 2017 Task 10 corpus are pre-
sented in Table 2, and for the ACL RD-TEC cor-
pus in Table 3. For the SemEval corpus, all five la-
belled multi-task learning models outperform both
examples of previous work, as well as our single-
task BiLSTM baseline, by some margin. For ACL
RD-TEC, three of out five multi-task learning la-
</p>
<p>1http://ronan.collobert.com/senna/
2http://nlp.stanford.edu/software/
CRF-NER.shtml
</p>
<p>3https://github.com/clab/
stack-lstm-ner
</p>
<p>belled labelled perform better than the single-task
BiLSTM baseline.
</p>
<p>On the SemEval corpus, the F1 error reduc-
tion of of the best labelled model over the Stan-
ford tagger is 9.64%. The lexicalised Finkel et al.
(2005) model shows a surprisingly competitive
performance on the ACL RD-TEC corpus, where
it is only 2 points in F1 behind our best per-
forming labelled model and on par with our best-
performing unlabelled model. Results with Lam-
ple et al. (2016), on the other hand, are lower than
the Finkel et al. (2005) baseline. This might be
due to the model having a large set of parameters
to model state transitions which poses a difficulty
for small training datasets.
</p>
<p>Overall, multi-task models show bigger im-
provements over baselines for the SemEval cor-
pus, and all models achieve better results on
ACL RD-TEC. Statistics shown in Table 1 help
to explain this. Most noticeably, the SemEval
dataset contains a significantly higher proportion
of long keyphrases than the ACL dataset. Interest-
ingly, ACL RD-TEC contains a large proportion
of keyphrases which only appear once in the train-
ing set (singletons), significantly fewer keyphrases
and more keyphrase type, but that does not seem
to impact results as much as a high proportion of
long keyphrases.
</p>
<p>All models struggle with semantically vague or
broad keyphrases (e.g. ‘items’, ‘scope’, ‘key’)
and long keyphrases, especially those containing
clauses (e.g. ‘complete characterisation of the ox-
ide particles’, ‘earley deduction proof procedure
for definite clauses’). The multi-task models gen-
erally outperform the BiLSTM baseline for long
phrases (e.g. ‘language-independent system for
</p>
<p>343</p>
<p />
</div>
<div class="page"><p />
<p>Unlabelled Labelled
Method Precision Recall F1 Precision Recall F1
Finkel et al. (2005) 77.89 50.27 61.10 49.90 27.97 35.85
Lample et al. (2016) 71.92 49.37 58.55 41.36 28.47 33.72
</p>
<p>BiLSTM 81.58 57.86 67.71 45.80 32.48 38.01
</p>
<p>BiLSTM + Chunking 82.88 52.08 63.96 55.54 34.90 42.86
BiLSTM + Framenet 77.86 56.05 65.18 54.04 38.91 45.24
BiLSTM + Hyperlinks 76.59 60.53 67.62 46.99 44.09 41.13
BiLSTM + Multi-word 74.80 70.18 72.42 46.99 44.09 45.49
BiLSTM + Super-sense 83.70 51.76 63.93 56.94 35.25 43.54
</p>
<p>Table 2: Results for keyphrase boundary classification on the SemEval 2017 Task 10 corpus
</p>
<p>Unlabelled Labelled
Method Precision Recall F1 Precision Recall F1
Finkel et al. (2005) 84.16 80.08 82.07 59.97 53.86 56.75
Lample et al. (2016) 65.60 86.06 74.45 31.30 41.07 35.53
</p>
<p>BiLSTM 83.40 80.36 81.85 59.62 57.45 58.51
</p>
<p>BiLSTM + Chunking 83.36 79.46 81.37 59.26 57.24 57.84
BiLSTM + Framenet 84.11 79.39 81.68 60.64 57.24 58.89
BiLSTM + Hyperlinks 83.94 79.12 81.46 60.18 56.73 58.40
BiLSTM + Multi-word 84.86 76.92 80.69 59.81 54.21 56.87
BiLSTM + Super-sense 84.67 78.29 81.36 61.35 56.73 58.95
</p>
<p>Table 3: Results for keyphrase boundary classification on the ACL RD-TEC corpus
</p>
<p>automatic discovery of text in parallel translation’,
‘honeycomb network of graphite bricks’). Being
able to recognise long keyphrases correctly is part
of the reason our multi-task models outperform
the baselines, especially on the SemEval dataset,
which contains many such long keyphrases.
</p>
<p>6 Related Work
</p>
<p>Multi-Task Learning Hard sharing of all hid-
den layers was introduced in Caruana (1993), and
popularised in NLP by Collobert et al. (2011a).
Several variants have been introduced, including
hard sharing of selected layers (Søgaard and Gold-
berg, 2016) and sharing of parts (subspaces) of
layers (Liu et al., 2015). Søgaard and Goldberg
(2016) show that hard parameter sharing is an ef-
fective regulariser, also on heterogeneous tasks
such as the ones considered here. Hard parameter
sharing has been studied for several tasks, includ-
ing CCG super tagging (Søgaard and Goldberg,
2016), text normalisation (Bollman and Søgaard,
2016), neural machine translation (Dong et al.,
2015; Luong et al., 2016), and super-sense tag-
ging (Martı́nez Alonso and Plank, 2017). Shar-
ing of information can further be achieved by ex-
tending LSTMs with an external memory shared
</p>
<p>across tasks (Liu et al., 2016). A further in-
stance of multi-task learning is to optimise a su-
pervised training objective jointly with an unsu-
pervised training objective, as shown in Yu et al.
(2016) for natural language generation and auto-
encoding, and in Rei (2017) for different sequence
labelling tasks and language modelling.
</p>
<p>Boundary Classification KBC is very similar to
named entity recognition (NER), though arguably
harder. Deep neural networks have been applied
to NER in Collobert et al. (2011b); Lample et al.
(2016). Other successful methods rely on condi-
tional random fields, thereby modelling the proba-
bility of each output label conditioned on the label
at the previous time step. Lample et al. (2016),
currently state-of-the-art for NER, stack CRFs on
top of recurrent neural networks. We leave explor-
ing such models in combination with multi-task
learning for future work.
</p>
<p>Keyphrase detection methods specific to the sci-
entific domain often use keyphrase gazetteers as
features or exploit citation graphs (Hasan and Ng,
2014). However, previous methods relied on cor-
pora annotated for type-level identification, not
for mention-level identification (Kim et al., 2010;
Sterckx et al., 2016). While most applications
</p>
<p>344</p>
<p />
</div>
<div class="page"><p />
<p>rely on extracting keyphrases (as types), this has
the unfortunate consequence that previous work
ignores acronyms and other short-hand forms re-
ferring to methods, metrics, etc. Further, relying
on gazetteers makes overfitting likely, obtaining
lower scores on out-of-gazetteer keyphrases.
</p>
<p>7 Conclusions and Future Work
</p>
<p>We present a new state of the art for keyphrase
boundary classification, using data from related,
auxiliary tasks; in particular, super-sense tag-
ging and identification of multi-word expressions.
Deep multi-task learning improves significantly
on previous approaches to KBC, with error reduc-
tions of up to 9.64%, mostly due to better identifi-
cation and labelling of long keyphrases.
</p>
<p>In future work, we want to explore alterna-
tive multi-task learning regimes to hard parameter
sharing and experiment with additional auxiliary
tasks. The auxiliary tasks considered here are stan-
dard NLP tasks, hyperlink prediction aside. Other
tasks may be more directly relevant such as pre-
dicting the layout of calls for papers for scientific
conferences, or predicting hashtags in tweets by
scientists, since both data sources contain scien-
tific keyphrases.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank Elsevier for supporting this
work.
</p>
<p>References
Isabelle Augenstein, Mrinal Das, Sebastian Riedel,
</p>
<p>Lakshmi Vikraman, and Andrew McCallum. 2017.
SemEval-2017 Task 10 : Extracting Keyphrases and
Relations from Scientific Publications. In Proceed-
ings of SemEval, to appear.
</p>
<p>Jonathan Baxter. 2000. A model of inductive bias
learning. Journal of Artificial Intelligence Research
12:149–198.
</p>
<p>Marcel Bollman and Anders Søgaard. 2016. Improving
historical spelling normalization with bi-directional
LSTMs and multi-task learning. In Proceedings of
COLING.
</p>
<p>Rich Caruana. 1993. Multitask Learning: A
Knowledge-Based Source of Inductive Bias. In Pro-
ceedings of ICML.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011a. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research
12:2493–2537.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011b. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research
12:2493–2537.
</p>
<p>Dipanjan Das, Desai Chen, Andre Martins, Nathan
Schneider, and Noah Smith. 2014. Frame-semantic
parsing. Computational linguistics 40(1):9–56.
</p>
<p>Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-Task Learning for Mul-
tiple Language Translation. In Proceedings of ACL.
</p>
<p>Jenny Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by Gibbs sam-
pling. In Proceedings of ACL.
</p>
<p>Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise Phoneme Classification with Bidirectional
LSTM and other Neural Network Architectures.
Neural Networks 18(5):602–610.
</p>
<p>Kazi Saidul Hasan and Vincent Ng. 2014. Automatic
Keyphrase Extraction: A Survey of the State of the
Art. In Proceedings of ACL.
</p>
<p>Anders Johannsen, Dirk Hovy, Héctor Martı́nez, Bar-
bara Plank, and Anders Søgaard. 2014. More or less
supervised supersense tagging of Twitter. In Pro-
ceedings of *SEM.
</p>
<p>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 Task 5 :
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of SemEval.
</p>
<p>Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural Architectures for Named Entity Recognition.
In Proceedings of NAACL-HLT . pages 260–270.
</p>
<p>Pengfei Liu, Shafiq Joty, and Helen Meng. 2015. Fine-
grained Opinion Mining with Recurrent Neural Net-
works and Word Embeddings. In Proceedings of
EMNLP.
</p>
<p>Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Deep Multi-Task Learning with Shared Memory for
Text Classification. In Proceedings of EMNLP.
</p>
<p>Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task Se-
quence to Sequence Learning. In Proceedings of
ICLR.
</p>
<p>Héctor Martı́nez Alonso and Barbara Plank. 2017.
When is multitask learning effective? Semantic se-
quence prediction under varying data conditions. In
Proceedings of EACL.
</p>
<p>Andreas Maurer. 2007. Bounds for Linear Multi Task
Learning. Journal of Machine Learning Research
7:117–139.
</p>
<p>345</p>
<p />
</div>
<div class="page"><p />
<p>Behrang QasemiZadeh and Anne-Kathrin Schumann.
2016. The ACL RD-TEC 2.0: A Language Re-
source for Evaluating Term Extraction and Entity
Recognition Methods. In Proceedings of LREC.
</p>
<p>Marek Rei. 2017. Semi-supervised Multitask Learning
for Sequence Labeling. In Proceedings of ACL, to
appear.
</p>
<p>Nathan Schneider and Noah Smith. 2015. A Corpus
and Model Integrating Multiword Expressions and
Supersenses. Proceedings of NAACL-HLT .
</p>
<p>Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of ACL.
</p>
<p>Valentin Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010. Profiting from Mark-Up: Hyper-Text
Annotations for Guided Parsing. In Proceedings of
ACL.
</p>
<p>Lucas Sterckx, Cornelia Caragea, Thomas Demeester,
and Chris Develder. 2016. Supervised Keyphrase
Extraction as Positive Unlabeled Learning. In Pro-
ceedings of EMNLP.
</p>
<p>Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online
Segment to Segment Neural Transduction. In Pro-
ceedings of EMNLP.
</p>
<p>346</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 347–351
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2055
</p>
<p>Cardinal Virtues: Extracting Relation Cardinalities from Text
</p>
<p>Paramita Mirza1, Simon Razniewski2, Fariz Darari2, Gerhard Weikum1
</p>
<p>1 Max Planck Institute for Informatics
2 Free University of Bozen-Bolzano
</p>
<p>{paramita, weikum}@mpi-inf.mpg.de
{razniewski, darari}@inf.unibz.it
</p>
<p>Abstract
</p>
<p>Information extraction (IE) from text has
largely focused on relations between in-
dividual entities, such as who has won
which award. However, some facts are
never fully mentioned, and no IE method
has perfect recall. Thus, it is beneficial
to also tap contents about the cardinalities
of these relations, for example, how many
awards someone has won. We introduce
this novel problem of extracting cardinali-
ties and discuss specific challenges that set
it apart from standard IE. We present a dis-
tant supervision method using conditional
random fields. A preliminary evaluation
results in precision between 3% and 55%,
depending on the difficulty of relations.
</p>
<p>1 Introduction
</p>
<p>Motivation Information extraction (IE) can in-
fer relations between named entities from text
(e.g., (Mitchell et al., 2015; Del Corro and
Gemulla, 2013; Mausam et al., 2012)), yield-
ing for example which awards an athlete has
won, or instances of family relations like spouses,
children, etc. These methods can be harnessed
for summarization, question answering (QA), and
more. For populating knowledge bases (KBs), the
IE output is usually cast into subject-predicate-
object (SPO) triples, such as 〈BarackObama,
hasChild, Malia〉, or sometimes n-ary tuples
such as 〈MichaelPhelps, hasWon, OlympicGold,
200mButterfly, 2016〉.
</p>
<p>IE has focused on capturing full SPO triples (or
n-ary facts) with all arguments bound to entities
for relation P. However, news, biographies or dis-
cussion forums often contain numeric expressions
that reveal cardinalities of relations. Phrases such
as “her two children” or “his 28th medal” are valu-
</p>
<p>able cues for quantifying the hasChild and hasWon
relations. This can be harnessed in QA for cases
like “Who won the most Olympic medals?”
</p>
<p>An important application of relation cardinali-
ties is KB curation. KBs are notoriously incom-
plete, contain erroneous triples, and are limited in
keeping up with the pace of real-world changes.
For example, a KB may contain only 10 of the 28
Olympic medals that Phelps has won, or may in-
correctly list 3 children for Obama. Extracting the
cardinalities of relations for given subject entities
can address all of these issues.
</p>
<p>Relation cardinalities are disregarded by virtu-
ally all IE methods. Open IE methods (Mausam
et al., 2012; Del Corro and Gemulla, 2013) cap-
ture triples (or quadruples) such as 〈Obama, has,
two children〉. However, there is no way to in-
terpret the numeric expression in the O slot of
this triple. While IE methods that hinge on pre-
specified relations for KB population (e.g., NELL
(Mitchell et al., 2015)) can already capture nu-
meric values for explicitly stated attributes such
as 〈Berlin2016attack, hasNumOfVictims, 32〉, they
are currently not able to learn them.
</p>
<p>This paper addresses the novel task of extracting
relation cardinalities. For a given subject entity
s and predicate p, we aim to infer the cardinality
|{〈S, P,O〉 | S = s, P = p}| directly from text,
without having to observe anyO entities. This task
poses several challenges:
• IE Training. Most IE methods build on seed-
</p>
<p>based distant supervision. However, if the un-
derlying KB is not complete, taking the counts
of SPO triples for a given SP pair may result in
wrong seeds, which can lead to poor patterns.
• Compositionality. The cardinality of an SP pair
</p>
<p>for a relation may depend on several cardinality
mentions. For example, when observing “An-
gelina has two sons and three daughters”, one
could infer the children cardinality by summing.
</p>
<p>347</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2055">https://doi.org/10.18653/v1/P17-2055</a></div>
</div>
<div class="page"><p />
<p>• Linguistic Variance. In addition to cardinal
numbers, cardinality IE should also pay atten-
tion to number-related terms, e.g., “Angelina
gives birth to twins”, or ordinal information,
e.g., “Angelina’s fourth child”, which can reveal
lower bounds on relation cardinalities.
</p>
<p>Approach and Contribution Our method
learns patterns of phrases that contain cardinal
numbers, relying on the distant supervision
approach by counting facts for given SP pairs.
Our technical contributions are as follows: (i)
we provide a statistical analysis of numeric
information in Wikipedia articles; (ii) we develop
a CRF-based extraction method for relation
cardinalities that achieves precision scores of up
to 55%; (iii) we analyze further challenges in this
research and outline possible solutions.
</p>
<p>2 Related Work
</p>
<p>Knowledge Bases and Information Extraction
Automated KB construction is a major effort for
quite a while. Some approaches, such as YAGO
(Suchanek et al., 2007) or DBpedia (Auer et al.,
2007), focus on structured parts of Wikipedia,
while other approaches such as OLLIE (Mausam
et al., 2012), ClauseIE (Del Corro and Gemulla,
2013) or NELL (Mitchell et al., 2015), focus on
unstructured contents across the whole Web. In the
latter, usually the schema is also not predefined,
thus such approaches are called Open IE. Most
state-of-the-art systems now rely on distant super-
vision (Craven and Kumlien, 1999; Mintz et al.,
2009).
</p>
<p>Despite all efforts, KBs are immensely incom-
plete. For instance, the average number of children
per person in Wikidata (Vrandečić and Krötzsch,
2014) is just 0.02 (Razniewski et al., 2016).
</p>
<p>Numbers and Relation Cardinalities Numbers
in text are an important source of information.
Much work has been done on understanding num-
bers that express temporal information (Ling and
Weld, 2010; Strötgen and Gertz, 2010), and more
recently, on numbers that express physical quan-
tities or measures, either mentioned in text (Cha-
ganty and Liang, 2016) or in the context of web ta-
bles (Ibrahim et al., 2016; Neumaier et al., 2016).
</p>
<p>In contrast, numbers that express relation cardi-
nalities have received little attention so far. State-
of-the-art Open-IE systems either hardly extract
cardinality information or fail to extract cardinal-
</p>
<p>NE Tag Frequency
DATE, TIME, DURATION, SET (temporal) 54.28%
NUMBER 40.13%
</p>
<p>Relation cardinality 18.86%
PERCENT 2.92%
MONEY 2.25%
PERSON, LOCATION, ORGANIZATION 0.26%
ORDINAL 0.16%
</p>
<p>Table 1: NE-tags of numbers in Wikipedia.
</p>
<p>ities at all. While NELL, for instance, knows 13
relations about the number of casualties and in-
juries in disasters, they all contain only seed facts
and no learned facts. The only prior work we are
aware of is of Mirza et al. (2016), who use manu-
ally created patterns to mine children cardinalities
from Wikipedia. It is shown that with 30 manually
crafted patterns and simple filters it is possible to
extract 86,227 children-cardinality-assertions with
a precision of 94.3%.
</p>
<p>3 Relation Cardinalities
</p>
<p>Definition We define a mention that expresses
relation cardinalities as the following: “A cardi-
nal number that states the number of objects that
stand in a specific relation with a certain subject.”
</p>
<p>Using this definition, we analyzed how often
relation cardinalities occur in Wikipedia. Rely-
ing on the part-of-speech (PoS) tagger of Stan-
ford CoreNLP (Manning et al., 2014), we ex-
tracted numbers–i.e., words tagged as CD (cardi-
nal number)–from 10,000 random Wikipedia arti-
cles. The distribution of their named-entity (NE)
tags, according to Stanford NE-tagger, is shown in
Table 1. While temporal-related numbers are the
most frequent, around 40% are classified only as
unspecific NUMBER. By manually checking 100
random NUMBERs, we observed that 47 are rela-
tion cardinalities,1 i.e., approximately 18.86% of
all numbers in Wikipedia are relation cardinalities.
</p>
<p>We also analyzed the nouns frequently modified
by NUMBERs, based on their dependency paths,
finding people, games, children, times, members
and seasons among the top nouns. Coarse topic-
grouping of the nouns shows that most NUMBERs
are about sport (games, goals), followed by art-
work (seasons, books), politics and organization
(members, countries), and family (children).
</p>
<p>1Among the others are measures, age, or expressions like
“one of the...”.
</p>
<p>348</p>
<p />
</div>
<div class="page"><p />
<p>4 Relation Cardinality Extraction
</p>
<p>Ideally, we would like to make sense of all car-
dinality statements found in text. However, this
would require us to resolve the meaning of a large
set of vague predicates, which is in general a dif-
ficult task. We thus turn the problem around:
given a well defined relation/predicate p, a sub-
ject s and a corresponding text about s, we now
try to estimate the relation cardinality (i.e., the
count of 〈s, p, ∗〉 triples), based on cardinality as-
sertions found in the text. We chose four Wikidata
predicates that span various domains, child (P40),
spouse (P26), has part (P527) of a series of cre-
ative works (restricted to novel, book and film se-
ries), and contains administrative territorial entity
(P150). As the text source for subjects of each
predicate, we consider sentences containing num-
bers taken from their respective English Wikipedia
articles.
</p>
<p>Methodology We approach the problem via se-
quence labelling, i.e., given a sentence containing
at least one number, we aim to determine whether
each number in the sentence corresponds to the
cardinality of a certain relation. We build a Con-
ditional Random Field (CRF) based model with
CRF++ (Kudo, 2005) for each relation, taking as
features the context lemmas (window size of 5)
around the observed token t, along with bigrams
and trigrams containing t.
</p>
<p>To generate the training data, we rely on dis-
tant supervision, annotating candidate numbers2
</p>
<p>in the text as correct cardinalities whenever they
correspond to the exact triple count (count &gt; 0)
found in the knowledge base. Otherwise, they are
labelled as O (for Others), like the rest of non-
number tokens. Table 2 contains for each consid-
ered relation (p), the number of subjects (#s) in
Wikidata, which have links to English Wikipedia
pages and have at least one 〈s, p, ∗〉 triple.
</p>
<p>We predict the relation cardinality of a given
〈s, p〉 pair by selecting the number positively an-
notated with marginal probability–resulting from
forward-backward inference–higher than 0.1, and
choosing the one with the highest probability if
there are several.
</p>
<p>Experiments Two experimental settings are
considered: vanilla refers to the distant supervi-
sion approach explained above, while for only-
</p>
<p>2Numbers that are not labelled as DATE, TIME, DURA-
TION, SET, MONEY and PERCENT by Stanford NE-tagger.
</p>
<p>nummod, we only annotate a candidate number as
correct cardinality if it modifies a noun, i.e., there
is an incoming dependency relation of label num-
mod according to the Stanford Dependency Parser.
This is to exclude numbers as in “one of the rea-
sons...” from training examples. We also con-
sidered a naive baseline, which chooses a random
number from a pool of numbers existing in each
text about a certain subject.
</p>
<p>Furthermore, to estimate how well KB counts
are suited as ground truth, we compare them on the
the child relation with the manually-created num-
ber of children (P1971) property from Wikidata.
</p>
<p>Evaluation Results We manually annotated the
evaluation data with the true relation counts, since
the knowledge base is highly incomplete, and thus,
the triple counts are often incorrect. Whenever the
cardinality matches the true count, we also manu-
ally inspected how relevant the textual evidence–
the context surrounding the cardinal number–is for
the observed relation. Table 2 shows the perfor-
mance of our CRF-based method in finding the
correct relation cardinality, evaluated on manually
annotated 20 (has part), 100 (admin. terr. entity)
and 200 (child and spouse) randomly selected sub-
jects that have at least one object.
</p>
<p>The random-number baseline achieves a preci-
sion of 5% (has part), 3.5% (admin. territ. entity),
0% (spouse) and 11.2% (child). Compared to that,
especially using only-nummod, our method gives
encouraging results for has part, admin. territ. en-
tity and child, with 30-50% precision and around
30% F1-score. For spouse, the performance is
significantly lower, reasons are discussed below.
Furthermore, we can observe that using manual
ground truth as training data for the child rela-
tion can boost performance considerably. Still,
the performance is significantly below the state-
of-the-art in fact extraction, where child triples can
be extracted from Wikipedia text with 96% preci-
sion (Palomares et al., 2016).
</p>
<p>5 Analysis
</p>
<p>A qualitative analysis of the training data and eval-
uation results revealed three aspects that make ex-
tracting relation cardinalities difficult.
</p>
<p>Quality of Training Data Unlike training data
for normal fact extraction, which is generally
highly correct (e.g., YAGO claims 95% preci-
sion (Suchanek et al., 2007)), taking triple counts
</p>
<p>349</p>
<p />
</div>
<div class="page"><p />
<p>baseline vanilla only-nummod
p #s P P R F1 P R F1
has part (creative work series) 261 .050 .333 .316 .324 .353 .316 .333
contains admin. terr. entity 18,000 .034 .390 .188 .254 .548 .200 .293
spouse 45,917 0 .014 .011 .013 .028 .017 .021
child 35,057 .112 .151 .129 .139 .320 .219 .260
child (manual ground truth) 6,408 0.374 0.309 0.338 0.452 0.315 0.371
</p>
<p>Table 2: Number of Wikidata entities as subjects (#s) of each predicate (p), and evaluation results on
manually annotated randomly selected subjects that have at least an object.
</p>
<p>found in knowledge bases as ground truth gener-
ally gives wrong results. For example, our manual
evaluation of child shows that the triple count from
Wikidata is 46% lower than what the texts assert.
</p>
<p>As shown by the last row of Table 2, higher
quality of training data can considerably boost the
performance of cardinality extraction. Unfortu-
nately, manually curated data is generally difficult
to obtain. We see two avenues to tackle training
data quality:
1. Filtering ground truth. Instead of taking the
</p>
<p>counts of all entities as ground truth, one might
trade size for quality, e.g., using popular enti-
ties only, as for these there are chances that KBs
are more complete.
</p>
<p>2. Incompleteness-resilient distant supervision.
Triple counts in KBs are often lower than what
is correct, but rarely too high. Thus, an avenue
might be to label all numbers equal or higher
than the KB count as correct, instead of only
considering the equal ones. Given that differ-
ent cardinalities could then be labelled as cor-
rect, this would require a postprocessing step in
which conflicting counts are consolidated.
</p>
<p>Compositionality Around 16% of false posi-
tives in extracting child cardinalities can be at-
tributed to failures in identifying the correct count
for, e.g., ”They have two sons and one daughter
together; he has four children from an earlier re-
lationship.” This was also observed for other rela-
tions, e.g., “The Qidong county has 4 subdistricts,
17 towns and 3 townships under its juridiction.”
We see two avenues to tackle this problem:
1. Aggregating numbers. In training data genera-
</p>
<p>tion, one could label a sequence of number as
correct cardinalities if the sum of the numbers
is equal to the relation count. In the prediction
step, one might sum up all consecutive cardi-
nalities that are labelled with sufficient confi-
dence.
</p>
<p>2. Learning composition rules. One may try to
learn the composition of counts, for instance,
that children are composed of sons and daugh-
ters, then try to extract the composing cardinal-
ities.
</p>
<p>Linguistic Variance We observe that for the
spouse relation, expressing the count with car-
dinal numbers (“He has married four times”) is
only found for 4% of subjects. It is more com-
mon to express the count with ordinal numbers,
e.g., “John’s first wife, Mary, ...”, which allows us
to conclude that the spouse-count for John is at
least–and most probably more than–one. An ap-
proach to such relations might be to identify or-
dinals numbers that express lower bounds of rela-
tions. Subsequently, one could reason over these
bounds and try to infer relation counts.
</p>
<p>Our initial motivation was to make sense of the
so far ignored large fraction of numbers that ex-
press relation cardinalities. However, we noticed
quickly that relation cardinalities are frequently
also expressed without numbers at all. This is es-
pecially true for the case of count zero, which is
mostly expressed using negation (“He never mar-
ried”), and the count one, which is expressed us-
ing indefinite articles (“They have a child”) or
the signal-word only (“Their only child, James”).
Terms such as twins or trilogy are also ways to
express domain-specific relation cardinalities. We
see two avenues to approach this variance:
</p>
<p>1. Translation to numbers. For the 0’s and 1’s, a
possible approach is to translate certain kinds
of negation and indefinite articles into explicit
numbers (e.g., “do not have any children” →
“have 0 children”).
</p>
<p>2. Word similarity with cardinals. If a word bears
high similarity with cardinal numbers, possibly
also in other languages such as Latin or Greek,
one might consider it as a candidate number.
</p>
<p>350</p>
<p />
</div>
<div class="page"><p />
<p>6 Conclusion
</p>
<p>In this paper we have introduced the problem of
relation cardinality extraction. We believe that re-
lation cardinalities can be useful in a variety of
tasks. Our next goal is to make distant supervision
incompleteness-resilient and to deal with compo-
sitionality, hoping that these can improve the pre-
cision of our approach. We also aim to take ordi-
nals into account and to experiment with linguistic
transformation for the cases of cardinalities 0 and
1, hoping that these could boost the recall.
</p>
<p>A limitation of our work is also that we only
focus on Wikipedia articles, assume that all state-
ments are about the article’s subject, and just take
the statement with the highest confidence. In fu-
ture work we aim to include a larger article base in
combination with named entity recognition, coref-
erence resolution and a truth consolidation step.
</p>
<p>Acknowledgments
We thank Werner Nutt and Sebastian Rudolph for
their feedback on an earlier version of this work.
We thank the anonymous reviewers for their help-
ful comments. This work has been partially sup-
ported by the project “The Call for Recall”, funded
by the Free University of Bozen-Bolzano.
</p>
<p>References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
</p>
<p>Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A nucleus for a web of open data.
Springer.
</p>
<p>Arun Chaganty and Percy Liang. 2016. How much is
131 million dollars? putting numbers in perspec-
tive with compositional descriptions. In ACL. pages
578–587.
</p>
<p>Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology. pages 77–86.
</p>
<p>Luciano Del Corro and Rainer Gemulla. 2013.
ClausIE: clause-based open information extraction.
In WWW. ACM, pages 355–366.
</p>
<p>Yusra Ibrahim, Mirek Riedewald, and Gerhard
Weikum. 2016. Making sense of entities and quan-
tities in web tables. In CIKM. pages 1703–1712.
</p>
<p>Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Software available at http://crfpp. sourceforge.net .
</p>
<p>Xiao Ling and Daniel S Weld. 2010. Temporal in-
formation extraction. In AAAI. volume 10, pages
1385–1390.
</p>
<p>Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. ACL (System Demonstra-
tions) pages 55–60.
</p>
<p>Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In EMNLP. pages
523–534.
</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In ACL. pages 1003–
1011.
</p>
<p>Paramita Mirza, Simon Razniewski, and Werner Nutt.
2016. Expanding Wikidatas parenthood informa-
tion by 178%, or how to mine relation cardinalities.
ISWC Posters &amp; Demos .
</p>
<p>Tom M. Mitchell, William W. Cohen, Estevam R. Hr-
uschka Jr., Partha Pratim Talukdar, Justin Bet-
teridge, Andrew Carlson, Bhavana Dalvi Mishra,
Matthew Gardner, Bryan Kisiel, Jayant Krishna-
murthy, Ni Lao, Kathryn Mazaitis, Thahir Mo-
hamed, Ndapandula Nakashole, Emmanouil Anto-
nios Platanios, Alan Ritter, Mehdi Samadi, Burr Set-
tles, Richard C. Wang, Derry Tanti Wijaya, Abhi-
nav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm
Greaves, and Joel Welling. 2015. Never-ending
learning. In AAAI. pages 2302–2310.
</p>
<p>Sebastian Neumaier, Jürgen Umbrich, Josiane Xavier
Parreira, and Axel Polleres. 2016. Multi-level se-
mantic labelling of numerical values. In ISWC.
pages 428–445.
</p>
<p>Thomas Palomares, Youssef Ahres, Juhana Kan-
gaspunta, and Christopher Ré. 2016. Wikipedia
knowledge graph with DeepDive. In ICWSM. pages
65–71.
</p>
<p>Simon Razniewski, Fabian M. Suchanek, and Werner
Nutt. 2016. But what do we actually know? Pro-
ceedings of AKBC pages 40–44.
</p>
<p>Jannik Strötgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normaliza-
tion of temporal expressions. In SemEval Workshop.
pages 321–324.
</p>
<p>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: a core of semantic knowl-
edge. WWW pages 697–706.
</p>
<p>Denny Vrandečić and Markus Krötzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Commu-
</p>
<p>nications of the ACM 57(10):78–85.
</p>
<p>351</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352–357
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2056
</p>
<p>Integrating Deep Linguistic Features in Factuality Prediction
over Unified Datasets
</p>
<p>Gabriel Stanovsky1, Judith Eckle-Kohler2, Yevgeniy Puzikov2,
Ido Dagan1 and Iryna Gurevych2
</p>
<p>1Bar-Ilan University Computer Science Department, Ramat Gan, Israel
2Ubiquitous Knowledge Processing Lab (UKP), Technische Universitat Darmstadt, Germany
</p>
<p>gabriel.stanovsky@gmail.com
</p>
<p>www.ukp.tu-darmstadt.de
</p>
<p>dagan@cs.biu.ac.il
</p>
<p>Abstract
</p>
<p>Previous models for the assessment of
commitment towards a predicate in a sen-
tence (also known as factuality prediction)
were trained and tested against a specific
annotated dataset, subsequently limiting
the generality of their results. In this work
we propose an intuitive method for map-
ping three previously annotated corpora
onto a single factuality scale, thereby en-
abling models to be tested across these
corpora. In addition, we design a novel
model for factuality prediction by first ex-
tending a previous rule-based factuality
prediction system and applying it over an
abstraction of dependency trees, and then
using the output of this system in a super-
vised classifier. We show that this model
outperforms previous methods on all three
datasets. We make both the unified factu-
ality corpus and our new model publicly
available.
</p>
<p>1 Introduction
</p>
<p>Factuality prediction is the task of determining the
level of commitment towards a predicate in a sen-
tence according to a specific source, e.g., the au-
thor (Saurı́ and Pustejovsky, 2009). For instance,
the author uses linguistic cues to mark the embed-
ded proposition as factual in (1) (cue: surprising),
as uncertain in (2) and (3) (cues: risk, might), and
as counterfactual (cue: did not manage) or uncer-
tain (cue: will not manage) in (4).
</p>
<p>(1) It is not surprising that they work.
</p>
<p>(2) She takes the risk to find out the truth.
</p>
<p>(3) She might find out the truth.
</p>
<p>(4) He did/will not manage to be in time.
</p>
<p>Detecting factuality is hard as the linguistic means
used to express it closely interact. For example,
lexical cues, such as the proposition-embedding
predicates in (1) and (4) interact with negation (in
(1), (4)) and tense (in (4)).
</p>
<p>Detecting factuality has many potential applica-
tions. For instance, in knowledge base population,
only propositions marked as factual should be ad-
mitted into the knowledge base, while hypotheti-
cal or negated ones should be left out. Similarly,
for argumentation analysis and question answer-
ing, factuality can play a major role in backing a
specific claim or supporting evidence for an an-
swer to a question at hand.
</p>
<p>Recent research efforts have approached the
factuality task from two complementing direc-
tions: automatic prediction and large scale anno-
tation. Previous attempts for automatic factuality
prediction either took a rule-based, deep syntac-
tic approach (Lotan et al., 2013; Saurı́ and Puste-
jovsky, 2012) or a machine learning approach over
more shallow features (Lee et al., 2015). In terms
of annotation, each effort was largely carried out
independently of the others, picking up different
factuality flavors and different annotation scales.
</p>
<p>In correlation, the proposed algorithms have
targeted a single annotated resource which they
aim to recover. Subsequently, this separation be-
tween annotated corpora has prevented a compar-
ison across datasets. Further, the models are non-
portable, inhibiting advancements in one dataset
to carry over to any of the other annotations.
</p>
<p>Our contribution in this work is twofold. First,
we suggest that the task can benefit from a uni-
fied representation. We exemplify this by mapping
the representation of two recent datasets (Fact-
Bank (Saurı́ and Pustejovsky, 2009) and MEAN-
TIME (Minard et al., 2016)) onto the [−3,+3]
scale, as annotated by (Lee et al., 2015). This
unification allows us to test the generality of mod-
</p>
<p>352</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2056">https://doi.org/10.18653/v1/P17-2056</a></div>
</div>
<div class="page"><p />
<p>els which were previously applicable on a single
dataset. Second, we design a new model for fac-
tuality prediction that extends TruthTeller (Lotan
et al., 2013), which employed implicative signa-
tures (MacCartney and Manning, 2009; Karttunen,
2012) over dependency trees using a large predi-
cate lexicon. We first extend TruthTeller’s lexicon
by about 40% through a semi-automatic process
(following Eckle-Kohler (2016)). We then apply
TruthTeller’s rules over an abstraction of depen-
dency trees (Stanovsky et al., 2016), which repre-
sents predicate-argument structures more consis-
tently, thereby allowing TruthTeller rules to ap-
ply on a wider range of syntactic constructions.
Finally, we surpass previous methods by using
the output from TruthTeller as deep linguistically-
informed features in a supervised classifier, thus
successfully integrating a rule-based approach in
a machine learning framework.
</p>
<p>Overall, we hope that our unified representation
will enable training and testing on larger, more di-
verse datasets, and that the good performance of
our new model indicates its usability across differ-
ent flavors of factuality prediction. We make both
the unified factuality corpus and the new model
publicly available.1
</p>
<p>2 Background
</p>
<p>Factuality prediction requires the identification of
uncertainty, a concept which largely corresponds
to the linguistic notion of modality (Hacquard,
2011). Modality expresses possibilities and ne-
cessities by means of negation, modal verbs (may,
might, can), main verbs (agree, refuse), adjectives
(dishonest), future tense (will, won’t), and more.
Looking at the numerous and varied possibilities
language offers to express all the different shades
of modality, it is clear that factuality does not as-
sume any fixed set of discrete values either. In-
stead, the underlying linguistic system forms a
continuous spectrum ranging from factual to coun-
terfactual (Saurı́ and Pustejovsky, 2009).
</p>
<p>While linguistic theory assigns a spectrum of
factuality values, recent years have seen many
practical efforts to capture the notion of factual-
ity in a consistent annotation (Saurı́ and Puste-
jovsky, 2009; Nissim et al., 2013; Lee et al., 2015;
OGorman et al., 2016; Minard et al., 2016; Ghia
et al., 2016). Each of these make certain deci-
</p>
<p>1https://github.com/gabrielStanovsky/
unified-factuality
</p>
<p>sions regarding the granularity of factuality that
they aim to extract. In the course of this work
we chose to set our focus on three of these anno-
tations: FactBank (Saurı́ and Pustejovsky, 2009),
MEANTIME (Minard et al., 2016) and the UW
corpus (Lee et al., 2015). We use these specific
corpora as they represent recent efforts, display a
range of different design choices (e.g., in their no-
tion of factuality and method of annotation), and
are made publicly available which ensures the ease
of the reproducibility of our experiments. Table
1 sums the properties and variations of these cor-
pora. For example, we can see that: (1) the UW
corpus uses a continuous scale and is annotated by
crowdsourcing, while MEANTIME and FactBank
were annotated discretely by experts, (2) Fact-
Bank annotates factuality from different perspec-
tives, and (3) MEANTIME is significantly smaller
compared to the other corpora.
</p>
<p>In parallel with the creation of these annotated
resources, several efforts were made to predict fac-
tuality in an automatic manner. The methods for
doing so can be largely divided into rule-based
systems which examine deep linguistic features,
and machine learning algorithms which generally
extract more shallow features. The De Facto fac-
tuality profiler (Saurı́ and Pustejovsky, 2012) and
TruthTeller algorithms (Lotan et al., 2013) take the
rule-based approach and assign a discrete anno-
tation of factuality (following the values assigned
by FactBank) using a deterministic rule-based top-
down approach on dependency trees, changing the
factuality assessment when encountering factual-
ity affecting predicates or modality and negation
cues (following implicative signatures by Kart-
tunen (2012)). In addition to a factuality assess-
ment, TruthTeller assigns three values per predi-
cate in the sentence: (1) implicative signature from
a hand-coded lexicon indicating how this predi-
cate changes the factuality of its embedded clause,
in positive and negative contexts, (2) clause truth,
marking the factuality assessment of the entire
clause, and (3) negation and uncertainty, indicat-
ing whether this predicate is affected by negation
or modality. Both of these algorithms rely on a
hand-written lexicon of predicates, indicating how
they modify the factuality status of their embed-
ded predicates (e.g., refuse negates while assure
asserts it). In this work we will make use of the
more recent TruthTeller which uses a much larger
lexicon of 1,700 predicates (verbs, adjectives and
</p>
<p>353</p>
<p />
</div>
<div class="page"><p />
<p>Corpus #Tokens/Sentences Factuality Values Type Annotators Perspective
Original Our mapping
</p>
<p>FactBank 77231 / 3839
</p>
<p>Factual (CT+/-) +3.0 / -3.0
</p>
<p>Discrete Experts
Author’s and
discourse-internal
sources
</p>
<p>Probable (PR+/-) +2.0 / -2.0
Possible (PS+/-) +1.0 / -1.0
Unknown (Uu/CTu ) 0.0
</p>
<p>MEANTIME† 9743 / 631
Fact / Counterfact +3.0 / -3.0
</p>
<p>Discrete Experts Author’sPossibility (uncertain) +1.5 / -1.5
Possibility (future) +0.5 / -0.5
</p>
<p>UW 106371 / 4234 [-3.0, 3.0] Continuous Crowdsource Author’s
</p>
<p>Table 1: Factuality annotation statistics and mappings used in this paper - the number of tokens and
sentences in each corpus, the original factuality value with the corresponding converted value to UW
scale, the type of annotation (discrete or continuous), the annotators’ proficiency, and the perspective to
which the annotation refers. †This is an abstraction over the original MEANTIME annotation (suggested
by the MEANTIME authors), which is composed of polarity, certainty and temporality.
</p>
<p>nouns) compared to De Facto’s lexicon, which
contains 646 predicates.
</p>
<p>In a separate attempt which we will call UW sys-
tem, Lee et al. (2015) have used SVM regression
techniques to predict a continuous factuality value
from lexical and syntactic features (lemma, part
of speech, and dependency paths). Similarly to
the TruthTeller approach, they also predict a single
factuality value pertaining to the author’s commit-
ment towards the predicate.
</p>
<p>3 Unified Factuality Representation
</p>
<p>We achieve a unified representation by map-
ping FactBank and MEANTIME onto the UW
[−3,+3] range in a simple automatic rule-based
manner.
</p>
<p>Table 1 describes these rules (see column “Our
mapping”), which were hand-written by consult-
ing the annotation guidelines of each of the cor-
pora. Specifically, in converting FactBank we
take only the author’s perspective annotations as
these comply with the annotations of the other
corpora, and for MEANTIME we use their pro-
posed abstraction into factual, uncertain and pos-
sible (in the future). We map from the discrete
values (MEANTIME and FactBank) to the contin-
uous scale (UW) since this conversion is lossless:
if two events receive different factuality values in
the original annotation, they will also differ in the
unified representation, and vice versa. Further-
more, since FactBank and MEANTIME are both
discrete, it is not clear a priori how to map between
them.
</p>
<p>Label distribution Given the above conversion,
we can plot the label distribution of all three cor-
pora on the same scale (Figure 1). This analysis
</p>
<p>−3 −2 −1 0 1 2 3
0
</p>
<p>2000
</p>
<p>4000
</p>
<p>6000
</p>
<p>8000 MEANTIME
</p>
<p>FactBank
</p>
<p>UW
</p>
<p>Figure 1: Histogram of factuality values in Fact-
Bank(red), UW (blue), and MEANTIME(green).
</p>
<p>reveals that all corpora are significantly skewed to-
wards the factual end of the scale, where the ma-
jority of the annotation mass is located. In par-
ticular, we find that MEANTIME is especially bi-
ased, assigning the factual value (+3) to 90% of
its event annotations. Overall, we suspect that this
is an inherent trait of the news domain which tends
to be more factual than other text types (e.g., edu-
cational texts or opinion pieces).
</p>
<p>4 Model
</p>
<p>Following the automatic conversion which
achieves a unified representation for our three
datasets, we devise a factuality prediction model
composed of three main components: (1) aug-
mentation of the TruthTeller lexicon with about
800 adjectival, nominal and verbal predicates, (2)
syntactic re-ordering with PropS (Stanovsky et al.,
2016), (3) application of TruthTeller on top of
PropS trees (Lotan et al., 2013). In the following
we describe these components.
</p>
<p>354</p>
<p />
</div>
<div class="page"><p />
<p>Don was dishonest when he said he paid taxes
</p>
<p>nsubj
acomp
</p>
<p>advmod
</p>
<p>nsubj
</p>
<p>advcl
</p>
<p>nsubj
ccomp
</p>
<p>dobj
</p>
<p>Don was dishonest when he said he paid taxes
</p>
<p>prop of
mod
</p>
<p>subj
</p>
<p>mod
</p>
<p>subj
comp
</p>
<p>dobj
</p>
<p>Figure 2: Dependency tree (top, obtained with
spaCy) versus PropS representation (bottom, ob-
tained via the online demo). Note that PropS
posits dishonest as the head of said, while the de-
pendency tree obstructs this relation.
</p>
<p>Extending TruthTeller’s lexicon We extended
the TruthTeller lexicon of single-word predicates
by integrating a large resource of modality mark-
ers. Following the approach of Eckle-Kohler
(2016), we first induced the modality status of En-
glish adjectives and nouns from the subcategoriza-
tion frames of their German counterparts listed
in a large valency lexicon (using the “IMSLex
German Lexicon” (Fitschen, 2004) and Google
Translate for obtaining the translations2). We fo-
cused on four modality classes (the classes wh-
factual and wh/if-factual indicating factuality, and
the two classes future-orientation and non-factual,
indicating uncertainty)3 and semi-automatically
mapped them to the signatures used in TruthTeller.
We performed the same kind of mapping for
the modality classes of English verbs provided
by Eckle-Kohler (2016). The result of this process
extended TruthTeller’s lexicon by roughly 40%
(265 adjectives, 281 nouns, and 133 verbs).
</p>
<p>Integrating PropS with TruthTeller PropS
was recently presented as an abstraction over de-
pendency trees. Most convenient in our case is its
re-ordering of non-verbal predicates (adjectival,
conditional, non-lexical, etc.) such that each pred-
icate is the direct head of its respective arguments.
For example, for adjectival predication, compare
the different parses in Figure 2. PropS positions
dishonest as the head of said, which is subse-
quently the head of paid. This chain allows the im-
plicative signature encoded in TruthTeller to cap-
ture this complex relation. The dependency syn-
</p>
<p>2We used the translation function available as part of
Google Sheets. https://www.google.com/sheets
and removed all translation pairs with English multi-words.
</p>
<p>3In Eckle-Kohler (2016), these are the classes containing
the majority of the verb types.
</p>
<p>tax, in contrast, obstructs this relation by positing
dishonest as a leaf node under when. The overall
consistency of PropS annotation allows the top-
down approach of TruthTeller to apply to predi-
cates beyond the verbal case.
</p>
<p>Finally, we take as features all four TruthTeller
annotations (see Section 2) of the target predicate,
its PropS head and its children (padding or trun-
cating to 4 children). For a fair comparison with
the UW system, we use these features to train an
SVM regression (Basak et al., 2007) model to pre-
dict the final factuality value.
</p>
<p>5 Evaluation
</p>
<p>In this section we describe the experiments we car-
ried out on the three unified datasets (FactBank,
MEANTIME, and UW). For a fair comparison,
we use the same train, development, test split of
the datasets for all systems. We preprocess the
data with the spaCy Python library.4 In all our
experiments we compute the metrics used in Lee
et al. (2015): (1) Mean Absolute Error5 (MAE),
which computes the absolute fit of the model and
(2) Pearson correlation coefficient between auto-
matic predictions and gold labels, especially infor-
mative in biased test sets as it assesses how well
the model captures the variability in the gold data.
</p>
<p>5.1 Baselines
</p>
<p>We test the performance of our model on the uni-
fied factuality corpus against that of several algo-
rithms, representing the state-of-the-art (SoA) in
competing approaches.
</p>
<p>Rule-based approach For a SoA rule-based ap-
proach we use TruthTeller with extended lexicon
as described in Section 4. We convert its dis-
crete predictions to the [-3, +3] scale using a hand-
written conversion table, similarly to our mapping
of FactBank annotations.
</p>
<p>Supervised approach The SoA for supervised
learning is represented by the features from the
UW system. We note that for practical issues, we
did not use the same solver6, but instead used sup-
port vector regression (SVR) model with a linear
kernel (as implemented in the scikit-learn Python
</p>
<p>4https://spacy.io
5Note that in our case this ranges between 0 (perfect per-
</p>
<p>formance) and 6 (worst performance).
6UW used the IBM CPLEX Optimizer
</p>
<p>355</p>
<p />
</div>
<div class="page"><p />
<p>library7). All hyperparameters were tuned on the
development set.
</p>
<p>Semantic representation approach In addition
to the rule-based and supervised approaches, we
experimented with a semantic abstraction of the
sentence. For that end, we extracted features in-
spired by the UW system on the popular AMR
formalism (Banarescu et al., 2013) using a SoA
parser (Pust et al., 2015). Our hope was that
this would improve performance by focusing on
the more semantically-significant portions of the
predicate-argument structure. In particular, we ex-
tracted the following features from the predicted
AMR structures: immediate parents, grandparents
and siblings of the target node, lemma and POS
tag of the target and preceding token in the sen-
tence, and a Boolean feature based on the AMR
polarity role (indicating semantic negation).
</p>
<p>All-factual approach Finally, we compare
against an all-factual baseline which assigns +3.0
to all predicates. Since the task is by nature heav-
ily biased towards the factual label, it is interest-
ing to compare against such a simple (yet strong)
lower bound. See the supplemental material for a
technical elaboration on the baselines implemen-
tation.
</p>
<p>5.2 Results
</p>
<p>Several observations can be made following the
results on our test sets (Table 2).
</p>
<p>Rule-based baseline is a good starting point
The rule-based performance is well correlated
with the gold predictions on FactBank and UW,
showing its off-the-shelf usability.
</p>
<p>Supervised setting improves performance
Adding our features provided a predictive signal
for factuality assessment on all test sets. More
significant improvement is observed in the larger
FactBank and UW corpora.
</p>
<p>UW achieves good correlation UW gives a
more diverse annotation thanks to its richer fea-
ture set (including lemma and dependency path).
While this hurts MAE in some scenarios, it over-
all leads to good correlation with the gold data.
</p>
<p>MEANTIME proves especially hard None of
the systems were able to surpass the all-factual
baseline in terms of MAE on MEANTIME. This
</p>
<p>7http://scikit-learn.org/
</p>
<p>Dataset FactBank UW MEANTIME
MAE r MAE r MAE r
</p>
<p>All-factual .80 0 .78 0 .31 0
UW feat.† .81 .66 .51 .71 .56 .33
AMR .66 .66 .64 .58 .44 .30
Rule-based .75 .62 .72 .63 .35 .23
Supervised .59 .71 .42 .66 .34 .47
</p>
<p>Table 2: Performance of the baselines against
our new supervised model (bottom). †The perfor-
mance of UW features on MEANTIME and Fact-
Bank uses a different solver from that in Lee et al.
(2015). See Section 5 for details.
</p>
<p>is due to its much smaller size and heavy factual
bias (assigning +3.0 to 90% of the predicates).
</p>
<p>AMR models achieve comparable performance
While AMR provides a more abstract represen-
tation, many aspects of factuality (interaction of
verb tenses, modal verbs, negation) are not mod-
eled. Noisy automatic parses also diminish the
positive effect of richer feature representation.
</p>
<p>6 Conclusions and Future Work
</p>
<p>We presented an intuitive method for mapping
FactBank and MEANTIME onto the UW scale,
and presented a novel factuality model which ex-
tends TruthTeller and applies it over PropS’ ab-
straction of dependency trees. An interesting di-
rection for future work is to address the inher-
ent bias in the data towards the factual end of the
scale by uniformly bucketing the factuality values,
which will affect the way the evaluation is carried
out on top of these annotations.
</p>
<p>We made both the unified representation and the
trained model publicly available8, hoping that it
will enable factuality research across larger, more
diverse datasets.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank the anonymous review-
ers for their helpful comments. This work was
supported in part by grants from the MAGNET
program of the Israeli Office of the Chief Scien-
tist (OCS) and by the German Research Founda-
tion through the German-Israeli Project Coopera-
tion (DIP, grant DA 1600/1-1).
</p>
<p>8https://github.com/gabrielStanovsky/
unified-factuality
</p>
<p>356</p>
<p />
</div>
<div class="page"><p />
<p>References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
</p>
<p>Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking .
</p>
<p>Debasish Basak, Srimanta Pal, and Dipak Chan-
dra Patranabis. 2007. Support vector regression.
Neural Information Processing-Letters and Reviews
11(10):203–224.
</p>
<p>Judith Eckle-Kohler. 2016. Verbs taking clausal and
non-finite arguments as signals of modality revis-
iting the issue of meaning grounded in syntax. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016).
Association for Computational Linguistics, volume
Volume 1: Long Papers, pages 811–822.
</p>
<p>Arne Fitschen. 2004. Ein Computerlinguistisches
Lexikon als komplexes System. PhD Thesis, Uni-
versität Stuttgart, Germany.
</p>
<p>Elisa Ghia, Lennart Kloppenburg, Malvina Nissim,
Paola Pietrandrea, and Valerio Cervoni. 2016. A
construction-centered approach to the annotation of
modality. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Sara Goggi,
Marko Grobelnik, Bente Maegaard, Joseph Mari-
ani, Helene Mazo, Asuncion Moreno, Jan Odijk,
and Stelios Piperidis, editors, Proceedings of the
12thISO Workshop on Interoperable Semantic Anno-
tation. European Language Resources Association
(ELRA).
</p>
<p>Valentine Hacquard. 2011. Modality. In Claudia
Maienborn, Klaus von Heusinger, and Paul Port-
ner, editors, Semantics: An International Handbook
of Natural Language Meaning. HSK 33.2, Berlin:
Mouton de Gruyter, pages 1484–1515.
</p>
<p>Lauri Karttunen. 2012. Simple and Phrasal Implica-
tives. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics. Montréal,
Canada, pages 124–131.
</p>
<p>Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-
moyer. 2015. Event detection and factuality assess-
ment with non-expert supervision. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing (EMNLP). Associa-
tion for Computational Linguistics, Lisbon, Portu-
gal.
</p>
<p>Amnon Lotan, Asher Stern, and Ido Dagan. 2013.
Truthteller: Annotating predicate truth. In HLT-
NAACL. pages 752–757.
</p>
<p>Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In Proceedings
of the Eighth International Conference on Computa-
tional Semantics. Tilburg, The Netherlands, IWCS-8
’09, pages 140–156.
</p>
<p>Anne-Lyse Minard, Manuela Speranza, Ruben Urizar,
Begona Altuna, Marieke van Erp, Anneleen Schoen,
and Chantal van Son. 2016. Meantime, the news-
reader multilingual event and time corpus. Proceed-
ings of LREC2016 .
</p>
<p>Malvina Nissim, Paola Pietrandrea, Andrea Sanso, and
Caterina Mauri. 2013. Cross-Linguistic Annotation
of Modality: a Data-Driven Hierarchical Model. In
Proceedings of the 9th Joint ISO - ACL SIGSEM
Workshop on Interoperable Semantic Annotation.
Potsdam, Germany, pages 7–14.
</p>
<p>Tim OGorman, Kristin Wright-Bettner, and Martha
Palmer. 2016. Richer event description: Integrating
event coreference with temporal, causal and bridg-
ing annotation. Computing News Storylines page 47.
</p>
<p>Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel
Marcu, and Jonathan May. 2015. Parsing english
into abstract meaning representation using syntax-
based machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1143–
1154.
</p>
<p>Roser Saurı́ and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
resources and evaluation 43(3):227.
</p>
<p>Roser Saurı́ and James Pustejovsky. 2012. Are You
Sure That This Happened? Assessing the Factuality
Degree of Events in Text. Computational Linguis-
tics 38(2):261–299.
</p>
<p>Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav
Goldberg. 2016. Getting more out of syntax with
props. arXiv preprint .
</p>
<p>357</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2057
</p>
<p>Question Answering on Knowledge Bases and Text
using Universal Schema and Memory Networks
</p>
<p>Rajarshi Das∗♠ Manzil Zaheer∗♥ Siva Reddy♣ and Andrew McCallum♠
♠College of Information and Computer Sciences, University of Massachusetts Amherst
</p>
<p>♥School of Computer Science, Carnegie Mellon University
♣School of Informatics, University of Edinburgh
</p>
<p>{rajarshi, mccallum}@cs.umass.edu, manzilz@cs.cmu.edu
siva.reddy@ed.ac.uk
</p>
<p>Abstract
</p>
<p>Existing question answering methods infer
answers either from a knowledge base or
from raw text. While knowledge base (KB)
methods are good at answering composi-
tional questions, their performance is often
affected by the incompleteness of the KB.
Au contraire, web text contains millions
of facts that are absent in the KB, how-
ever in an unstructured form. Universal
schema can support reasoning on the union
of both structured KBs and unstructured
text by aligning them in a common embed-
ded space. In this paper we extend uni-
versal schema to natural language question
answering, employing memory networks
to attend to the large body of facts in the
combination of text and KB. Our models
can be trained in an end-to-end fashion on
question-answer pairs. Evaluation results
on SPADES fill-in-the-blank question an-
swering dataset show that exploiting uni-
versal schema for question answering is
better than using either a KB or text alone.
This model also outperforms the current
state-of-the-art by 8.5 F1 points.
</p>
<p>1 Introduction
Question Answering (QA) has been a long-
standing goal of natural language processing. Two
main paradigms evolved in solving this problem:
1) answering questions on a knowledge base; and
2) answering questions using text.
</p>
<p>Knowledge bases (KB) contains facts expressed
in a fixed schema, facilitating compositional rea-
soning. These attracted research ever since the
early days of computer science, e.g., BASEBALL
(Green Jr et al., 1961). This problem has matured
into learning semantic parsers from parallel ques-
tion and logical form pairs (Zelle and Mooney,
</p>
<p>1996; Zettlemoyer and Collins, 2005), to recent
scaling of methods to work on very large KBs like
Freebase using question and answer pairs (Berant
et al., 2013). However, a major drawback of this
paradigm is that KBs are highly incomplete (Dong
et al., 2014). It is also an open question whether
KB relational structure is expressive enough to rep-
resent world knowledge (Stanovsky et al., 2014;
Gardner and Krishnamurthy, 2017)
</p>
<p>The paradigm of exploiting text for questions
started in the early 1990s (Kupiec, 1993). With
the advent of web, access to text resources became
abundant and cheap. Initiatives like TREC QA
competitions helped popularizing this paradigm
(Voorhees et al., 1999). With the recent advances
in deep learning and availability of large public
datasets, there has been an explosion of research in
a very short time (Rajpurkar et al., 2016; Trischler
et al., 2016; Nguyen et al., 2016; Wang and Jiang,
2016; Lee et al., 2016; Xiong et al., 2016; Seo et al.,
2016; Choi et al., 2016). Still, text representation is
unstructured and does not allow the compositional
reasoning which structured KB supports.
</p>
<p>An important but under-explored QA paradigm
is where KB and text are exploited together (Fer-
rucci et al., 2010). Such combination is attractive
because text contains millions of facts not present
in KB, and a KB’s generative capacity represents
infinite number of facts that are never seen in text.
However QA inference on this combination is chal-
lenging due to the structural non-uniformity of KB
and text. Distant supervision methods (Bunescu
and Mooney, 2007; Mintz et al., 2009; Riedel et al.,
2010; Yao et al., 2010; Zeng et al., 2015) address
this problem partially by means of aligning text pat-
terns with KB. But the rich and ambiguous nature
of language allows a fact to be expressed in many
different forms which these models fail to capture.
Universal schema (Riedel et al., 2013) avoids the
alignment problem by jointly embedding KB facts
</p>
<p>358</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2057">https://doi.org/10.18653/v1/P17-2057</a></div>
</div>
<div class="page"><p />
<p>kb
:h
</p>
<p>as
_c
</p>
<p>ity
kb
</p>
<p>:h
as
</p>
<p>_c
om
</p>
<p>pa
ny
</p>
<p>kb
:p
</p>
<p>re
sid
</p>
<p>en
t_
</p>
<p>of
</p>
<p>ar
g 
</p>
<p>2 
is 
</p>
<p>th
e f
</p>
<p>irs
t 
</p>
<p>no
n-
</p>
<p>wh
ite
</p>
<p> p
re
</p>
<p>sid
en
</p>
<p>t o
f 
</p>
<p>ar
g 
</p>
<p>1
</p>
<p>... ...
</p>
<p>USA/
Obama
</p>
<p>USA/
Google
</p>
<p>...
</p>
<p>...
</p>
<p>...
</p>
<p>1
</p>
<p>...
</p>
<p>USA/
NYC
</p>
<p>1
</p>
<p>1
</p>
<p>USA/
Facebook 11
</p>
<p>1
</p>
<p>1
</p>
<p>USA    has  elected _blank_ our    first  african-american    president
</p>
<p>Bidirectional LSTM
</p>
<p>Attention Layer
</p>
<p>Affine+Softmax
</p>
<p>Ba
rac
</p>
<p>k 
</p>
<p>Ob
am
</p>
<p>a
Do
</p>
<p>nal
d 
</p>
<p>Tru
mpHil
</p>
<p>lary US
A
</p>
<p>NY
C..
</p>
<p>.
..
</p>
<p>.
</p>
<p>..
.
</p>
<p>..
.
..
</p>
<p>.
..
</p>
<p>.
..
</p>
<p>.
..
</p>
<p>.
..
</p>
<p>.
</p>
<p>1
</p>
<p>ar
g2
</p>
<p> is
 
</p>
<p>he
ad
</p>
<p>qu
ar
</p>
<p>ter
ed
</p>
<p>  
in
</p>
<p> ar
g 
</p>
<p>1
</p>
<p>Figure 1: Memory network attending the facts in the universal schema (matrix on the left). The color
gradients denote the attention weight on each fact.
</p>
<p>and text into a uniform structured representation,
allowing interleaved propagation of information.
Figure 1 shows a universal schema matrix which
has pairs of entities as rows, and Freebase and
textual relations in columns. Although universal
schema has been extensively used for relation ex-
traction, this paper shows its applicability to QA.
Consider the question USA has elected blank ,
our first african-american president with its answer
Barack Obama. While Freebase has a predicate for
representing presidents of USA, it does not have
one for ‘african-american’ presidents. Whereas in
text, we find many sentences describing the pres-
idency of Barack Obama and his ethnicity at the
same time. Exploiting both KB and text makes it
relatively easy to answer this question than relying
on only one of these sources.
</p>
<p>Memory networks (MemNN; Weston et al. 2015)
are a class of neural models which have an external
memory component for encoding short and long
term context. In this work, we define the mem-
ory components as observed cells of the universal
schema matrix, and train an end-to-end QA model
on question-answer pairs.
</p>
<p>The contributions of the paper are as follows
(a) We show that universal schema representation
is a better knowledge source for QA than either
KB or text alone, (b) On the SPADES dataset (Bisk
et al., 2016), containing real world fill-in-the-blank
questions, we outperform state-of-the-art semantic
parsing baseline, with 8.5 F1 points. (c) Our analy-
sis shows how individual data sources help fill the
weakness of the other, thereby improving overall
performance.
</p>
<p>2 Background
</p>
<p>Problem Definition Given a question q with
words w1,w2, . . . ,wn, where these words contain
one blank and at least one entity, our goal is to
fill in this blank with an answer entity qa using
a knowledge base K and text T . Few example
question answer pairs are shown in Table 2.
</p>
<p>Universal Schema Traditionally universal
schema is used for relation extraction in the
context of knowledge base population. Rows in
the schema are formed by entity pairs (e.g. USA,
NYC), and columns represent the relation between
them. A relation can either be a KB relation, or it
could be a pattern of text that exist between these
two entities in a large corpus. The embeddings of
entities and relation types are learned by low-rank
matrix factorization techniques. Riedel et al.
(2013) treat textual patterns as static symbols,
whereas recent work by Verga et al. (2016)
replaces them with distributed representation of
sentences obtained by a RNN. Using distributed
representation allows reasoning on sentences that
are similar in meaning but different on the surface
form. We too use this variant to encode our textual
relations.
</p>
<p>Memory Networks MemNNs are neural atten-
tion models with external and differentiable mem-
ory. MemNNs decouple the memory component
from the network thereby allowing it store external
information. Previously, these have been success-
fully applied to question answering on KB where
the memory is filled with distributed representation
of KB triples (Bordes et al., 2015), or for read-
</p>
<p>359</p>
<p />
</div>
<div class="page"><p />
<p>ing comprehension (Sukhbaatar et al., 2015; Hill
et al., 2016), where the memory consists of dis-
tributed representation of sentences in the compre-
hension. Recently, key-value MemNN are intro-
duced (Miller et al., 2016) where each memory slot
consists of a key and value. The attention weight
is computed only by comparing the question with
the key memory, whereas the value is used to com-
pute the contextual representation to predict the
answer. We use this variant of MemNN for our
model. Miller et al. (2016), in their experiments,
store either KB triples or sentences as memories
but they do not explicitly model multiple memories
containing distinct data sources like we do.
</p>
<p>3 Model
Our model is a MemNN with universal schema as
its memory. Figure 1 shows the model architecture.
</p>
<p>Memory: Our memory M comprise of both KB
and textual triples from universal schema. Each
memory cell is in the form of key-value pair. Let
(s, r,o) ∈ K represent a KB triple. We represent
this fact with distributed key k ∈ R2d formed by
concatenating the embeddings s ∈ Rd and r ∈ Rd
of subject entity s and relation r respectively. The
embedding o ∈ Rd of object entity o is treated as
its value v.
</p>
<p>Let (s, [w1, . . . ,arg1, . . . ,arg2,wn], o) ∈ T rep-
resent a textual fact, where arg1 and arg2 corre-
spond to the positions of the entities ‘s’ and ‘o’. We
represent the key as the sequence formed by replac-
ing arg1 with ‘s’ and arg2 with a special ‘ blank ’
token, i.e., k = [w1, . . . ,s, . . . , blank , wn] and
value as just the entity ‘o’. We convert k to a dis-
tributed representation using a bidirectional LSTM
(Hochreiter and Schmidhuber, 1997; Graves and
Schmidhuber, 2005), where k ∈ R2d is formed by
concatenating the last states of forward and back-
ward LSTM, i.e., k =
</p>
<p>[−−−−→
LSTM(k);
</p>
<p>←−−−−
LSTM(k)
</p>
<p>]
.
</p>
<p>The value v is the embedding of the object entity o.
Projecting both KB and textual facts to R2d offers
a unified view of the knowledge to reason upon.
In Figure 1, each cell in the matrix represents a
memory containing the distributed representation
of its key and value.
</p>
<p>Question Encoder: A bidirectional LSTM is
also used to encode the input question q to a dis-
tributed representation q ∈ R2d similar to the key
encoding step above.
</p>
<p>Attention over cells: We compute attention
weight of a memory cell by taking the dot prod-
</p>
<p>uct of its key k with a contextual vector c which
encodes most important context in the current iter-
ation. In the first iteration, the contextual vector is
the question itself. We only consider the memory
cells that contain at least one entity in the question.
For example, for the input question in Figure 1,
we only consider memory cells containing USA.
Using the attention weights and values of memory
cells, we compute the context vector ct for the next
iteration t as follows:
</p>
<p>ct = Wt
</p>
<p>(
ct−1 +Wp ∑
</p>
<p>(k,v)∈M
(ct−1 ·k)v
</p>
<p>)
</p>
<p>where c0 is initialized with question embedding
q, Wp is a projection matrix, and Wt represents
the weight matrix which considers the context in
previous hop and the values in the current iteration
based on their importance (attention weight). This
multi-iterative context selection allows multi-hop
reasoning without explicitly requiring a symbolic
query representation.
</p>
<p>Answer Entity Selection: The final contextual
vector ct is used to select the answer entity qa
(among all 1.8M entities in the dataset) which has
the highest inner product with it.
</p>
<p>4 Experiments
</p>
<p>4.1 Evaluation Dataset
</p>
<p>We use Freebase (Bollacker et al., 2008) as our
KB, and ClueWeb (Gabrilovich et al., 2013) as our
text source to build universal schema. For evalua-
tion, literature offers two options: 1) datasets for
text-based question answering tasks such as answer
sentence selection and reading comprehension; and
2) datasets for KB question answering.
</p>
<p>Although the text-based question answering
datasets are large in size, e.g., SQuAD (Rajpurkar
et al., 2016) has over 100k questions, answers to
these are often not entities but rather sentences
which are not the focus of our work. Moreover
these texts may not contain Freebase entities at all,
making these skewed heavily towards text. Com-
ing to the alternative option, WebQuestions (Berant
et al., 2013) is widely used for QA on Freebase.
This dataset is curated such that all questions can
be answered on Freebase alone. But since our goal
is to explore the impact of universal schema, testing
on a dataset completely answerable on a KB is not
ideal. WikiMovies dataset (Miller et al., 2016) also
has similar properties. Gardner and Krishnamurthy
(2017) created a dataset with motivations similar to
</p>
<p>360</p>
<p />
</div>
<div class="page"><p />
<p>Model Dev. F1 Test F1
Bisk et al. (2016) 32.7 31.4
ONLYKB 39.1 38.5
ONLYTEXT 25.3 26.6
ENSEMBLE. 39.4 38.6
UNISCHEMA 41.1 39.9
</p>
<p>Table 1: QA results on SPADES.
</p>
<p>ours, however this is not publicly released during
the submission time.
</p>
<p>Instead, we use SPADES (Bisk et al., 2016) as
our evaluation data which contains fill-in-the-blank
cloze-styled questions created from ClueWeb. This
dataset is ideal to test our hypothesis for following
reasons: 1) it is large with 93K sentences and 1.8M
entities; and 2) since these are collected from Web,
most sentences are natural. A limitation of this
dataset is that it contains only the sentences that
have entities connected by at least one relation in
Freebase, making it skewed towards Freebase as
we will see (§ 4.4). We use the standard train, dev
and test splits for our experiments. For text part of
universal schema, we use the sentences present in
the training set.
</p>
<p>4.2 Models
We evaluate the following models to measure the
impact of different knowledge sources for QA.
</p>
<p>ONLYKB: In this model, MemNN memory con-
tains only the facts from KB. For each KB triple
(e1,r,e2), we have two memory slots, one for
(e1,r,e2) and the other for its inverse (e2,ri,e1).
</p>
<p>ONLYTEXT: SPADES contains sentences with
blanks. We replace the blank tokens with the an-
swer entities to create textual facts from the train-
ing set. Using every pair of entities, we create a
memory cell similar to as in universal schema.
</p>
<p>ENSEMBLE This is an ensemble of the above
two models. We use a linear model that combines
the scores from, and use an ensemble to combine
the evidences from individual models.
</p>
<p>UNISCHEMA This is our main model with uni-
versal schema as its memory, i.e., it contains mem-
ory slots corresponding to both KB and textual
facts.
</p>
<p>4.3 Implementation Details
The dimensions of word, entity and relation em-
beddings, and LSTM states were set to d =50. The
word and entity embeddings were initialized with
word2vec (Mikolov et al., 2013) trained on 7.5
</p>
<p>Question Answer
</p>
<p>1. USA have elected blank , our first
african-american president.
</p>
<p>Obama
</p>
<p>2. Angelina has reportedly been threatening
to leave blank .
</p>
<p>Brad Pitt
</p>
<p>3. Spanish is more often a second and
weaker language among many blank .
</p>
<p>Latinos
</p>
<p>4. blank is the third largest city in the
United States.
</p>
<p>Chicago
</p>
<p>5. blank was Belshazzar ’s father. Nabonidus
</p>
<p>Table 2: A few questions on which ONLYKB fails
to answer but UNISCHEMA succeeds.
</p>
<p>million ClueWeb sentences containing entities in
Freebase subset of SPADES. The network weights
were initialized using Xavier initialization (Glorot
and Bengio, 2010). We considered up to a maxi-
mum of 5k KB facts and 2.5k textual facts for a
question. We used Adam (Kingma and Ba, 2015)
with the default hyperparameters (learning rate=1e-
3, β1=0.9, β2=0.999, ε=1e-8) for optimization. To
overcome exploding gradients, we restricted the
magnitude of the `2 norm of the gradient to 5. The
batch size during training was set to 32.
</p>
<p>To train the UNISCHEMA model, we initialized
the parameters from a trained ONLYKB model.
We found that this is crucial in making the UNIS-
CHEMA to work. Another caveat is the need to em-
ploy a trick similar to batch normalization (Ioffe
and Szegedy, 2015). For each minibatch, we nor-
malize the mean and variance of the textual facts
and then scale and shift to match the mean and
variance of the KB memory facts. Empirically, this
stabilized the training and gave a boost in the final
performance.
</p>
<p>4.4 Results and Discussions
</p>
<p>Table 1 shows the main results on SPADES. UNIS-
CHEMA outperforms all our models validating our
hypothesis that exploiting universal schema for QA
is better than using either KB or text alone. De-
spite SPADES creation process being friendly to
Freebase, exploiting text still provides a significant
improvement. Table 2 shows some of the ques-
tions which UNISCHEMA answered but ONLYKB
failed. These can be broadly classified into (a)
relations that are not expressed in Freebase (e.g.,
african-american presidents in sentence 1); (b) in-
tentional facts since curated databases only rep-
resent concrete facts rather than intentions (e.g.,
threating to leave in sentence 2); (c) compara-
tive predicates like first, second, largest, smallest
(e.g., sentences 3 and 4); and (d) providing addi-
</p>
<p>361</p>
<p />
</div>
<div class="page"><p />
<p>Model Dev. F1
ONLYKB correct 39.1
ONLYTEXT correct 25.3
UNISCHEMA correct 41.1
ONLYKB or ONLYTEXT got it correct 45.9
</p>
<p>Both ONLYKB and ONLYTEXT got it correct 18.5
ONLYKB got it correct and ONLYTEXT did not 20.6
ONLYTEXT got it correct and ONLYKB did not 6.80
</p>
<p>Both UNISCHEMA and ONLYKB got it correct 34.6
UNISCHEMA got it correct and ONLYKB did not 6.42
ONLYKB got it correct and UNISCHEMA did not 4.47
</p>
<p>Both UNISCHEMA and ONLYTEXT got it correct 19.2
UNISCHEMA got it correct and ONLYTEXT did not 21.9
ONLYTEXT got it correct and UNISCHEMA did not 6.09
</p>
<p>Table 3: Detailed results on SPADES.
</p>
<p>tional type constraints (e.g., in sentence 5, Freebase
does not have a special relation for father. It can be
expressed using the relation parent along with the
type constraint that the answer is of gender male).
</p>
<p>We have also anlalyzed the nature of UNIS-
CHEMA attention. In 58.7% of the cases the at-
tention tends to prefer KB facts over text. This is as
expected since KBs facts are concrete and accurate
than text. In 34.8% of cases, the memory prefers
to attend text even if the fact is already present in
the KB. For the rest (6.5%), the memory distributes
attention weight evenly, indicating for some ques-
tions, part of the evidence comes from text and part
of it from KB. Table 3 gives a more detailed quan-
titative analysis of the three models in comparison
with each other.
</p>
<p>To see how reliable is UNISCHEMA, we gradu-
ally increased the coverage of KB by allowing only
a fixed number of randomly chosen KB facts for
each entity. As Figure 2 shows, when the KB cov-
erage is less than 16 facts per entity, UNISCHEMA
outperforms ONLYKB by a wide-margin indicat-
ing UNISCHEMA is robust even in resource-scarce
scenario, whereas ONLYKB is very sensitive to
the coverage. UNISCHEMA also outperforms EN-
SEMBLE showing joint modeling is superior to en-
semble on the individual models. We also achieve
the state-of-the-art with 8.5 F1 points difference.
Bisk et al. use graph matching techniques to con-
vert natural language to Freebase queries whereas
even without an explicit query representation, we
outperform them.
</p>
<p>5 Related Work
</p>
<p>A majority of the QA literature that focused on
exploiting KB and text either improves the infer-
</p>
<p>Figure 2: Performance on varying the number of
available KB facts during test time. UNISCHEMA
model consistently outperforms ONLYKB
</p>
<p>ence on the KB using text based features (Krish-
namurthy and Mitchell, 2012; Reddy et al., 2014;
Joshi et al., 2014; Yao and Van Durme, 2014; Yih
et al., 2015; Neelakantan et al., 2015b; Guu et al.,
2015; Xu et al., 2016b; Choi et al., 2015; Savenkov
and Agichtein, 2016) or improves the inference on
text using KB (Sun et al., 2015).
</p>
<p>Limited work exists on exploiting text and KB
jointly for question answering. Gardner and Krish-
namurthy (2017) is the closest to ours who generate
a open-vocabulary logical form and rank candidate
answers by how likely they occur with this logi-
cal form both in Freebase and text. Our models
are trained on a weaker supervision signal without
requiring the annotation of the logical forms.
</p>
<p>A few QA methods infer on curated databases
combined with OpenIE triples (Fader et al., 2014;
Yahya et al., 2016; Xu et al., 2016a). Our work
differs from them in two ways: 1) we do not need
an explicit database query to retrieve the answers
(Neelakantan et al., 2015a; Andreas et al., 2016);
and 2) our text-based facts retain complete senten-
tial context unlike the OpenIE triples (Banko et al.,
2007; Carlson et al., 2010).
</p>
<p>6 Conclusions
</p>
<p>In this work, we showed universal schema is a
promising knowledge source for QA than using
KB or text alone. Our results conclude though KB
is preferred over text when the KB contains the fact
of interest, a large portion of queries still attend to
text indicating the amalgam of both text and KB is
superior than KB alone.
</p>
<p>362</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgments
</p>
<p>We sincerely thank Luke Vilnis for helpful insights.
This work was supported in part by the Center
for Intelligent Information Retrieval and in part
by DARPA under agreement number FA8750-13-
2-0020. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
those of the sponsor.
</p>
<p>References
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
</p>
<p>Dan Klein. 2016. Learning to Compose Neural Net-
works for Question Answering. In NAACL.
</p>
<p>Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open Information Extraction from the Web. In IJ-
CAI.
</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In EMNLP.
</p>
<p>Yonatan Bisk, Siva Reddy, John Blitzer, Julia Hock-
enmaier, and Mark Steedman. 2016. Evaluating In-
duced CCG Parsers on Grounded Semantic Parsing.
In EMNLP.
</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In ICDM.
</p>
<p>Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015. Large-scale simple question
answering with memory networks. CoRR .
</p>
<p>Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In ACL.
</p>
<p>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Jr. Estevam R. Hruschka, and Tom M.
Mitchell. 2010. Toward an Architecture for Never-
ending Language Learning. In AAAI.
</p>
<p>Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia
Polosukhin, Jakob Uszkoreit, and Jonathan Berant.
2016. Hierarchical question answering for long doc-
uments. arXiv preprint arXiv:1611.01839 .
</p>
<p>Eunsol Choi, Tom Kwiatkowski, and Luke Zettlemoyer.
2015. Scalable Semantic Parsing with Partial On-
tologies. In ACL.
</p>
<p>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge Vault: A Web-scale Approach to Probabilistic
Knowledge Fusion. New York, NY, USA, KDD ’14.
</p>
<p>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In KDD. ACM, pages
1156–1165.
</p>
<p>David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, and others. 2010. Building Watson: An
overview of the DeepQA project. AI magazine .
</p>
<p>Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. Facc1: Freebase annota-
tion of clueweb corpora. (http://lemurproject.
org/clueweb09/.
</p>
<p>Matt Gardner and Jayant Krishnamurthy. 2017. Open-
Vocabulary Semantic Parsing with both Distribu-
tional Statistics and Formal Knowledge. In AAAI.
</p>
<p>Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In AISTATS.
</p>
<p>Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works .
</p>
<p>Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference. ACM, pages 219–224.
</p>
<p>K. Guu, J. Miller, and P. Liang. 2015. Traversing
knowledge graphs in vector space. In EMNLP.
</p>
<p>Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2016. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. ICLR .
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation .
</p>
<p>Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. In ICML. JMLR
Workshop and Conference Proceedings.
</p>
<p>Mandar Joshi, Uma Sawant, and Soumen Chakrabarti.
2014. Knowledge Graph and Corpus Driven Seg-
mentation and Answer Inference for Telegraphic
Entity-seeking Queries. In EMNLP.
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. ICLR .
</p>
<p>363</p>
<p />
</div>
<div class="page"><p />
<p>Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly Supervised Training of Semantic Parsers. In
EMNLP.
</p>
<p>Julian Kupiec. 1993. MURAX: A robust linguistic ap-
proach for question answering using an on-line en-
cyclopedia. In SIGIR. ACM.
</p>
<p>Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Di-
panjan Das. 2016. Learning recurrent span repre-
sentations for extractive question answering. arXiv
preprint arXiv:1611.01436 .
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS.
</p>
<p>Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason Weston.
2016. Key-value memory networks for directly read-
ing documents. In EMNLP .
</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL.
</p>
<p>Arvind Neelakantan, Quoc V Le, and Ilya Sutskever.
2015a. Neural programmer: Inducing latent pro-
grams with gradient descent. arXiv preprint
arXiv:1511.04834 .
</p>
<p>Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015b. Compositional vector space models
for knowledge base completion. In ACL.
</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. MS MARCO: A Human Generated MA-
chine Reading COmprehension Dataset. CoRR
abs/1611.09268.
</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ Questions
for Machine Comprehension of Text. In EMNLP.
Austin, Texas.
</p>
<p>Siva Reddy, Mirella Lapata, and Mark Steedman. 2014.
Large-scale semantic parsing without question-
answer pairs. TACL 2.
</p>
<p>Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In ECML PKDD.
</p>
<p>Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL.
</p>
<p>Denis Savenkov and Eugene Agichtein. 2016. When
a knowledge base is not enough: Question answer-
ing over knowledge bases with external text data. In
SIGIR. ACM.
</p>
<p>Minjoon Seo, Sewon Min, Ali Farhadi, and Han-
naneh Hajishirzi. 2016. Query-reduction net-
works for question answering. arXiv preprint
arXiv:1606.04582 .
</p>
<p>Gabriel Stanovsky, Omer Levy, and Ido Dagan. 2014.
Proposition Knowledge Graphs. COLING 2014 .
</p>
<p>Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In NIPS.
</p>
<p>Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai,
Jingjing Liu, and Ming-Wei Chang. 2015. Open do-
main question answering via semantic enrichment.
In WWW. ACM.
</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. NewsQA: A Machine Compre-
hension Dataset. CoRR abs/1611.09830.
</p>
<p>Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multilin-
gual relation extraction using compositional univer-
sal schema .
</p>
<p>Ellen M Voorhees et al. 1999. The trec-8 question an-
swering track report. In Trec. volume 99, pages 77–
82.
</p>
<p>Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905 .
</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes.
2015. Memory networks. In ICLR .
</p>
<p>Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic Coattention Networks For Question
Answering. arXiv preprint arXiv:1611.01604 .
</p>
<p>Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2016a. Hybrid Question Answering
over Knowledge Base and Free Text. In COLING.
</p>
<p>Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,
and Dongyan Zhao. 2016b. Question Answering on
Freebase via Relation Extraction and Textual Evi-
dence. In ACL.
</p>
<p>Mohamed Yahya, Denilson Barbosa, Klaus Berberich,
Qiuyue Wang, and Gerhard Weikum. 2016. Rela-
tionship queries on extended knowledge graphs. In
Proceedings of the Ninth ACM International Confer-
ence on Web Search and Data Mining. ACM, pages
605–614.
</p>
<p>Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In EMNLP.
</p>
<p>Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation Extraction over Structured Data: Question
Answering with Freebase. In ACL.
</p>
<p>364</p>
<p />
</div>
<div class="page"><p />
<p>Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic Parsing via Staged
Query Graph Generation: Question Answering with
Knowledge Base. In ACL.
</p>
<p>John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI. Portland, Oregon.
</p>
<p>Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction
via piecewise convolutional neural networks. In
EMNLP.
</p>
<p>Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Gram-
mars. In UAI. Edinburgh, Scotland.
</p>
<p>365</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 366–371
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2058
</p>
<p>Differentiable Scheduled Sampling for Credit Assignment
</p>
<p>Kartik Goyal
Carnegie Mellon University
</p>
<p>Pittsbrugh, PA, USA
kartikgo@cs.cmu.edu
</p>
<p>Chris Dyer
DeepMind
</p>
<p>London, UK
cdyer@google.com
</p>
<p>Taylor Berg-Kirkpatrick
Carnegie Mellon University
</p>
<p>Pittsburgh, PA, USA
tberg@cs.cmu.edu
</p>
<p>Abstract
</p>
<p>We demonstrate that a continuous relax-
ation of the argmax operation can be
used to create a differentiable approxima-
tion to greedy decoding for sequence-to-
sequence (seq2seq) models. By incorpo-
rating this approximation into the sched-
uled sampling training procedure (Bengio
et al., 2015)–a well-known technique for
correcting exposure bias–we introduce a
new training objective that is continuous
and differentiable everywhere and that can
provide informative gradients near points
where previous decoding decisions change
their value. In addition, by using a related
approximation, we demonstrate a similar
approach to sampled-based training. Fi-
nally, we show that our approach outper-
forms cross-entropy training and sched-
uled sampling procedures in two sequence
prediction tasks: named entity recognition
and machine translation.
</p>
<p>1 Introduction
</p>
<p>Sequence-to-Sequence (seq2seq) models have
demonstrated excellent performance in several
tasks including machine translation (Sutskever
et al., 2014), summarization (Rush et al., 2015),
dialogue generation (Serban et al., 2015), and
image captioning (Xu et al., 2015). However,
the standard cross-entropy training procedure for
these models suffers from the well-known prob-
lem of exposure bias: because cross-entropy train-
ing always uses gold contexts, the states and con-
texts encountered during training do not match
those encountered at test time. This issue has been
addressed using several approaches that try to in-
corporate awareness of decoding choices into the
training optimization. These include reinforce-
ment learning (Ranzato et al., 2016; Bahdanau
</p>
<p>et al., 2017), imitation learning (Daumé et al.,
2009; Ross et al., 2011; Bengio et al., 2015),
and beam-based approaches (Wiseman and Rush,
2016; Andor et al., 2016; Daumé III and Marcu,
2005). In this paper, we focus on one the simplest
to implement and least computationally expensive
approaches, scheduled sampling (Bengio et al.,
2015), which stochastically incorporates contexts
from previous decoding decisions into training.
</p>
<p>While scheduled sampling has been empiri-
cally successful, its training objective has a draw-
back: because the procedure directly incorporates
greedy decisions at each time step, the objective is
discontinuous at parameter settings where previ-
ous decisions change their value. As a result, gra-
dients near these points are non-informative and
scheduled sampling has difficulty assigning credit
for errors. In particular, the gradient does not pro-
vide information useful in distinguishing between
local errors without future consequences and cas-
cading errors which are more serious.
</p>
<p>Here, we propose a novel approach based on
scheduled sampling that uses a differentiable ap-
proximation of previous greedy decoding deci-
sions inside the training objective by incorporat-
ing a continuous relaxation of argmax. As a re-
sult, our end-to-end relaxed greedy training ob-
jective is differentiable everywhere and fully con-
tinuous. By making the objective continuous at
points where previous decisions change value, our
approach provides gradients that can respond to
cascading errors. In addition, we demonstrate a
related approximation and reparametrization for
sample-based training (another training scenario
considered by scheduled sampling (Bengio et al.,
2015)) that can yield stochastic gradients with
lower variance than in standard scheduled sam-
pling. In our experiments on two different tasks,
machine translation (MT) and named entity recog-
nition (NER), we show that our approach out-
performs both cross-entropy training and standard
</p>
<p>366</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2058">https://doi.org/10.18653/v1/P17-2058</a></div>
</div>
<div class="page"><p />
<p>↵ = 1
</p>
<p>↵ = 10
</p>
<p>✓
</p>
<p>ob
je
</p>
<p>ct
iv
</p>
<p>e(
✓)
</p>
<p>ŷi�1(✓) = ‘dog’ŷi�1(✓) = ‘kitten’ ŷi�1(✓) = ‘cat’
</p>
<p>Figure 1: Discontinuous scheduled sampling objective (red)
and continuous relaxations (blue and purple).
</p>
<p>scheduled sampling procedures with greedy and
sampled-based training.
</p>
<p>2 Discontinuity in Scheduled Sampling
</p>
<p>While scheduled sampling (Bengio et al., 2015) is
an effective way to rectify exposure bias, it can-
not differentiate between cascading errors, which
can lead to a sequence of bad decisions, and local
errors, which have more benign effects. Specifi-
cally, scheduled sampling focuses on learning op-
timal behavior in the current step given the fixed
decoding decision of the previous step. If a previ-
ous bad decision is largely responsible for the cur-
rent error, the training procedure has difficulty ad-
justing the parameters accordingly. The following
machine translation example highlights this credit
assignment issue:
</p>
<p>Ref: The cat purrs . Pred: The dog barks .
</p>
<p>At step 3, the model prefers the word ‘barks’ af-
ter incorrectly predicting ‘dog’ at step 2. To cor-
rect this error, the scheduled sampling procedure
would increase the score of ‘purrs’ at step 3, con-
ditioned on the fact that the model predicted (in-
correctly) ‘dog’ at step 2, which is not the ideal
learning behaviour. Ideally, the model should be
able to backpropagate the error from step 3 to the
source of the problem which occurred at step 2,
where ‘dog’ was predicted instead of ‘cat’.
</p>
<p>The lack of credit assignment during training
is a result of discontinuity in the objective func-
tion used by scheduled sampling, as illustrated in
Figure 1. We denote the ground truth target sym-
bol at step i by y∗i , the embedding representa-
tion of word y by e(y), and the hidden state of
a seq2seq decoder at step i as hi. Standard cross-
entropy training defines the loss at each step to be
log p(y∗i |hi(e(y∗i−1), hi−1)), while scheduled sam-
pling uses loss log p(y∗i |hi(e(ŷi−1), hi−1)), where
</p>
<p>{ e(dog)
e(cat) ↵-soft argmax
</p>
<p>{peaked softmaxargmax
</p>
<p>hi�1 hi
</p>
<p>e(kitten)
</p>
<p>ŷi�1 = dog
</p>
<p>ēi�1
</p>
<p>si�1(dog)
</p>
<p>si�1(kitten)
</p>
<p>si�1(cat)
</p>
<p>X
</p>
<p>y
</p>
<p>e(y) · exp [↵ · si�1(y)]
Z
</p>
<p>Figure 2: Relaxed greedy decoder that uses a continuous
approximation of argmax as input to the decoder state at next
time step.
</p>
<p>ŷi−1 refers the model’s prediction at the previ-
ous step.1 Here, the model prediction ŷi−1 is
obtained by argmaxing over the output softmax
layer. Hence, in addition to the intermediate hid-
den states and final softmax scores, the previ-
ous model prediction, ŷi−1, itself depends on the
model parameters, θ, and ideally, should be back-
propagated through, unlike the gold target symbol
y∗i−1 which is independent of model parameters.
However, the argmax operation is discontinuous,
and thus the training objective (depicted in Fig-
ure 1 as the red line) exhibits discontinuities at pa-
rameter settings where the previous decoding deci-
sions change value (depicted as changes from ‘kit-
ten’ to ‘dog’ to ‘cat’). Because these change points
represent discontinuities, their gradients are unde-
fined and the effect of correcting an earlier mistake
(for example ‘dog’ to ‘cat’) as the training proce-
dure approaches such a point is essentially hidden.
</p>
<p>In our approach, described in detail in the next
section, we attempt to fix this problem by incorpo-
rating a continuous relaxation of the argmax op-
eration into the scheduled sampling procedure in
order to form an approximate but fully continuous
objective. Our relaxed approximate objective is
depicted in Figure 1 as blue and purple lines, de-
pending on temperature parameter αwhich trades-
off smoothness and quality of approximation.
</p>
<p>3 Credit Assignment via Relaxation
</p>
<p>In this section we explain in detail the continu-
ous relaxation of greedy decoding that we will use
to build a fully continuous training objective. We
also introduce a related approach for sample-based
training.
</p>
<p>1For the sake of simplicity, the ‘always sample’ variant of
scheduled sampling is described (Bengio et al., 2015).
</p>
<p>367</p>
<p />
</div>
<div class="page"><p />
<p>3.1 Soft Argmax
</p>
<p>In scheduled sampling, the embedding for the best
scoring word at the previous step is passed as an
input to the current step. This operation2 can be
expressed as
</p>
<p>êi−1 =
∑
</p>
<p>y
</p>
<p>e(y)1[∀y′ 6= y si−1(y) &gt; si−1(y′)]
</p>
<p>where y is a word in the vocabulary, si−1(y) is the
output score of that word at the previous step, and
êi−1 is the embedding passed to the next step. This
operation can be relaxed by replacing the indicator
function with a peaked softmax function with hy-
perparameter α to define a soft argmax procedure:
</p>
<p>ēi−1 =
∑
</p>
<p>y
</p>
<p>e(y) · exp (α si−1(y))∑
y′ exp (α si−1(y
</p>
<p>′))
</p>
<p>As α → ∞, the equation above approaches the
true argmax embedding. Hence, with a finite and
large α, we get a linear combination of all the
words (and therefore a continuous function of the
parameters) that is dominated heavily by the word
with maximum score.
</p>
<p>3.2 Soft Reparametrized Sampling
</p>
<p>Another variant of scheduled sampling is to pass
a sampled embedding from the softmax distribu-
tion at the previous step to the current step instead
of the argmax. This is expected to enable better
exploration of the search space during optimiza-
tion due to the added randomness and hence re-
sult in a more robust model. In this section, we
discuss and review an approximation to the Gum-
bel reparametrization trick that we use as a mod-
ule in our sample-based decoder. This approxima-
tion was proposed by Maddison et al. (2017) and
Jang et al. (2016), who showed that the same soft
argmax operation introduced above can be used
for reducing variance of stochastic gradients when
sampling from softmax distributions. Unlike soft
argmax, this approach is not a fully continuous ap-
proximation to the sampling operation, but it does
result in much more informative gradients com-
pared to naive scheduled sampling procedure.
</p>
<p>The Gumbel reparametrization trick shows that
sampling from a categorical distribution can be
refactored into sampling from a simple distribu-
tion followed by a deterministic transformation
</p>
<p>2Assuming there are no ties for the sake of simplicity.
</p>
<p>as follows: (i) sampling an independent Gum-
bel noise G for each element in the categori-
cal distribution, typically done by transforming
a sample from the uniform distribution: U ∼
Uniform(0, 1) as G = −log(−log U), then
(ii) adding it componentwise to the unnormalized
score of each element, and finally (iii) taking an
argmax over the vector. Using the same argmax
softening procedure as above, they arrive at an ap-
proximation to the reparametrization trick which
mitigates some of the gradient’s variance intro-
duced by sampling. The approximation is3:
</p>
<p>ẽi−1 =
∑
</p>
<p>y
</p>
<p>e(y) · exp (α (si−1(y) +Gy))∑
y′ exp (α (si−1(y
</p>
<p>′) +Gy′))
</p>
<p>We will use this ‘concrete’ approximation of soft-
max sampling in our relaxation of scheduled sam-
pling with a sample-based decoder. We discuss
details in the next section. Note that our orig-
inal motivation based on removing discontinuity
does not strictly apply to this sampling procedure,
which still yields a stochastic gradient due to sam-
pling from the Gumbel distribution. However, this
approach is conceptually related to greedy relax-
ations since, here, the soft argmax reparametriza-
tion reduces gradient variance which may yield a
more informative training signal. Intuitively, this
approach results in the gradient of the loss to be
more aware of the sampling procedure compared
to naive scheduled sampling and hence carries for-
ward information about decisions made at previ-
ous steps. The empirical results, discussed later,
show similar gains to the greedy scenario.
</p>
<p>3.3 Differentiable Relaxed Decoders
With the argmax relaxation introduced above, we
have a recipe for a fully differentiable greedy de-
coder designed to produce informative gradients
near change points. Our final training network for
scheduled sampling with relaxed greedy decod-
ing is shown in Figure 2. Instead of conditioning
the current hidden state, hi, on the argmax em-
bedding from the previous step, êi−1, we use the
α-soft argmax embedding, ēi−1, defined in Sec-
tion 3.1. This removes the discontinuity in the
original greedy scheduled sampling objective by
passing a linear combination of embeddings, dom-
inated by the argmax, to the next step. Figure 1
</p>
<p>3This is different from using the expected softmax embed-
ding because our approach approximates the actual sampling
process instead of linearly weighting the embeddings by their
softmax probabilities
</p>
<p>368</p>
<p />
</div>
<div class="page"><p />
<p>illustrates the effect of varying α. As α increases,
we more closely approximate the greedy decoder.
</p>
<p>As in standard scheduled sampling, here we
minimize the cross-entropy based loss at each time
step. Hence the computational complexity of our
approach is comparable to standard seq2seq train-
ing. As we discuss in Section 5, mixing model
predictions randomly with ground truth symbols
during training (Bengio et al., 2015; Daumé et al.,
2009; Ross et al., 2011), while annealing the prob-
ability of using the ground truth with each epoch,
results in better models and more stable train-
ing. As a result, training is reliant on the anneal-
ing schedule of two important hyperparameters: i)
ground truth mixing probability and ii) the α pa-
rameter used for approximating the argmax func-
tion. For output prediction, at each time step, we
can still output the hard argmax, depicted in Fig-
ure 2.
</p>
<p>For the case of scheduled sampling with
sample-based training–where decisions are sam-
pled rather than chosen greedily (Bengio et al.,
2015)–we conduct experiments using a related
training procedure. Instead of using soft argmax,
we use the soft sample embedding, ẽi−1, defined
in Section 3.2. Apart from this difference, training
is carried out using the same procedure.
</p>
<p>4 Related Work
</p>
<p>Gormley et al. (2015)’s approximation-aware
training is conceptually related, but focuses on
variational decoding procedures. Hoang et al.
(2017) also propose continuous relaxations of de-
coders, but are focused on developing better infer-
ence procedures. Grefenstette et al. (2015) suc-
cessfully use a soft approximation to argmax in
neural stack mechanisms. Finally, Ranzato et al.
(2016) experiment with a similarly motivated ob-
jective that was not fully continuous, but found it
performed worse than the standard training.
</p>
<p>5 Experimental Setup
</p>
<p>We perform experiments with machine translation
(MT) and named entity recognition (NER).
</p>
<p>Data: For MT, we use the same dataset (the
German-English portion of the IWSLT 2014 ma-
chine translation evaluation campaign (Cettolo
et al., 2014)), preprocessing and data splits as Ran-
zato et al. (2016). For named entity recognition,
we use the CONLL 2003 shared task data (Tjong
</p>
<p>Kim Sang and De Meulder, 2003) for German lan-
guage and use the provided data splits. We per-
form no preprocessing on the data.The output vo-
cabulary length for MT is 32000 and 10 for NER.
</p>
<p>Implementation details: For MT, we use a
seq2seq model with a simple attention mechanism
(Bahdanau et al., 2015), a bidirectional LSTM en-
coder (1 layer, 256 units), and an LSTM decoder
(1 layer, 256 units). For NER, we use a seq2seq
model with an LSTM encoder (1 layer, 64 units)
and an LSTM decoder (1 layer, 64 units) with a
fixed attention mechanism that deterministically
attends to the ith input token when decoding the
ith output, and hence does not involve learning of
attention parameters. 4
</p>
<p>Hyperparameter tuning: We start by training
with actual ground truth sequences for the first
epoch and decay the probability of selecting the
ground truth token as an inverse sigmoid (Bengio
et al., 2015) of epochs with a decay strength pa-
rameter k. We also tuned for different values of α
and explore the effect of varying α exponentially
(annealing) with the epochs. In table 1, we report
results for the best performing configuration of de-
cay parameter and the α parameter on the valida-
tion set. To account for variance across randomly
started runs, we ran multiple random restarts (RR)
for all the systems evaluated and always used the
RR with the best validation set score to calculate
test performance.
</p>
<p>Comparison We report validation and test met-
rics for NER and MT tasks in Table 1, F1 and
BLEU respectively. ‘Greedy’ in the table refers
to scheduled sampling with soft argmax decisions
(either soft or hard) and ‘Sample’ refers the cor-
responding reparametrized sample-based decod-
ing scenario. We compare our approach with two
baselines: standard cross-entropy loss minimiza-
tion for seq2seq models (‘Baseline CE’) and the
standard scheduled sampling procedure (Bengio
et al. (2015)). We report results for two variants
of our approach: one with a fixed α parameter
throughout the training procedure (α-soft fixed),
and the other in which we vary α exponentially
with the number of epochs (α-soft annealed).
</p>
<p>4Fixed attention refers to the scenario when we use the
bidirectional LSTM encoder representation of the source se-
quence token at time step t while decoding at time step t in-
stead of using a linear combination of all the input sequences
weighted according to the attention parameters in the stan-
dard attention mechanism based models.
</p>
<p>369</p>
<p />
</div>
<div class="page"><p />
<p>Table 1: Result on NER and MT. We compare our approach (α-soft argmax with fixed and annealed temperature) with standard
cross entropy training (Baseline CE) and discontinuous scheduled sampling (Bengio et al. (2015)). ‘Greedy’ and ‘Sample’ refer
to Section 3.1 and Section 3.2.
</p>
<p>Training procedure NER (F1) MT (BLEU)
</p>
<p>Dev Test Dev Test
Baseline CE 49.43 53.32 20.35 19.11
</p>
<p>Greedy Sample Greedy Sample
Dev Test Dev Test Dev Test Dev Test
</p>
<p>Bengio et al. (2015) 49.75 54.83 50.90 54.60 20.52 19.85 20.40 19.69
α-soft fixed 51.65 55.88 51.13 56.25 21.32 20.28 20.48 19.69
α-soft annealed 51.43 56.33 50.99 54.20 21.28 20.18 21.36 20.60
</p>
<p>6 Results
</p>
<p>All three approaches improve over the standard
cross-entropy based seq2seq training. Moreover,
both approaches using continuous relaxations
(greedy and sample-based) outperform standard
scheduled sampling (Bengio et al., 2015). The
best results for NER were obtained with the re-
laxed greedy decoder with annealed α which
yielded an F1 gain of +3.1 over the standard
seq2seq baseline and a gain of +1.5 F1 over stan-
dard scheduled sampling. For MT, we obtain
the best results with the relaxed sample-based de-
coder, which yielded a gain of +1.5 BLEU over
standard seq2seq and a gain of +0.75 BLEU over
standard scheduled sampling.
</p>
<p>We observe that the reparametrized sample-
based method, although not fully continuous end-
to-end unlike the soft greedy approach, results
in good performance on both the tasks, partic-
ularly MT. This might be an effect of stochas-
tic exploration of the search space over the out-
put sequences during training and hence we ex-
pect MT to benefit from sampling due to a much
larger search space associated with it. We also ob-
serve that annealing α results in good performance
which suggests that a smoother approximation to
the loss function in the initial stages of training is
helpful in guiding the learning in the right direc-
tion. However, in our experiments we noticed that
</p>
<p>k 100 10 1 Always
NER (F1) 56.33 55.88 55.30 54.83
</p>
<p>Table 2: Effect of different schedules for scheduled sampling
on NER. k is the decay strength parameter. Higher k cor-
responds to gentler decay schedules. Always refers to the
case when predictions at the previous predictions are always
passed on as inputs to the next step.
</p>
<p>the performance while annealing α was sensitive
to the hyperparameter associated with the anneal-
ing schedule of the mixing probability in sched-
uled sampling during training.
</p>
<p>The computational complexity of our approach
is comparable to that of standard seq2seq train-
ing. However, instead of a vocabulary-sized max
and lookup, our approach requires a matrix multi-
plication. Practically, we observed that on GPU
hardware, all the models for both the tasks had
similar speeds which suggests that our approach
leads to accuracy gains without compromising
run-time. Moreover, as shown in Table 2, we ob-
serve that a gradual decay of mixing probability
consistently compared favorably to more aggres-
sive decay schedules. We also observed that the
‘always sample’ case of relaxed greedy decoding,
in which we never mix in ground truth inputs (see
Bengio et al. (2015)), worked well for NER but
resulted in unstable training for MT. We reckon
that this is an effect of large difference between
the search space associated with NER and MT.
</p>
<p>7 Conclusion
</p>
<p>Our positive results indicate that mechanisms for
credit assignment can be useful when added to
the models that aim to ameliorate exposure bias.
Further, our results suggest that continuous relax-
ations of the argmax operation can be used as ef-
fective approximations to hard decoding during
training.
</p>
<p>Acknowledgements
</p>
<p>We thank Graham Neubig for helpful discussions.
We also thank the three anonymous reviewers for
their valuable feedback.
</p>
<p>370</p>
<p />
</div>
<div class="page"><p />
<p>References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei
</p>
<p>Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Asso-
ciation for Computational Linguistics.
</p>
<p>Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In International
Conference on Learning Representations.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.
</p>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems. pages 1171–1179.
</p>
<p>Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, and Marcello Federico. 2014. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In
Proceedings of the International Workshop on Spo-
ken Language Translation, Hanoi, Vietnam.
</p>
<p>Hal Daumé, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine learn-
ing 75(3):297–325.
</p>
<p>Hal Daumé III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings
of the 22nd international conference on Machine
learning. ACM, pages 169–176.
</p>
<p>Matthew R. Gormley, Mark Dredze, and Jason Eisner.
2015. Approximation-aware dependency parsing by
belief propagation. Transactions of the Association
for Computational Linguistics (TACL) .
</p>
<p>Edward Grefenstette, Karl Moritz Hermann, Mustafa
Suleyman, and Phil Blunsom. 2015. Learning to
transduce with unbounded memory. In Advances
in Neural Information Processing Systems. pages
1828–1836.
</p>
<p>Cong Duy Vu Hoang, Gholamreza Haffari, and Trevor
Cohn. 2017. Decoding as continuous optimiza-
tion in neural machine translation. arXiv preprint
arXiv:1701.02854 .
</p>
<p>Eric Jang, Shixiang Gu, and Ben Poole. 2016. Cate-
gorical reparameterization with gumbel-softmax. In
International Conference on Learning Representa-
tions.
</p>
<p>Chris J Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The concrete distribution: A continuous re-
laxation of discrete random variables. In Interna-
tional Conference on Learning Representations.
</p>
<p>Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In International
Conference on Learning Representations.
</p>
<p>Stéphane Ross, Geoffrey J Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In AIS-
TATS. volume 1, page 6.
</p>
<p>Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Empirical Methods in Nat-
ural Language Processing.
</p>
<p>Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2015. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In AAAI’16 Pro-
ceedings of the Thirtieth AAAI Conference on Artifi-
cial Intelligence.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.
</p>
<p>Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4.
Association for Computational Linguistics, pages
142–147.
</p>
<p>Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In Empirical Methods in Natural Lan-
guage Processing.
</p>
<p>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C Courville, Ruslan Salakhutdinov, Richard S
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. In ICML. volume 14, pages 77–81.
</p>
<p>371</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 372–377
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2059
</p>
<p>A Deep Network with Visual Text Composition Behavior
</p>
<p>Hongyu Guo
</p>
<p>National Research Council Canada
</p>
<p>1200 Montreal Road, Ottawa, Ontario, K1A 0R6
</p>
<p>hongyu.guo@nrc-cnrc.gc.ca
</p>
<p>Abstract
</p>
<p>While natural languages are composi-
</p>
<p>tional, how state-of-the-art neural mod-
</p>
<p>els achieve compositionality is still un-
</p>
<p>clear. We propose a deep network, which
</p>
<p>not only achieves competitive accuracy for
</p>
<p>text classification, but also exhibits com-
</p>
<p>positional behavior. That is, while creating
</p>
<p>hierarchical representations of a piece of
</p>
<p>text, such as a sentence, the lower layers of
</p>
<p>the network distribute their layer-specific
</p>
<p>attention weights to individual words. In
</p>
<p>contrast, the higher layers compose mean-
</p>
<p>ingful phrases and clauses, whose lengths
</p>
<p>increase as the networks get deeper until
</p>
<p>fully composing the sentence.
</p>
<p>1 Introduction
</p>
<p>Deep neural networks leverage task-specific archi-
</p>
<p>tectures to develop hierarchical representations of
</p>
<p>the input, where higher level representations are
</p>
<p>derived from lower level features (Conneau et al.,
</p>
<p>2016). Such hierarchical representations have
</p>
<p>visually demonstrated compositionality in im-
</p>
<p>age processing, i.e., pixels combine to form
</p>
<p>shapes and then contours (Farabet et al., 2013;
</p>
<p>Zeiler and Fergus, 2014). Natural languages are
</p>
<p>also compositional, i.e., words combine to form
</p>
<p>phrases and then sentences. Yet unlike in vision,
</p>
<p>how deep neural models in NLP, which mainly
</p>
<p>operate on distributed word embeddings, achieve
</p>
<p>compositionality, is still unclear (Li et al., 2015,
</p>
<p>2016).
</p>
<p>We propose an Attention Gated Transforma-
</p>
<p>tion (AGT) network, where each layer’s feature
</p>
<p>generation is gated by a layer-specific attention
</p>
<p>mechanism (Bahdanau et al., 2014). Specifically,
</p>
<p>through distributing its attention to the original
</p>
<p>given text, each layer of the networks tends to in-
</p>
<p>crementally retrieve new words and phrases from
</p>
<p>the original text. The new knowledge is then com-
</p>
<p>bined with the previous layer’s features to create
</p>
<p>the current layer’s representation, thus resulting
</p>
<p>in composing longer or new phrases and clauses
</p>
<p>while creating higher layers’ representations of the
</p>
<p>text.
</p>
<p>Experiments on the Stanford Sentiment Tree-
</p>
<p>bank (Socher et al., 2013) dataset show that the
</p>
<p>AGT method not only achieves very competitive
</p>
<p>accuracy, but also exhibits compositional behav-
</p>
<p>ior via its layer-specific attention. We empirically
</p>
<p>show that, given a piece of text, e.g., a sentence,
</p>
<p>the lower layers of the networks select individ-
</p>
<p>ual words, e.g, negative and conjunction words
</p>
<p>not and though, while the higher layers aim at
</p>
<p>composing meaningful phrases and clauses such
</p>
<p>as negation phrase not so much, where the phrase
</p>
<p>length increases as the networks get deeper until
</p>
<p>fully composing the whole sentence. Interestingly,
</p>
<p>after composing the sentence, the compositions of
</p>
<p>different sentence phrases compete to become the
</p>
<p>dominating features of the end task.
</p>
<p>Figure 1: An AGT network with three layers.
</p>
<p>S1
+ tanh 
</p>
<p>S2
+ 
</p>
<p>tanh 
</p>
<p>S0
+ 
</p>
<p>softmax 
</p>
<p>σ 
</p>
<p>+ 
</p>
<p>fully connected 
</p>
<p>fully connected  
</p>
<p>+ 
</p>
<p>σ 
</p>
<p>in
p
</p>
<p>u
t 
</p>
<p>te
x
</p>
<p>t 
</p>
<p>Layer 1, y0 
</p>
<p>Layer 2, y1 
</p>
<p>Layer 3, y2:   
</p>
<p>372</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2059">https://doi.org/10.18653/v1/P17-2059</a></div>
</div>
<div class="page"><p />
<p>2 Attention Gated Transformation
</p>
<p>Network
</p>
<p>Our AGT network was inspired by the Highway
</p>
<p>Networks (Srivastava et al., 2015a,b), where each
</p>
<p>layer is equipped with a transform gate.
</p>
<p>2.1 Transform Gate for Information Flow
</p>
<p>Consider a feedforward neural network with mul-
</p>
<p>tiple layers. Each layer l typically applies a non-
linear transformation f (e.g., tanh, parameterized
by W
</p>
<p>f
l ), on its input, which is the output of the
</p>
<p>most recent previous layer (i.e., yl−1), to produce
its output yl. Here, l = 0 indicates the first layer
and y0 is equal to the given input text x, namely
y0 = x:
</p>
<p>yl = f(yl−1, W
f
l ) (1)
</p>
<p>While in a highway network (the left column of
</p>
<p>Figure 1), an additional non-linear transform gate
</p>
<p>function Tl is added to the l
th(l &gt; 0) layer:
</p>
<p>yl = f(yl−1, W
f
l )Tl + yl−1(1 − Tl) (2)
</p>
<p>where the function Tl expresses how much of the
representation yl is produced by transforming the
yl−1 (first term in Equation 2), and how much
is just carrying from yl−1 (second term in Equa-
tion 2). Here Tl is typically defined as:
</p>
<p>Tl = σ(W
t
l yl−1 + b
</p>
<p>t
l) (3)
</p>
<p>where W tl is the weight matrix and b
t
l the bias vec-
</p>
<p>tor; σ is the non-linear activation function.
With transform gate T , the networks learn to de-
</p>
<p>cide if a feature transformation is needed at each
</p>
<p>layer. Suppose σ represents a sigmoid function.
In such case, the output of T lies between zero
and one. Consequently, when the transform gate
</p>
<p>is one, the networks pass through the transforma-
</p>
<p>tion f over yl−1 and block the pass of input yl−1;
when the gate is zero, the networks pass through
</p>
<p>the unmodified yl−1, while the transformation f
over yl−1 is suppressed.
</p>
<p>The left column of Figure 1 reflects the high-
</p>
<p>way networks as proposed by (Srivastava et al.,
</p>
<p>2015b). Our AGT method adds the right two
</p>
<p>columns of Figure 1. That is, 1) the transform gate
</p>
<p>Tl now is not a function of yl−1, but a function of
the selection vector s+l , which is determined by the
attention distributed to the given input x by the lth
</p>
<p>layer (will be discussed next), and 2) the function
</p>
<p>f takes as input the concatenation of yl−1 and s
+
l
</p>
<p>to create feature representation yl. These changes
result in an attention gated transformation when
</p>
<p>forming hierarchical representations of the text.
</p>
<p>2.2 Attention Gated Transformation
</p>
<p>In AGT, the activation of the transform gate at each
</p>
<p>layer depends on a layer-specific attention mecha-
</p>
<p>nism. Formally, given a piece of text x, such as a
sentence with N words, it can be represented as a
matrix B ∈ IRN×d. Each row of the matrix corre-
sponds to one word, which is represented by a d-
dimensional vector as provided by a learned word
</p>
<p>embedding table. Consequently, the selection vec-
</p>
<p>tor s+l , for the l
th layer, is the softmax weighted
</p>
<p>sum over the N word vectors in B:
</p>
<p>s+l =
N∑
</p>
<p>n=1
</p>
<p>dl,nB[n : n] (4)
</p>
<p>with the weight (i.e., attention) dl,n computed as:
</p>
<p>dl,n =
exp(ml,n)∑N
</p>
<p>n=1 exp(ml,n)
(5)
</p>
<p>ml,n = w
m
l tanh(W
</p>
<p>m
l (B[n : n])) (6)
</p>
<p>here, wml and W
m
l are the weight vector and
</p>
<p>weight matrix, respectively. By varying the at-
</p>
<p>tention weight dl,n, the s
+
l can focus on different
</p>
<p>rows of the matrix B, namely different words of
the given text x, as illustrated by different color
curves connecting to s+ in Figure 1. Intuitively,
one can consider s+ as a learned word selection
component: choosing different sets of words of the
</p>
<p>given text x by distributing its distinct attention.
Having built one s+ for each layer from the
</p>
<p>given text x, the activation of the transform gate
for layer l (l &gt; 0) (i.e., Equation 3) is calculated:
</p>
<p>Tl = σ(W
t
l s
</p>
<p>+
l + b
</p>
<p>t
l) (7)
</p>
<p>To generate feature representation yl, the function
f takes as input the concatenation of yl−1 and s
</p>
<p>+
l .
</p>
<p>That is, Equation 2 becomes:
</p>
<p>yl =
</p>
<p>{
s+l , l = 0
</p>
<p>f([yl−1; s
+
l ], W
</p>
<p>f
l )Tl + yl−1(1 − Tl), l &gt; 0
</p>
<p>(8)
</p>
<p>where [...;...] denotes concatenation. Thus, at
</p>
<p>each layer l, the gate Tl can regulate either pass-
ing through yl−1 to form yl, or retrieving novel
</p>
<p>373</p>
<p />
</div>
<div class="page"><p />
<p>knowledge from the input text x to augment yl−1
to create a better representation for yl.
</p>
<p>Finally, as depicted in Figure 1, the feature rep-
</p>
<p>resentation of the last layer of the AGT is fed into
</p>
<p>two fully connected layers followed by a softmax
</p>
<p>function to produce a distribution over the possi-
</p>
<p>ble target classes. For training, we use multi-class
</p>
<p>cross entropy loss.
</p>
<p>Note that, Equation 8 indicates that the repre-
</p>
<p>sentation yl depends on both s
+
l and yl−1. In other
</p>
<p>words, although Equation 7 states that the gate ac-
</p>
<p>tivation at layer l is computed by s+l , the gate acti-
vation is also affected by yl−1, which embeds the
information from the layers below l.
</p>
<p>Intuitively, the AGT networks are encouraged
</p>
<p>to consider new words/phrases from the input text
</p>
<p>at higher layers. Consider the fact that the s+0 at
the bottom layer of the AGT only deploys a lin-
</p>
<p>ear transformation of the bag-of-words features.
</p>
<p>If no new words are used at higher layers of
</p>
<p>the networks, it will be challenge for the AGT
</p>
<p>to sufficiently explore different combinations of
</p>
<p>word sets of the given text, which may be im-
</p>
<p>portant for building an accurate classifier. In con-
</p>
<p>trast, through tailoring its attention for new words
</p>
<p>at different layers, the AGT enables the words
</p>
<p>selected by a layer to be effectively combined
</p>
<p>with words/phrases selected by its previous lay-
</p>
<p>ers to benefit the accuracy of the classification task
</p>
<p>(more discussions are presented in Section 3.2).
</p>
<p>3 Experimental Studies
</p>
<p>3.1 Main Results
</p>
<p>The Stanford Sentiment Treebank data contains
</p>
<p>11,855 movie reviews (Socher et al., 2013). We
</p>
<p>use the same splits for training, dev, and test
</p>
<p>data as in (Kim, 2014) to predict the fine-
</p>
<p>grained 5-class sentiment categories of the sen-
</p>
<p>tences. For comparison purposes, following (Kim,
</p>
<p>2014; Kalchbrenner et al., 2014; Lei et al., 2015),
</p>
<p>we trained the models using both phrases and
</p>
<p>sentences, but only evaluate sentences at test
</p>
<p>time. Also, we initialized all of the word em-
</p>
<p>beddings (Cherry and Guo, 2015; Chen and Guo,
</p>
<p>2015) using the 300 dimensional pre-trained vec-
</p>
<p>tors from GloVe (Pennington et al., 2014). We
</p>
<p>learned 15 layers with 200 dimensions each,
</p>
<p>which requires us to project the 300 dimensional
</p>
<p>word vectors; we implemented this using a lin-
</p>
<p>ear transformation, whose weight matrix and bias
</p>
<p>term are shared across all words, followed by
</p>
<p>a tanh activation. For optimization, we used
Adadelta (Zeiler, 2012), with learning rate of
</p>
<p>0.0005, mini-batch of 50, transform gate bias of
</p>
<p>1, and dropout (Srivastava et al., 2014) rate of
</p>
<p>0.2. All these hyperparameters were determined
</p>
<p>through experiments on the validation-set.
</p>
<p>AGT 50.5
high-order CNN 51.2
tree-LSTM 51.0
DRNN 49.8
PVEC 48.7
DCNN 48.5
DAN 48.2
CNN-MC 47.4
CNN 47.2
RNTN 45.7
NBoW 44.5
RNN 43.2
SVM 38.3
</p>
<p>Table 1: Test-set accuracies obtained; results ex-
</p>
<p>cept the AGT are drawn from (Lei et al., 2015).
</p>
<p>Figure 2: Soft attention distribution (top) and
</p>
<p>phrase length distribution (bottom) on the test set.
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
</p>
<p>attention weight
</p>
<p>layer1
</p>
<p>layer2
</p>
<p>layer3
</p>
<p>layer4
</p>
<p>layer5
</p>
<p>layer6
</p>
<p>layer7
</p>
<p>layer8
</p>
<p>layer9
</p>
<p>layer10
</p>
<p>layer11
</p>
<p>layer12
</p>
<p>layer13
</p>
<p>layer14
</p>
<p>layer15
</p>
<p>phrase length
</p>
<p>1−gram 2−gram 3−gram 4−gram
</p>
<p>layer1
</p>
<p>layer2
</p>
<p>layer3
</p>
<p>layer4
</p>
<p>layer5
</p>
<p>layer6
</p>
<p>layer7
</p>
<p>layer8
</p>
<p>layer9
</p>
<p>layer10
</p>
<p>layer11
</p>
<p>layer12
</p>
<p>layer13
</p>
<p>layer14
</p>
<p>layer15
</p>
<p>Table 1 presents the test-set accuracies obtained
</p>
<p>by different strategies. Results in Table 1 indi-
</p>
<p>cate that the AGT method achieved very competi-
</p>
<p>tive accuracy (with 50.5%), when compared to the
</p>
<p>state-of-the-art results obtained by the tree-LSTM
</p>
<p>(51.0%) (Tai et al., 2015; Zhu et al., 2015) and
</p>
<p>high-order CNN approaches (51.2%) (Lei et al.,
</p>
<p>2015).
</p>
<p>Top subfigure in Figure 2 depicts the distribu-
</p>
<p>374</p>
<p />
</div>
<div class="page"><p />
<p>Figure 3: Transform gate activities of the test-set
</p>
<p>(top) and the first sentence in Figure 4 (bottom).
</p>
<p>0.0 0.2 0.4 0.6 0.8
</p>
<p>0
2
</p>
<p>4
6
</p>
<p>8
10 layer1
</p>
<p>layer2
</p>
<p>layer3
</p>
<p>layer4
</p>
<p>layer5
</p>
<p>layer6
</p>
<p>layer7
</p>
<p>layer8
</p>
<p>layer9
</p>
<p>layer10
</p>
<p>layer11
</p>
<p>layer12
</p>
<p>layer13
</p>
<p>layer14
</p>
<p>layer15
</p>
<p>0.0 0.2 0.4 0.6
</p>
<p>0
2
</p>
<p>4
6
</p>
<p>8
10 layer1
</p>
<p>layer2
</p>
<p>layer3
</p>
<p>layer4
</p>
<p>layer5
</p>
<p>layer6
</p>
<p>layer7
</p>
<p>layer8
</p>
<p>layer9
</p>
<p>layer10
</p>
<p>layer11
</p>
<p>layer12
</p>
<p>layer13
</p>
<p>layer14
</p>
<p>layer15
</p>
<p>tions of the attention weights created by differ-
</p>
<p>ent layers on all test data, where the attention
</p>
<p>weights of all words in a sentence, i.e., dl,n in
Equation 4, are normalized to the range between
</p>
<p>0 and 1 within the sentence. The figure indicates
</p>
<p>that AGT generated very spiky attention distribu-
</p>
<p>tion. That is, most of the attention weights are ei-
</p>
<p>ther 1 or 0. Based on these narrow, peaked bell
</p>
<p>curves formed by the normal distributions for the
</p>
<p>attention weights of 1 and 0, we here consider a
</p>
<p>word has been selected by the networks if its at-
</p>
<p>tention weight is larger than 0.95, i.e., receiving
</p>
<p>more than 95% of the full attention, and a phrase
</p>
<p>has been composed and selected if a set of consec-
</p>
<p>utive words all have been selected.
</p>
<p>In the bottom subfigure of Figure 2 we present
</p>
<p>the distribution of the phrase lengths on the test
</p>
<p>set. This figure indicates that the middle layers of
</p>
<p>the networks e.g., 8th and 9th, have longer phrases
</p>
<p>(green and blue curves) than others, while the lay-
</p>
<p>ers at the two ends contain shorter phrases (red and
</p>
<p>pink curves).
</p>
<p>In Figure 3, we also presented the transform
</p>
<p>gate activities on all test sentences (top) and that
</p>
<p>of the first example sentence in Figure 4 (bottom).
</p>
<p>These curves suggest that the transform gates at
</p>
<p>the middle layers (green and blue curves) tended
</p>
<p>to be close to zero, indicating the pass-through of
</p>
<p>lower layers’ representations. On the contrary, the
</p>
<p>gates at the two ends (red and pink curves) tended
</p>
<p>to be away from zero with large tails, implying
</p>
<p>the retrieval of new knowledge from the input text.
</p>
<p>These are consistent with the results below.
</p>
<p>Figure 4 presents three sentences with various
</p>
<p>lengths from the test set, with the attention weights
</p>
<p>numbered and then highlighted in heat map. Fig-
</p>
<p>ure 4 suggests that the lower layers of the networks
</p>
<p>selected individual words, while the higher layers
</p>
<p>aimed at phrases. For example, the first and sec-
</p>
<p>ond layers seem to select individual words carry-
</p>
<p>ing strong sentiment (e.g., predictable, bad, never
</p>
<p>and delicate), and conjunction and negation words
</p>
<p>(e.g., though and not). Also, meaningful phrases
</p>
<p>were composed and selected by later layers, such
</p>
<p>as not so much, not only... but also, bad taste, bad
</p>
<p>luck, emotional development, and big screen. In
</p>
<p>addition, in the middle layer, i.e., the 8th layer,
</p>
<p>the whole sentences were composed by filtering
</p>
<p>out uninformative words, resulting in concise ver-
</p>
<p>sions, as follows (selected words and phrases are
</p>
<p>highlighted in color blocks).
</p>
<p>1) though plot predictable movie never
</p>
<p>feels formulaic attention nuances emo-
</p>
<p>tional development delicate characters
</p>
<p>2) bad company leaves bad taste not
</p>
<p>only bad luck but also staleness script
</p>
<p>3) not so much movie picture big screen
</p>
<p>Interestingly, if relaxing the word selection cri-
</p>
<p>teria, e.g., including words receiving more than the
</p>
<p>median, rather than 95%, of the full attention, the
</p>
<p>sentences recruited more conjunction and modifi-
</p>
<p>cation words, e.g., because, for, a, its and on, thus
</p>
<p>becoming more readable and fluent:
</p>
<p>1) though plot is predictable movie never
</p>
<p>feels formulaic because attention is on
</p>
<p>nuances emotional development delicate
</p>
<p>characters
</p>
<p>2) bad company leaves a bad taste not
</p>
<p>only because its bad luck timing but also
</p>
<p>staleness its script
</p>
<p>3) not so much a movie a picture book
</p>
<p>for big screen
</p>
<p>Now, consider the AGT’s compositional behavior
</p>
<p>for a specific sentence, e.g., the last sentence in
</p>
<p>Figure 4. The first layer solely selected the word
</p>
<p>not (with attention weight of 1 and all other words
</p>
<p>with weights close to 0), but the 2nd to 4th lay-
ers gradually pulled out new words book, screen
</p>
<p>and movie from the given text. Incrementally,
</p>
<p>the 5th and 6th layers further selected words to
form phrases not so much, picture book, and big
</p>
<p>screen. Finally, the 7th and 8th layers added some
</p>
<p>375</p>
<p />
</div>
<div class="page"><p />
<p>Figure 4: Three sentences from the test set and their attention received from the 15 layers (L1 to L15).
though the plot is predicta, the movie never feels formula, becaus the attentionis on the nuance of the emotionadevelopmof the delicatecharacte
</p>
<p>L1 0.84 0.00 0.06 0.00 1.00 0.00 0.00 0.00 0.97 0.29 0.93 0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.63 0.00 0.00 0.21 0.00 0.00 0.00 1.00 0.01
</p>
<p>L2 0.97 0.00 0.33 0.01 1.00 0.00 0.00 0.00 1.00 0.78 0.99 0.00 0.00 0.00 0.38 0.01 0.00 0.00 0.90 0.00 0.00 0.79 0.02 0.00 0.00 1.00 0.03
</p>
<p>L3 1.00 0.00 0.79 0.03 1.00 0.01 0.00 0.15 1.00 0.97 1.00 0.01 0.05 0.00 0.86 0.03 0.01 0.00 0.99 0.00 0.00 0.98 0.15 0.00 0.00 1.00 0.16
</p>
<p>L4 1.00 0.01 0.98 0.29 1.00 0.00 0.01 0.98 1.00 1.00 1.00 0.00 0.27 0.01 0.99 0.29 0.08 0.01 1.00 0.02 0.01 1.00 0.53 0.02 0.01 1.00 0.74
</p>
<p>L5 1.00 0.03 0.99 0.49 1.00 0.01 0.03 0.99 1.00 1.00 1.00 0.01 0.44 0.03 0.99 0.49 0.16 0.03 1.00 0.04 0.03 1.00 0.70 0.04 0.03 1.00 0.84
</p>
<p>L6 1.00 0.06 0.99 0.71 1.00 0.02 0.06 1.00 1.00 1.00 1.00 0.02 0.66 0.06 1.00 0.71 0.32 0.06 1.00 0.07 0.06 1.00 0.84 0.07 0.06 1.00 0.92
</p>
<p>L7 1.00 0.08 1.00 0.83 1.00 0.03 0.08 1.00 1.00 1.00 1.00 0.03 0.76 0.08 1.00 0.83 0.44 0.08 1.00 0.10 0.08 1.00 0.90 0.10 0.08 1.00 0.95
</p>
<p>L8 1.00 0.10 1.00 0.94 1.00 0.04 0.10 1.00 1.00 1.00 1.00 0.04 0.85 0.10 1.00 0.94 0.60 0.10 1.00 0.12 0.10 1.00 0.95 0.12 0.10 1.00 0.98
</p>
<p>L9 1.00 0.05 1.00 0.94 1.00 0.02 0.05 1.00 1.00 1.00 1.00 0.02 0.79 0.05 1.00 0.94 0.49 0.05 1.00 0.07 0.05 1.00 0.92 0.07 0.05 1.00 0.98
</p>
<p>L10 1.00 0.00 0.99 0.81 1.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.44 0.00 1.00 0.81 0.15 0.00 1.00 0.01 0.00 1.00 0.75 0.01 0.00 1.00 0.93
</p>
<p>L11 1.00 0.00 0.99 0.68 1.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.24 0.00 1.00 0.68 0.07 0.00 1.00 0.01 0.00 1.00 0.59 0.01 0.00 1.00 0.88
</p>
<p>L12 0.99 0.00 0.98 0.51 1.00 0.01 0.00 0.99 1.00 1.00 1.00 0.01 0.08 0.00 0.99 0.51 0.03 0.00 0.99 0.00 0.00 1.00 0.42 0.00 0.00 1.00 0.81
</p>
<p>L13 0.99 0.00 0.90 0.17 1.00 0.01 0.00 0.62 1.00 0.98 1.00 0.01 0.02 0.00 0.98 0.17 0.02 0.00 0.99 0.00 0.00 1.00 0.18 0.00 0.00 1.00 0.43
</p>
<p>L14 0.92 0.00 0.65 0.04 1.00 0.00 0.00 0.04 1.00 0.93 0.99 0.00 0.00 0.00 0.88 0.04 0.00 0.00 0.96 0.00 0.00 0.99 0.05 0.00 0.00 1.00 0.14
</p>
<p>L15 0.80 0.00 0.60 0.03 1.00 0.00 0.00 0.02 0.99 0.91 0.99 0.00 0.00 0.00 0.84 0.03 0.00 0.00 0.93 0.00 0.00 0.98 0.04 0.00 0.00 1.00 0.12
</p>
<p>bad companleaves a bad taste , not only becaus of its bad luck timing , but also the staleneof its script
</p>
<p>L1 0.95 0.02 0.25 0.00 0.95 0.05 0.00 0.92 0.05 0.00 0.00 0.00 0.95 0.90 0.05 0.00 0.60 0.00 0.00 1.00 0.00 0.00 0.01
</p>
<p>L2 1.00 0.16 0.85 0.00 1.00 0.46 0.01 1.00 0.69 0.01 0.01 0.00 1.00 0.99 0.18 0.01 0.93 0.02 0.01 1.00 0.01 0.00 0.05
</p>
<p>L3 1.00 0.70 0.99 0.00 1.00 0.93 0.02 1.00 0.98 0.07 0.02 0.01 1.00 1.00 0.47 0.02 1.00 0.19 0.02 1.00 0.02 0.01 0.49
</p>
<p>L4 1.00 0.75 1.00 0.00 1.00 0.99 0.00 1.00 1.00 0.27 0.02 0.02 1.00 1.00 0.75 0.00 1.00 0.65 0.01 1.00 0.02 0.02 0.98
</p>
<p>L5 1.00 0.84 1.00 0.03 1.00 1.00 0.01 1.00 1.00 0.44 0.04 0.07 1.00 1.00 0.81 0.01 1.00 0.79 0.03 1.00 0.04 0.07 0.99
</p>
<p>L6 1.00 0.91 1.00 0.10 1.00 1.00 0.02 1.00 1.00 0.66 0.07 0.17 1.00 1.00 0.88 0.02 1.00 0.90 0.06 1.00 0.07 0.17 1.00
</p>
<p>L7 1.00 0.93 1.00 0.21 1.00 1.00 0.03 1.00 1.00 0.76 0.10 0.29 1.00 1.00 0.91 0.03 1.00 0.94 0.08 1.00 0.10 0.29 1.00
</p>
<p>L8 1.00 0.95 1.00 0.59 1.00 1.00 0.04 1.00 1.00 0.85 0.12 0.50 1.00 1.00 0.94 0.04 1.00 0.98 0.10 1.00 0.12 0.50 1.00
</p>
<p>L9 1.00 0.89 1.00 0.64 1.00 1.00 0.02 1.00 1.00 0.79 0.07 0.42 1.00 1.00 0.92 0.02 1.00 0.98 0.05 1.00 0.07 0.42 1.00
</p>
<p>L10 1.00 0.57 1.00 0.19 1.00 1.00 0.00 1.00 1.00 0.44 0.01 0.10 1.00 1.00 0.80 0.00 1.00 0.92 0.00 1.00 0.01 0.10 1.00
</p>
<p>L11 1.00 0.34 1.00 0.09 1.00 0.99 0.00 1.00 0.99 0.24 0.01 0.04 1.00 1.00 0.73 0.00 1.00 0.85 0.00 1.00 0.01 0.04 0.99
</p>
<p>L12 1.00 0.13 1.00 0.06 1.00 0.98 0.01 1.00 0.92 0.08 0.00 0.02 1.00 1.00 0.67 0.01 0.97 0.70 0.00 1.00 0.00 0.02 0.99
</p>
<p>L13 1.00 0.08 1.00 0.02 1.00 0.90 0.01 1.00 0.53 0.02 0.00 0.01 1.00 1.00 0.46 0.01 0.94 0.37 0.00 1.00 0.00 0.01 0.86
</p>
<p>L14 1.00 0.01 0.98 0.01 1.00 0.45 0.00 0.96 0.11 0.00 0.00 0.00 1.00 1.00 0.21 0.00 0.70 0.07 0.00 1.00 0.00 0.00 0.31
</p>
<p>L15 1.00 0.01 0.96 0.00 1.00 0.31 0.00 0.92 0.06 0.00 0.00 0.00 1.00 1.00 0.18 0.00 0.48 0.04 0.00 1.00 0.00 0.00 0.27
</p>
<p>not so much a movie as a picturebook for the big screen
L1 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00
L2 1.00 0.01 0.01 0.00 0.01 0.01 0.00 0.01 0.03 0.02 0.01 0.05 0.02
L3 1.00 0.06 0.07 0.00 0.16 0.02 0.00 0.07 0.18 0.09 0.02 0.19 0.20
L4 1.00 0.59 0.41 0.00 0.98 0.06 0.00 0.59 0.54 0.29 0.01 0.59 0.76
L5 1.00 0.80 0.60 0.03 0.99 0.12 0.03 0.74 0.70 0.50 0.03 0.72 0.87
L6 1.00 0.92 0.78 0.10 1.00 0.24 0.10 0.87 0.84 0.71 0.06 0.83 0.94
L7 1.00 0.96 0.87 0.21 1.00 0.33 0.21 0.93 0.88 0.81 0.08 0.89 0.97
L8 1.00 0.99 0.95 0.59 1.00 0.41 0.59 0.98 0.91 0.89 0.10 0.96 0.99
L9 1.00 0.99 0.95 0.64 1.00 0.26 0.64 0.98 0.85 0.85 0.05 0.96 0.99
L10 1.00 0.93 0.82 0.19 1.00 0.05 0.19 0.91 0.56 0.52 0.00 0.90 0.95
L11 1.00 0.85 0.71 0.09 1.00 0.02 0.09 0.85 0.34 0.29 0.00 0.85 0.91
L12 1.00 0.68 0.55 0.06 0.99 0.01 0.06 0.78 0.15 0.13 0.00 0.77 0.84
L13 1.00 0.19 0.23 0.02 0.63 0.00 0.02 0.27 0.04 0.05 0.00 0.53 0.48
L14 1.00 0.03 0.06 0.01 0.05 0.00 0.01 0.04 0.01 0.01 0.00 0.21 0.07
L15 1.00 0.02 0.05 0.00 0.02 0.00 0.00 0.02 0.01 0.01 0.00 0.15 0.03
</p>
<p>conjunction and quantification words a and for to
</p>
<p>make the sentence more fluent. This recursive
</p>
<p>composing process resulted in the sentence “not
</p>
<p>so much a movie a picture book for big screen”.
</p>
<p>Interestingly, Figures 4 and 2 also imply that,
</p>
<p>after composing the sentences by the middle layer,
</p>
<p>the AGT networks shifted to re-focus on shorter
</p>
<p>phrases and informative words. Our analysis on
</p>
<p>the transform gate activities suggests that, dur-
</p>
<p>ing this re-focusing stage the compositions of sen-
</p>
<p>tence phrases competed to each others, as well as
</p>
<p>to the whole sentence composition, for the domi-
</p>
<p>nating task-specific features to represent the text.
</p>
<p>3.2 Further Observations
</p>
<p>As discussed at the end of Section 2.2, intuitively,
</p>
<p>including new words at different layers allows
</p>
<p>the networks to more effectively explore different
</p>
<p>combinations of word sets of the given text than
</p>
<p>that of using all words only at the bottom layer
</p>
<p>of the networks. Empirically, we observed that, if
</p>
<p>with only s+0 in the AGT network, namely remov-
ing s+i for i &gt; 0, the test-set accuracy dropped
from 50.5% to 48.5%. In other words, transform-
</p>
<p>ing a linear combination of the bag-of-words fea-
</p>
<p>tures was insufficient for obtaining sufficient ac-
</p>
<p>curacy for the classification task. For instance, if
</p>
<p>being augmented with two more selection vectors
</p>
<p>s+i , namely removing s
+
i for i &gt; 2, the AGT was
</p>
<p>able to improve its accuracy to 49.0%.
</p>
<p>Also, we observed that the AGT networks
</p>
<p>tended to select informative words at the lower
</p>
<p>layers. This may be caused by the recursive form
</p>
<p>of Equation 8, which suggests that the words re-
</p>
<p>trieved by s+0 have more chance to combine with
and influence the selection of other feature words.
</p>
<p>In our study, we found that, for example, the top
</p>
<p>3 most frequent words selected by the first layer
</p>
<p>of the AGT networks were all negation words: n’t,
</p>
<p>never, and not. These are important words for sen-
</p>
<p>timent classification (Zhu et al., 2014).
</p>
<p>In addition, like the transform gate in the High-
</p>
<p>way networks (Srivastava et al., 2015a) and the
</p>
<p>forget gate in the LSTM (Gers et al., 2000), the
</p>
<p>attention-based transform gate in the AGT net-
</p>
<p>works is sensitive to its bias initialization. We
</p>
<p>found that initializing the bias to one encouraged
</p>
<p>the compositional behavior of the AGT networks.
</p>
<p>4 Conclusion and Future Work
</p>
<p>We have presented a novel deep network. It
</p>
<p>not only achieves very competitive accuracy for
</p>
<p>text classification, but also exhibits interesting
</p>
<p>text compositional behavior, which may shed light
</p>
<p>on understanding how neural models work in
</p>
<p>NLP tasks. In the future, we aim to apply the
</p>
<p>AGT networks to incrementally generating natu-
</p>
<p>ral text (Guo, 2015; Hu et al., 2017).
</p>
<p>376</p>
<p />
</div>
<div class="page"><p />
<p>References
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In ICLR 2015.
</p>
<p>Boxing Chen and Hongyu Guo. 2015. Representation
based translation evaluation metrics. In ACL (2).
pages 150–155.
</p>
<p>Colin Cherry and Hongyu Guo. 2015. The unreason-
able effectiveness of word representations for twit-
ter named entity recognition. In HLT-NAACL. pages
735–745.
</p>
<p>Alexis Conneau, Holger Schwenk, Loı̈c Barrault, and
Yann LeCun. 2016. Very deep convolutional net-
works for natural language processing. CoRR
abs/1606.01781.
</p>
<p>Clément Farabet, Camille Couprie, Laurent Najman,
and Yann LeCun. 2013. Learning hierarchical fea-
tures for scene labeling. IEEE Trans. Pattern Anal.
Mach. Intell. 35(8):1915–1929.
</p>
<p>Felix A. Gers, Jürgen A. Schmidhuber, and Fred A.
Cummins. 2000. Learning to forget: Continual pre-
diction with lstm. Neural Comput. 12(10):2451–
2471.
</p>
<p>Hongyu Guo. 2015. Generating text with deep rein-
forcement learning. In NIPS2015 Deep Reinforce-
ment Learning Workshop.
</p>
<p>Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Controllable
text generation. arXiv preprint arXiv:1703.00955 .
</p>
<p>Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. CoRR abs/1404.2188.
</p>
<p>Yoon Kim. 2014. Convolutional neural networks for
sentence classification. CoRR abs/1408.5882.
</p>
<p>Tao Lei, Regina Barzilay, and Tommi S. Jaakkola.
2015. Molding cnns for text: non-linear, non-
consecutive convolutions. CoRR abs/1508.04112.
</p>
<p>Jiwei Li, Xinlei Chen, Eduard H. Hovy, and Dan Ju-
rafsky. 2015. Visualizing and understanding neural
models in NLP. CoRR abs/1506.01066.
</p>
<p>Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-
derstanding neural networks through representation
erasure. arXiv preprint arXiv:1612.08220 .
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.
</p>
<p>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In EMNLP ’13.
</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. J. Mach. Learn. Res. 15(1).
</p>
<p>Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015a. Highway networks. CoRR
abs/1505.00387.
</p>
<p>Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015b. Training very deep networks.
In Proceedings of the 28th International Confer-
ence on Neural Information Processing Systems.
NIPS’15, pages 2377–2385.
</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. CoRR abs/1503.00075.
</p>
<p>Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR abs/1212.5701.
</p>
<p>Matthew D. Zeiler and Rob Fergus. 2014. Visualizing
and understanding convolutional networks. In Com-
puter Vision - ECCV 2014 - 13th European Con-
ference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part I. pages 818–833.
</p>
<p>Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In ACL
(1). pages 304–313.
</p>
<p>Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In ICML. pages 1604–1612.
</p>
<p>377</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 378–384
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2060
</p>
<p>Neural System Combination for Machine Translation
</p>
<p>Long Zhou†, Wenpeng Hu†, Jiajun Zhang†∗, Chengqing Zong†‡
†University of Chinese Academy of Sciences, Beijing, China
</p>
<p>National Laboratory of Pattern Recognition, CASIA, Beijing, China
‡CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China
</p>
<p>{long.zhou,wenpeng.hu,jjzhang,cqzong}@nlpr.ia.ac.cn
</p>
<p>Abstract
</p>
<p>Neural machine translation (NMT) be-
comes a new approach to machine trans-
lation and generates much more fluen-
t results compared to statistical machine
translation (SMT). However, SMT is usu-
ally better than NMT in translation ade-
quacy. It is therefore a promising direction
to combine the advantages of both NMT
and SMT. In this paper, we propose a neu-
ral system combination framework lever-
aging multi-source NMT, which takes as
input the outputs of NMT and SMT sys-
tems and produces the final translation.
Extensive experiments on the Chinese-
to-English translation task show that our
model archives significant improvemen-
t by 5.3 BLEU points over the best single
system output and 3.4 BLEU points over
the state-of-the-art traditional system com-
bination methods.
</p>
<p>1 Introduction
</p>
<p>Neural machine translation has significantly im-
proved the quality of machine translation in re-
cent several years (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2015; Junczys-Dowmunt et al., 2016a). Although
most sentences are more fluent than translations
by statistical machine translation (SMT) (Koehn
et al., 2003; Chiang, 2005), NMT has a problem
to address translation adequacy especially for the
rare and unknown words. Additionally, it suf-
fers from over-translation and under-translation to
some extent (Tu et al., 2016). Compared to N-
MT, SMT, such as phrase-based machine trans-
lation (PBMT, (Koehn et al., 2003)) and hierar-
chical phrase-based machine translation (HPMT,
</p>
<p>∗Corresponding author.
</p>
<p>(Chiang, 2005)), does not need to limit the vo-
cabulary and can guarantee translation coverage
of source sentences. It is obvious that NMT and
SMT have different strength and weakness. In or-
der to take full advantages of both NMT and SMT,
system combination can be a good choice.
</p>
<p>Traditionally, system combination has been ex-
plored respectively in sentence-level, phrase-level,
and word-level (Kumar and Byrne, 2004; Feng
et al., 2009; Chen et al., 2009). Among them,
word-level combination approaches that adop-
t confusion network for decoding have been quite
successful (Rosti et al., 2007; Ayan et al., 2008;
Freitag et al., 2014). However, these approaches
are mainly designed for SMT without considering
the features of NMT results. NMT opts to produce
diverse words and free word order, which are quite
different from SMT. And this will make it hard to
construct a consistent confusion network. Further-
more, traditional system combination approaches
cannot guarantee the fluency of the final transla-
tion results.
</p>
<p>In this paper, we propose a neural system
combination framework, which is adapted from
the multi-source NMT model (Zoph and Knight,
2016). Different encoders are employed to mod-
el the semantics of the source language input and
each best translation produced by different NMT
and SMT systems. The encoders produce multiple
context vector representations, from which the de-
coder generates the final output word by word. S-
ince the same training data is used for NMT, SMT
and neural system combination, we further design
a smart strategy to simulate the real training data
for neural system combination.
</p>
<p>Specifically, we make the following contribu-
tions in this paper:
</p>
<p>• We propose a neural system combination
method, which is adapted from multi-source
</p>
<p>378</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2060">https://doi.org/10.18653/v1/P17-2060</a></div>
</div>
<div class="page"><p />
<p> !
</p>
<p> 
</p>
<p>"!
</p>
<p> 
</p>
<p>!
nm
</p>
<p> 
</p>
<p>   
</p>
<p>!"
</p>
<p>!
</p>
<p>"
nm
</p>
<p> 
</p>
<p>   
!"
</p>
<p> 
</p>
<p>!
</p>
<p>nz
 
</p>
<p>nz
n
</p>
<p>n
</p>
<p>mz
   
</p>
<p>!"
</p>
<p> 
</p>
<p>#"
</p>
<p> 
</p>
<p>"
pm
</p>
<p> 
</p>
<p>   
</p>
<p>!"
</p>
<p>!
</p>
<p>"
pm
</p>
<p> 
</p>
<p>   
!"
</p>
<p> 
</p>
<p>!
</p>
<p>pz
 
</p>
<p>pz p
p
</p>
<p>mz
   
</p>
<p>!"
</p>
<p> 
</p>
<p>#"
</p>
<p> 
</p>
<p>"
hm
</p>
<p> 
</p>
<p>   
</p>
<p>!"
</p>
<p>!
</p>
<p>"
hm
</p>
<p> 
</p>
<p>   
!"
</p>
<p> 
</p>
<p>!
</p>
<p>hz
 
</p>
<p>hz h
h
</p>
<p>mz
   
</p>
<p>!"jS jS  jS  
 
</p>
<p> !jS
</p>
<p> jy  jy
</p>
<p> 
jS
</p>
<p> jy  
</p>
<p> 
</p>
<p>   
</p>
<p> 
</p>
<p>      
!
</p>
<p>n
</p>
<p>j  
p
</p>
<p>j  
p
</p>
<p>j 
p
</p>
<p>jm 
</p>
<p> j 
 j  j 
</p>
<p>!
</p>
<p> 
</p>
<p>n
</p>
<p>j 
n
</p>
<p>jm  
h
</p>
<p>j  
h
</p>
<p>j 
h
</p>
<p>jm 
</p>
<p>jnc
</p>
<p>jpc jhc
</p>
<p> 
</p>
<p> !"#$!%
</p>
<p>&amp;''!(')#(
</p>
<p>      
</p>
<p>*("#$!%+,-. *("#$!%+/0-. *("#$!%+1/-.
</p>
<p>jc
</p>
<p> 
nZ  
</p>
<p>pZ  hZ
</p>
<p>Figure 1: The architecture of Neural System Combination Model.
</p>
<p>NMT model and can accommodate both
source inputs and different system transla-
tions. It combines the fluency of NMT and
adequacy (especially the ability to address
rare words) of SMT.
</p>
<p>• We design a good strategy to construct appro-
priate training data for neural system combi-
nation.
</p>
<p>• The extensive experiments on Chinese-
English translation show that our mod-
el archives significant improvement by 3.4
BLEU points over the state-of-the-art system
combination methods and 5.3 BLEU points
over the best individual system output.
</p>
<p>2 Neural Machine Translation
</p>
<p>The encoder-decoder NMT with an attention
mechanism (Bahdanau et al., 2015) has been pro-
posed to softly align each decoder state with the
encoder states, and computes the conditional prob-
ability of the translation given the source sentence.
</p>
<p>The encoder is a bidirectional neural network
with gated recurrent units (GRU) (Cho et al.,
2014) which reads an input sequence X =
(x1, x2, ..., xm) and encodes it into a sequence of
hidden states H = h1, h2, ..., hm.
</p>
<p>The decoder is a recurrent neural network that
predicts a target sequence Y = (y1, y2, ..., yn).
Each word yj is predicted based on a recurrent hid-
den state sj , the previously predicted word yj−1,
and a context vector cj . cj is obtained from the
</p>
<p>weighted sum of the annotations hi. We use the
latest implementation of attention-based NMT1.
</p>
<p>3 Neural System Combination for
Machine Translation
</p>
<p>Macherey and Och (2007) gave empirical evi-
dence that these systems to be combined need to
be almost uncorrelated in order to be beneficial for
system combination. Since NMT and SMT are t-
wo kinds of translation models with large differ-
ences, we attempt to build a neural system com-
bination model, which can take advantage of the
different systems.
</p>
<p>Model: Figure 1 illustrates the neural system
combination framework, which can take as input
the source sentence and the results of MT system-
s. Here, we use MT results as inputs to detail the
model.
</p>
<p>Formally, given the result sequences Z(Zn, Zp,
and Zh) of three MT systems for the same source
sentence and previously generated target sequence
Y&lt;j = (y1, y2, ..., yj−1), the probability of the
next target word yj is
</p>
<p>p(yj |Y&lt;j , Z) = softmax(f(cj , yj−1, sj)) (1)
</p>
<p>Here f(.) is a non-linear function, yj−1 represents
the word embedding of the previous prediction
word, and sj is the state of decoder at time step
j, calculated by
</p>
<p>sj = GRU(s̃j−1, cj) (2)
</p>
<p>1https://github.com/nyu-dl/dl4mt-tutorial
</p>
<p>379</p>
<p />
</div>
<div class="page"><p />
<p>System MT03 MT04 MT05 MT06 Ave
PBMT 37.47 41.20 36.41 36.03 37.78
HPMT 38.05 41.47 36.86 36.04 38.10
NMT 37.91 38.95 36.02 36.65 37.38
Jane (Freitag et al., 2014) 39.83 42.75 38.63 39.10 40.08
Multi 40.64 44.81 38.80 38.26 40.63
Multi+Source 42.16 45.51 40.28 39.03 41.75
Multi+Ensemble 41.67 45.95 40.37 39.02 41.75
Multi+Source+Ensemble 43.55 47.09 42.02 41.10 43.44
</p>
<p>Table 1: Translation results (BLEU score) for different machine translation and system combination
methods. Jane is a open source machine translation system combination toolkit that uses confusion
network decoding. Best and important results per category are highlighted.
</p>
<p>s̃j−1 = GRU(sj−1, yj−1) (3)
</p>
<p>where sj−1 is previous hidden state, s̃j−1 is an in-
termediate state. And cj is the context vector of
system combination obtained by attention mecha-
nism, which is computed as weighted sum of the
context vectors of three MT systems, just as illus-
trated in the middle part of Figure 1.
</p>
<p>cj =
K∑
</p>
<p>k=1
</p>
<p>βjkcjk (4)
</p>
<p>where K is the number of MT systems, and βjk is
a normalized item calculated as follows:
</p>
<p>βjk =
exp(s̃j−1 · cjk)∑
k′ exp(s̃j−1 · cjk′ )
</p>
<p>(5)
</p>
<p>Here, we calculate kth MT system context cjk as a
weighted sum of the source annotations:
</p>
<p>cjk =
m∑
</p>
<p>i=1
</p>
<p>αkjihi (6)
</p>
<p>where hi = [
−→
h i;
←−
h i] is the annotation of zi from
</p>
<p>a bi-directional GRU, and its weight αkji is com-
puted by
</p>
<p>αkji =
exp(eji)∑m
l=1 exp(ejl)
</p>
<p>(7)
</p>
<p>where eji = vTa tanh(Was̃j−1+Uahi) scores how
well s̃j−1 and hi match.
</p>
<p>Training Data Simulation: The neural sys-
tem combination framework should be trained on
the outputs of multiple translation systems and the
gold target translations. In order to keep consis-
tency in training and testing, we design a strategy
</p>
<p>to simulate the real scenario. We randomly divide
the training corpus into two parts, then reciprocal-
ly train the MT system on one half and translate
the source sentences of the other half into target
translations. The MT translations and the gold tar-
get reference can be available.
</p>
<p>4 Experiments
</p>
<p>We perform our experiments on the Chinese-
English translation task. The MT systems par-
ticipating in system combination are PBMT, H-
PMT and NMT. The evaluation metric is case-
insensitive BLEU (Papineni et al., 2002).
</p>
<p>4.1 Data preparation
</p>
<p>Our training data consists of 2.08M sentence
pairs extracted from LDC corpus. We use NIST
2003 Chinese-English dataset as the validation set,
NIST 2004-2006 datasets as test sets. We list all
the translation methods as follows:
</p>
<p>• PBMT: It is the start-of-the-art phrase-based
SMT system. We use its default setting and
train a 4-gram language model on the target
portion of the bilingual training data.
</p>
<p>• HPMT: It is a hierarchical phrase-based
SMT system, which uses its default config-
uration as PBMT in Moses.
</p>
<p>• NMT: It is an attention-based NMT system,
with the same setting given in section 2.
</p>
<p>4.2 Training Details
</p>
<p>The hyper-parameters used in our neural combi-
nation system are described as follows. We limit
both Chinese and English vocabulary to 30k in our
experiments. The number of hidden units is 1000
</p>
<p>380</p>
<p />
</div>
<div class="page"><p />
<p> !
</p>
<p> "
</p>
<p>!#
</p>
<p>!$
</p>
<p>!%
</p>
<p>!&amp;
</p>
<p>!'
</p>
<p>()*+ ,(*+ -*+ ./01 *2345 67829:1 6;0&lt;1=&gt;31
</p>
<p> 
!"
#
$
</p>
<p>Figure 2: Translation results (RIBES score) for d-
ifferent machine translation and system combina-
tion methods.
</p>
<p>and the word embedding dimension is 500 for all
source and target word. The network parameter-
s are updated with Adadelta algorithm. We adopt
beam search with beam size b=10 at test time.
</p>
<p>As to confusion-network-based system Jane, we
use its default configuration and train a 4-gram
language model on target data and 10M Xinhua
portion of Gigaword corpus.
</p>
<p>4.3 Main Results
We compare our neural combination system with
the best individual engines, and the state-of-the-art
traditional combination system Jane (Freitag et al.,
2014). Table 1 shows the BLEU of different mod-
els on development data and test data. The BLEU
score of the multi-source neural combination mod-
el is 2.53 higher than the best single model H-
PMT. The source language input gives a further
improvement of +1.12 BLEU points.
</p>
<p>As shown in Table 1, Jane outperforms the best
single MT system by 1.92 BLEU points. Howev-
er, our neural combination system with source lan-
guage gets an improvement of 1.67 BLEU points
over Jane. Furthermore, when augmenting our
neural combination system with ensemble decod-
ing 2, it leads to another significant boost of +1.69
BLEU points.
</p>
<p>4.4 Word Order of Translation
We evaluate word order by the automatic evalua-
tion metrics RIBES (Isozaki et al., 2010), whose
score is a metric based on rank correlation coef-
ficients with word precision. RIBES is known to
have stronger correlation with human judgements
than BLEU for English as discussed in Isozaki et
al. (2010).
</p>
<p>2We use four neural combination models in ensemble
model.
</p>
<p>System MT03 MT04 MT05 MT06 Ave
NMT 1086 1145 1020 708 989.8
Ours 869 1023 909 609 852.5
</p>
<p>Table 2: The number of unknown words in the re-
sults of NMT and our model.
</p>
<p>System MT03 MT04 MT05 MT06 Ave
E-NMT 39.14 40.78 37.31 37.89 38.78
Jane 40.61 43.28 39.05 39.18 40.53
Ours 43.61 47.65 42.02 41.17 43.61
</p>
<p>Table 3: Translation results (BLEU score) when
we replace original NMT with strong E-NMT,
which uses ensemble strategy with four NMT
models. All results of system combination are
based on strong outputs of E-NMT.
</p>
<p>Figure 2 illustrates experimental results of
RIBES scores, which demonstrates that our neu-
ral combination model outperforms the best result
of single MT system and Jane. Additionally, al-
though BLEU point of Jane is higher than single
NMT system, the word order of Jane is worse in
terms of RIBES.
</p>
<p>4.5 Rare and Unknown Words Translation
</p>
<p>It is difficult for NMT systems to handle rare
words, because low-frequency words in training
data cannot capture latent translation mappings in
neural network model. However, we do not need
to limit the vocabulary in SMT, which are often
able to translate rare words in training data. As
shown in Table 2, the number of unknown words
of our proposed model is 137 fewer than original
NMT model.
</p>
<p>Table 4 shows an example of system combina-
tion. The Chinese word zǔzhı̄wǎng is an out-of-
vocabulary(OOV) for NMT and the baseline N-
MT cannot correctly translate this word. Although
PBMT and HPMT translate this word well, they
does not conform to the grammar. By combining
the merits of NMT and SMT, our model gets the
correct translation.
</p>
<p>4.6 Effect of Ensemble Decoding
</p>
<p>The performance of candidate systems is very im-
portant to the result of system combination, and
we use ensemble strategy with four NMT mod-
els to improve the performance of original NMT
system. As shown in Table 3, the E-NMT with
</p>
<p>381</p>
<p />
</div>
<div class="page"><p />
<p>Source °éÝ|� ïá
éX"
Pinyin hǎnshān yě yǔ kǒngbù zǔzhı̄wǎng jiànlı̀ le liánxı̀"
Reference hussein has also established ties with terrorist networks .
PBMT hussein also has established relations and terrorist group .
HPMT hussein also and terrorist group established relations .
NMT hussein also established relations with UNK .
Jane hussein also has established relations with .
Multi hussein also has established relations with the terrorist group .
</p>
<p>Table 4: Translation examples of single system, Jane and our proposed model.
</p>
<p>ensemble strategy outperforms the original NMT
system by +1.40 BLEU points, and it has become
the best sytem in all MT systems, which is +0.68
BLEU points higher than HPMT.
</p>
<p>After replacing original NMT with strong E-
NMT , Jane outperforms original result by +0.45
BLEU points, and our model gets an improvemen-
t of +3.08 BLEU points over Jane. Experiments
further demonstrate that our proposed model is ef-
fective and robust for system combination.
</p>
<p>5 Related Work
</p>
<p>The recently proposed neural machine translation
has drawn more and more attention. Most of the
existing approaches and models mainly focus on
designing better attention models (Luong et al.,
2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng
et al., 2016), better strategies for handling rare and
unknown words (Luong et al., 2015b; Li et al.,
2016; Zhang and Zong, 2016a; Sennrich et al.,
2016b) , exploiting large-scale monolingual da-
ta (Cheng et al., 2016; Sennrich et al., 2016a;
Zhang and Zong, 2016b), and integrating SMT
techniques (Shen et al., 2016; Junczys-Dowmunt
et al., 2016b; He et al., 2016).
</p>
<p>Our focus in this work is aiming to take advan-
tage of NMT and SMT by system combination,
which attempts to find consensus translations a-
mong different machine translation systems. In
past several years, word-level, phrase-level and
sentence-level system combination methods were
well studied (Bangalore et al., 2001; Rosti et al.,
2008; Li and Zong, 2008; Li et al., 2009; Heafield
and Lavie, 2010; Freitag et al., 2014; Ma and Mck-
eown, 2015; Zhu et al., 2016), and reported state-
of-the-art performances in benchmarks for SMT.
Here, we propose a neural system combination
model which combines the advantages of NMT
and SMT efficiently.
</p>
<p>Recently, Niehues et al. (2016) use phrase-
</p>
<p>based SMT to pre-translate the inputs into target
translations. Then a NMT system generates the
final hypothesis using the pre-translation. More-
over, multi-source MT has been proved to be very
effective to combine multiple source languages
(Och and Ney, 2001; Zoph and Knight, 2016; Firat
et al., 2016a,b; Garmash and Monz, 2016). Unlike
previous works, we adapt multi-source NMT for
system combination and design a good strategy to
simulate the real training data for our neural sys-
tem combination.
</p>
<p>6 Conclusion and Future Work
</p>
<p>In this paper, we propose a novel neural system
combination framework for machine translation.
The central idea is to take advantage of NMT and
SMT by adapting the multi-source NMT model.
The neural system combination method cannot on-
ly address the fluency of NMT and the adequacy
of SMT, but also can accommodate the source sen-
tences as input. Furthermore, our approach can
further use ensemble decoding to boost the per-
formance compared to traditional system combi-
nation methods.
</p>
<p>Experiments on Chinese-English datasets show
that our approaches obtain significant improve-
ments over the best individual system and the
state-of-the-art traditional system combination
methods. In the future work, we plan to encode n-
best translation results to further improve the sys-
tem combination quality. Additionally, it is inter-
esting to extend this approach to other tasks like
sentence compression and text abstraction.
</p>
<p>Acknowledgments
</p>
<p>The research work has been funded by the Natu-
ral Science Foundation of China under Grant No.
61333018 and No. 61673380, and it is also sup-
ported by the Strategic Priority Research Program
of the CAS under Grant No. XDB02070007.
</p>
<p>382</p>
<p />
</div>
<div class="page"><p />
<p>References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
</p>
<p>Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of COLING 2008.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Nuural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR 2015.
</p>
<p>Srinivas Bangalore, German Bordel, and Giuseppe
Richardi. 2001. Computing consensus translation
from multiple machine translation systems. In Pro-
ceedings of IEEE ASRU.
</p>
<p>Boxing Chen, Min Zhang, Haizhou Li, and Aiti Aw.
2009. A comparative study of hypothesis alignment
and its improvement for machine translation system
combination. In Proceedings of ACL 2009.
</p>
<p>Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu,
Maosong Sun, and Yang Liu. 2016. Semi-supervised
learning for neural machine translation. In Pro-
ceedings of ACL 2016.
</p>
<p>David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL 2005.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar
Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014.
Learning phrase representations using RNN en-
coderõdecoder for statistical machine translation.
In Proceedings of EMNLP 2014.
</p>
<p>Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan
Lu. 2009. Lattice-based system combination for sta-
tistical machine translation. In Proceedings of ACL
2009.
</p>
<p>Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016a. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of NAACL-HLT 2016.
</p>
<p>Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,
Fatos T. Yarman Vural, and Kyunghyun Cho. 2016b.
Zero-resource translation with multi-lingual neural
machine translation. In Proceedings of EMNLP
2016.
</p>
<p>Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: open source machine translation sys-
tem combiantion. In Proceedings of EACL 2014.
</p>
<p>Ekaterina Garmash and Christof Monz. 2016. Ensem-
ble learning for multi-source neural machine trans-
lation. In Proceedings of COLING 2016.
</p>
<p>Wei He, Zhongjun He, Hua Wu, and Haifeng Wang.
2016. Improved neural machine translation with
SMT features. In Proceedings of AAAI 2016.
</p>
<p>Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source. The
Prague Bulletin of Machematical Linguistics.
</p>
<p>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic e-
valuation of translation quality for distant language
pairs. In Proceedings of EMNLP 2010.
</p>
<p>Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu
Hoang. 2016a. Is neural machine translation ready
for deployment? A case study on 30 translation di-
rections. In Proceedings of IWSLT 2016.
</p>
<p>Marcin Junczys-Dowmunt, Tomasz Dwojak, and Rico
Sennrich. 2016b. The AMU-UEDIN submission to
the WMT16 news translation task: attention-based
NMT models as feature functions in phrase-based
SMT . In Proceedings of WMT 2016.
</p>
<p>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
EMNLP 2013.
</p>
<p>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of ACL NAACL 2013.
</p>
<p>Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL 2004.
</p>
<p>Maoxi Li, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2009. The CASIA statistical machine trans-
lation system for IWSLT 2009. In Proceedings of
IWSLT2009.
</p>
<p>Maoxi Li and Chengqing Zong. 2008. Word reorder-
ing alignment for combination of statistical machine
translation systems. In Proceedings of the Inter-
national Symposium on Chinese Spoken Language
Processing.
</p>
<p>Xiaoqing Li, Jiajun Zhang, and Chengqing Zong. 2016.
Towards zero unknown word in neural machine
translation. In Proceedings of IJCAI 2016.
</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015a. Effective approaches to attention-
based neural machine translation. In Proceedings
of EMNLP 2015.
</p>
<p>Minh-Thang Luong, Ilya Sutskever, Quoc V Le, Oriol
Vinyals, and Wojciech Zaremba. 2015b. Addressing
the rare word problem in neural machine transla-
tion. In Proceedings of ACL 2015.
</p>
<p>Wei-Yun Ma and Kathleen Mckeown. 2015. System
combination for machine translation through para-
phrasing. In Proceedings of EMNLP 2015.
</p>
<p>Wolfgang Macherey and Franz Josef Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems. In
Proceedings of EMNLP 2007.
</p>
<p>383</p>
<p />
</div>
<div class="page"><p />
<p>Fandong Meng, Zhengdong Lu, Hang Li, and Qun Li-
u. 2016. Interactive attention for neural machine
translation. In Proceedings of COLING 2016.
</p>
<p>Haitao Mi, Baskaran Sankaran, Zhiguo Wang, Niyu
Ge, and Abe Ittycheriah. 2016a. A coverage em-
bedding model for neural machine translation. In
Proceedings of EMNLP 2016.
</p>
<p>Haitao Mi, Zhiguo Wang, Niyu Ge, and Abe Ittycheri-
ah. 2016b. Supervised attentions for neural machine
translation. In Proceedings of EMNLP 2016.
</p>
<p>Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex
Waibel. 2016. Pre-translation for neural machine
translation. In Proceedings of COLING 2016.
</p>
<p>Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Proceedings of MT
Summit.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a methof for automatic e-
valuation of machine translation. In Proceedings of
ACL 2002.
</p>
<p>Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
ACL 2007.
</p>
<p>Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
appplication to machine translation systems combi-
nation. In Proceedings of the Third ACL Workshop
on Statistical Machine Translation.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of ACL
2016.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of ACL 2016.
</p>
<p>Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of ACL 2016.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural network-
s. In Proceedings of NIPS 2014.
</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In Proceedings of ACL 2016.
</p>
<p>Jiajun Zhang and Chengqing Zong. 2016a. Bridging
neural machine translation and bilingual dictionar-
ies. arXiv preprint arXiv:1610.07272.
</p>
<p>Jiajun Zhang and Chengqing Zong. 2016b. Exploit-
ing source-side monolingual data in neural machine
translation.. In Proceedings of EMNLP 2016.
</p>
<p>Junguo Zhu, Muyun Yang, Sheng Li, and Tiejun Zhao.
2016. Sentence-level paraphrasing for machine
translation system combination. In Proceedings of
ICYCSEE 2016.
</p>
<p>Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of NAACL-HLT
2016.
</p>
<p>384</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 385–391
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2061
</p>
<p>An Empirical Comparison of Domain Adaptation Methods for
Neural Machine Translation
</p>
<p>Chenhui Chu1∗, Raj Dabre2, and Sadao Kurohashi2
1Institute for Datability Science, Osaka University
2Graduate School of Informatics, Kyoto University
</p>
<p>chu@ids.osaka-u.ac.jp, {raj, kuro}@nlp.ist.i.kyoto-u.ac.jp
</p>
<p>Abstract
</p>
<p>In this paper, we propose a novel do-
main adaptation method named “mixed
fine tuning” for neural machine transla-
tion (NMT). We combine two existing ap-
proaches namely fine tuning and multi do-
main NMT. We first train an NMT model
on an out-of-domain parallel corpus, and
then fine tune it on a parallel corpus which
is a mix of the in-domain and out-of-
domain corpora. All corpora are aug-
mented with artificial tags to indicate spe-
cific domains. We empirically compare
our proposed method against fine tuning
and multi domain methods and discuss its
benefits and shortcomings.
</p>
<p>1 Introduction
</p>
<p>One of the most attractive features of neural ma-
chine translation (NMT) (Bahdanau et al., 2015;
Cho et al., 2014; Sutskever et al., 2014) is that it
is possible to train an end to end system without
the need to deal with word alignments, translation
rules and complicated decoding algorithms, which
are a characteristic of statistical machine transla-
tion (SMT) systems. However, it is reported that
NMT works better than SMT only when there is an
abundance of parallel corpora. In the case of low
resource domains, vanilla NMT is either worse
than or comparable to SMT (Zoph et al., 2016).
</p>
<p>Domain adaptation has been shown to be effec-
tive for low resource NMT. The conventional do-
main adaptation method is fine tuning, in which
an out-of-domain model is further trained on in-
domain data (Luong and Manning, 2015; Sen-
nrich et al., 2016b; Servan et al., 2016; Freitag
and Al-Onaizan, 2016). However, fine tuning
</p>
<p>∗This work was done when the first author was a re-
searcher of Japan Science and Technology Agency.
</p>
<p>tends to overfit quickly due to the small size of
the in-domain data. On the other hand, multi do-
main NMT (Kobus et al., 2016) involves training
a single NMT model for multiple domains. This
method adds tags “&lt;2domain&gt;” to the source sen-
tences in the parallel corpora to indicate domains
without any modifications to the NMT system ar-
chitecture. However, this method has not been
studied for domain adaptation in particular.
</p>
<p>Motivated by these two lines of studies, we
propose a new domain adaptation method called
“mixed fine tuning,” where we first train an NMT
model on an out-of-domain parallel corpus, and
then fine tune it on a parallel corpus that is a
mix of the in-domain and out-of-domain corpora.
Fine tuning on the mixed corpus instead of the in-
domain corpus can address the overfitting prob-
lem. All corpora are augmented with artificial tags
to indicate specific domains. We tried two dif-
ferent corpora settings on two different language
pairs:
</p>
<p>• Manually created resource poor corpus
(Chinese-to-English translation): Using the
NTCIR data (patent domain; resource rich)
(Goto et al., 2013) to improve the translation
quality for the IWSLT data (TED talks; re-
source poor) (Cettolo et al., 2015).
</p>
<p>• Automatically extracted resource poor cor-
pus (Chinese-to-Japanese translation): Using
the ASPEC data (scientific domain; resource
rich) (Nakazawa et al., 2016) to improve the
translation quality for the Wiki data (resource
poor). The parallel corpus of the latter do-
main was automatically extracted (Chu et al.,
2016a).
</p>
<p>We observed that “mixed fine tuning” works sig-
nificantly better than methods that use fine tuning
</p>
<p>385</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2061">https://doi.org/10.18653/v1/P17-2061</a></div>
</div>
<div class="page"><p />
<p>Source-Target  
(out-of-domain) NMT 
</p>
<p>Model 
(out-of-domain) 
</p>
<p>Source-Target   
(in-domain) NMT 
</p>
<p>Model  
(in-domain) 
</p>
<p>Init
ializ
</p>
<p>e 
</p>
<p>Figure 1: Fine tuning for domain adaptation.
</p>
<p>and domain tag based approaches separately. Our
contributions are twofold:
</p>
<p>• We propose a novel method that combines the
best of existing approaches and show that it is
effective.
</p>
<p>• To the best of our knowledge this is the first
work on an empirical comparison of various
domain adaptation methods.
</p>
<p>2 Related Work
</p>
<p>Fine tuning has also been explored for various
NLP tasks using neural networks such as sen-
timent analysis and paraphrase detection (Mou
et al., 2016). Tag based NMT has also been shown
to be effective for control the politeness of trans-
lations (Sennrich et al., 2016a) and multilingual
NMT (Johnson et al., 2016).
</p>
<p>Besides fine tuning and multi domain NMT us-
ing tags, another direction of domain adaptation
for NMT is using in-domain monolingual data. Ei-
ther training an in-domain recurrent neural net-
work (RNN) language model for the NMT de-
coder (Gülçehre et al., 2015) or generating syn-
thetic data by back translating target in-domain
monolingual data (Sennrich et al., 2016b) have
been studied.
</p>
<p>3 Methods for Comparison
</p>
<p>All the methods that we compare are simple and
do not need any modifications to the NMT system.
</p>
<p>3.1 Fine Tuning
Fine tuning is the conventional way for domain
adaptation, and thus serves as a baseline in this
study. In this method, we first train an NMT sys-
tem on a resource rich out-of-domain corpus till
convergence, and then fine tune its parameters on
a resource poor in-domain corpus (Figure 1).
</p>
<p>3.2 Multi Domain
The multi domain method is originally motivated
by (Sennrich et al., 2016a), which uses tags to
</p>
<p>control the politeness of NMT translations. The
overview of this method is shown in the dotted
section in Figure 2. In this method, we simply con-
catenate the corpora of multiple domains with two
small modifications:
</p>
<p>• Appending the domain tag “&lt;2domain&gt;” to
the source sentences of the respective cor-
pora.1 This primes the NMT decoder to gen-
erate sentences for the specific domain.
</p>
<p>• Oversampling the smaller corpus so that the
training procedure pays equal attention to
each domain.
</p>
<p>We can further fine tune the multi domain model
on the in-domain data, which is named as “multi
domain + fine tuning.”
</p>
<p>3.3 Mixed Fine Tuning
The proposed mixed fine tuning method is a com-
bination of the above methods (shown in Figure
2). The training procedure is as follows:
</p>
<p>1. Train an NMT model on out-of-domain data
till convergence.
</p>
<p>2. Resume training the NMT model from step
1 on a mix of in-domain and out-of-domain
data (by oversampling the in-domain data) till
convergence.
</p>
<p>By default, we utilize domain tags, but we also
consider settings where we do not use them (i.e.,
“w/o tags”). We can further fine tune the model
from step 2 on the in-domain data, which is named
as “mixed fine tuning + fine tuning.”
</p>
<p>Note that in the “fine tuning” method, the vo-
cabulary obtained from the out-of-domain data is
used for the in-domain data; while for the “multi
domain” and “mixed fine tuning” methods, we use
a vocabulary obtained from the mixed in-domain
and out-of-domain data for all the training stages.
Regarding development data, for fine tuning, an
out-of-domain development set is first used for
training the out-of-domain NMT model, then an
in-domain development set is used for fine tuning;
For multi-domain, a mix of in-domain and out-of-
domain development sets are used; For mixed fine
tuning, an out-of-domain development set is first
used for training the out-of-domain NMT model,
then a mix of in-domain and out-of-domain devel-
opment sets are used for mixed fine tuning.
</p>
<p>1We verified the effectiveness of the domain tags by com-
paring against a setting that does not use them, see the “w/o
tags” settings in Tables 1 and 2.
</p>
<p>386</p>
<p />
</div>
<div class="page"><p />
<p>NMT Model 
</p>
<p>(mixed) 
merge 
</p>
<p>Append in-domain 
</p>
<p>tag (&lt;2in-domain&gt;) 
</p>
<p>NMT Model  
</p>
<p>(out-of-domain) 
</p>
<p>Ini
tia
</p>
<p>liz
e 
</p>
<p>NMT Model 
</p>
<p>(in-domain) 
</p>
<p>In
iti
</p>
<p>al
iz
</p>
<p>e 
</p>
<p>Append out-of-domain  
</p>
<p>tag (&lt;2out-of-domain&gt;) 
</p>
<p>Oversample the smaller  
</p>
<p>in-domain corpus 
</p>
<p>Source-Target  
</p>
<p>(out-of-domain) 
</p>
<p>Source-Target 
</p>
<p>(in-domain) 
</p>
<p>Figure 2: Mixed fine tuning with domain tags for domain adaptation (The section in the dotted rectangle
denotes the multi domain method).
</p>
<p>4 Experimental Settings
</p>
<p>We conducted NMT domain adaptation experi-
ments in two different settings as follows:
</p>
<p>4.1 High Quality In-domain Corpus Setting
</p>
<p>Chinese-to-English translation was the focus of
the high quality in-domain corpus setting. We uti-
lized the resource rich patent out-of-domain data
to augment the resource poor spoken language in-
domain data. The patent domain MT was con-
ducted on the Chinese-English subtask (NTCIR-
CE) of the patent MT task at the NTCIR-10 work-
shop2 (Goto et al., 2013). The NTCIR-CE task
uses 1M, 2k, and 2k sentences for training, devel-
opment, and testing, respectively. The spoken do-
main MT was conducted on the Chinese-English
subtask (IWSLT-CE) of the TED talk MT task at
the IWSLT 2015 workshop (Cettolo et al., 2015).
The IWSLT-CE task contains 209,491 sentences
for training. We used the dev 2010 set for devel-
opment, containing 887 sentences. We evaluated
all methods on the 2010, 2011, 2012, and 2013
test sets, containing 1570, 1245, 1397, and 1261
sentences, respectively.
</p>
<p>4.2 Low Quality In-domain Corpus Setting
</p>
<p>Chinese-to-Japanese translation was the focus of
the low quality in-domain corpus setting. We
utilized the resource rich scientific out-of-domain
data to augment the resource poor Wikipedia (es-
sentially open) in-domain data. The scientific do-
main MT was conducted on the Chinese-Japanese
paper excerpt corpus (ASPEC-CJ)3 (Nakazawa
et al., 2016), which is one subtask of the workshop
on Asian translation (WAT)4 (Nakazawa et al.,
</p>
<p>2http://ntcir.nii.ac.jp/PatentMT-2/
3http://lotus.kuee.kyoto-u.ac.jp/ASPEC/
4http://orchid.kuee.kyoto-u.ac.jp/WAT/
</p>
<p>2015). The ASPEC-CJ task uses 672315, 2090,
and 2107 sentences for training, development, and
testing, respectively. The Wikipedia domain task
was conducted on a Chinese-Japanese corpus au-
tomatically extracted from Wikipedia (WIKI-CJ)
(Chu et al., 2016a) using the ASPEC-CJ corpus as
a seed. The WIKI-CJ task contains 136013, 198,
and 198 sentences for training, development, and
testing, respectively.
</p>
<p>4.3 MT Systems
</p>
<p>For NMT, we used the KyotoNMT system5
</p>
<p>(Cromieres et al., 2016). The NMT settings were
the same as (Cromieres et al., 2016) except that
we used a vocabulary size of 32k for all the ex-
periments, and did not ensemble independently
trained parameters. The sizes of the source and
target vocabularies, the source and target side em-
beddings, the hidden states, the attention mecha-
nism hidden states, and the deep softmax output
with a 2-maxout layer were set to 32,000, 620,
1000, 1000, and 500, respectively. We used 2-
layer LSTMs for both the source and target sides.
ADAM was used as the learning algorithm, with
a dropout rate of 20% for the inter-layer dropout,
and L2 regularization with a weight decay coef-
ficient of 1e-6. The mini batch size was 64, and
sentences longer than 80 tokens were discarded.
We early stopped the training process when we
observed that the BLEU score of the development
set converges. For testing, we ensembled the three
parameters of the best development loss, the best
development BLEU, and the final parameters in a
single training run. Beam size was set to 100. The
maximum length of the translation was set to 2,
and 1.5 times of the source sentences for Chinese-
to-English, and Chinese-to-Japanese, respectively.
</p>
<p>5https://github.com/fabiencro/knmt
</p>
<p>387</p>
<p />
</div>
<div class="page"><p />
<p>IWSLT-CE
System NTCIR-CE test 2010 test 2011 test 2012 test 2013 average
IWSLT-CE SMT - 12.73 16.27 14.01 14.67 14.31
IWSLT-CE NMT - 6.75 9.08 9.05 7.29 7.87
NTCIR-CE SMT 29.54 3.57 4.70 4.21 4.74 4.33
NTCIR-CE NMT 37.11 2.23 2.83 2.55 2.85 2.60
Fine tuning 17.37 13.93 18.99 16.12 17.12 16.41
Multi domain 36.40 13.42 19.07 16.56 17.54 16.34
Multi domain w/o tags 37.32 12.57 17.40 15.02 15.96 14.97
Multi domain + Fine tuning 14.47 13.18 18.03 16.41 16.80 15.82
Mixed fine tuning 37.01 15.04 20.96 18.77 18.63 18.01
Mixed fine tuning w/o tags 39.67 14.47 20.53 18.10 17.97 17.43
Mixed fine tuning + Fine tuning 32.03 14.40 19.53 17.65 17.94 17.11
</p>
<p>Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE.
</p>
<p>For performance comparison, we also con-
ducted experiments on phrase based SMT (PB-
SMT). We used the Moses PBSMT system (Koehn
et al., 2007) for all of our MT experiments. For
the respective tasks, we trained 5-gram language
models on the target side of the training data us-
ing the KenLM toolkit6 with interpolated Kneser-
Ney discounting, respectively. In all of our ex-
periments, we used the GIZA++ toolkit7 for word
alignment; tuning was performed by minimum er-
ror rate training (Och, 2003), and it was re-run for
every experiment.
</p>
<p>For both MT systems, we preprocessed the data
as follows. For Chinese, we used KyotoMorph8
</p>
<p>for segmentation, which was trained on the CTB
version 5 (CTB5) and SCTB (Chu et al., 2016b).
For English, we tokenized and lowercased the sen-
tences using the tokenizer.perl script in Moses.
Japanese was segmented using JUMAN9 (Kuro-
hashi et al., 1994).
</p>
<p>For NMT, we further split the words into sub-
words using byte pair encoding (BPE) (Sennrich
et al., 2016c), which has been shown to be effec-
tive for the rare word problem in NMT. Another
motivation of using sub-words is making the dif-
ferent domains share more vocabulary, which is
important especially for the resource poor domain.
For the Chinese-to-English tasks, we trained two
BPE models on the Chinese and English vocabu-
laries, respectively. For the Chinese-to-Japanese
tasks, we trained a joint BPE model on both of the
Chinese and Japanese vocabularies, because Chi-
nese and Japanese could share some vocabularies
of Chinese characters. The number of merge op-
erations was set to 30k for all the tasks.
</p>
<p>6https://github.com/kpu/kenlm/
7http://code.google.com/p/giza-pp
8https://bitbucket.org/msmoshen/kyotomorph-beta
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
</p>
<p>System ASPEC-CJ WIKI-CJ
WIKI-CJ SMT - 36.83
WIKI-CJ NMT - 18.29
ASPEC-CJ SMT 36.39 17.43
ASPEC-CJ NMT 42.92 20.01
Fine tuning 22.10 37.66
Multi domain 42.52 35.79
Multi domain w/o tags 40.78 33.74
Multi domain + Fine tuning 22.78 34.61
Mixed fine tuning 42.56 37.57
Mixed fine tuning w/o tags 41.86 37.23
Mixed fine tuning + Fine tuning 31.63 37.77
</p>
<p>Table 2: Domain adaptation results (BLEU-4
scores) for WIKI-CJ using ASPEC-CJ.
</p>
<p>5 Results
</p>
<p>Tables 1 and 2 show the translation results on
the Chinese-to-English and Chinese-to-Japanese
tasks, respectively. The entries with SMT and
NMT are the PBSMT and NMT systems, respec-
tively; others are the different methods described
in Section 3. In both tables, the numbers in bold
indicate the best system and all systems that were
not significantly different from the best system.
The significance tests were performed using the
bootstrap resampling method (Koehn, 2004) at
p &lt; 0.05.
</p>
<p>We can see that without domain adaptation,
the SMT systems perform significantly better than
the NMT system on the resource poor domains,
i.e., IWSLT-CE and WIKI-CJ; while on the re-
source rich domains, i.e., NTCIR-CE and ASPEC-
CJ, NMT outperforms SMT. Directly using the
SMT/NMT models trained on the out-of-domain
data to translate the in-domain data shows bad
performance. With our proposed “Mixed fine
tuning” domain adaptation method, NMT signif-
icantly outperforms SMT on the in-domain tasks.
</p>
<p>Comparing different domain adaptation meth-
ods, “Mixed fine tuning” shows the best perfor-
</p>
<p>388</p>
<p />
</div>
<div class="page"><p />
<p>mance. We believe the reason for this is that
“Mixed fine tuning” can address the over-fitting
problem of “Fine tuning.” We observed that both
fine-tuning and mixed fine-tuning tends to con-
verge after 1 epoch of training, and thus we early
stopped training soon after 1 epoch. After 1 epoch
of training, fine-tuning overfitted very quickly,
while mixed fine-tuning did not overfit. In ad-
dition, “Mixed fine tuning” does not worsen the
quality of out-of-domain translations, while “Fine
tuning” and “Multi domain” do. One shortcoming
of “Mixed fine tuning” is that compared to “Fine
tuning,” it took a longer time for the fine tuning
process, as the time until convergence is essen-
tially proportional to the size of the data used for
fine tuning. Note that training as long as “Mixed
fine tuning” is not helpful for “Fine tuning” due to
overfitting.
</p>
<p>“Multi domain” performs either as well as
(IWSLT-CE) or worse than (WIKI-CJ) “Fine tun-
ing,” but “Mixed fine tuning” performs either sig-
nificantly better than (IWSLT-CE) or is compara-
ble to (WIKI-CJ) “Fine tuning.” We believe the
performance difference between the two tasks is
due to their unique characteristics. As WIKI-CJ
data is of relatively poorer quality, mixing it with
out-of-domain data does not have the same level
of positive effects as those obtained by the IWSLT-
CE data.
</p>
<p>The domain tags are helpful for both “Multi do-
main” and “Mixed fine tuning.” Essentially, fur-
ther fine tuning on in-domain data does not help
for both “Multi domain” and “Mixed fine tun-
ing.” We believe that there are two reasons for
this. Firstly, the “Multi domain” and “Mixed fine
tuning” methods already utilize the in-domain data
used for fine tuning. Secondly, fine tuning on the
small in-domain data overfits very quickly. Actu-
ally, we observed that adding fine-tuning on top
of both “Multi domain” and “Mixed fine tuning”
overfits at the beginning of training.
</p>
<p>Mixed fine tuning performs significantly better
on the out-domain NTCIR-CE test set without tags
as compared to with tags (39.67 v.s. 37.01). We
believe the reason for this is that without tags the
IWSLT-CE in-domain data can contribute more to
the out-of-domain NTCIR-CE data. With tags, the
NMT training tends to learn a model that pays
equal attention to each domain. Without tags, the
NMT training pays more attention to the NTCIR-
CE data as it contains much longer sentences, al-
</p>
<p>though we oversampled the IWSLT-CE data. As
the IWSLT-CE data is TED talks, there could be
some vocabulary and content overlaps between the
IWSLT-CE the NTCIR-CE data, and thus append-
ing the IWSLT-CE data to the NTCIR-CE data can
benefit for the NTCIR-CE translation. In the case
of WIKI-CJ and ASPEC-CJ, due to the low qual-
ity of WIKI-CJ, appending WIKI-CJ to ASPEC-
CJ does not improve the ASPEC-CJ translation.
</p>
<p>6 Conclusion
</p>
<p>In this paper, we proposed a novel domain adapta-
tion method named “mixed fine tuning” for NMT.
We empirically compared our proposed method
against fine tuning and multi domain methods, and
have shown that it is effective but is sensitive to the
quality of the in-domain data used. The presented
methods are language and domain independent,
and thus we believe that the general observations
also hold on other languages and domains. Fur-
thermore, we believe the contribution in this paper
can be helpful for domain adaptation of other NN
based natural language processing tasks.
</p>
<p>In the future, we plan to incorporate an RNN
model into our architecture to leverage abundant
in-domain monolingual corpora. We also plan
on exploring the effects of synthetic data by back
translating large in-domain monolingual corpora.
</p>
<p>Acknowledgments
</p>
<p>This work was partly supported by JSPS and DST
under the Japan-India Science Cooperative Pro-
gram. We are very appreciated to Prof. Daisuke
Kawahara, Dr. Toshiaki Nakazawa, and Dr. Fa-
bien Cromieres for helping improving the writing
quality of this paper. We also thank the anony-
mous reviewers for their insightful comments.
</p>
<p>References
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In In Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR 2015). International Conference
on Learning Representations, San Diego, USA.
</p>
<p>M Cettolo, J Niehues, S Stüker, L Bentivogli, R Cat-
toni, and M Federico. 2015. The iwslt 2015 evalua-
tion campaign. In Proceedings of the Twelfth Inter-
national Workshop on Spoken Language Translation
(IWSLT).
</p>
<p>389</p>
<p />
</div>
<div class="page"><p />
<p>Kyunghyun Cho, Bart van Merriënboer, Çalar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
1724–1734. http://www.aclweb.org/anthology/D14-
1179.
</p>
<p>Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2016a.
Parallel sentence extraction from comparable cor-
pora with neural network features. In Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC 2016). Eu-
ropean Language Resources Association (ELRA),
Paris, France.
</p>
<p>Chenhui Chu, Toshiaki Nakazawa, Daisuke Kawa-
hara, and Sadao Kurohashi. 2016b. SCTB: A
Chinese treebank in scientific domain. In Pro-
ceedings of the 12th Workshop on Asian Lan-
guage Resources (ALR12). The COLING 2016 Or-
ganizing Committee, Osaka, Japan, pages 59–67.
http://aclweb.org/anthology/W16-5407.
</p>
<p>Fabien Cromieres, Chenhui Chu, Toshiaki Nakazawa,
and Sadao Kurohashi. 2016. Kyoto univer-
sity participation to wat 2016. In Proceed-
ings of the 3rd Workshop on Asian Transla-
tion (WAT2016). The COLING 2016 Organiz-
ing Committee, Osaka, Japan, pages 166–174.
http://aclweb.org/anthology/W16-4616.
</p>
<p>Markus Freitag and Yaser Al-Onaizan. 2016. Fast
domain adaptation for neural machine translation.
arXiv preprint arXiv:1612.06897 .
</p>
<p>Isao Goto, Ka-Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Conference. Na-
tional Institute of Informatics (NII), Tokyo, Japan,
pages 260–286.
</p>
<p>Çaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loı̈c Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015.
On using monolingual corpora in neural ma-
chine translation. CoRR abs/1503.03535.
http://arxiv.org/abs/1503.03535.
</p>
<p>Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Viégas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2016.
Google’s multilingual neural machine translation
system: Enabling zero-shot translation. CoRR
abs/1611.04558. http://arxiv.org/abs/1611.04558.
</p>
<p>Catherine Kobus, Josep Crego, and Jean Senellart.
2016. Domain control for neural machine transla-
tion. arXiv preprint arXiv:1612.06140 .
</p>
<p>Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004. Association for Computational Lin-
guistics, Barcelona, Spain, pages 388–395.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 177–180.
http://www.aclweb.org/anthology/P/P07/P07-2045.
</p>
<p>Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements of
Japanese morphological analyzer JUMAN. In Pro-
ceedings of the International Workshop on Sharable
Natural Language. pages 22–28.
</p>
<p>Minh-Thang Luong and Christopher D Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In Proceedings of the 12th
International Workshop on Spoken Language Trans-
lation. Da Nang, Vietnam, pages 76–79.
</p>
<p>Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
479–489. https://aclweb.org/anthology/D16-1046.
</p>
<p>Toshiaki Nakazawa, Hideya Mino, Isao Goto, Gra-
ham Neubig, Sadao Kurohashi, and Eiichiro Sumita.
2015. Overview of the 2nd Workshop on Asian
Translation. In Proceedings of the 2nd Workshop on
Asian Translation (WAT2015). Kyoto, Japan, pages
1–28.
</p>
<p>Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. Aspec: Asian
scientific paper excerpt corpus. In Proceedings of
the Tenth International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association (ELRA), Paris,
France.
</p>
<p>Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics. Association for Com-
putational Linguistics, Sapporo, Japan, pages 160–
167. http://www.aclweb.org/anthology/P03-1021.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016a. Controlling politeness in neu-
ral machine translation via side constraints. In
Proceedings of the 2016 Conference of the
</p>
<p>390</p>
<p />
</div>
<div class="page"><p />
<p>North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies. Association for Computational Lin-
guistics, San Diego, California, pages 35–40.
http://www.aclweb.org/anthology/N16-1005.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Improving neural machine translation
models with monolingual data. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 86–96.
http://www.aclweb.org/anthology/P16-1009.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016c. Neural machine translation of
rare words with subword units. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1715–1725.
http://www.aclweb.org/anthology/P16-1162.
</p>
<p>Christophe Servan, Josep Crego, and Jean Senel-
lart. 2016. Domain specialization: a post-training
domain adaptation for neural machine translation.
arXiv preprint arXiv:1612.06141 .
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014. Sequence to sequence learning with
neural networks. In Proceedings of the 27th
International Conference on Neural Informa-
tion Processing Systems. MIT Press, Cam-
bridge, MA, USA, NIPS’14, pages 3104–3112.
http://dl.acm.org/citation.cfm?id=2969033.2969173.
</p>
<p>Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016. pages 1568–1575.
http://aclweb.org/anthology/D/D16/D16-1163.pdf.
</p>
<p>391</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 392–398
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2062
</p>
<p>Efficient Extraction of Pseudo-Parallel Sentences
from Raw Monolingual Data Using Word Embeddings
</p>
<p>Benjamin Marie Atsushi Fujita
National Institute of Information and Communications Technology
</p>
<p>3-5 Hikaridai, Seika-cho, Souraku-gun, Kyoto, 619-0289, Japan
{bmarie, atsushi.fujita}@nict.go.jp
</p>
<p>Abstract
</p>
<p>We propose a new method for extracting
pseudo-parallel sentences from a pair of
large monolingual corpora, without rely-
ing on any document-level information.
Our method first exploits word embed-
dings in order to efficiently evaluate tril-
lions of candidate sentence pairs and then
a classifier to find the most reliable ones.
We report significant improvements in do-
main adaptation for statistical machine
translation when using a translation model
trained on the sentence pairs extracted
from in-domain monolingual corpora.
</p>
<p>1 Introduction
</p>
<p>Parallel corpus is an indispensable resource for
statistical and neural machine translation. Gener-
ally, using more sentence pairs to train a transla-
tion system makes it able to produce better trans-
lations. However, for most language pairs and do-
mains, parallel corpora remain scarce due mainly
to the cost of their creation (Germann, 2001).
</p>
<p>In the last two decades, numerous methods
have been proposed to extract parallel sentences
from comparable corpora. In addition to compa-
rable corpora in large quantity, to the best of our
knowledge, all previous methods heavily rely on
document-level information and/or lexical trans-
lation models, such as those for statistical ma-
chine translation (SMT) systems (Zhao and Vo-
gel, 2002; Fung and Cheung, 2004; Munteanu
and Marcu, 2005; Tillmann and Xu, 2009) and
manually-created bilingual lexicon (Utiyama and
Isahara, 2003). The most successful approaches
use cross-lingual information retrieval techniques
(Abdul Rauf and Schwenk, 2011; S, tefănescu
et al., 2012) to extract sentence pairs from com-
parable documents. Using such document pairs
</p>
<p>has the strong advantage that it drastically reduces
the search space; we need to consider only sen-
tence pairs in each document pair instead of scor-
ing all sentence pairs in the two monolingual cor-
pora. However, in many cases, we do not have
access to document-level information. Only Till-
mann and Xu (2009) have explored this scenario
using efficient caching strategies to extract use-
ful sentence pairs from nearly one trillion candi-
dates in comparable data. Yet, their approach is
tightly related to the exploitation of accurate lex-
ical translation models and does not allow us to
introduce other features. The reliance on lexical
translation models implies that we must have al-
ready access to parallel data sufficiently large for
obtaining accurate estimates. Nevertheless, the
most useful sentence pairs for SMT are actually
the ones that contain infrequent or even unseen to-
kens in these parallel data. Relying only on lexical
translation models thus seems rather inadequate to
extract sentence pairs containing numerous infre-
quent or unseen tokens, and may actually be more
prone to extract sentence pairs that contain words
and phrases for which we already have accurate
translation probability estimates.
</p>
<p>This paper proposes a new method that exploits
word embeddings to efficiently extract pseudo-
parallel sentences1 from raw monolingual data
without using any document-level information.
We report significant improvements of translation
quality in a domain adaptation scenario for SMT.
</p>
<p>2 Sentence pair extraction
</p>
<p>During the sentence pair extraction, we do not
assume an access to document-level information.
Our method thus has to be efficient in evaluating
</p>
<p>1As in previous work, we regard the sentence pairs ex-
tracted by our method as “pseudo-parallel” because they are
not necessarily parallel. As shown by Goutte et al. (2012),
even very noisy parallel corpora may be useful for SMT.
</p>
<p>392</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2062">https://doi.org/10.18653/v1/P17-2062</a></div>
</div>
<div class="page"><p />
<p>trillions of sentence pairs hypothesized from two
monolingual corpora, each containing millions of
sentences. To achieve this computationally chal-
lenging task, we need a fast way to compute some
similarity between the source and target sentences,
without relying on large lexical translation models
that may not be available or accurate enough in
some low-resourced conditions.
</p>
<p>2.1 Step 1: Filtering with sentence
embeddings
</p>
<p>Assuming the availability of large-scale mono-
lingual data, our method exploits word embed-
dings (Mikolov et al., 2013b) that are fast to es-
timate. First, word embeddings for each language
are learned from the given monolingual data. This
enables us to evaluate arbitrary sentence pair given
all the words it contains, which is not fully guaran-
teed by a lexical translation model as some tokens
may be out-of-vocabulary (OOV). We then pro-
ceed to the projection of all the source word em-
beddings to the target embedding space, following
Mikolov et al. (2013a),2 in order to represent both
source and target words in the same space.
</p>
<p>To compute the similarity between arbitrary
sentence pairs, we represent each sentence by av-
eraging the embeddings of its constituent words.3
</p>
<p>As a result of this first step, our method keeps
for each source sentence the n closest target sen-
tences (n being small, for instance with a value of
100) according to the similarity score.
</p>
<p>2.2 Step 2: Refining with a classifier
</p>
<p>Given a far smaller search space, this second
step evaluates and re-ranks the remaining sentence
pairs, incorporating more complex features to train
a classifier. We use a total of five features.
</p>
<p>For each sentence pair, we use the score com-
puted in the first step and a more accurate sim-
ilarity score based on alignments between word
embeddings, following the work in Kajiwara and
</p>
<p>2Despite the availability of more accurate methods (Coul-
mance et al., 2015; Duong et al., 2016) we choose this method
considering its low computational cost and its reasonable
need of external resources to estimate the translation matrix,
i.e., only a small bilingual dictionary.
</p>
<p>3As shown by Adi et al. (2016), this can be effective to en-
code sentence-level information such as content and length,
while being computationally more efficient than other meth-
ods, such as inducing paragraph vectors (Le and Mikolov,
2014) and using LSTM auto-encoders (Li et al., 2015). Our
decision also relies on the promising accuracy of linear pro-
jection of word (not sentence) embeddings across different
languages (Mikolov et al., 2013a).
</p>
<p>Komachi (2016). They found out that the average
of the cosine similarity between all the best word
pairs, for each source word, taken from the sen-
tence pair, shown in Eq. (1), was a good indicator
of similarity between two sentences.
</p>
<p>S(x,y) =
1
</p>
<p>|x|
</p>
<p>|x|∑
</p>
<p>i=1
</p>
<p>max
j
φ(xembi , y
</p>
<p>emb
j ) (1)
</p>
<p>where x and y are respectively the source and tar-
get sentences, |x| the length of x, and φ the co-
sine similarity between the embeddings in the tar-
get language space of the i-th word in x, i.e., xembi ,
and the j-th word in y, i.e., yembj . The computa-
tion of this score can be highly costly, depending
on the sentence length and the number of dimen-
sions of the word embeddings. Thus, we compute
this score only for the source to target direction,
unlike Kajiwara and Komachi (2016).
</p>
<p>In many situations, we may also have an access
to a lexical translation model trained on some par-
allel data. We therefore incorporate the scores pro-
posed by Tillmann and Xu (2009), but considering
one probability for each translation direction, in-
stead of summing them up, so that our classifier
can optimize their weight separately.
</p>
<p>P (x|y) =
|x|∑
</p>
<p>i=1
</p>
<p>1
</p>
<p>|x| log(
1
</p>
<p>|y|
</p>
<p>|y|∑
</p>
<p>j=1
</p>
<p>p(xtoki |ytokj )) (2)
</p>
<p>P (y|x) =
|y|∑
</p>
<p>j=1
</p>
<p>1
</p>
<p>|y| log(
1
</p>
<p>|x|
</p>
<p>|x|∑
</p>
<p>i=1
</p>
<p>p(ytokj |xtoki )) (3)
</p>
<p>where xtoki is the i-th token in x, y
tok
j the j-th to-
</p>
<p>ken in y and p the probability given by an already
estimated lexical translation model.
</p>
<p>Our last feature is the length ratio of the source
and target sentences (Munteanu and Marcu, 2005).
</p>
<p>To assign a real-valued score to each sentence
pair in order to filter and rank them, we train
a Maximum Entropy (ME) classifier, following
Munteanu and Marcu (2005). ME classifier suits
particularly well our situation, since we deal with
a small number of dense features and have hun-
dred millions of sentence pairs to classify quickly.
</p>
<p>Positive examples for training the classifier can
be obtained straightforwardly: we use true sen-
tence pairs sampled from parallel data, different
from the one used to train the lexical transla-
tion model. As for negative examples, Munteanu
and Marcu (2005) randomly paired sentences from
</p>
<p>393</p>
<p />
</div>
<div class="page"><p />
<p>their parallel data using two constraints: a length
ratio not greater than two, and a coverage con-
straint that considers a negative example only if
more than half of the words of the source sentence
has a translation in the given target sentence ac-
cording to some bilingual lexicon. However, from
a large parallel corpus, one can easily retrieve an-
other target sentence, almost identical, containing
most of the words that the true target sentence also
contains. In this case, the negative example will be
almost as semantically close as the positive one,
weakening the discriminative power of the fea-
tures based on word embeddings. To circumvent
this problem, we generate negative examples, as
many as positive examples, without using this cov-
erage constraint.
</p>
<p>Having assigned a score for each sentence pair,
we make a pseudo-parallel corpus selecting the
target sentence with the best score for each source
sentence and retaining only the sentence pairs with
a score above some threshold, th . This pseudo-
parallel corpus can then be used to train a new
phrase table.
</p>
<p>3 Experiments
</p>
<p>We evaluated our method in a scenario of domain
adaptation for phrase-based SMT (PBSMT). In
this scenario, we assumed a lot of general-domain
parallel data to train a general-domain phrase ta-
ble and a lot of in-domain monolingual data as our
source of in-domain pseudo-parallel sentences.
</p>
<p>3.1 Data and SMT system
</p>
<p>We experimented with the French–English lan-
guage pair, both translation directions, on the med-
ical domain. We used Moses (Koehn et al., 2007)
to train, tune, and test our PBSMT systems. The
general-domain phrase table was trained on Eu-
roparl V74 (1.99M sentences). The in-domain
monolingual data were prepared by applying the
NLTK5 sentence segmenter to the concatenation
of all the monolingual corpora provided for the
WMT’14 medical translation task.6 As the source
of extracting in-domain sentence pairs, we ran-
domly sampled 1M sentences (33M tokens) from
the French data and 5M sentences (164M tokens)
from the English data. Given pseudo-parallel sen-
tences extracted by our method from these data
</p>
<p>4http://statmt.org/europarl/
5http://www.nltk.org/
6http://statmt.org/wmt14/medical-task/
</p>
<p>(see Section 3.2), we trained an in-domain phrase
table. Moses exploits the two phrase tables, i.e.,
general-domain and in-domain ones, with its mul-
tiple decoding path ability. The PBSMT systems
used one language model trained on the entire tar-
get in-domain monolingual data concatenated to
the target side of Europarl and News Crawl data
provided by WMT’15.7 The development and test
data used to tune and evaluate the PBSMT sys-
tems were excerpts of the EMEA parallel corpus
(Carpuat et al., 2012).
</p>
<p>3.2 Parameters for sentence pair extraction
</p>
<p>We used word2vec8 to learn word embeddings
with the parameters -cbow 1 -window 10
-negative 15 -sample 1e-4 -iter
15 -min-count 1, specifying 800 and 300
dimensions for the source and target languages,
respectively,9 on the same data used to train the
language models. The translation matrix used
to project the source word embeddings to the
target embedding space was trained on a bilingual
lexicon containing the 5k10 most frequent French
tokens,11 from Europarl, and their most probable
single token in English given by the Europarl
phrase table. The first step of our method eval-
uated five trillion (1M×5M) sentence pairs and
retained the 100 closest target sentences for each
source sentence.
</p>
<p>The second step then dealt with only 100M
(1M×100) sentence pairs. The lexical translation
probabilities used to compute our features were
given by the Europarl lexical translation models.
We used Scikit-learn12 to train the ME clas-
sifier, with default parameters, on 5k positive and
5k negative examples13 randomly generated from
the MultiUn corpus.14 According to the classi-
fier’s score, only the 1-best target sentence for
each source sentence was retained. We discarded
sentence pairs having a score lower than a thresh-
</p>
<p>7http://statmt.org/wmt15/
8http://word2vec.googlecode.com/
9Mikolov et al. (2013a) observed that a more accurate pro-
</p>
<p>jection is obtained when using a greater number of dimen-
sions on the source side than that for the target side.
</p>
<p>10Vulić and Korhonen (2016) demonstrated that 5k word
pairs is enough to train a useful translation matrix.
</p>
<p>11We extracted sentence pairs regarding French and En-
glish as source and target languages, respectively, but used
the resulted parallel corpus for both translation directions.
</p>
<p>12http://scikit-learn.org/
13We chose this number empirically through observing the
</p>
<p>classification accuracy on a set of held-out sentence pairs.
14http://opus.lingfil.uu.se/MultiUN.php
</p>
<p>394</p>
<p />
</div>
<div class="page"><p />
<p>System
Fr→En En→Fr #extracted speed
</p>
<p>BLEU #OOV BLEU #OOV pairs (#pairs/sec)
</p>
<p>Only general-domain phrase table 25.9 3,134 23.1 3,099 - -
</p>
<p>Baseline (Tillmann and Xu (2009)) 27.2 2,729 24.7 2,661 121k 1.22M
</p>
<p>Proposed method
(th = 0.955) 28.0 2,607 25.4 2,533 121k
</p>
<p>14.46M
(th = 0.700) 28.6 1,985 26.4 1,955 361k
</p>
<p>Proposed method
(th = 0.600) 26.1 3,064 23.2 3,077 11k 19.21M
</p>
<p>w/ cov. constraint
</p>
<p>Table 1: BLEU scores (Papineni et al., 2002) averaged over 3 tuning runs, obtained when added an
in-domain phrase table to the system, created either by the baseline method or by our work with or
without the coverage constraint activated (denoted “w/ cov. constraint”). Bold scores indicate statistical
significance (p &lt; 0.01) of the score over the baseline system, measured by approximate randomization
using MultEval (Clark et al., 2011). We also present the number of OOV tokens in the test set and the
number of sentence pairs actually used to train the in-domain phrase table. The speed of the method to
evaluate sentence pairs from monolingual data was measured with 100 CPU threads (Xeon E5-2600) on
1 trillion sentence pairs randomly sampled.
</p>
<p>old value. We examined {0.5, 0.6, 0.7, 0.8, 0.9}
as the threshold value through tuning PBSMT sys-
tems, and determined 0.7 to be optimal.
</p>
<p>We regarded the method proposed by Tillmann
and Xu (2009) as a baseline, because it does not
rely on document-level information, as ours. Un-
like our method, in addition to the constraint based
on length ratio, this method also used the cover-
age constraint. As discussed in Section 2.2, this
constraint speeds up the extraction, but sacrifices
source sentences with numerous OOV due to its
heavy reliance on a bilingual lexicon learned from
parallel data. To measure the effect of the cov-
erage constraint, we also activated it in some of
our experiments using our method. Then, as for
our method, we discarded sentence pairs having a
score lower than a threshold value and found the
threshold value of -10 to be the best among {-15,
-12, -10, -7}.
</p>
<p>3.3 Results
</p>
<p>Table 1 presents the results. Both the baseline and
our methods outperformed the system using only
the general-domain phrase table in both translation
directions. This may be explained by the pres-
ence of highly parallel sentences in the in-domain
monolingual data, from Wikipedia articles for in-
stance, that can be retrieved by both methods.
</p>
<p>Our method significantly outperformed the
baseline, with 1.4 and 1.7 BLEU points gains re-
spectively for Fr→En and En→Fr. Our method,
</p>
<p>with the optimal threshold of 0.7, extracted 361k
sentence pairs from the in-domain monolingual
data, while the baseline method extracted only
121k sentence pairs due presumably to the use of
the coverage constraint that might remove source
sentences with a high OOV ratio. Less OOV to-
kens remained with the system using our method,
highlighting the positive effect of exploiting word
embeddings in addition to lexical translation mod-
els. Activating the coverage constraint on our
method was harmful and was significantly worse
than the baseline. This constraint excludes can-
didate sentence pairs by relying only on general-
domain lexical translation models, while our clas-
sifier is trained to use word embeddings that are
more robust but unhelpful to discriminate the
remaining candidates. Therefore, the optimal
threshold value allowed the extraction of only 11k
sentence pairs. In contrast, without this constraint,
even with a high threshold value of 0.955 that
retrieved as many sentence pairs as the baseline
method, the extracted sentence pairs resulted in
a significantly higher BLEU score than the base-
line method, with a slightly better lexical cover-
age. Last but not least, our method is 11.9 times
faster than the baseline method.
</p>
<p>4 Feature contribution
</p>
<p>To evaluate the impact of the features used during
classification, we performed a feature ablation ex-
periment. The results for the EMEA translation
</p>
<p>395</p>
<p />
</div>
<div class="page"><p />
<p>Feature set th Fr→En En→Fr
all 0.7 28.6 26.4
</p>
<p>-avg. emb. 0.7 28.8 26.1
-max. al. emb. 0.7 29.0 26.1
</p>
<p>-max. al. emb. 0.8 28.4 25.6
-lex. prob. 0.8 28.3 26.0
-length 0.6 28.9 26.4
</p>
<p>Table 2: Results (BLEU) obtained without us-
ing some of the features during the classification
(see Section 2.2). The features removed, indepen-
dently, are the following: averaged word embed-
dings (avg. emb.), maximum alignment between
embeddings (max. al. emb.), lexical translation
probabilities (lex. prob.) and the length ratio of
the source and target sentences (length). The “th”
column indicates the threshold value for the clas-
sifier’s score above which we retain the sentence
pairs. This value was selected among the val-
ues {0.5,0.6,0.7,0.8,0.9}with respect to the BLEU
score on the development data, through the tuning
of the PBSMT system, for each configuration.
</p>
<p>task are reported in Table 2.
For both translation directions, the features that
</p>
<p>have the most important were the ones based on
lexical translaiton probabilities and alignments be-
tween embeddings. For instance, in En→Fr trans-
lation, removing them led to a significant drop of
0.4 and 0.8 BLEU points, respectively.
</p>
<p>For the Fr→En translation direction, surpris-
ingly, we observed improvements on the test set
for all configurations, except when removing ei-
ther of the above two types of features. However,
we did not observe such improvements for the
En→Fr translation direction; removing any fea-
ture(s) consistently led to a lower or equal BLEU
score. Feature ablation did not improve the perfor-
mance on the development set for both translation
directions, neither.
</p>
<p>5 Classifier accuracy
</p>
<p>To better understand the performance of our
method, we also evaluated the accuracy of the
classifier used in step 2 (see Section 2.2). Note
that this evaluation does not intend to show how
well the classifier retrieves useful pseudo-parallel
sentences. We cannot directly evaluate it, as we do
not have an evaluation data set that contains gold
pseudo-parallel sentences at hand.
</p>
<p>A set of in-domain truly parallel sentences was
used for our evaluation. We selected the 50k
first source sentences from the held-out in-domain
EMEA parallel corpus,15 and used each one of
them to make two sentence pairs in order to obtain
a positive and a negative example. For the positive
example, the source sentence is associated to its
correct translation from the EMEA corpus, while
for the negative example, we associated the source
sentence with a target sentence randomly extracted
from the EMEA corpus. The classifier has then to
decide if the sentence pair is correct or incorrect.
</p>
<p>The classifier is the same one that was presented
in Section 3.2 and trained on the MultiUn parallel
data. On our EMEA evaluation data set, this clas-
sifier achieves an accuracy of 85.98%. This high
accuracy highlights the potential of our method
in retrieving highly, or truly, parallel sentences if
such kinds of sentence pairs exist in the monolin-
gual data exploited by our approach.
</p>
<p>6 Conclusion and future work
</p>
<p>We presented a method for extracting pseudo-
parallel sentences from a pair of large monolingual
corpora, without relying on any document-level
information. Our domain adaptation experiments
showed that our method outperformed the state-of-
the-art method by more efficiently extracting more
useful sentence pairs from in-domain monolingual
data. In addition to the improved BLEU scores,
our method provides a better handling of OOV, ig-
nored by other methods that strongly rely on al-
ready trained lexical translation models.
</p>
<p>Our method can further be speeded up by some
approximation, such as local sensitive hashing, or
by using a smaller number of dimensions for word
embeddings. We leave the study of their impact
to our future work. We believe that our work is
also useful for other downstream tasks that need
comparable or pseudo-parallel sentences, such as
parallel phrase extraction (Hewavitharana and Vo-
gel, 2016) and adaptation of neural machine trans-
lation systems (Luong and Manning, 2015; Freitag
and Al-Onaizan, 2016).
</p>
<p>Acknowledgments
</p>
<p>We would like to thank all the reviewers for their
valuable comments and suggestions.
</p>
<p>15We used the EMEA training data provided by the same
workshop on domain adaptation (Carpuat et al., 2012) that re-
leased the development and test data used in our experiments.
</p>
<p>396</p>
<p />
</div>
<div class="page"><p />
<p>References
Sadaf Abdul Rauf and Holger Schwenk. 2011. Paral-
</p>
<p>lel sentence generation from comparable corpora for
improved smt. Machine Translation 25(4):341–375.
https://doi.org/10.1007/s10590-011-9114-9.
</p>
<p>Yosshi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2016. Fine-grained
analysis of sentence embeddings using auxiliary
prediction tasks. In Proceedings of ICLR 2017.
https://arxiv.org/pdf/1608.04207v3.pdf.
</p>
<p>Marine Carpuat, Hal Daumé III, Alexander Fraser,
Chris Quirk, Fabienne Braune, Ann Clifton, Ann
Irvine, Jagadeesh Jagarlamudi, John Morgan, Ma-
jid Razmara, Aleš Tamchyna, Katharine Henry,
and Rachel Rudinger. 2012. Domain adaptation
in machine translation: Final report. In 2012
Johns Hopkins Summer Workshop Final Report.
http://hal3.name/damt/.
</p>
<p>Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of ACL-HLT .
Portland, Oregon. http://aclweb.org/anthology/P11-
2031.
</p>
<p>Jocelyn Coulmance, Jean-Marc Marty, Guillaume
Wenzek, and Amine Benhalloum. 2015. Trans-
gram, fast cross-lingual word-embeddings. In
Proceedings of EMNLP. Lisbon, Portugal.
https://doi.org/10.18653/v1/D15-1131.
</p>
<p>Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven
Bird, and Trevor Cohn. 2016. Learning crosslin-
gual word embeddings without bilingual cor-
pora. In Proceedings of EMNLP. Austin, Texas.
http://aclweb.org/anthology/D16-1136.
</p>
<p>Markus Freitag and Yaser Al-Onaizan. 2016.
Fast domain adaptation for neural ma-
chine translation. CoRR abs/1612.06897.
https://arxiv.org/abs/1612.06897.
</p>
<p>Pascale Fung and Percy Cheung. 2004. Min-
ing very-non-parallel corpora: Parallel sentence
and lexicon extraction via bootstrapping and em.
In Proceedings of EMNLP. Barcelona, Spain.
http://aclweb.org/anthology/W04-3208.
</p>
<p>Ulrich Germann. 2001. Building a statistical ma-
chine translation system from scratch: How much
bang for the buck can we expect? In Pro-
ceedings of the ACL Workshop on Data-Driven
Methods in Machine Translation. Toulouse, France.
http://aclweb.org/anthology/W01-1409.
</p>
<p>Cyril Goutte, Marine Carpuat, and George Foster.
2012. The impact of sentence alignment er-
rors on phrase-based machine translation perfor-
mance. In Proceedings of AMTA. San Diego,
USA. http://www.mt-archive.info/AMTA-2012-
Goutte.pdf.
</p>
<p>Sanjika Hewavitharana and Stephan Vogel.
2016. Extracting parallel phrases from com-
parable data for machine translation. Nat-
ural Language Engineering 22(4):549–573.
https://doi.org/10.1017/S1351324916000139.
</p>
<p>Tomoyuki Kajiwara and Mamoru Komachi. 2016.
Building a monolingual parallel corpus for
text simplification using sentence similarity
based on alignment between word embed-
dings. In Proceedings of COLING. Osaka,
Japan. http://aclweb.org/anthology/C16-1109.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL. Prague, Czech Repub-
lic. http://aclweb.org/anthology/P07-2045.
</p>
<p>Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. CoRR
abs/1405.4053. https://arxiv.org/abs/1405.4053.
</p>
<p>Jiwei Li, Minh-Thang Luong, and Dan Jurafsky.
2015. A hierarchical neural autoencoder for para-
graphs and documents. CoRR abs/1506.01057.
https://arxiv.org/abs/1506.01057.
</p>
<p>Minh-Thang Luong and Christopher D. Manning.
2015. Stanford neural machine translation sys-
tems for spoken language domains. In Proceed-
ings of IWSLT . Da Nang, Vietnam. http://www.mt-
archive.info/15/IWSLT-2015-luong.pdf.
</p>
<p>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a. Exploiting similarities among languages
for machine translation. CoRR abs/1309.4168.
http://arxiv.org/abs/1309.4168.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In Proceedings of NIPS.
https://papers.nips.cc/paper/5021-distributed-
representations-of-words-and-phrases-and-their-
compositionality.pdf.
</p>
<p>Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics
31(4):477–504. http://aclweb.org/anthology/J05-
4003.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-Jing Zhu. 2002. Bleu: a method for
automatic evaluation of machine translation.
In Proceedings of ACL. Philadelphia, USA.
http://aclweb.org/anthology/P02-1040.
</p>
<p>Dan S, tefănescu, Radu Ion, and Sabine Hunsicker.
2012. Hybrid parallel sentence mining from
comparable corpora. In Proceedings of EAMT .
Trento, Italy. http://www.mt-archive.info/EAMT-
2012-Stefanescu.pdf.
</p>
<p>397</p>
<p />
</div>
<div class="page"><p />
<p>Christoph Tillmann and Jian-ming Xu. 2009. A simple
sentence-level extraction algorithm for comparable
data. In Proceedings of HLT-NAACL. Boulder, Col-
orado. http://aclweb.org/anthology/N09-2024.
</p>
<p>Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning japanese-english news articles
and sentences. In Proceedings of ACL. Sapporo,
Japan. http://aclweb.org/anthology/P03-1010.
</p>
<p>Ivan Vulić and Anna Korhonen. 2016. On the role
of seed lexicons in learning bilingual word embed-
dings. In Proceedings of ACL. Berlin, Germany.
http://aclweb.org/anthology/P16-1024.
</p>
<p>Bing Zhao and Stephan Vogel. 2002. Adap-
tive parallel sentences mining from web
bilingual news collection. In Proceed-
ings of IEEE ICDM. Maebashi, Japan.
http://dl.acm.org/citation.cfm?id=844380.844785.
</p>
<p>398</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 399–403
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2063
</p>
<p>Feature Hashing for Language and Dialect Identification
</p>
<p>Shervin Malmasi
Harvard Medical School, Boston, MA
</p>
<p>Macquarie University, Australia
shervin.malmasi@mq.edu.au
</p>
<p>Mark Dras
Macquarie University
</p>
<p>Sydney, Australia
mark.dras@mq.edu.au
</p>
<p>Abstract
</p>
<p>We evaluate feature hashing for language
identification (LID), a method not previ-
ously used for this task. Using a standard
dataset, we first show that while feature
performance is high, LID data is highly
dimensional and mostly sparse (&gt;99.5%)
as it includes large vocabularies for many
languages; memory requirements grow as
languages are added. Next we apply hash-
ing using various hash sizes, demonstrat-
ing that there is no performance loss with
dimensionality reductions of up to 86%.
We also show that using an ensemble of
low-dimension hash-based classifiers fur-
ther boosts performance. Feature hashing
is highly useful for LID and holds great
promise for future work in this area.
</p>
<p>1 Introduction
</p>
<p>Language Identification (LID) is the task of deter-
mining the language of a text, at the document,
sub-document or even sentence level. LID is a
fundamental preprocessing task in NLP and is also
used in lexicography, machine translation and in-
formation retrieval. It is widely used for filtering
to select documents in a specific language; e.g.
LID can filter webpages or tweets by language.
</p>
<p>Although LID has been widely studied, several
open issues remain (Hughes et al., 2006). Current
goals include developing models that can iden-
tify thousands of languages; extending the task to
more fine-grained dialect identification; and mak-
ing LID functionality more readily available to
users/developers. A common challenge among
these goals is dealing with high dimensional fea-
ture spaces. LID differs from traditional text cate-
gorization tasks in some important aspects. Stan-
dard tasks, such as topic classification, are usually
</p>
<p>performed within a single language, and the max-
imum feature space size is a function of the single
language’s vocabulary. However, LID must deal
with vocabulary from many languages and the fea-
ture space grows prodigiously.
</p>
<p>This raises immediate concerns about memory
requirements for such systems and portends im-
plementation issues for applying the systems to
dozens, hundreds or even thousands of languages.
Recent LID work has reported results on datasets
including over 1,300 languages (Brown, 2014), al-
beit using small samples. Such models are going
to include an extraordinarily large feature space,
and individual vectors for each sample are going
to be extremely sparse. LID is usually done using
n-grams and as the number of languages and/or
n gets larger, the feature space will become pro-
hibitively large or impractical for real-world use.
</p>
<p>For high dimensional input, traditional dimen-
sionality reduction methods (e.g. PCA, LDA) can
be computationally expensive. Feature selection
methods, e.g. those using entropy, are simpler but
still expensive. Recently, feature hashing has been
shown to be a very effective dimensionality re-
duction method (Weinberger et al., 2009). It has
proven to be useful in numerous machine learning
applications, particularly for handling extremely
high dimensional data. It also provides numerous
other benefits, which we describe in §2.1.
</p>
<p>Although hashing could be tremendously use-
ful for LID, to our knowledge no such experiments
have been reported to date. It is unclear how colli-
sions of features from different languages would
affect its application for LID. Accordingly, the
aims of the present work are to: (1) evaluate the
effectiveness of hashing for LID; (2) compare its
performance to the standard n-gram approach; (3)
assess the role of hash size (and collision rate) on
accuracy for different feature types; and (4) deter-
mine if ensemble methods can boost performance.
</p>
<p>399</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2063">https://doi.org/10.18653/v1/P17-2063</a></div>
</div>
<div class="page"><p />
<p>2 Related Work
2.1 Language and Dialect Identification
</p>
<p>Work in language identification (LID) dates back
to the seminal work of Beesley (1988), Dunning
(1994) and Cavnar and Trenkle (1994). Automatic
LID methods have since been widely used in NLP
research and applications. Recently, attention has
turned to discriminating between close languages,
such as Malay-Indonesian and Croatian-Serbian
(Ljubešić et al., 2007), or even dialects/varieties of
one language, e.g. Arabic dialects (Malmasi et al.,
2015). This has been the focus of the “Discrim-
inating Similar Language” (DSL) shared task se-
ries in recent years. In this work we use data from
the 2016 task (Malmasi et al., 2016).
</p>
<p>The 2016 task used data from 12 different lan-
guages/dialects. A training and development set
consisting of 20,000 sentences from each lan-
guage and an unlabelled test set of 1,000 sentences
per language was used for evaluation. Most par-
ticipants relied on multi-class discriminative clas-
sifiers trained with word unigrams and character
n-grams (Malmasi et al., 2016).
</p>
<p>2.2 Feature Hashing
</p>
<p>Feature hashing is a method for mapping a high-
dimensional input to a low-dimensional space us-
ing hashing (Weinberger et al., 2009). Hashing has
proven to be simple, efficient and effective. It has
been applied to various tasks including protein se-
quence classification (Caragea et al., 2012), senti-
ment analysis (Da Silva et al., 2014), and malware
detection (Jang et al., 2011).
</p>
<p>This method uses a hash function h(x) to arbi-
trarily map input to a hash key of a specified size.
The hash size, e.g. 218, determines the size of the
mapped feature space. Hash functions are many-
to-one mappings. Collision occur when distinct
inputs yield the same output, i.e. h(a) = h(b). The
collision rate is affected by the hash size. From
a learning perspective, collisions cause random
clustering of features and introduce noise; unre-
lated features map to the same vector index and
may degrade the learner’s accuracy. However, it
has been shown that “the interference between in-
dependently hashed subspaces is negligible with
high probability” (Weinberger et al., 2009).
</p>
<p>A positive by-product of hashing is that it elim-
inates the need for a feature dictionary. In NLP,
bag-of-words models require a full pass over the
data to identify the vocabulary for each feature
type (e.g. n-grams) and build a feature index.
</p>
<p>Eliminating this has many benefits: it simpli-
fies implementation of feature extraction meth-
ods, reduces memory overhead, and facilitates dis-
tributed computing. Global statistics, e.g. totals
and per-class feature counts, are usually required
for feature selection and dimensionality reduction
methods. Feature hashing may eliminate the need
for full processing of the data to calculate these.
</p>
<p>3 Data and Experimental Setup
Our methodology is based on the results of 2016
DSL Shared Task (Malmasi et al., 2016) and we
use their dataset. The DSL task is performed at
the sentence level, making it more challenging.
</p>
<p>3.1 Data
A key shortcoming in LID research has been
the absence of a common dataset for evaluation
(Hughes et al., 2006), a need that has been met
by the corpora released as part of the DSL shared
task series. We use the DSLCC 3.0 corpus from
the 2016 DSL task.1 This allows us to compare
our findings to that of the 17 participants. Using a
standard, publicly available dataset also facilitates
replicability of our results. The 2016 task used
data from 12 different languages and varieties,2
</p>
<p>including training/development sets composed of
20,000 sentences per language. An unlabelled test
set of 1,000 sentences per language was used for
evaluation. The total sentences for training and
testing are 240k and 12k, respectively. We report
our results on the standard test set.
</p>
<p>3.2 Classifier and Evaluation
Participants applied various methods, but the task
organizers note that linear classifiers, particularly
SVMs, were the most successful (Malmasi et al.,
2016). This is unsurprising as SVMs have been
very successful for text classification and we adopt
this method. The data is balanced across classes,
so accuracy is used as the evaluation metric.
</p>
<p>3.3 Features
Most DSL entries use surface features, with words
and high-order character n-grams being particu-
larly successful. We apply character n-grams of
order 1–6 (CH1-6) and word unigrams (WD1).
1http://ttg.uni-saarland.de/resources/DSLCC/
2Bosnian (BS), Argentine Spanish (ES AR), Peninsular
Spanish (ES ES), Mexican Spanish (ES MX), Canadian
French (FR CA), Hexagonal French (FR FR), Croatian
(HR), Indonesian (ID), Malay (MY), Brazilian Portuguese
(PT BR), European Portuguese (PT PT) and Serbian (SR).
</p>
<p>400</p>
<p />
</div>
<div class="page"><p />
<p>4 Feature Performance &amp; Dimensionality
In our first experiment we examine the feature
space in our dataset and establish the memory re-
quirements and accuracy of the feature types we
use (character 1–6 n-grams and word unigrams).
</p>
<p>For each feature type we extract the training
vectors and use it to train a linear SVM model
which is used to classify the standard test set. We
report the feature’s accuracy on the test set along
with some statistics about the data: the number of
features in the training data, the number of out-of-
vocabulary (OOV) features3 in the test data, and
the sparsity4 of the training data matrix. These re-
sults are shown in Figure 1.
</p>
<p>CH1 CH2 CH3 CH4 CH5 CH6 WD1
Feat Count 272 6.1k 61.5k 358.3k 1.4m 3.7m 443.9k
</p>
<p>Test OOV 7 57 0.7k 5.5k 26.0k 84.6k 10.7k
</p>
<p>Sparsity (%) 88.17 98.04 99.7 99.94 99.98 99.99 99.99
</p>
<p>Test Acc. (%) 64.49 77.49 85.77 88.21 89.09 89.2 87.93
</p>
<p>0%
</p>
<p>20%
</p>
<p>40%
</p>
<p>60%
</p>
<p>80%
</p>
<p>100%
</p>
<p>1
10
100
</p>
<p>1,000
10,000
</p>
<p>100,000
1,000,000
</p>
<p>10,000,000
</p>
<p>Fe
at
ur
e 
Co
</p>
<p>un
ts
</p>
<p>Figure 1: Details of our feature spaces. Train-
ing set feature counts and test set OOV counts are
shown as bars (left axis, logarithmic scale). Train-
ing data sparsity and test set accuracy are shown
as lines (in %, right axis).
</p>
<p>These results reveal a number of interesting
patterns. Character n-grams perform better than
word unigrams. The winner of the 2016 DSL task
combined various character n-grams into an SVM
model to obtain 89.4% accuracy on the test set.
We obtain the best result of 89.2% using charac-
ter 6-grams alone. The character n-gram feature
grow rapidly, from 61k trigrams to 3.7m 6-grams.
While accuracy plateaus at 6-grams, there is only
a 1% improvement from 4- to 6-grams, but a huge
feature space increase. The number of OOV test
features also increases, but is relatively small.
</p>
<p>The sparsity analysis is also revealing, showing
that for many features only around 0.1% of the
training matrix contains non-zero values. We can
expect that this sparsity will rapidly grow with the
addition of more classes to the dataset. This poses
a huge problem for practical applications of LID
for discriminating large number of languages.
</p>
<p>In this regard, we can also assess how the di-
mensionality cumulatively increases as languages
3i.e. features present in the test set but not the training set.
4Matrix sparsity is the proportion of zero-valued elements.
</p>
<p>are added, shown in Figure 2. Features increase
even as similar languages are added; we expect
this trend to continue if more classes are added.
</p>
<p>0
0.5
1
1.5
2
2.5
3
3.5
4
</p>
<p>0
</p>
<p>100000
</p>
<p>200000
</p>
<p>300000
</p>
<p>400000
</p>
<p>500000
</p>
<p>BS
</p>
<p>ES
_A
</p>
<p>R
</p>
<p>ES
_E
S
</p>
<p>ES
_M
</p>
<p>X
</p>
<p>FR
_C
</p>
<p>A
</p>
<p>FR
_F
R HR ID M
Y
</p>
<p>PT
_B
</p>
<p>R
</p>
<p>PT
_P
T SR
</p>
<p>Ch
ar
 n
‐g
r a
m
s (
in
 m
</p>
<p>ill
io
ns
)
</p>
<p>W
or
d 
U
ni
gr
am
</p>
<p> C
ou
</p>
<p>nt
</p>
<p>WORD1 CHAR4 CHAR5 CHAR6
</p>
<p>Figure 2: Feature growth rate as classes are added.
Words (bars, left axis) and character n-grams
(lines, right axis) grow as languages are added.
</p>
<p>5 Feature Hashing Performance
Having established baseline performance for our
features using the standard approach, we now
experiment with applying feature hashing to the
same data in order to evaluate its effectiveness for
this task and compare it to the standard approach.
</p>
<p>We also assess the effect of hash size on the fea-
ture collision rate, and in turn, on classification
accuracy. To do this we test each feature5 using
hash sizes in the range 210 (1024) to 222 (2.1m)
features, which covers most of our feature types.
Our hash function is implemented using the signed
32-bit version of MurmurHash3.6
</p>
<p>We report each feature’s accuracy at every hash
size, with the smallest hash that yields maximal
accuracy considered to be the best result. Each
feature is compared against its performance using
the full feature space (baseline). These results,
along with the reduction in the feature space for
the best results, are listed in Table 1.
</p>
<p>Our first observation is that every feature
matches baseline performance at a hash size
smaller than its full feature space. This demon-
strates that feature hashing is useful for LID.
</p>
<p>We can also assess the effect of feature colli-
sions using the results, which we plot in Figure 3.
We note that at the same hash size, features with a
larger space perform worse. That is, with a 212
</p>
<p>hash size, CHAR4 outperforms 5- and 6-grams.
This is evidence of performance degradation due
to hash collision. However, we see that when us-
ing an appropriately sized hash, feature collisions
between languages do not degrade performance.
5Except character unigrams which only have 272 features.
6https://github.com/aappleby/smhasher
</p>
<p>401</p>
<p />
</div>
<div class="page"><p />
<p>Hash Size
</p>
<p>Feature Baseline 210 211 212 213 214 215 216 217 218 219 220 221 222 ∆ feats
</p>
<p>CHAR2 0.77 0.74 0.76 0.77 0.77 0.77 0.77 0.77 0.77 0.77 0.77 0.77 0.77 0.77 -33%
</p>
<p>CHAR3 0.86 0.74 0.79 0.82 0.84 0.85 0.86 0.86 0.86 0.86 0.86 0.86 0.86 0.86 -47%
</p>
<p>CHAR4 0.88 0.70 0.76 0.80 0.83 0.86 0.86 0.88 0.88 0.88 0.88 0.88 0.88 0.88 -82%
</p>
<p>CHAR5 0.89 0.65 0.72 0.77 0.81 0.84 0.86 0.87 0.88 0.89 0.89 0.89 0.89 0.89 -81%
</p>
<p>CHAR6 0.89 0.58 0.67 0.73 0.78 0.82 0.84 0.87 0.87 0.88 0.89 0.89 0.89 0.89 -86%
</p>
<p>WORD1 0.88 0.70 0.74 0.78 0.81 0.83 0.85 0.86 0.87 0.88 0.88 0.88 0.88 0.88 -41%
</p>
<p>Table 1: Test set accuracy for hashed features at each hash size. Baseline is accuracy without hashing.
Best result (w/ smallest hash) per row in bold. Last column is the best result’s reduction in dimensionality.
We observe that every feature matches its baseline at a hash size smaller than its full feature space.
</p>
<p>0.6
0.62
0.64
0.66
0.68
</p>
<p>0.7
0.72
0.74
0.76
0.78
</p>
<p>0.8
0.82
0.84
0.86
0.88
</p>
<p>0.9
</p>
<p>10 11 12 13 14 15 16 17 18 19 20 21 22
Hash Size Exponent
</p>
<p>CHAR4
CHAR5
CHAR6
</p>
<p>Figure 3: Performance (left axis) of 3 features at
different hash sizes. Higher order n-grams per-
form worse at lower hash sizes due to collisions.
</p>
<p>We also analyze the memory reduction achieved
via hashing by calculating the relative difference
in dimensionality between the best result and the
full feature set, listed in the last column of Table 1.
We see very significant reductions of up to 86%
in dimensionality without any performance loss.
Character 4-grams yield very competitive results
(0.88) with a large feature space reduction of 82%
using a hash size of 216.
</p>
<p>6 Hashing-based Ensemble Classifier
</p>
<p>Ensemble classifiers combine multiple learners
with the aim of improving accuracy through en-
hanced decision making. They have been ap-
plied to many tasks and shown to achieve bet-
ter results compared to single-classifier methods
(Oza and Tumer, 2008). By aggregating the out-
puts of multiple classifiers their outputs are gen-
erally considered to be more robust. Ensembles
have been successfully used for LID, e.g. winning
</p>
<p>the 2015 task (Malmasi and Dras, 2015a). They
also achieve state-of-the-art performance for Na-
tive Language Identification (Malmasi and Dras,
2017). Could an ensemble composed of low-
dimension hash-based classifiers achieve compet-
itive performance?
</p>
<p>In order to assess this we created an ensemble
of our features with a hash size of 216. Eval-
uating against the test set, a hard voting ensem-
ble achieved 88.7% accuracy while a probability-
based combination obtained 89.2%. Comparing to
the winning shared task accuracy of 89.4%, this is
an excellent result given that only 65,536 features
were used by our system. Ensemble combination
boosted our best single-model 216 hash size result
by 1.1%. This highlights the utility of ensemble
methods for hashing-based feature spaces. It also
shows that model combination can compensate for
small performance losses caused by hashing.
</p>
<p>7 Discussion and Conclusion
</p>
<p>We presented the first application of feature hash-
ing for language identification. Results show that
hashing is highly effective for LID and can amelio-
rate the dimensionality issues that can impose pro-
hibitive memory requirements for some LID tasks.
We further showed that reduced feature spaces
with as few as 65k features can be combined in en-
semble classifiers to boost performance. We also
demonstrated the effect of hash collision on accu-
racy, and outlined the type of analysis needed to
choose the correct hash size for a given feature.
</p>
<p>Hashing provided dimensionality reductions of
up to 86% without performance degradation. This
is impressive considering that no feature selection
or analysis was performed, making it highly effi-
</p>
<p>402</p>
<p />
</div>
<div class="page"><p />
<p>cient. This reduction facilitates model loading and
training as we also showed that LID data is ex-
tremely sparse, with over 99% of our training ma-
trix cells containing zeros. This is particularly use-
ful for limited memory scenarios (e.g. handheld or
embedded devices). It may also enable the use
of methods requiring dense data representations,
something often infeasible for large datasets.
</p>
<p>Another key advantage of hashing is that it
eliminates the need for maintaining a feature dic-
tionary, making it easy to develop feature extrac-
tion modules. This greatly simplifies paralleliza-
tion, lending itself to online learning and dis-
tributed systems, which are important issues for
LID systems in our experience.
</p>
<p>Hashing also holds promise for facilitating the
use of deep learning methods for LID. In the 2016
DSL task, such systems performed “poorly com-
pared to traditional classifiers”; participants cited
“memory requirements and long training times”
spanning several days (Malmasi et al., 2016). Fea-
ture hashing has recently been used to compress
neural networks (Chen et al., 2015) and its appli-
cation for deep learning-based text classification
may provide insightful results.
</p>
<p>There are also downsides to hashing, including
the inability to interpret feature weights and model
parameters, and some minor performance loss.
</p>
<p>Future work in this area includes evaluation on
larger datasets, as well as cross-corpus experi-
ments, which may also be insightful. The ap-
plication of these methods to other text classifi-
cation tasks, particularly those dealing with lan-
guage varieties such as Native Language Identifi-
cation (Malmasi and Dras, 2015b), could also pro-
vide a deeper understanding about how they work.
</p>
<p>References
Kenneth R Beesley. 1988. Language identifier: A com-
</p>
<p>puter program for automatic natural-language iden-
tification of on-line text. In Proceedings of the 29th
Annual Conference of the American Translators As-
sociation. page 54.
</p>
<p>Ralf D Brown. 2014. Non-linear Mapping for Im-
proved Identification of 1300+ Languages. In
EMNLP.
</p>
<p>Cornelia Caragea, Adrian Silvescu, and Prasenjit Mi-
tra. 2012. Protein sequence classification using fea-
ture hashing. Proteome science 10(1):S14.
</p>
<p>William B. Cavnar and John M. Trenkle. 1994. N-
Gram-Based Text Categorization. In Proceedings of
SDAIR-94. Las Vegas, US, pages 161–175.
</p>
<p>Wenlin Chen, James T Wilson, Stephen Tyree, Kilian Q
Weinberger, and Yixin Chen. 2015. Compressing
neural networks with the hashing trick. In ICML.
pages 2285–2294.
</p>
<p>Nadia FF Da Silva, Eduardo R Hruschka, and Este-
vam R Hruschka. 2014. Tweet sentiment analysis
with classifier ensembles. Decision Support Systems
66:170–179.
</p>
<p>Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
</p>
<p>Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources .
</p>
<p>Jiyong Jang, David Brumley, and Shobha Venkatara-
man. 2011. Bitshred: feature hashing malware for
scalable triage and semantic analysis. In Proceed-
ings of the 18th ACM conference on Computer and
communications security. ACM, pages 309–320.
</p>
<p>Nikola Ljubešić, Nives Mikelić, and Damir Boras.
2007. Language indentification: How to distinguish
similar languages? In Information Technology In-
terfaces. pages 541–546.
</p>
<p>Shervin Malmasi and Mark Dras. 2015a. Language
Identification using Classifier Ensembles. In Pro-
ceedings of LT4VarDial 2015.
</p>
<p>Shervin Malmasi and Mark Dras. 2015b. Multilingual
Native Language Identification. In Natural Lan-
guage Engineering.
</p>
<p>Shervin Malmasi and Mark Dras. 2017. Native Lan-
guage Identification using Stacked Generalization.
arXiv preprint arXiv:1703.06541 .
</p>
<p>Shervin Malmasi, Eshrag Refaee, and Mark Dras.
2015. Arabic Dialect Identification using a Parallel
Multidialectal Corpus. In Proceedings of PACLING
2015. pages 209–217.
</p>
<p>Shervin Malmasi, Marcos Zampieri, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, and Jörg Tiedemann.
2016. Discriminating between Similar Languages
and Arabic Dialect Identification: A Report on the
Third DSL Shared Task. In Proceedings of the 3rd
Workshop on Language Technology for Closely Re-
lated Languages, Varieties and Dialects (VarDial).
Osaka, Japan, pages 1–14.
</p>
<p>Nikunj C Oza and Kagan Tumer. 2008. Classifier en-
sembles: Select real-world applications. Informa-
tion Fusion 9(1):4–20.
</p>
<p>Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In Pro-
ceedings of the 26th Annual International Confer-
ence on Machine Learning. ACM, pages 1113–
1120.
</p>
<p>403</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 404–410
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2064
</p>
<p>Detection of Chinese Word Usage Errors for Non-Native Chinese
Learners with Bidirectional LSTM
</p>
<p>Yow-Ting Shiue, Hen-Hsen Huang and Hsin-Hsi Chen
Department of Computer Science and Information Engineering
</p>
<p>National Taiwan University
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan
</p>
<p>orina1123@gmail.com,hhhuang@nlg.csie.ntu.edu.tw,hhchen@ntu.edu.tw
</p>
<p>Abstract
</p>
<p>Selecting appropriate words to compose a
sentence is one common problem faced by
non-native Chinese learners. In this pa-
per, we propose (bidirectional) LSTM se-
quence labeling models and explore vari-
ous features to detect word usage errors in
Chinese sentences. By combining CWIN-
DOW word embedding features and POS
information, the best bidirectional LSTM
model achieves accuracy 0.5138 and MRR
0.6789 on the HSK dataset. For 80.79% of
the test data, the model ranks the ground-
truth within the top two at position level.
</p>
<p>1 Introduction
</p>
<p>Recently, more and more people around the world
choose Chinese as their second language. That re-
sults in an increasing need for automatic grammat-
ical error detection and correction (GEC) tools.
To measure the performance of GEC systems in
a standardized manner, several shared tasks have
been conducted for English (Dale and Kilgarriff,
2011; Dale et al., 2012; Ng et al., 2013, 2014) and
Chinese (Yu et al., 2014; Lee et al., 2015, 2016).
</p>
<p>In Chinese sentences, a word usage error
(WUE) is a grammatically or semantically incor-
rect token which is written in a wrong form itself,
or is an existent word but is improper for its con-
text (refer to example (E1)). In fact, many Chi-
nese WUEs result from subtle semantic unsuitabil-
ity instead of violation of syntactic constraints. In
example (E1), both權力 (power) and權利 (right)
are nouns in Chinese, and both versions are gram-
matically correct. It is difficult to formulate an ex-
plicit rule for recognizing this kind of errors.
</p>
<p>(E1)人們有 (*權力,權利)吃安全的食品。
( People have the (*power, right) to enjoy safe
food. )
</p>
<p>Shiue and Chen (2016) adopted the HSK cor-
pus, a dynamic composition corpus built by Bei-
jing Language and Culture University, to study the
detection of WUEs. Instead of specific position in-
formation, their model only determines whether a
sentence segment contains WUEs. Huang et al.
(2016) used the HSK corpus to study the prepo-
sition selection problem. They proposed gated
recurrent unit (GRU)-based models to select the
most suitable one from a closed set of Chinese
prepositions given the sentential context. Al-
though their approach can be utilized to detect and
correct preposition errors, it is still worth inves-
tigating how to recognize WUEs involving other
types of words such as verbs and nouns.
</p>
<p>In the past few years, distributed word rep-
resentations derived from neural network mod-
els (Mikolov et al., 2013a; Pennington et al.,
2014) have become popular among various stud-
ies in natural language processing. Beyond sur-
face forms, these low-dimensional vector repre-
sentations can encode syntactic and semantic in-
formation implicitly (Mikolov et al., 2013b). Be-
cause WUEs involve syntactic or semantic prob-
lems, vector representations could be promising
for finding the erroneous tokens.
</p>
<p>One challenging aspect of dealing with gram-
matical errors is that the errors usually do not stand
on their own, but are dependent on the context
(Chollampatt et al., 2016). Therefore, we need a
model that considers the sequence of words in a
sentence as a whole to determine which position
needs correction. One possible model for this task
is the Long Short-Term Memory (LSTM) model
(Hochreiter and Schmidhuber, 1997), which pro-
cesses sequential data and generates the output
based not only on the information of the current
time step, but also on the past information stored
in the memory layer. Rei and Yannakoudakis
(2016) adopted neural network models, including
</p>
<p>404</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2064">https://doi.org/10.18653/v1/P17-2064</a></div>
</div>
<div class="page"><p />
<p>LSTM, to detect errors in English learner writ-
ing. However, they mainly focused on compar-
ing different composition architectures under the
same word representation, so it remained unclear
to what extent pre-trained word embeddings can
help. Huang and Wang (2016) used LSTM for
Chinese grammatical error diagnosis, but their
models are trained only on learner data, without
external well-formed text. That means the per-
formance might be limited by the relatively small
amount of annotated sentences written by foreign
learners.
</p>
<p>This paper utilizes LSTM and its extension
(Bidirectional LSTM) along with the information
derived from external resources to deal with Chi-
nese WUE detection. Several types of pre-trained
word embeddings and additional token-level fea-
tures are considered. Each token in a sentence will
be labeled correct or incorrect. Experimental re-
sults show that our models can rank the ground-
truth error position toward the top of the candidate
list.
</p>
<p>2 WUE Detection Based on Bidirectional
LSTM
</p>
<p>We formulate the Chinese WUE detection task as
a sequence labeling problem. Each token, the fun-
damental unit after word segmentation, is labeled
either correct (0) or incorrect (1).
</p>
<p>We utilize the LSTM model for labeling. LSTM
models long sequences better than simple re-
current neural network (RNN) does, since it is
equipped with input, output and forget gates to
control how much information is used. The ability
of LSTM to capture longer dependencies among
time steps makes it suitable for modeling the com-
plex dependencies of the erroneous token on the
other parts of the sentence.
</p>
<p>We train the LSTM model with the Adam op-
timizer (Kingma and Ba, 2014) implemented in
Keras (Chollet, 2015). The loss function is bi-
nary cross entropy. The batch size and the initial
learning rate is set to 32 and 0.001 respectively.
The training process is stopped when the valida-
tion accuracy does not increase for two consecu-
tive epochs. The model with the highest validation
accuracy is selected as the final model.
</p>
<p>We apply a sigmoid activation function before
the output layer, so the output score of each token,
which is between 0 and 1, can be interpreted as the
predicted level of incorrectness. With these scores,
</p>
<p>our system can output a ranked list of candidate
error positions. The positions with the highest in-
correctness scores will be marked as incorrect. In
(E2) we show an example labeling result of our
system. The tokens 差 (bad) and 知識 (knowl-
edge), with the highest scores, are most likely to
be incorrect.
</p>
<p>(E2) 學習 的 知知知識識識 也 很 差差差
0.056 0.035 0.153 0.039 0.030 0.429
</p>
<p>( The knowledge learned is also very bad. )
Bidirectional LSTM (Schuster and Paliwal,
</p>
<p>1997) is an extension of LSTM which includes a
backward LSTM layer. Both information before
and after the current time step are taken into con-
sideration. We need the “future” information to
detect the error in example (E3). The incorrect-
ness of the token 留在 (left at) cannot be deter-
mined without considering its object我們 (us).
</p>
<p>(E3)店是爸爸 (*留在,留給)我們的。
( The store is our father left (*at,to) us. )
</p>
<p>3 Sequence Embedding Features
</p>
<p>We consider the word sequence in a sentence
and the corresponding POS tag sequence. They
are mapped to sequences of real-valued vectors
through an embedding layer. These vectors are
also updated during the training process.
</p>
<p>3.1 Word Embeddings
We set the word embedding size to 400. Besides
randomly initialized embedding, we also tried sev-
eral types of pre-trained word vectors. To train
the word embeddings, we utilize the Chinese part
of the ClueWeb09 dataset1. The Chinese part was
extracted and segmented by Yu et al. (2012).
</p>
<p>CBOW/Skip-gram Word Embeddings
We trained word vectors with the two architec-
tures included in the word2vec software (Mikolov
et al., 2013a). The continuous bag-of-words
model (CBOW) uses the words in a context win-
dow to predict the target word, while the skip-
gram model (SG) uses the target word to predict
every word in the context window.
</p>
<p>CWINDOW/Structured Skip-gram Word
Embeddings
Taking the order of the context words into con-
sideration, we also employ the continuous win-
dow model (CWIN) and the structured skip-gram
</p>
<p>1http://lemurproject.org/clueweb09.php
</p>
<p>405</p>
<p />
</div>
<div class="page"><p />
<p>model (Struct-SG) (Ling et al., 2015). The former
replaces the summation of context word vectors
in CBOW with a concatenation operation, and the
latter applies different projection matrices for pre-
dicting context words in different relative position
with the target word.
</p>
<p>3.2 POS Embeddings
The POS embeddings are randomly initialized.
We set the embedding size to 20, which is slightly
smaller than the number of different POS tags (30)
in our dataset.
</p>
<p>4 Token Features
</p>
<p>In addition to representing each token as a real-
valued vector, we also incorporate some abstract
features. These features are derived from the
Google Chinese Web 5-gram corpus (Liu et al.,
2010) and will be referred to as “n-gram features”.
</p>
<p>4.1 Out-of-Vocabulary Indicator
This feature is simply a bit indicating whether a
word is an out-of-vocabulary word or not. If a to-
ken never appears in the Web 5-gram corpus, the
bit is set to 1; otherwise it is set to 0.
</p>
<p>4.2 N-gram Probability Features
We compute the n-gram probability of each token
using the occurrence count in the Web 5-gram cor-
pus. We consider only up to trigrams since the
probabilities are mostly zero when n &gt; 3. Given
the limited amount of available learner data, these
probabilities may serve as useful features indicat-
ing how likely an expression is valid in Chinese.
</p>
<p>5 Experiments
</p>
<p>5.1 Dataset
We obtain the “wrong” part of the HSK dataset
used in (Shiue and Chen, 2016). Each sentence
segment has exactly one token-level position that
is erroneous. Word segmentation and POS tagging
are performed with the Stanford CoreNLP toolkit
(Manning et al., 2014). We filter out any sentence
segment whose corrected version differs from it
by more than one token due to segmentation is-
sue. That is, we only focus on the cases in which
the error can be corrected by replacing one single
token. After filtering, we end up with 10,510 sen-
tence segments. We use 10% data for validation
and testing respectively, and the remaining 80%
data as the training set.
</p>
<p>5.2 Evaluation
Accuracy
We use the detection accuracy as our main eval-
uation metric. A test instance is regarded as cor-
rect only if our system gives the highest score of
incorrectness for the ground-truth position. This
metric is relatively strict as the average length of
the sentence segments in our dataset is 9.24. The
McNemar’s test is adopted to perform statistical
significance test.
</p>
<p>Mean Reciprocal Rank (MRR)
The mean reciprocal rank rewards the test in-
stances for which the model ranks the ground-truth
near the top of the candidate list. MRR is defined
as 1N
</p>
<p>∑N
i=1
</p>
<p>1
rank(i) , where N is the total number
</p>
<p>of test instances and rank(i) is the rank of the
ground-truth position of test instance i.
</p>
<p>Hit@k Rate
The Hit@k rate regards a test instance as correct
if the answer is ranked within the top k places. In
the experiments, k is set to 2. We report this met-
ric since one of the most common types of WUEs
is collocation error. In example (E2), the prob-
lem involves a pair of words, i.e., the adjective差
(bad) is not a suitable modifier of the noun 知識
(knowledge). (E4) and (E5) are both acceptable.
</p>
<p>(E4)學習的知識也很不不不足足足
( The knowledge learned is also insufficient. )
(E5)學習的態態態度度度也很差
( The attitude of learning is also very bad. )
Which correction is better highly depends on
</p>
<p>the context or even the intended meaning in the
writer’s mind. If the model proposes two poten-
tially erroneous tokens which are closely related
to each other, it can be useful for Chinese learners.
</p>
<p>Hit@r% Rate
Finding the exact position of the error could be
more challenging in a longer sentence segment.
We propose another hit rate measure which takes
the segment length (len) into account. Specif-
ically, we regard one test instance as correct if
the answer is ranked within the top max(1, blen ∗
r%c) candidates. We report hit@20%. That is, for
segments shorter than 10 tokens, the system is al-
lowed to propose one candidate; for those whose
length is between 10 and 14, the system is al-
lowed to propose two, and so on. Equivalently,
this measure judges whether our system can rank
the ground-truth error position within the top 20%
</p>
<p>406</p>
<p />
</div>
<div class="page"><p />
<p>Model Features Accuracy MRR Hit@2 Hit@20%
Random Baseline - 0.1239 0.3312 0.2478 0.1611
</p>
<p>LSTM
</p>
<p>Rand. Init. Word Embedding 0.4186 0.6010 0.7222 0.6565
CBOW 0.4072 0.5923 0.7155 0.6432
CBOW + POS 0.4263 0.6150 0.7564 0.6908
CBOW + POS + n-gram 0.4386 0.6204 0.7526 0.6755
SG 0.4072 0.5910 0.7146 0.6365
SG + POS 0.4301 0.6170 0.7593 0.6965
SG + POS + n-gram 0.4386 0.6205 0.7507 0.6755
CWIN 0.4853 0.6537 0.7774 0.7031
CWIN + POS 0.4681 0.6435 0.7783 0.7022
CWIN + POS + n-gram 0.4700 0.6502 0.7945 0.7269
Struct-SG 0.4710 0.6412 0.7650 0.6889
Struct-SG + POS 0.4757 0.6441 0.7593 0.6822
Struct-SG + POS + n-gram 0.4881 0.6577 0.7840 0.7184
</p>
<p>Bi-LSTM
</p>
<p>CWIN 0.4795 0.6547 0.7840 0.7174
CWIN + POS 0.5138 0.6789 0.8097 0.7479
CWIN + POS + n-gram 0.4948 0.6719 0.8173 0.7507
Struct-SG 0.4710 0.6412 0.7650 0.6889
Struct-SG + POS 0.4757 0.6441 0.7593 0.6822
Struct-SG + POS + n-gram 0.4948 0.6658 0.8040 0.7374
</p>
<p>Table 1: Performance of the LSTM/Bi-LSTM sequence labeling models with different sets of features.
</p>
<p>of the candidate list. This metric compromises Ac-
curacy and Hit@k.
</p>
<p>6 Results and Analysis
</p>
<p>Table 1 shows the performance of our WUE de-
tection models with different input features. The
random baseline is a system randomly choosing
one token as the incorrect position. The LSTM
model using only randomly initialized word em-
beddings largely outperforms the random base-
line. The pre-trained CBOW/SG word embed-
dings seem not very useful, leading to detection
performance slightly lower than the model with
random initial word embeddings. For both CBOW
and SG, introducing the POS sequence improves
the detection accuracy by about 2% and also im-
proves all other measurements. The n-gram fea-
tures further increase the accuracy by about 1%.
</p>
<p>On the other hand, the CWIN and Struct-SG
embeddings themselves are very powerful. In-
corporating the POS and n-gram features leads
to only slight improvements in terms of accu-
racy. Despite the small impact on accuracy, the
n-gram features bring obvious improvements on
hit@2 and hit@20% rates, indicating that they
do facilitate the model in promoting the rank of
the ground-truth position. Under the same set of
features, all models with CWIN/Struct-SG signif-
icantly outperform their CBOW/SG counterparts
(p &lt; 0.05).
</p>
<p>Bidirectional LSTM (Bi-LSTM) further en-
hance the performance of LSTM. The Bi-LSTM
</p>
<p>with CWIN+POS features achieves the best accu-
racy and MRR, and significantly outperforms its
LSTM counterpart (p &lt; 0.005). Bi-LSTM with
CWIN+POS+n-gram features achieves the best
Hit@2 and Hit@20%. To take a closer look, we
analyze the performance of the two types of mod-
els on different length of segments in Table 2. We
use the versions with all set of features and report
hit@20% rates. Using Bi-LSTM leads to some
improvement on short (≤ 9 tokens) segments, and
larger improvement on mid-length (10~14 tokens)
ones. Even longer (≥ 15 tokens) segments are
relatively rare since foreign learners seldom con-
struct complex sentences.
</p>
<p>In Section 5.2 we justify the use of the hit@2
metric by pointing out that a WUE usually in-
volves a pair of words dependent on each other.
We can verify whether the top two candidates pro-
posed by our model are closely related by exam-
ining the dependency distance. We take the out-
put of the Bi-LSTM model with CWIN+POS+n-
gram features and analyze the error cases where
the model ranks the ground-truth error position
second. We use the dependency parsing output of
CoreNLP to construct an undirected graph, where
</p>
<p>Length (# tests) # proposed LSTM Bi-LSTM
≤ 9 (645) 1 0.7426 0.7659
</p>
<p>10~14 (317) 2 0.6908 0.7319
≥ 15 (89) ≥ 3 0.7416 0.7079
</p>
<p>Table 2: Hit@20% rates of LSTM and Bi-LSTM
on segments with different lengths.
</p>
<p>407</p>
<p />
</div>
<div class="page"><p />
<p># correct (c1 = a) 520 (49.48%)
# tests where c2 = a 339 (32.25%)
Average dis(c1, c2) when c2 = a 2.07
# tests where c2 = a and dis(c1, c2) = 1 129 (12.27%)
</p>
<p>Table 3: Summary of the analysis of the depen-
dency between the top two candidates proposed by
the CWIN+POS+n-gram Bi-LSTM model. a de-
notes the ground-truth error position. c1 and c2
denote the first and the second candidate positions
proposed by the model. dis(c1, c2) is the distance
between c1 and c2 on the dependency graph.
</p>
<p>POS (# tests) CWIN CWIN+POS
VV (325) 0.8123 0.8185
NN (282) 0.6879 0.7447
AD (134) 0.6194 0.7015
</p>
<p>Table 4: Hit@20% rates of Bi-LSTM models
with or without POS features on three most fre-
quent POS tags of the erroneous token.
</p>
<p>each dependency corresponds to an edge, and cal-
culate the shortest distance between the top two
candidates in these cases. The results are summa-
rized in Table 3. The average distance (2.07) is
small compared to the average length of the seg-
ments (9.24), indicating that our model can con-
sider the dependencies among words when rank-
ing the candidate positions.
</p>
<p>A factor that might limit the effectiveness of
POS features is that the POS tagger trained on
well-formed text may not perform well on noisy
learner data. In fact, for 26.7% of the test data,
the POS tag of the original erroneous token dif-
fers from that of its corrected version. We com-
pare the performance of the model with or without
POS features on three most frequent POS tags in
Table 4. As can be seen, the POS information of
the erroneous segment, which potentially contains
errors, can still be helpful for detecting anomaly
of the segment. In example (E6) we show the
scores of incorrectness predicted by models with
or without POS features. The ”DEC + AD” con-
struction is invalid in Chinese, so in this case the
error can be detected more easily if POS informa-
tion is available.
</p>
<p>(E6) 應該 有 別人 的 *盡盡盡力力力
POS tag VV VE NN DEC AD
</p>
<p>w/o POS 0.048 0.226 0.030 0.016 0.042
w/ POS 0.010 0.066 0.031 0.071 0.077
</p>
<p>( There should be someone else’s *utmost. )
</p>
<p>Word Error rate Precision Recall
產生 (generate) 0.571 (8/14) 0.700 (7/10) 0.875 (7/8)
經驗 (experience) 0.500 (5/10) 0.667 (4/6) 0.800 (4/5)
發生 (happen) 0.455 (5/11) 0.571 (4/7) 0.800 (4/5)
而 (so) 0.417 (20/48) 0.550 (11/20) 0.550 (11/20)
</p>
<p>Table 5: Precision/recall of Bi-LSTM mod-
els with CWIN+POS features on four most com-
monly misused (err rate(w) &gt; 0.4) words.
</p>
<p>In Table 5 we show the precision/recall of the
Bi-LSTM model with CWIN+POS features on
four most commonly misused words. The error
rate of a word w is calculated on the test set by
err rate(w) = # segments in which w is misused# segments containing w . We
exclude words that occur in less than 10 segments
regardless of their error rates. In general, our
model achieves high recall and fair precision. Dis-
criminating correct and wrong usage of the con-
junction 而 (so), which often connects more than
one segment, seems to be the most difficult. For
example, in (E7) the inappropriateness of而 can-
not be recognized unless we consider the wider
context of this segment.
</p>
<p>(E7) (*而,並)當成此生做人的道理
( ..., (*so,and) take it as a lifelong way to
behave around others. )
</p>
<p>7 Conclusion
</p>
<p>In this paper we propose an LSTM-based se-
quence labeling model for detecting WUEs in
sentences written by non-native Chinese learn-
ers. The experimental results suggest that the
CWIN/Struct-SG embeddings, which consider
word orders, are better word features for Chinese
WUE detection. Moreover, Bi-LSTM is more pre-
ferred than LSTM. While a wrong usage often in-
volves more than one token, making it difficult
to determine which one should be corrected, the
best model can rank the ground-truth error posi-
tion within the top two in 80.97% of the cases.
</p>
<p>One possible future direction is to exploit more
sophisticated structural information such as de-
pendency paths. Moreover, it is also worth study-
ing how to extend our system to cope with the cor-
rection task.
</p>
<p>Acknowledgments
</p>
<p>This research was partially supported by Min-
istry of Science and Technology, Taiwan, un-
der grants MOST-104-2221-E-002-061-MY3 and
MOST-105-2221-E-002-154-MY3.
</p>
<p>408</p>
<p />
</div>
<div class="page"><p />
<p>References
</p>
<p>Shamil Chollampatt, Kaveh Taghipour, and Hwee Tou
Ng. 2016. Neural network translation models for
grammatical error correction. In Proceedings of the
Twenty-Fifth International Joint Conference on Ar-
tificial Intelligence. pages 2768–2774.
</p>
<p>François Chollet. 2015. Keras. https://github.
com/fchollet/keras.
</p>
<p>Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition
and determiner error correction shared task. In
Proceedings of the Seventh Workshop on Build-
ing Educational Applications Using NLP. Associ-
ation for Computational Linguistics, pages 54–62.
http://aclweb.org/anthology/W12-2006.
</p>
<p>Robert Dale and Adam Kilgarriff. 2011. Help-
ing our own: The hoo 2011 pilot shared task.
In Proceedings of the 13th European Work-
shop on Natural Language Generation. Associa-
tion for Computational Linguistics, pages 242–249.
http://aclweb.org/anthology/W11-2838.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Hen-Hsen Huang, Yen-Chi Shao, and Hsin-Hsi Chen.
2016. Chinese preposition selection for grammat-
ical error diagnosis. In Proceedings of COLING
2016, the 26th International Conference on Com-
putational Linguistics: Technical Papers. The COL-
ING 2016 Organizing Committee, pages 888–899.
http://aclweb.org/anthology/C16-1085.
</p>
<p>Shen Huang and Houfeng Wang. 2016. Bi-lstm neu-
ral networks for chinese grammatical error diag-
nosis. In Proceedings of the 3rd Workshop on
Natural Language Processing Techniques for Edu-
cational Applications (NLPTEA 2016). The COL-
ING 2016 Organizing Committee, pages 148–154.
http://aclweb.org/anthology/W16-4919.
</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
</p>
<p>Lung-Hao Lee, Gaoqi Rao, Liang-Chih Yu, Endong
Xun, Baolin Zhang, and Li-Ping Chang. 2016.
Overview of nlp-tea 2016 shared task for chinese
grammatical error diagnosis. In Proceedings of
the 3rd Workshop on Natural Language Processing
Techniques for Educational Applications (NLPTEA
2016). The COLING 2016 Organizing Commit-
tee, pages 40–48. http://aclweb.org/anthology/W16-
4906.
</p>
<p>Lung-Hao Lee, Liang-Chih Yu, and Li-Ping Chang.
2015. Overview of the nlp-tea 2015 shared
task for chinese grammatical error diagnosis.
</p>
<p>In Proceedings of the 2nd Workshop on Nat-
ural Language Processing Techniques for Edu-
cational Applications (NLPTEA 2015). Associa-
tion for Computational Linguistics, pages 1–6.
https://doi.org/10.18653/v1/W15-4401.
</p>
<p>Wang Ling, Chris Dyer, W. Alan Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies (HLT-NAACL).
Association for Computational Linguistics, pages
1299–1304. https://doi.org/10.3115/v1/N15-1142.
</p>
<p>Fang Liu, Meng Yang, and Dekang Lin. 2010. Chinese
web 5-gram version 1. Linguistic Data Consortium,
Philadelphia .
</p>
<p>Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations. Associ-
ation for Computational Linguistics, pages 55–60.
https://doi.org/10.3115/v1/P14-5010.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies (HLT-NAACL). Associ-
ation for Computational Linguistics, pages 746–751.
http://aclweb.org/anthology/N13-1090.
</p>
<p>Tou Hwee Ng, Mei Siew Wu, Ted Briscoe, Chris-
tian Hadiwinoto, Hendy Raymond Susanto, and
Christopher Bryant. 2014. The conll-2014 shared
task on grammatical error correction. In Proceed-
ings of the Eighteenth Conference on Computational
Natural Language Learning: Shared Task. Asso-
ciation for Computational Linguistics, pages 1–14.
https://doi.org/10.3115/v1/W14-1701.
</p>
<p>Tou Hwee Ng, Mei Siew Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The conll-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task. Association for Computational Linguistics,
pages 1–12. http://aclweb.org/anthology/W13-
3601.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
</p>
<p>409</p>
<p />
</div>
<div class="page"><p />
<p>for Computational Linguistics, pages 1532–1543.
https://doi.org/10.3115/v1/D14-1162.
</p>
<p>Marek Rei and Helen Yannakoudakis. 2016. Composi-
tional sequence labeling models for error detection
in learner writing. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 1181–1191.
https://doi.org/10.18653/v1/P16-1112.
</p>
<p>Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.
</p>
<p>Yow-Ting Shiue and Hsin-Hsi Chen. 2016. Detecting
word usage errors in chinese sentences for learn-
ing chinese as a foreign language. In Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC 2016). Eu-
ropean Language Resources Association (ELRA),
pages 220–224.
</p>
<p>Chi-Hsin Yu, Yi jie Tang, and Hsin-Hsi Chen. 2012.
Development of a web-scale chinese word n-gram
corpus with parts of speech information. In Pro-
ceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC
2012). European Language Resources Association
(ELRA), pages 320–324.
</p>
<p>Liang-Chih Yu, Lung-Hao Lee, and Li-Ping Chang.
2014. Overview of grammatical error diagnosis for
learning chinese as a foreign language. In Pro-
ceedings of the 1st Workshop on Natural Language
Processing Techniques for Educational Applications
(NLPTEA 2014). pages 42–47.
</p>
<p>410</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 411–416
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2065
</p>
<p>Automatic Compositor Attribution in the First Folio of Shakespeare
</p>
<p>Maria Ryskina∗ Hannah Alpert-Abrams† Dan Garrette‡ Taylor Berg-Kirkpatrick∗
</p>
<p>∗ Language Technologies Institute, Carnegie Mellon University, {mryskina,tberg}@cs.cmu.edu
†Comparative Literature Program, University of Texas at Austin, halperta@gmail.com
</p>
<p>‡ Google, dhgarrette@google.com
</p>
<p>Abstract
</p>
<p>Compositor attribution, the clustering of
pages in a historical printed document by
the individual who set the type, is a bib-
liographic task that relies on analysis of
orthographic variation and inspection of
visual details of the printed page. In
this paper, we introduce a novel unsuper-
vised model that jointly describes the tex-
tual and visual features needed to distin-
guish compositors. Applied to images of
Shakespeare’s First Folio, our model pre-
dicts attributions that agree with the man-
ual judgements of bibliographers with an
accuracy of 87%, even on text that is the
output of OCR.
</p>
<p>1 Introduction
</p>
<p>Within literary studies, the field of bibliography
has an unusually long tradition of quantitative
analysis. One particularly relevant area is that
of compositor attribution—the clustering of pages
in a historical printed document by the individ-
ual (the compositor) who set the type. Like sty-
lometry, a long-standing area of NLP that has
largely focused on attributing the authorship of
text (Holmes, 1994; Hope, 1994; Juola, 2006;
Koppel et al., 2009; Jockers and Witten, 2010),
the analysis of orthographic patterns is fundamen-
tal to compositor attribution. Additionally, com-
positor attribution often makes use of visual fea-
tures, such as whitespace layout, introducing new
challenges. These analyses have traditionally been
done by hand, but efforts are painstaking due to the
difficulty of manually recording these features.
</p>
<p>In this paper, we present an unsupervised model
specifically designed for compositor attribution
that incorporates both textual and visual sources
of evidence traditionally used by bibliographers
(Hinman, 1963; Taylor, 1981; Blayney, 1991).
</p>
<p>spelling
variation
</p>
<p>spacing variation
medial comma
</p>
<p>Figure 1: The compositor of the left page tended to use the
spellings doe and deere, while the compositor for the right
page used spellings do and deare, indicating these pages
were likely set by different people. The varying width of the
medial comma whitespace also distinguishes the typesetters.
</p>
<p>Our model jointly describes the patterns of vari-
ation both in orthography and in the whitespace
between glyphs, allowing us to cluster pages
by discovering patterns of similarity and differ-
ence. When applied to digital scans of histori-
cal printed documents, our approach learns ortho-
graphic and whitespace preferences of individual
compositors and predicts groupings of pages set
by the same compositor.1 This is, to our knowl-
edge, the first attempt to perform compositor at-
tribution automatically. Prior work has proposed
automatic approaches to authorship attribution—
which is typically viewed as the supervised prob-
lem of identifying a particular author given sam-
ples of their writing. In contrast, compositor attri-
bution lacks supervision because compositors are
unknown and, in addition, focuses on different lin-
guistic patterns. We explain spellings of words
conditioned on word choice, not the word choice
itself.
</p>
<p>1The validity of compositor attribution has sparked an on-
going and heated debate among bibliographers (McKenzie,
1969, 1984; Rizvi, 2016); while some reject parts or all of
this approach, it continues to be cited in authoritative bib-
liographical texts (Gaskell, 2007; Blayney, 1996). Without
taking a position in this debate, we seek only to automate
the methods that remain in use by particular bibliographers
(Blayney, 1996; Burrows, 2013).
</p>
<p>411</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2065">https://doi.org/10.18653/v1/P17-2065</a></div>
</div>
<div class="page"><p />
<p>ci
</p>
<p>sik
</p>
<p>mij
</p>
<p>dij
</p>
<p>dear
</p>
<p>deere
Ki Ji
</p>
<p>I
</p>
<p>Compositor
</p>
<p>C! !! !! !
</p>
<p>Edit operation weights:
</p>
<p>C
</p>
<p>wc
Word variant weights:
</p>
<p>Figure 2: In our model, a compositor
ci is generated for page i from a multi-
nomial prior. Then, each diplomatic
word, dij , is generated conditioned on
ci and the corresponding modern word,
mij , from a distribution parameterized
by weight vectorwc. Finally, each me-
dial comma spacing width (measured
in pixels), sik, is generated conditioned
on ci from a distribution parameterized
by θc.
</p>
<p>To evaluate our approach, we fit our model to
digital scans of Shakespeare’s First Folio (1623)—
a document with well established manual judge-
ments of compositor attribution. We find that even
when relying on noisy OCR transcriptions of tex-
tual content, our model predicts compositor attri-
butions that agree with manual annotations 87% of
the time, outperforming several simpler baselines.
Our approach opens new possibilities for consid-
ering patterns across a larger vocabulary of words
and at a higher visual resolution than has been pos-
sible historically. Such a tool may enable scalable
first-pass analysis in understudied domains as a
complement to humanistic studies of composition.
</p>
<p>2 Background
</p>
<p>In this paper we focus on modeling the same types
of observations made by scholars and demonstrate
agreement with authoritative attributions. We use
compositor studies of Shakespeare’s First Folio
to inform our approach, drawing on the meth-
ods proposed by Hinman (1963), Howard-Hill
(1973), and Taylor (1981). Hinman’s landmark
1963 study clustered the pages of the First Fo-
lio according to five different compositors based
on variations in spelling among three common
words. Figure 1, for example, shows portions of
two pages of the First Folio with different spelling
variants for the words dear and do: one compos-
itor used deere and doe, while the other used
deare and do. Hinman relied on the assumption
that each compositor was consistent in their pref-
erences for the sake of convenience in the typeset-
ting process (Blayney, 1991). Subsequent studies
looked at larger sets of words and more general
orthographic preferences (e.g. the preference to
terminate words with -ie instead of -y), lead-
ing to modifications of Hinman’s original analysis
</p>
<p>(Howard-Hill, 1973; Taylor, 1981). In this paper
we propose a probabilistic model designed to cap-
ture both word-specific preferences and general
orthographic patterns. To separate the effect of
the compositor from the choices made by the au-
thor or editor, we condition on a modernized (col-
lated) version of Shakespeare’s text as was done
by scholars.
</p>
<p>Visual features, including typeface usage and
whitespace layout, also inform compositor attribu-
tion. For example, the highlighted spacing in Fig-
ure 1 shows different choices after medial commas
(commas that occur before the end of the line).
Bibliographers produced new hypotheses about
how many compositors were involved in produc-
tion based on the analysis of the use of spaces
before and after punctuation (Howard-Hill, 1973;
Taylor, 1981). We additionally incorporate this
source of evidence into to our automatic approach
by modeling pixel-level whitespace distances.
</p>
<p>Bibliographers also use contextual information
to inform their analyses, including copy text or-
thography, printing house records, collation, type
case usage, and the use of type with cast-on
spaces. In our model, we restrict our analysis to
only those features that can be derived from the
OCR output and simple visual analysis.
</p>
<p>3 Model
</p>
<p>Our computational approach to compositor attri-
bution operates on the sources of evidence that
have been considered by bibliographers. In par-
ticular, we focus on jointly modeling patterns
of orthographic variation and spacing preferences
across pages of a document, treating composi-
tor assignments as latent variables in a generative
model. We assume access to a diplomatic tran-
scription of the document (a transcription faith-
</p>
<p>412</p>
<p />
</div>
<div class="page"><p />
<p>ful to the original orthography), which we auto-
matically align with a modernized version.2 We
experiment with both manually and automatically
(OCR) produced transcriptions, and assume ac-
cess to pixel-level spacing information on each
page, which can be extracted using OCR as de-
scribed in Section 4.
</p>
<p>Figure 2 shows the generative process. In
our model, each of I total pages is generated
independently. The compositor assignment for
the ith page is represented by the variable ci ∈
{1, . . . , C} and is generated from a multinomial
prior. For page i, each diplomatic word, dij , is
generated conditioned on the corresponding mod-
ern word, mij , and the compositor who set the
page, ci. Finally, the model produces the pixel
width of the space after each medial comma, sik,
again conditioned on the compositor, ci. The joint
distribution for page i, conditioned on modern
text, takes the following form:
</p>
<p>P ({dij}, {sik}, ci|{mij}) =
P (ci) [Prior on compositors]
</p>
<p>·
Ji∏
</p>
<p>j=1
</p>
<p>P (dij |mij , ci;wci) [Orthographic model]
</p>
<p>·
Ki∏
</p>
<p>k=1
</p>
<p>P (sik|ci;θci) [Whitespace model]
</p>
<p>3.1 Orthographic Preference Model
</p>
<p>We choose the parameterization of the distribu-
tion of diplomatic words in order to capture two
types of spelling preference: (1) general prefer-
ences for certain character groups (such as -ie)
and (2) preferences that only pertain to a particu-
lar word and do not indicate a larger pattern. Since
it is unknown which of the two behaviors is dom-
inant, we let the model describe both and learn to
separate their effects. Using a log-linear parame-
terization,
</p>
<p>P (d|m, c;w) ∝ exp(w&gt;c f(m, d))
</p>
<p>we introduce features to capture both effects.
Here, f(m, d) is a feature function defined on
modern word m paired with diplomatic word d,
whilewc is a weight vector corresponding to com-
positor c.
</p>
<p>2Modern editions are common for many books that are of
interest to bibliographers, though future work could consider
how to cope with their absence.
</p>
<p>To capture word-specific preferences we add an
indicator feature for each pair of modern word m
and diplomatic spelling d. We refer to these as
WORD features below. To capture general ortho-
graphic preferences we introduce an additional set
of features based on the edit operations involved in
the computation of Levenshtein distance between
m and d. In particular, each operation is added
as a separate feature, both with and without local
context (previous or next character of the modern
word). We refer to this group as EDIT features.
The weight vector for each compositor represents
their unique biases, as shown in the depiction of
these parameters in Figure 2.
</p>
<p>3.2 Whitespace Preference Model
Manual analysis of spacing has revealed differ-
ences across pages. In particular, the choice of
spaced or non-spaced punctuation marks is hy-
pothesized by biobliographers to be indicative of
compositor preference and specific typecase. We
add whitespace distance to our model to capture
those observations. While bibliographers only
made a coarse distinction between spaced or non-
spaced commas, in our model we generate medial
comma spacing widths, sik, that are measured in
pixels to enable finer-grained analysis. We use a
simple multinomial parameterization where each
pixel width is treated as a separate outcome up to
some maximum allowable width:
</p>
<p>sik|ci ∼Mult(θci)
Here, θc represents the vector of multinomial
spacing parameters corresponding to compositor
c. We choose this parameterization because it can
capture non-unimodal whitespace preference dis-
tributions, as depicted in Figure 2, and it makes
learning simple.
</p>
<p>3.3 Learning and Inference
Modern and diplomatic words and spacing vari-
ables are observed, while compositor assignments
are latent. In order to fit the model to an input doc-
ument we estimate the orthographic preference pa-
rameters, wc, and spacing preference parameters,
θc, for each compositor using EM. The E-step is
accomplished via a tractable sum over composi-
tor assignments, while the M-step for wc is ac-
complished via gradient ascent (Berg-Kirkpatrick
et al., 2010). The M-step for spacing parameters,
θc, uses the standard multinomial update. Pre-
dicting compositor groups is accomplished via an
</p>
<p>413</p>
<p />
</div>
<div class="page"><p />
<p>Model Setup
Bodleian Transcription Ocular OCR Transcription
</p>
<p>Hinman Attr Blayney Attr Hinman Attr Blayney Attr
1-to-1 M-to-1 1-to-1 M-to-1 1-to-1 M-to-1 1-to-1 M-to-1
</p>
<p>RANDOM 22.5 49.6 16.7 49.6 22.5 49.6 16.7 49.6
BASIC w/ HINMAN 67.9 71.8 60.4 67.3 66.6 70.5 47.1 63.8
</p>
<p>w/ AUTO 64.3 81.0 58.8 81.3 64.9 81.1 53.7 80.7
FEAT w/ EDIT 75.3 79.1 77.1 83.1 76.8 77.4 76.1 76.0
</p>
<p>w/ EDIT + WORD 81.1 81.1 80.7 80.6 75.1 75.0 74.4 74.4
w/ EDIT + SPACE 87.6 87.5 87.3 87.2 86.7 86.6 85.9 85.8
w/ ALL 83.8 83.7 83.5 83.4 82.5 82.4 82.4 82.2
</p>
<p>Table 1: The experi-
mental results for all se-
tups of the model. In the
experiments with BASIC
model, we compare the
short HINMAN word list
with the automatically
filtered AUTO word list.
We show results for sev-
eral variants of our full
model, labeled as FEAT,
both with and without
spacing generation. A
random baseline is in-
cluded for comparison.
</p>
<p>independent argmax over each ci. In all experi-
ments we run 75 iterations of EM with 100 ran-
dom restarts, choosing the learned parameters cor-
responding the best model likelihood.
</p>
<p>4 Experiments
</p>
<p>Data: To evaluate our model when it has ac-
cess to perfectly transcribed historical text, we use
the Bodleian diplomatic transcription of the First
Folio.3 To test whether our approach can also
work with untranscribed books, we ran the Ocu-
lar OCR system (Berg-Kirkpatrick et al., 2013) on
the Bodleian facsimile images to create an auto-
matic diplomatic transcription. In both cases, we
used Ocular’s estimates of glyph bounding boxes
on the complete First Folio images to extract spac-
ing information. The modern text was taken from
MIT Complete Works of Shakespeare4 and was
aligned with diplomatic transcriptions by running
a word-level edit distance calculation. The ex-
tracted substitutions form the model’s observed
modern-diplomatic word pairs.
</p>
<p>Evaluation: To compare the recovered attribu-
tion with those proposed by bibliographers, we
evaluate against an authoritative attribution com-
piled by Peter Blayney (1996) which includes the
work of various scholars (Hinman, 1963; Howard-
Hill, 1973, 1976, 1980; Taylor, 1981; O’Connor,
1975; Werstine, 1982). We also evaluate our sys-
tem against an earlier, highly influential model
proposed by Hinman (1963), which we approx-
imate by reverting certain compositor divisions
in Blayney’s attribution. Hinman’s attribution
posited five compositors, while Blayney’s posited
eight. In experiments, we set the model’s maxi-
mum number of compositors to C = 5 when eval-
uating on Hinman’s attribution, and use C = 8
</p>
<p>3http://firstfolio.bodleian.ox.ac.uk/
4http://shakespeare.mit.edu/
</p>
<p>with Blayney’s. We compute the one-to-one
and many-to-one accuracy, mapping the recov-
ered page groups to the gold compositors to max-
imize accuracy, as is standard for many unsuper-
vised clustering tasks, e.g. POS induction (see
Christodoulopoulos et al. (2010)).
</p>
<p>BASIC model variant: We evaluate a simple base-
line model that uses a multinomial parameteriza-
tion for generating diplomatic words and does not
incorporate spacing information. We use two dif-
ferent options for selection of spelling variants to
be considered by the model. First, we consider
only the three words selected by Hinman: do, go
and here (referred to as HINMAN). Second, we
use a larger, automatically selected, word list (re-
ferred to as AUTO). Here, we select all modern
words with frequency greater than 70 that are not
names and that exhibit sufficient variance in diplo-
matic spellings (most common diplomatic spelling
occurs in less than 80% of aligned tokens). For
our full model, described in the next section, we
always use the larger AUTO word list.
</p>
<p>FEAT model variant: We run experiments with
several variants of our full model, described in
Section 3 (referred to as FEAT since they use
a feature-based parameterization of diplomatic
word generation.) We try ablations of WORD and
EDIT features, as well as model variants with and
without the spacing generation component (re-
ferred to as SPACE.) We refer to the full model
that includes both types of features and spacing
generation as ALL.
</p>
<p>5 Results
</p>
<p>Our experimental results are presented in Table 1.
The BASIC variant, modeled after Hinman’s orig-
inal procedure, substantially outperforms the ran-
dom baseline, with the HINMAN word list outper-
forming the larger AUTO word list. However, use
</p>
<p>414</p>
<p />
</div>
<div class="page"><p />
<p>! ! ! ! ! ! ! !
</p>
<p>!u   o
u   w!
u   DEL!
</p>
<p>B AC E D
</p>
<p>! ! ! !
</p>
<p>! ! ! !! ! ! !
</p>
<p>! ! ! !
</p>
<p>! ! ! !
</p>
<p>! ! ! !
</p>
<p>Comp B Comp AComp C Comp E Comp D
</p>
<p>! ! ! !
</p>
<p>Figure 3: Learned behaviors of the Fo-
lio compositors. Our model only de-
tected the presence of five compositors
(ranked according to number of pages
the compositor set in our model’s pre-
diction). Compositor D’s habit of omit-
ting u (yong vs. young) and compos-
itor C’s usage of spaced medial commas
were also noticed in Taylor (1981).
</p>
<p>of the larger word list with feature-based models
yields large gains in all scenarios, including evalu-
ation on Hinman’s original attributions and while
using OCR diplomatic transcriptions. The best-
performing model for both manually transcribed
and OCR text uses EDIT features in conjunction
with spacing generation and achieves an accuracy
of up to 87%. Including WORD features on top
of this leads to slightly reduced performance, per-
haps as a result of the substantially increased num-
ber of free parameters. In the OCR scenario, the
addition of WORD features on top of EDIT de-
creases accuracy, unlike the same experiment with
the manual transcription. This is possibly a result
of the reduced reliability of full word forms due to
mistakes in OCR.
</p>
<p>Particularly interesting is the result that spac-
ing, rarely a factor considered in NLP models,
improves the accuracy significantly for our sys-
tem when compared with EDIT features alone.
Because pixel-level visual information and arbi-
trary orthographic patterns are also the most diffi-
cult features to measure manually, our results give
strong evidence to the assertion that NLP-style
models can aid bibliographers.
</p>
<p>6 Discussion
</p>
<p>The results on OCR (character error rate for most
plays≈ 10−15%) transcripts are only marginally
worse than those on manual transcripts, which
shows that our approach can be generalized for the
common case where manual diplomatic transcrip-
tions are not available. For our experiments, we
also chose a common modern edition of Shake-
speare instead of more carefully produced mod-
ernized transcription of the facsimile—our goal
being to again show that this approach can be
generalized, perhaps to documents where careful
modernizations of the facsimile are not available.
Together, these results suggest that our model may
be sufficiently robust to aid bibliographers in their
analysis of less studied texts.
</p>
<p>Figure 3 shows an example of the feature
</p>
<p>weights and spacing parameters learned by the
FEAT w/ ALL model. Our statistical approach is
able to successfully explain some of the observa-
tions scholars made. For example, Taylor (1981)
notices that compositors C and D prefer to omit
u in young but A does not. Our model reflects
this by giving u → DEL high weight for D and
low weight for A. However, the weight of a single
feature is difficult to interpret in isolation. This
might be the reason why our model only moder-
ately agrees in case of compositor C. Another ex-
ample can be seen in spacing patterns: according
to Taylor (1981), compositor C uses spaced me-
dial commas unlike A and D. Our model learns
the same behavior.
</p>
<p>7 Conclusion
</p>
<p>Our primary goal is to scale the methods of com-
positor attribution, including both textual and vi-
sual modes of evidence, for use across books and
corpora. By using principled statistical techniques
and considering evidence at a larger scale, we of-
fer a more robust approach to compositor identi-
fication than has previously been possible. The
fact that our system works well on OCR texts
means that we are not restricted to only those
documents for which we have manually produced
transcriptions, opening up the possibility for bib-
liographic study on a much larger class of texts.
Though we are unable to incorporate the kinds
of world knowledge used by bibliographers, our
ability to include more information and more fine-
grained information allows us to recreate their re-
sults. Having validated these techniques on the
First Folio, where historical claims are well es-
tablished, we hope future work can extend these
methods and their application.
</p>
<p>Acknowledgements
</p>
<p>We thank the three anonymous reviewers for their
valuable feedback. This project is funded in part
by the NSF under grant 1618044.
</p>
<p>415</p>
<p />
</div>
<div class="page"><p />
<p>References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
</p>
<p>John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
</p>
<p>Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.
2013. Unsupervised transcription of historical doc-
uments. In Proceedings of the Association for Com-
putational Linguistics.
</p>
<p>Peter W. M. Blayney. 1991. The First Folio of Shake-
speare: In Conjunction with the Exhibition at the
Folger Shakespeare Library, Washington, DC, April
1, 1991-September 21, 1991. Folger Shakespeare
Library.
</p>
<p>Peter W. M. Blayney, editor. 1996. The First Folio of
Shakespeare: The Norton Facsimile. Norton.
</p>
<p>Ian R. Burrows. 2013. ”The peryod of my blisse”:
Commas, ends and utterance in Solyman and
Perseda. Textual Cultures: Texts, Contexts, Inter-
pretation 8(2):95–120.
</p>
<p>Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
</p>
<p>Philip Gaskell. 2007. A New Introduction to Bibliogra-
phy. Oak Knoll Press.
</p>
<p>Charlton Hinman. 1963. The printing and proof-
reading of the First Folio of Shakespeare, volume 1.
Oxford: Clarendon Press.
</p>
<p>David I. Holmes. 1994. Authorship attribution. Com-
puters and the Humanities 28(2):87–106.
</p>
<p>Jonathan Hope. 1994. The authorship of Shakespeare’s
plays: A socio-linguistic study. Cambridge Univer-
sity Press.
</p>
<p>Trevor H. Howard-Hill. 1973. The compositors of
Shakespeare’s Folio Comedies. Studies in bibliog-
raphy 26:61–106.
</p>
<p>Trevor H. Howard-Hill. 1976. Compositors B and E in
the Shakespeare First Folio and Some Recent Stud-
ies. Self-published.
</p>
<p>Trevor H. Howard-Hill. 1980. New light on compos-
itor E of the Shakespeare First Folio. The Library
6(2):156–178.
</p>
<p>Matthew L. Jockers and Daniela M. Witten. 2010. A
comparative study of machine learning methods for
authorship attribution. Literary and Linguistic Com-
puting 25(2):215–223.
</p>
<p>Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval 1(3):233–
334.
</p>
<p>Moshe Koppel, Jonathan Schler, and Shlomo Arga-
mon. 2009. Computational methods in authorship
attribution. J. Am. Soc. Inf. Sci. Technol. 60(1):9–
26. https://doi.org/10.1002/asi.v60:1.
</p>
<p>D. F. McKenzie. 1969. Printers of the mind: Some
notes on bibliographical theories and printing-house
practices. Studies in Bibliography 22:1–75.
</p>
<p>D. F. McKenzie. 1984. Stretching a point: Or, the case
of the spaced-out comps. Studies in Bibliography
37:106–121.
</p>
<p>John O’Connor. 1975. Compositors D and F of the
Shakespeare First Folio. Studies in Bibliography
28:81–117.
</p>
<p>Pervez Rizvi. 2016. Use of spellings for compositor
attribution in the First Folio. The Papers of the Bib-
liographical Society of America 110:1–53.
</p>
<p>Gary Taylor. 1981. The shrinking compositor A of
the Shakespeare First Folio. Studies in Bibliography
34:96–117.
</p>
<p>Paul Werstine. 1982. Cases and compositors in the
Shakespeare First Folio Comedies. Studies in Bib-
liography 35:206–234.
</p>
<p>416</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 417–421
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2066
</p>
<p>STAIR Captions:
Constructing a Large-Scale Japanese Image Caption Dataset
</p>
<p>Yuya Yoshikawa Yutaro Shigeto Akikazu Takeuchi
Software Technology and Artificial Intelligence Research Laboratory (STAIR Lab)
</p>
<p>Chiba Institute of Technology
2-17-1, Tsudanuma, Narashino, Chiba, Japan.
</p>
<p>{yoshikawa,shigeto,takeuchi}@stair.center
</p>
<p>Abstract
</p>
<p>In recent years, automatic generation of im-
age descriptions (captions), that is, image
captioning, has attracted a great deal of at-
tention. In this paper, we particularly con-
sider generating Japanese captions for im-
ages. Since most available caption datasets
have been constructed for English lan-
guage, there are few datasets for Japanese.
To tackle this problem, we construct a
large-scale Japanese image caption dataset
based on images from MS-COCO, which
is called STAIR Captions. STAIR Captions
consists of 820,310 Japanese captions for
164,062 images. In the experiment, we
show that a neural network trained using
STAIR Captions can generate more natu-
ral and better Japanese captions, compared
to those generated using English-Japanese
machine translation after generating En-
glish captions.
</p>
<p>1 Introduction
Integrated processing of natural language and im-
ages has attracted attention in recent years. The
Workshop on Vision and Language held in 2011
has since become an annual event1. In this
research area, methods to automatically gener-
ate image descriptions (captions), that is, image
captioning, have attracted a great deal of atten-
tion (Karpathy and Fei-Fei, 2015; Donahue et al.,
2015; Vinyals et al., 2015; Mao et al., 2015) .
</p>
<p>Image captioning is to automatically generate a
caption for a given image. By improving the qual-
ity of image captioning, image search using nat-
ural sentences and image recognition support for
</p>
<p>1In recent years it has been held as a joint workshop such
as EMNLP and ACL; https://vision.cs.hacettepe.
edu.tr/vl2017/
</p>
<p>visually impaired people by outputting captions as
sounds can be made available. Recognizing vari-
ous images and generating appropriate captions for
the images necessitates the compilation of a large
number of image and caption pairs.
</p>
<p>In this study, we consider generating image cap-
tions in Japanese. Since most available caption
datasets have been constructed for English lan-
guage, there are few datasets for Japanese. A
straightforward solution is to translate English cap-
tions into Japanese ones by using machine trans-
lation such as Google Translate. However, the
translated captions may be literal and unnatural
because image information cannot be reflected in
the translation. Therefore, in this study, we con-
struct a Japanese image caption dataset, and for
given images, we aim to generate more natural
Japanese captions than translating the generated
English captions into the Japanese ones.
</p>
<p>The contributions of this paper are as follows:
</p>
<p>• We constructed a large-scale Japanese image
caption dataset, STAIR Captions, which con-
sists of Japanese captions for all the images in
MS-COCO (Lin et al., 2014) (Section 3).
</p>
<p>• We confirmed that quantitatively and qualita-
tively better Japanese captions than the ones
translated from English captions can be gen-
erated by applying a neural network-based
image caption generation model learned on
STAIR Captions (Section 5).
</p>
<p>STAIR Captions is available for download from
http://captions.stair.center.
</p>
<p>2 Related Work
</p>
<p>Some English image caption datasets have been
proposed (Krishna et al., 2016; Kuznetsova et al.,
2013; Ordonez et al., 2011; Vedantam et al.,
</p>
<p>417</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2066">https://doi.org/10.18653/v1/P17-2066</a></div>
</div>
<div class="page"><p />
<p>2015; Isola et al., 2014). Representative ex-
amples are PASCAL (Rashtchian et al., 2010),
Flickr3k (Rashtchian et al., 2010; Hodosh et al.,
2013), Flickr30k (Young et al., 2014) —an ex-
tension of Flickr3k—, and MS-COCO (Microsoft
Common Objects in Context) (Lin et al., 2014).
</p>
<p>As detailed in Section 3, we annotate Japanese
captions for the images in MS-COCO. Note that
when annotating the Japanese captions, we did not
refer to the original English captions in MS-COCO.
</p>
<p>MS-COCO is a dataset constructed for research
on image classification, object recognition, and En-
glish caption generation. Since its release, MS-
COCO has been used as a benchmark dataset for
image classification and caption generation. In ad-
dition, many studies have extended MS-COCO by
annotating additional information about the images
in MS-COCO2.
</p>
<p>Recently, a few caption datasets in lan-
guages other than English have been con-
structed (Miyazaki and Shimizu, 2016;
Grubinger et al., 2006; Elliott et al., 2016).
In particular, the study of Miyazaki and Shimizu
(2016) is closest to the present study. As in our
study, they constructed a Japanese caption dataset
called YJ Captions. The main difference between
STAIR Captions and YJ Captions is that STAIR
Captions provides Japanese captions for a greater
number of images. In Section 3, we highlight this
difference by comparing the statistics of STAIR
Captions and YJ Captions.
</p>
<p>3 STAIR Captions
</p>
<p>3.1 Annotation Procedure
This section explains how we constructed STAIR
Captions. We annotated all images (164,062 im-
ages) in the 2014 edition of MS-COCO. For each
image, we provided five Japanese captions. There-
fore, the total number of captions was 820,310.
Following the rules for publishing datasets created
based on MS-COCO, the Japanese captions we
created for the test images are excluded from the
public part of STAIR Captions.
</p>
<p>To annotate captions efficiently, we first devel-
oped a web system for caption annotation. Figure 1
shows the example of the annotation screen in the
web system. Each annotator looks at the displayed
image and writes the corresponding Japanese de-
scription in the text box under the image. By
</p>
<p>2http://mscoco.org/external/
</p>
<p>Figure 1: Example of annotation screen of web
system for caption annotation.
</p>
<p>pressing the send (送信) button, a single task is
completed and the next task is started.
</p>
<p>To concurrently and inexpensively annotate cap-
tions by using the above web system, we asked
part-time job workers and crowd-sourcing work-
ers to perform the caption annotation. The work-
ers annotated the images based on the following
guidelines. (1) A caption must contain more than
15 letters. (2) A caption must follow the da/dearu
style (one of writing styles in Japanese). (3) A
caption must describe only what is happening in
an image and the things displayed therein. (4) A
caption must be a single sentence. (5) A caption
must not include emotions or opinions about the
image. To guarantee the quality of the captions
created in this manner, we conducted sampling in-
spection of the annotated captions, and the captions
not in line with the guidelines were removed. The
entire annotation work was completed by about
2,100 workers in about half a year.
</p>
<p>3.2 Statistics
This section introduces the quantitative character-
istics of STAIR Captions. In addition, we compare
it to YJ Captions (Miyazaki and Shimizu, 2016),
a dataset with Japanese captions for the images in
MS-COCO like in STAIR Captions.
</p>
<p>Table 1 summarizes the statistics of the datasets.
Compared with YJ Captions, overall, the numbers
of Japanese captions and images in STAIR Cap-
tions are 6.23x and 6.19x, respectively. In the pub-
lic part of STAIR Captions, the numbers of images
and Japanese captions are 4.65x and 4.67x greater
than those in YJ Captions, respectively. That the
numbers of images and captions are large in STAIR
Captions is an important point in image caption
</p>
<p>418</p>
<p />
</div>
<div class="page"><p />
<p>Table 1: Comparison of dataset specifications.
Numbers in the brackets indicate statistics of public
part of STAIR Captions.
</p>
<p>Ours YJ Captions
</p>
<p># of images 164,062 (123,287) 26,500
# of captions 820,310 (616,435) 131,740
</p>
<p>Vocabulary size 35,642 (31,938) 13,274
Avg. # of chars 23.79 (23.80) 23.23
</p>
<p>generation because it reduces the possibility of un-
known scenes and objects appearing in the test im-
ages. The vocabulary of STAIR Captions is 2.69x
larger than that of YJ Captions. Because of the
large vocabulary of STAIR Captions, it is expected
that the caption generation model can learn and
generate a wide range of captions. The average
numbers of characters per a sentence in STAIR
Captions and in YJ Captions are almost the same.
</p>
<p>4 Image Caption Generation
In this section, we briefly review the caption gen-
eration method proposed by Karpathy and Fei-Fei
(2015), which is used in our experiments (Sec-
tion 5).
</p>
<p>This method consists of a convolutional neu-
ral network (CNN) and long short-term memory
(LSTM)3. Specifically, CNN first extracts features
from a given image, and then, LSTM generates a
caption from the extracted features.
</p>
<p>Let I be an image, and the corresponding caption
be Y = (y1, y2, · · · , yn). Then, caption generation
is defined as follows:
</p>
<p>x(im) = CNN(I),
h0 = tanh
</p>
<p>(
W(im)x(im) + b(im)
</p>
<p>)
,
</p>
<p>c0 = 0,
ht, ct = LSTM (xt, ht−1, ct−1) (t ≥ 1),
</p>
<p>yt = softmax (Woht + bo) ,
</p>
<p>where CNN(·) is a function that outputs the image
features extracted by CNN, that is, the final layer
of CNN, and yt is the tth output word. The input
xt at time t is substituted by a word embedding
vector corresponding to the previous output, that
is, yt−1. The generation process is repeated until
LSTM outputs the symbol that indicates the end of
sentence.
</p>
<p>3 Although their original paper used RNN, they reported
in the appendix that LSTM performed better than RNN. Thus,
we used LSTM.
</p>
<p>In the training phase, given the training data,
we train W(im), b(im),W∗, b∗, CNN, and LSTM pa-
rameters, where ∗ represents wild card.
</p>
<p>5 Experiments
In this section, we perform an experiment which
generates Japanese captions using STAIR Cap-
tions. The aim of this experiment is to show the
necessity of a Japanese caption dataset. In par-
ticular, we evaluate quantitatively and qualitatively
how fluent Japanese captions can be generated by
using a neural network-based caption generation
model trained on STAIR Captions.
</p>
<p>5.1 Experimental Setup
5.1.1 Evaluation Measure
Following the literature (Chen et al., 2015;
Karpathy and Fei-Fei, 2015), we use
BLEU (Papineni et al., 2002), ROUGE (Lin,
2004), and CIDEr (Vedantam et al., 2015) as eval-
uation measures. Although BLEU and ROUGE
were developed originally for evaluating machine
translation and text summarization, we use them
here because they are often used for measuring the
quality of caption generation.
</p>
<p>5.1.2 Comparison Methods
In this experiment, we evaluate the following cap-
tion generation methods.
</p>
<p>• En-generator → MT: A pipeline method
of English caption generation and English-
Japanese machine translation. This method
trains a neural network, which generates En-
glish captions, with MS-COCO. In the test
phase, given an image, we first generate an
English caption to the image by the trained
neural network, and then translate the gen-
erated caption into Japanese one by machine
translation. Here, we use Google translate4
for machine translation. This method is the
baseline.
</p>
<p>• Ja-generator: This method trains a neural net-
work using STAIR Captions. Unlike MS-
COCO → MT, this method directly generate
a Japanese caption from a given image .
</p>
<p>As mentioned in Section 4, we used the method
proposed by Karpathy and Fei-Fei (2015) as cap-
tion generation models for both En-generator →
MT and Ja-generator.
</p>
<p>4https://translate.google.com/
</p>
<p>419</p>
<p />
</div>
<div class="page"><p />
<p>Table 2: Experimental results of Japanese caption generation. The numbers in boldface indicate the best
score for each evaluation measure.
</p>
<p>BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE_L CIDEr
</p>
<p>En-generator → MT 0.565 0.330 0.204 0.127 0.449 0.324
Ja-generator 0.763 0.614 0.492 0.385 0.553 0.833
</p>
<p>Table 3: Examples of generated image cap-
tions. En-generator denotes the caption genera-
tor trained with MS-COCO. En-generator → MT
is the pipeline method: it first generates English
caption and performs machine translation subse-
quently. Ja-generator was trained with Japanese
captions.
</p>
<p>En-generator:
A double decker bus driving down a street.
</p>
<p>En-generator → MT:
ストリートを運転する二重デッカーバス。
</p>
<p>Ja-generator:
二階建てのバスが道路を走っている。
</p>
<p>En-generator:
A bunch of food that are on a table.
En-generator → MT:
テーブルの上にある食べ物の束。
</p>
<p>Ja-generator:
ドーナツがたくさん並んでいる。
</p>
<p>In both the methods, following
Karpathy and Fei-Fei, we only trained LSTM
parameters, while CNN parameters were fixed.
We used VGG with 16 layers as CNN, where the
VGG parameters were the pre-trained ones5. With
the optimization of LSTM, we used mini-batch
RMSProp, and the batch size was set to 20.
</p>
<p>5.1.3 Dataset Separation
Following the experimental setting in the previous
studies (Chen et al., 2015; Karpathy and Fei-Fei,
2015), we used 123,287 images included in the
MS-COCO training and validation sets and their
corresponding Japanese captions. We divided the
dataset into three parts, i.e., 113,287 images for
the training set, 5,000 images for the validation
set, and 5,000 images for the test set.
</p>
<p>The hyper-parameters of the neural network
were tuned based on CIDEr scores by using the
validation set. As preprocessing, we applied mor-
phological analysis to the Japanese captions using
MeCab6.
</p>
<p>5http://www.robots.ox.ac.uk/~vgg/research/
very_deep/
</p>
<p>6http://taku910.github.io/mecab/
</p>
<p>5.2 Results
Table 2 summarizes the experimental results. The
results show that Ja-generator, that is, the approach
in which Japanese captions were used as training
data, outperformed En-generator → MT, which
was trained without Japanese captions.
</p>
<p>Table 3 shows two examples where Ja-generator
generated appropriate captions, whereas En-
generator → MT generated unnatural ones. In
the example at the top in Table 3, En-generator
first generated the term, “A double decker bus.”
MT translated the term into as “二重デッカーバ
ス”, but the translation is word-by-word and in-
appropriate as a Japanese term. By contrast, Ja-
generator generated “二階建てのバス (two-story
bus),” which is appropriate as the Japanese transla-
tion of A double decker bus. In the example at the
bottom of the table, En-generator → MT yielded
the incorrect caption by translating “A bunch of
food” as “食べ物の束 (A bundle of food).” By
contrast, Ja-generator correctly recognized that the
food pictured in the image is a donut, and expressed
it as “ドーナツがたくさん (A bunch of donuts).”
6 Conclusion
In this paper, we constructed a new Japanese image
caption dataset called STAIR Captions. In STAIR
Captions, Japanese captions are provided for all
the images of MS-COCO. The total number of
Japanese captions is 820,310. To the best of our
knowledge, STAIR Captions is currently the largest
Japanese image caption dataset.
</p>
<p>In our experiment, we compared the perfor-
mance of Japanese caption generation by a neu-
ral network-based model with and without STAIR
Captions to highlight the necessity of Japanese
captions. As a result, we showed the necessity
of STAIR Captions. In addition, we confirmed
that Japanese captions can be generated simply by
adapting the existing caption generation method.
</p>
<p>In future work, we will analyze the experimental
results in greater detail. Moreover, by using both
Japanese and English captions, we will develop
multi-lingual caption generation models.
</p>
<p>420</p>
<p />
</div>
<div class="page"><p />
<p>References
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
</p>
<p>ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C. Lawrenc Zitnick. 2015. Microsoft COCO cap-
tions: Data collection and evaluation server. arXiv
preprint 1504.00325.
</p>
<p>Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In Proceedings of the IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR).
</p>
<p>D. Elliott, S. Frank, K. Sima’an, and L. Specia. 2016.
Multi30k: Multilingual english-german image de-
scriptions. In Workshop on Vision and Language.
pages 70–74.
</p>
<p>Michael Grubinger, Paul Clough, Henning Müller, and
Thomas Deselaers. 2006. The IAPR Benchmark:
A new evaluation resource for visual information
systems. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC).
</p>
<p>Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of
Artificial Intelligence Research 47:853–899.
</p>
<p>Phillip Isola, Jianxiong Xiao, Devi Parikh, and Antonio
Torralba. 2014. What makes a photograph memo-
rable? IEEE Transactions on Pattern Analysis and
Machine Intelligence 36(7):1469–1482.
</p>
<p>Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE Computer Society
Conference on Computer Vision and Pattern Recog-
nition (CVPR). pages 3128–3137.
</p>
<p>Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalanditis, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, Li Fei-Fei, Yannis Kalantidis,
Li-Jia Li, David A. Shamma, Michael S. Bernstein,
and Fei-Fei Li. 2016. Visual Genome: Connecting
language and vision using crowdsourced dense im-
age annotations. arXiv preprint arXiv:602.07332 .
</p>
<p>Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL). pages
790–796.
</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out.
pages 25–26.
</p>
<p>TY Lin, M Maire, S Belongie, J Hays, and P Perona.
2014. Microsoft COCO: Common objects in con-
text. In European Conference on Computer Vision
(ECCV). pages 740–755.
</p>
<p>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng
Huang, and Alan Yuille. 2015. Deep captioning with
multimodal recurrent neural networks (M-RNN). In
International Conference on Learning Representa-
tions (ICLR).
</p>
<p>Takashi Miyazaki and Nobuyuki Shimizu. 2016. Cross-
lingual image caption generation. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics (ACL). pages 1780–1790.
</p>
<p>Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
2011. Im2Text: Describing images using 1 million
captioned dhotographs. In Advances in Neural In-
formation Processing Systems (NIPS). pages 1143–
1151.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association of
Computational Linguistics (ACL). pages 311–318.
</p>
<p>Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon’s mechanical turk. In NAACL
HLT Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk. pages 139–
147.
</p>
<p>Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi
Parikh. 2015. CIDEr: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR). pages 4566–4575.
</p>
<p>Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR). pages 3156–3164.
</p>
<p>Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. 2014. From image descriptions to visual
denotations: New similarity metrics for semantic in-
ference over event descriptions. Transactions of the
Association for Computational Linguistics 2:67–78.
</p>
<p>421</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 422–426
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2067
</p>
<p>“Liar, Liar Pants on Fire”:
A New Benchmark Dataset for Fake News Detection
</p>
<p>William Yang Wang
Department of Computer Science
</p>
<p>University of California, Santa Barbara
Santa Barbara, CA 93106 USA
william@cs.ucsb.edu
</p>
<p>Abstract
</p>
<p>Automatic fake news detection is a chal-
lenging problem in deception detection,
and it has tremendous real-world politi-
cal and social impacts. However, statis-
tical approaches to combating fake news
has been dramatically limited by the lack
of labeled benchmark datasets. In this
paper, we present LIAR: a new, publicly
available dataset for fake news detection.
We collected a decade-long, 12.8K man-
ually labeled short statements in various
contexts from POLITIFACT.COM, which
provides detailed analysis report and links
to source documents for each case. This
dataset can be used for fact-checking re-
search as well. Notably, this new dataset
is an order of magnitude larger than pre-
viously largest public fake news datasets
of similar type. Empirically, we investi-
gate automatic fake news detection based
on surface-level linguistic patterns. We
have designed a novel, hybrid convolu-
tional neural network to integrate meta-
data with text. We show that this hybrid
approach can improve a text-only deep
learning model.
</p>
<p>1 Introduction
</p>
<p>In this past election cycle for the 45th President
of the United States, the world has witnessed a
growing epidemic of fake news. The plague of
fake news not only poses serious threats to the
integrity of journalism, but has also created tur-
moils in the political world. The worst real-world
impact is that fake news seems to create real-life
fears: last year, a man carried an AR-15 rifle and
walked in a Washington DC Pizzeria, because he
recently read online that “this pizzeria was harbor-
</p>
<p>ing young children as sex slaves as part of a child-
abuse ring led by Hillary Clinton”1. The man was
later arrested by police, and he was charged for
firing an assault rifle in the restaurant (Kang and
Goldman, 2016).
</p>
<p>The broadly-related problem of deception de-
tection (Mihalcea and Strapparava, 2009) is not
new to the natural language processing commu-
nity. A relatively early study by Ott et al. (2011)
focuses on detecting deceptive review opinions
in sentiment analysis, using a crowdsourcing ap-
proach to create training data for the positive class,
and then combine with truthful opinions from
TripAdvisor. Recent studies have also proposed
stylometric (Feng et al., 2012), semi-supervised
learning (Hai et al., 2016), and linguistic ap-
proaches (Pérez-Rosas and Mihalcea, 2015) to de-
tect deceptive text on crowdsourced datasets. Even
though crowdsourcing is an important approach to
create labeled training data, there is a mismatch
between training and testing. When testing on
real-world review datasets, the results could be
suboptimal since the positive training data was
created in a completely different, simulated plat-
form.
</p>
<p>The problem of fake news detection is more
challenging than detecting deceptive reviews,
since the political language on TV interviews,
posts on Facebook and Twitters are mostly short
statements. However, the lack of manually la-
beled fake news dataset is still a bottleneck
for advancing computational-intensive, broad-
coverage models in this direction. Vlachos and
Riedel (2014) are the first to release a public fake
news detection and fact-checking dataset, but it
only includes 221 statements, which does not per-
mit machine learning based assessments.
</p>
<p>To address these issues, we introduce the LIAR
1http://www.nytimes.com/2016/12/05/business/media/comet-
</p>
<p>ping-pong-pizza-shooting-fake-news-consequences.html
</p>
<p>422</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2067">https://doi.org/10.18653/v1/P17-2067</a></div>
</div>
<div class="page"><p />
<p>dataset, which includes 12,836 short statements
labeled for truthfulness, subject, context/venue,
speaker, state, party, and prior history. With such
volume and a time span of a decade, LIAR is an
order of magnitude larger than the currently avail-
able resources (Vlachos and Riedel, 2014; Ferreira
and Vlachos, 2016) of similiar type. Additionally,
in contrast to crowdsourced datasets, the instances
in LIAR are collected in a grounded, more natural
context, such as political debate, TV ads, Face-
book posts, tweets, interview, news release, etc. In
each case, the labeler provides a lengthy analysis
report to ground each judgment, and the links to
all supporting documents are also provided.
</p>
<p>Empirically, we have evaluated several pop-
ular learning based methods on this dataset.
The baselines include logistic regression, support
vector machines, long short-term memory net-
works (Hochreiter and Schmidhuber, 1997), and a
convolutional neural network model (Kim, 2014).
We further introduce a neural network architecture
to integrate text and meta-data. Our experiment
suggests that this approach improves the perfor-
mance of a strong text-only convolutional neural
networks baseline.
</p>
<p>2 LIAR: a New Benchmark Dataset
</p>
<p>The major resources for deceptive detection of re-
views are crowdsourced datasets (Ott et al., 2011;
Pérez-Rosas and Mihalcea, 2015). They are very
useful datasets to study deception detection, but
the positive training data are collected from a
simulated environment. More importantly, these
datasets are not suitable for fake statements detec-
tion, since the fake news on TVs and social media
are much shorter than customer reviews.
</p>
<p>Vlachos and Riedel (2014) are the first to
construct fake news and fact-checking datasets.
They obtained 221 statements from CHANNEL 42
</p>
<p>and POLITIFACT.COM3, a Pulitzer Prize-winning
website. In particular, PolitiFact covers a wide-
range of political topics, and they provide detailed
judgments with fine-grained labels. Recently, Fer-
reira and Vlachos (2016) have released the Emer-
gent dataset, which includes 300 labeled rumors
from PolitiFact. However, with less than a thou-
sand samples, it is impractical to use these datasets
as benchmarks for developing and evaluating ma-
chine learning algorithms for fake news detection.
</p>
<p>2http://blogs.channel4.com/factcheck/
3http://www.politifact.com/
</p>
<p>Statement: “The last quarter, it was just
announced, our gross domestic product
was below zero. Who ever heard of this?
Its never below zero.”
Speaker: Donald Trump
Context: presidential announcement
speech
Label: Pants on Fire
Justification: According to Bureau of
Economic Analysis and National Bu-
reau of Economic Research, the growth
in the gross domestic product has been
below zero 42 times over 68 years. Thats
a lot more than “never.” We rate his
claim Pants on Fire!
</p>
<p>Statement: “Newly Elected Republican
Senators Sign Pledge to Eliminate Food
Stamp Program in 2015.”
Speaker: Facebook posts
Context: social media posting
Label: Pants on Fire
Justification: More than 115,000 so-
cial media users passed along a story
headlined, “Newly Elected Republican
Senators Sign Pledge to Eliminate Food
Stamp Program in 2015.” But they failed
to do due diligence and were snook-
ered, since the story came from a pub-
lication that bills itself (quietly) as a
“satirical, parody website.” We rate the
claim Pants on Fire.
</p>
<p>Statement: “Under the health care law,
everybody will have lower rates, better
quality care and better access.”
Speaker: Nancy Pelosi
Context: on ’Meet the Press’
Label: False
Justification: Even the study that
Pelosi’s staff cited as the source of that
statement suggested that some people
would pay more for health insurance.
Analysis at the state level found the
same thing. The general understanding
of the word “everybody” is every per-
son. The predictions dont back that up.
We rule this statement False.
</p>
<p>Figure 1: Some random excerpts from the LIAR
dataset.
</p>
<p>423</p>
<p />
</div>
<div class="page"><p />
<p>Dataset Statistics
Training set size 10,269
Validation set size 1,284
Testing set size 1,283
Avg. statement length (tokens) 17.9
Top-3 Speaker Affiliations
Democrats 4,150
Republicans 5,687
None (e.g., FB posts) 2,185
</p>
<p>Table 1: The LIAR dataset statistics.
</p>
<p>Therefore, it is of crucial significance to introduce
a larger dataset to facilitate the development of
computational approaches to fake news detection
and automatic fact-checking.
</p>
<p>We show some random snippets from our
dataset in Figure 1. The LIAR dataset4 in-
cludes 12.8K human labeled short statements from
POLITIFACT.COM’s API5, and each statement is
evaluated by a POLITIFACT.COM editor for its
truthfulness. After initial analysis, we found du-
plicate labels, and merged the full-flop, half-flip,
no-flip labels into false, half-true, true labels re-
spectively. We consider six fine-grained labels for
the truthfulness ratings: pants-fire, false, barely-
true, half-true, mostly-true, and true. The distri-
bution of labels in the LIAR dataset is relatively
well-balanced: except for 1,050 pants-fire cases,
the instances for all other labels range from 2,063
to 2,638. We randomly sampled 200 instances to
examine the accompanied lengthy analysis reports
and rulings. Not that fact-checking is not a classic
labeling task in NLP. The verdict requires exten-
sive training in journalism for finding relevant evi-
dence. Therefore, for second-stage verifications,
we went through a randomly sampled subset of
the analysis reports to check if we agreed with the
reporters’ analysis. The agreement rate measured
by Cohens kappa was 0.82. We show the corpus
statistics in Table 1. The statement dates are pri-
marily from 2007-2016.
</p>
<p>The speakers in the LIAR dataset include a mix
of democrats and republicans, as well as a sig-
nificant amount of posts from online social me-
dia. We include a rich set of meta-data for each
speaker—in addition to party affiliations, current
</p>
<p>4https://www.cs.ucsb.edu/˜william/
data/liar_dataset.zip
</p>
<p>5http://static.politifact.com/api/
v2apidoc.html
</p>
<p>Figure 2: The proposed hybrid Convolutional
Neural Networks framework for integrating text
and meta-data.
</p>
<p>job, home state, and credit history are also pro-
vided. In particular, the credit history includes the
historical counts of inaccurate statements for each
speaker. For example, Mitt Romney has a credit
history vector h = {19, 32, 34, 58, 33}, which cor-
responds to his counts of “pants on fire”, “false”,
“barely true”, “half true”, “mostly true” for histor-
ical statements. Since this vector also includes the
count for the current statement, it is important to
subtract the current label from the credit history
when using this meta data vector in prediction ex-
periments.
</p>
<p>These statements are sampled from various
of contexts/venues, and the top categories in-
clude news releases, TV/radio interviews, cam-
paign speeches, TV ads, tweets, debates, Face-
book posts, etc. To ensure a broad coverage of
the topics, there is also a diverse set of subjects
discussed by the speakers. The top-10 most dis-
cussed subjects in the dataset are economy, health-
care, taxes, federal-budget, education, jobs, state-
budget, candidates-biography, elections, and im-
migration.
</p>
<p>3 Automatic Fake News Detection
</p>
<p>One of the most obvious applications of our
dataset is to facilitate the development of machine
learning models for automatic fake news detec-
tion. In this task, we frame this as a 6-way multi-
class text classification problem. And the research
questions are:
</p>
<p>• Based on surface-level linguistic realizations
only, how well can machine learning algo-
rithms classify a short statement into a fine-
grained category of fakeness?
</p>
<p>• Can we design a deep neural network archi-
tecture to integrate speaker related meta-data
</p>
<p>424</p>
<p />
</div>
<div class="page"><p />
<p>with text to enhance the performance of fake
news detection?
</p>
<p>Since convolutional neural networks architec-
tures (CNNs) (Collobert et al., 2011; Kim, 2014;
Zhang et al., 2015) have obtained the state-of-the-
art results on many text classification datasets, we
build our neural networks model based on a re-
cently proposed CNN model (Kim, 2014). Fig-
ure 2 shows the overview of our hybrid convo-
lutional neural network for integrating text and
meta-data.
</p>
<p>We randomly initialize a matrix of embedding
vectors to encode the metadata embeddings. We
use a convolutional layer to capture the depen-
dency between the meta-data vector(s). Then,
a standard max-pooling operation is performed
on the latent space, followed by a bi-directional
LSTM layer. We then concatenate the max-pooled
text representations with the meta-data representa-
tion from the bi-directional LSTM, and feed them
to fully connected layer with a softmax activation
function to generate the final prediction.
</p>
<p>4 LIAR: Benchmark Evaluation
</p>
<p>In this section, we first describe the experimental
setup, and the baselines. Then, we present the em-
pirical results and compare various models.
</p>
<p>4.1 Experimental Settings
</p>
<p>We used five baselines: a majority baseline, a reg-
ularized logistic regression classifier (LR), a sup-
port vector machine classifier (SVM) (Crammer
and Singer, 2001), a bi-directional long short-term
memory networks model (Bi-LSTMs) (Hochreiter
and Schmidhuber, 1997; Graves and Schmidhu-
ber, 2005), and a convolutional neural network
model (CNNs) (Kim, 2014). For LR and SVM,
we used the LIBSHORTTEXT toolkit6, which was
shown to provide very strong performances on
short text classification problems (Wang and Yang,
2015). For Bi-LSTMs and CNNs, we used Ten-
sorFlow for the implementation. We used pre-
trained 300-dimensional word2vec embeddings
from Google News (Mikolov et al., 2013) to
warm-start the text embeddings. We strictly tuned
all the hyperparameters on the validation dataset.
The best filter sizes for the CNN model was
(2,3,4). In all cases, each size has 128 filters. The
dropout keep probabilities was optimized to 0.8,
</p>
<p>6https://www.csie.ntu.edu.tw/˜cjlin/libshorttext/
</p>
<p>Models Valid. Test
Majority 0.204 0.208
SVMs 0.258 0.255
Logistic Regress0ion 0.257 0.247
Bi-LSTMs 0.223 0.233
CNNs 0.260 0.270
Hybrid CNNs
Text + Subject 0.263 0.235
Text + Speaker 0.277 0.248
Text + Job 0.270 0.258
Text + State 0.246 0.256
Text + Party 0.259 0.248
Text + Context 0.251 0.243
Text + History 0.246 0.241
Text + All 0.247 0.274
</p>
<p>Table 2: The evaluation results on the LIAR
dataset. The top section: text-only models. The
bottom: text + meta-data hybrid models.
</p>
<p>while no L2 penalty was imposed. The batch size
for stochastic gradient descent optimization was
set to 64, and the learning process involves 10
passes over the training data for text model. For
the hybrid model, we use 3 and 8 as filter sizes,
and the number of filters was set to 10. We con-
sidered 0.5 and 0.8 as dropout probabilities. The
hybrid model requires 5 training epochs.
</p>
<p>We used grid search to tune the hyperparame-
ters for LR and SVM models. We chose accuracy
as the evaluation metric, since we found that the
accuracy results from various models were equiv-
alent to f-measures on this balanced dataset.
</p>
<p>4.2 Results
</p>
<p>We outline our empirical results in Table 2. First,
we compare various models using text features
only. We see that the majority baseline on this
dataset gives about 0.204 and 0.208 accuracy on
the validation and test sets respectively. Standard
text classifier such as SVMs and LR models ob-
tained significant improvements. Due to overfit-
ting, the Bi-LSTMs did not perform well. The
CNNs outperformed all models, resulting in an ac-
curacy of 0.270 on the heldout test set. We com-
pare the predictions from the CNN model with
SVMs via a two-tailed paired t-test, and CNN was
significantly better (p &lt; .0001). When consider-
ing all meta-data and text, the model achieved the
best result on the test data.
</p>
<p>425</p>
<p />
</div>
<div class="page"><p />
<p>5 Conclusion
</p>
<p>We introduced LIAR, a new dataset for automatic
fake news detection. Compared to prior datasets,
LIAR is an order of a magnitude larger, which en-
ables the development of statistical and computa-
tional approaches to fake news detection. LIAR’s
authentic, real-world short statements from vari-
ous contexts with diverse speakers also make the
research on developing broad-coverage fake news
detector possible. We show that when combin-
ing meta-data with text, significant improvements
can be achieved for fine-grained fake news detec-
tion. Given the detailed analysis report and links to
source documents in this dataset, it is also possible
to explore the task of automatic fact-checking over
knowledge base in the future. Our corpus can also
be used for stance classification, argument min-
ing, topic modeling, rumor detection, and political
NLP research.
</p>
<p>References
Ronan Collobert, Jason Weston, Léon Bottou, Michael
</p>
<p>Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12(Aug):2493–2537.
</p>
<p>Koby Crammer and Yoram Singer. 2001. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. Journal of machine learning re-
search 2(Dec):265–292.
</p>
<p>Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2. Association for Computational
Linguistics, pages 171–175.
</p>
<p>William Ferreira and Andreas Vlachos. 2016. Emer-
gent: a novel data-set for stance classification. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
ACL.
</p>
<p>Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works 18(5):602–610.
</p>
<p>Zhen Hai, Peilin Zhao, Peng Cheng, Peng Yang, Xiao-
Li Li, Guangxia Li, and Ant Financial. 2016. De-
ceptive review spam detection via exploiting task re-
latedness and unlabeled data. In EMNLP.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Cecilia Kang and Adam Goldman. 2016. In washing-
ton pizzeria attack, fake news brought real guns. In
the New York Times.
</p>
<p>Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
</p>
<p>Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computa-
tional Linguistics, pages 309–319.
</p>
<p>Verónica Pérez-Rosas and Rada Mihalcea. 2015. Ex-
periments in open domain deception detection. In
EMNLP. pages 1120–1125.
</p>
<p>Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task definition and dataset construction.
Proceedings of the ACL 2014 Workshop on Lan-
guage Technology and Computational Social Sci-
ence .
</p>
<p>William Yang Wang and Diyi Yang. 2015. That’s
so annoying!!!: A lexical and frame-semantic em-
bedding based data augmentation approach to au-
tomatic categorization of annoying behaviors using
#petpeeve tweets. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2015). ACL, Lisbon, Portugal.
</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in neural information pro-
cessing systems. pages 649–657.
</p>
<p>426</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 427–432
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2068
</p>
<p>English Multiword Expression-aware
Dependency Parsing Including Named Entities
</p>
<p>Akihiko Kato and Hiroyuki Shindo and Yuji Matsumoto
Graduate School of Information and Science
</p>
<p>Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara, 630-0192, Japan
</p>
<p>{kato.akihiko.ju6, shindo, matsu} @is.naist.jp
</p>
<p>Abstract
</p>
<p>Because syntactic structures and spans of
multiword expressions (MWEs) are inde-
pendently annotated in many English syn-
tactic corpora, they are generally inconsis-
tent with respect to one another, which is
harmful to the implementation of an ag-
gregate system. In this work, we construct
a corpus that ensures consistency between
dependency structures and MWEs, in-
cluding named entities. Further, we ex-
plore models that predict both MWE-
spans and an MWE-aware dependency
structure. Experimental results show that
our joint model using additional MWE-
span features achieves an MWE recogni-
tion improvement of 1.35 points over a
pipeline model.
</p>
<p>1 Introduction
</p>
<p>To solve complex Natural Language Processing
(NLP) tasks that require deep syntactic analysis,
various levels of annotation such as parse trees and
named entities (NEs) must be consistent with one
another (Finkel and Manning, 2009). Otherwise,
it is usually impossible to combine these pieces of
information effectively.
</p>
<p>However, the standard syntactic corpus of En-
glish, Penn Treebank, is not concerned with con-
sistency between syntactic trees and spans of mul-
tiword expressions (MWEs). In Penn Treebank,
that is, an MWE-span does not always correspond
to a span dominated by a single non-terminal
node. Therefore, word-based dependency struc-
tures converted from Penn Treebank are generally
inconsistent with MWE-spans (Figure 1a). To mit-
igate this inconsistency, Kato et al. (2016) estab-
</p>
<p>(a) a word-based dependency structure
</p>
<p>(b) an MWE-aware dependency structure
</p>
<p>Figure 1: A word-based and an MWE-aware de-
pendency structure. In the former, a span of an
MWE (“a number of”) does not correspond to any
subtree. The MWE is represented as a single node
in the latter structure.
</p>
<p>lishes each span of functional MWEs 1 as a sub-
tree of a phrase structure in the Wall Street Journal
portion of Ontonotes (Pradhan et al., 2007).
</p>
<p>To pursue this direction further, we construct a
corpus such that dependency structures are consis-
tent with MWEs, by extending Kato et al. (2016)’s
corpus 2. As is the case with their corpus, each
MME is a syntactic unit in an MWE-aware de-
pendency structure from our corpus (Figure 1b).
Moreover, our corpus includes not only functional
MWEs but also NEs. Because NEs are highly pro-
ductive and occur more frequently than functional
MWEs, they are difficult to cover in a dictionary.
</p>
<p>Consistency between NE-spans and phrase
structures is not guaranteed because they are in-
dependently annotated in most syntactic corpora.
</p>
<p>1By functional MWEs, we mean MWEs that function ei-
ther as prepositions, conjunctions, determiners, pronouns, or
adverbs.
</p>
<p>2We release our dependency corpus at https:
//github.com/naist-cl-parsing/
mwe-aware-dependency. MWE-aware phrase struc-
tures will be distributed from LDC as a part of LDC2017T01.
</p>
<p>427</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2068">https://doi.org/10.18653/v1/P17-2068</a></div>
</div>
<div class="page"><p />
<p>Figure 2: Example of inconsistency between NE-
spans and phrase structures. A rectangle shows an
NE-span.
</p>
<p>MWE POS NNP RB IN others
MWE 20,992 3,796 2,424 737
Instances
MWE Types 11,875 377 92 52
</p>
<p>Table 1: Corpus statistics.
</p>
<p>For instance, in Figure 2, an NE-span is “Board of
Investment,” which is inconsistent with the syntac-
tic tree. Therefore, we resolve this inconsistency
by modifying phrase structures locally and estab-
lishing each NE as a subtree.
</p>
<p>Furthermore, to evaluate the constructed cor-
pus, we explore pipeline and joint models that pre-
dict both MWE-spans and an MWE-aware depen-
dency tree 3. Our experimental results show that
the proposed joint model with additional MWE-
span features achieves an MWE recognition im-
provement of 1.35 points over the pipeline model.
</p>
<p>2 MWE-aware Dependency Corpus
</p>
<p>To ensure consistency between MWE annotations
and dependency structures, we first integrate NE
</p>
<p>Type Non- Contiguous Crossing
of MWEs terminal children brackets
Functional 3,466 1,663 1,799
MWEs
NEs 18,625 2,252 144
</p>
<p>Table 2: Histogram tabling the consistency be-
tween MWE-spans and phrase structures.
</p>
<p>3Although Kato et al. (2016) conducts experiments re-
garding MWE-aware dependency parsing, they use gold
MWE-spans. This is not a realistic scenario. By contrast,
our parsing models do not use gold MWE-spans.
</p>
<p>annotations on Ontonotes 4 into phrase structures
such that functional MWEs are established as sub-
trees. Subsequently, we convert phrase structures
to dependency structures. We construct our corpus
by extending Kato et al. (2016)’s corpus 5, which
is itself built on a corpus by Shigeto et al. (2013).
Regarding MWE annotations, Shigeto et al. (2013)
first constructed an MWE dictionary by extract-
ing functional MWEs from the English-language
Wiktionary 6, and classified their occurrences in
Ontonotes into either MWE or literal usage. Kato
et al. (2016) integrated these MWE annotations
into phrase structures and established functional
MWEs as subtrees.
</p>
<p>Next, we describe the establishment of each
NE as a subtree. If an NE-span does not corre-
spond to any non-terminal in a phrase structure,
there are two possibilities: (A) the NE-span cor-
responds to multiple contiguous children of a sub-
tree, or (B) the NE-span has crossing brackets with
the spans in the parse tree (Finkel and Manning,
2009; Kato et al., 2016). In Case (A), we insert
a new non-terminal (“MWE NNP”) that governs
the NE-span 7. In Case (B), many instances corre-
spond to a noun phrase (NP) comprised of a nested
NP and a prepositional phrase (Figure 2). In the
main NP, a modifier, such as a determiner, an ad-
jective, or a possessive NP, precedes an NE. For
these instances, according to Finkel and Manning
(2009), we reduce Case (B) to Case (A) by moving
the modifier from the nested NP to the main NP.
Then, we establish each NE as a subtree by insert-
ing an MWE-specific non-terminal. Furthermore,
in some instances it is more reasonable to enlarge
NE-spans than to modify phrase structures. As
a typical example, there is an NE annotation that
covers only part of a coordination structure, such
as “Peter and Edward Bronfman,” where “Edward
Bronfman” is annotated as an NE. In this case,
we extend an original NE-span to the whole co-
ordination structure. We show the statistics for
the corpus in Table 1 8. This corpus has 27,949
MWE instances in 37,015 sentences. A histogram
</p>
<p>4We exploit NE annotations on Ontonotes Release 5.0
(LDC2013T19). We address traditional NEs, such as per-
sons, locations, and organizations, while omitting the follow-
ing: DATE, TIME, PERCENT, MONEY, QUANTITY, OR-
DINAL, and CARDINAL. Note that we only focus on multi-
word NEs.
</p>
<p>5https://catalog.ldc.upenn.edu/LDC2017T01
6https://en.wiktionary.org
7We do not require manual annotations for Case (A).
8NEs have NNP as an MWE-level POS tag.
</p>
<p>428</p>
<p />
</div>
<div class="page"><p />
<p>Figure 3: In the joint model, we directly infer an
MWE-aware dependency tree in which an MWE
(“a number of”) is represented as a head-initial
structure by a dependency parser.
</p>
<p>tabling the consistency between MWE-spans and
phrase structures is shown in Table 2. For tree-
to-dependency conversion, we first replace a sub-
tree corresponding to an MWE by a preterminal
node and its child node. The preterminal node has
an MWE-level POS (MWE POS) tag. The child
node is generated by joining all components of the
MWE with underscores. We then convert a phrase
structure into a Stanford-style dependency struc-
ture (Marneffe and Manning, 2008) (Figure 1b).
</p>
<p>3 Models for MWE identification and
MWE-aware dependency parsing
</p>
<p>In this section, we explore models that predict
both MWE-spans and an MWE-aware depen-
dency structure (Figure 1b).
</p>
<p>3.1 Pipeline Model
</p>
<p>The pipeline model involves the following three
steps. First, BIO tags encoding MWE-spans and
MWE POS tags, such as “B NNP” and “I DT” are
predicted by a sequential labeler based on Con-
ditional Random Fields (CRFs) (Lafferty et al.,
2001). Second, tokens belonging to each pre-
dicted MWE-span are concatenated into a sin-
gle node. Finally, an MWE-based dependency
structure (Figure 1b) is predicted by an arc-eager
transition-based parser. For the CRFs, in addi-
tion to word-form and character-based features,
we use 1- to 3-gram features based on dictionaries
of functional MWEs and NEs within 5-word win-
dows from a target token. For a dictionary of func-
tional MWEs, we use the dictionary by Shigeto
et al. (2013) (Section 2). Meanwhile, we create
a dictionary of NEs from a title list of English
Wikipedia articles, excepting stop words, provided
by UniNE 9. Regarding parsing features, we use
</p>
<p>9http://members.unine.ch/jacques.savoy/clef/englishST.txt
</p>
<p>baseline features and rich non-local features pro-
posed by Zhang and Nivre (2011).
</p>
<p>3.2 Joint Model
In the proposed joint model, MWE-spans and
MWE POS tags are encoded as dependency la-
bels, and conventional word-based dependency
parsing is performed by an arc-eager transition-
based parser. We use the same parsing features
used in the pipeline model. We convert MWEs
in MWE-aware dependency structures (Figure 1b)
to head-initial structures (Figure 3) that encode
MWE-spans and MWE POS tags. Note that
this representation is similar to Universal Depen-
dency (McDonald et al., 2013). When parsing, we
use constraints based on a history of transitions
and the dictionary of functional MWEs. This is
done to avoid invalid dependency trees. Because
NEs are highly productive, we do not use a con-
straint regarding NEs.
</p>
<p>Joint(+dict)
We designed additional features based on matches
with dictionaries of NEs and functional MWEs.
Hereafter, we refer to the joint model coupled with
these additional features as joint(+dict). For in-
stance, given a sentence that starts with “a number
of cities,” the additional features are as follows: a
/ B DT, number / I DT, of / I DT, cities / O. Based
on these additional features, we extend the base-
line features proposed by Zhang and Nivre (2011)
to develop MWE-specific features whose atomic
features include not only words and word-level
POS tags, but also BIO tags encoding MWE-spans
and MWE POS tags.
</p>
<p>Joint(+pred span)
Because dictionary matching is not concerned
with context, in this setting, we use MWE-spans
and MWE POS tags predicted by CRF, rather than
dictionary matching. Hereafter, we refer to this
as joint(+pred span). By using features extracted
from CRF predictions, we can mitigate error prop-
agation from sequential labeling and consider in-
formation from a full sentence. Moreover, we can
alleviate difficulties in predicting MWE-spans and
MWE POS tags encoded as head-initial structures
(Figure 3) by the parser.
</p>
<p>4 Experimental Setting
We split the Wall Street Journal (WSJ) portion of
Ontonotes, using sections 2-21 for training, and
section 23 for testing. For all models, we used
</p>
<p>429</p>
<p />
</div>
<div class="page"><p />
<p>Dependency Parsing MWE Recognition
All sentences First tokens of MWEs
</p>
<p>Model UAS LAS UAS LAS FUM FTM
Pipeline 91.39 89.42 84.06 78.22 91.40 91.32
Joint 91.15 89.18 81.93 77.74 89.03 88.79
Joint(+dict) 91.36 89.37 84.45 80.74 91.93 91.78
Joint(+pred span) 91.50 89.51 84.85 81.29 92.75 92.60
</p>
<p>Table 3: Experimental results on the test set.
</p>
<p>Dependency Parsing MWE Recognition
(First tokens of MWEs)
Functional MWEs NEs Functional MWEs NEs
</p>
<p>Model UAS LAS UAS LAS FUM FTM FUM
Pipeline 78.89 64.01 85.58 82.41 96.76 96.42 89.81
Joint 71.28 65.05 85.07 81.49 91.01 89.93 88.47
Joint(+dict) 79.93 73.70 85.79 82.82 97.94 97.25 90.16
Joint(+pred span) 81.31 74.74 85.89 83.23 97.59 96.91 91.32
</p>
<p>Table 4: Breakdown of experimental results by type of MWE. Note that UAS / LAS are calculated
regarding first tokens of MWEs. For NEs, the FTM is the same as the FUM because each NE always
takes NNP as an MWE-level POS tag, and is not repeated.
</p>
<p>the POS tags predicted by the Stanford POS tag-
ger (Toutanova et al., 2003) 10. For the pipeline
model and joint(+pred span), we used MWE-
spans and MWE POS tags predicted by CRF 11.
For dependency parsing, we used Redshift (Hon-
nibal et al., 2013) for all models, with a beam
size of 16 for decoding. For training, we re-
moved non-projective dependency trees. For test-
ing, we parsed all sentences. To evaluate pars-
ing, we used unlabeled and labeled attachment
scores (UAS/LAS) 12. For the pipeline model, we
converted each concatenated token correspond-
ing to an MWE into a head-initial structure and
compared this with the gold tree. For the joint
model, we directly compared a predicted tree
with the gold tree. To evaluate MWE recogni-
tion, we used the F-measure for untagged / tagged
MWEs (FUM/FTM) 13. For the pipeline model,
we compared the gold MWEs with predictions by
CRF. For the proposed joint model, we compared
the gold MWEs with predicted MWE-spans and
</p>
<p>10We used 20-way jackknifing for the training split. The
test split was automatically tagged by the POS tagger trained
on the training split.
</p>
<p>11We used 20-way jackknifing for the training split. The
test split was automatically tagged by the sequential labeler
trained on the training split.
</p>
<p>12When calculating UAS/LAS, we removed punctuation.
13FUM only focuses on MWE-spans, whereas FTM fo-
</p>
<p>cuses on both MWE-spans and MWE POS tags.
</p>
<p>MWE POS tags represented as dependency labels.
</p>
<p>5 Experimental Results and Discussion
We present the experimental results in Table 3.
Comparing the joint model with the pipeline
model, there is not much difference between these
models regarding UAS / LAS for all sentences.
However, the former is 2.13 / 0.48 points worse
than the latter in terms of UAS / LAS regarding the
first tokens of MWEs (1269 in 34,526 tokens), and
2.37 / 2.53 points worse than the latter regarding
FUM / FTM. These results suggest that the perfor-
mance of the joint model with no additional fea-
tures at predicting dependencies inside and around
MWEs is worse than the pipeline model. One of
the reasons for this is that the exploitation of head-
initial structures in the joint model (Figure 3) in-
volves the addition of MWE-specific labels. This
results in an increase in the total number of de-
pendency labels from 41 to 50. Because of this
broader output space, more search errors can oc-
cur in the joint model compared with the pipeline
model. Moreover, a breakdown by type of MWE
(Table 4) shows that most differences in perfor-
mance between these two models are related to
functional MWEs. These results suggest that con-
straints regarding functional MWEs during pars-
ing (3.2) are harmful to the joint model with no ad-
ditional features in terms of its performance with
</p>
<p>430</p>
<p />
</div>
<div class="page"><p />
<p>respect to functional MWEs.
</p>
<p>By adding MWE-specific features to the joint
model, however, we observe at least a 2.52 / 3.00
point improvement in terms of UAS / LAS regard-
ing the first tokens of MWEs, and a 2.90 / 2.99
point improvement regarding FUM / FTM. As a
result, we obtain a 1.35 / 1.28 point improvement
with joint(+pred span) compared with the pipeline
model in terms of FUM / FTM. A breakdown by
type of MWE shows that the addition of MWE-
specific features leads to a performance improve-
ment, especially for functional MWEs (Table 4).
These results suggest that MWE-specific features
are effective at both MWE recognition through de-
pendency parsing and the prediction of dependen-
cies connecting inside and outside of MWEs.
</p>
<p>Comparing the joint(+pred span) with the
joint(+dict), the former is 0.40 / 0.55 points better
than the latter in terms of UAS / LAS regarding the
first tokens of MWEs, and 0.82 / 0.82 points bet-
ter than the latter regarding FUM / FTM. We can
attribute this gain in performance to the additional
features extracted from more accurate predictions
of MWE-spans and MWE POS tags by CRF than
those by dictionary matching.
</p>
<p>6 Related Work
Whereas French Treebank is available for French
MWEs (Abeillé et al., 2003), there have been only
limited corpora for English MWE-aware depen-
dency parsing. Schneider et al. (2014) constructs
an MWE-annotated corpus on English Web Tree-
bank (Bies et al., 2012). However, this corpus is
relatively small as training data for a parser, and
its MWE annotations are not consistent with syn-
tactic trees. By contrast, our corpus covers the
whole of the WSJ portion of Ontonotes and en-
sures consistency between MWE annotations and
parse trees.
</p>
<p>Korkontzelos and Manandhar (2010) reports
an improvement in base-phrase chunking by pre-
grouping MWEs as words-with-spaces. They fo-
cus on compound nouns, adjective-noun construc-
tions, and named entities. However, they use gold
MWE-spans, and this is not a realistic setting. By
contrast, we use predicted MWE-spans.
</p>
<p>Three works concerned with a French MWE-
aware syntactic parsing are relevant. First, Green
et al. (2013) proposes a method for recognizing
contiguous MWEs as a part of constituency pars-
ing by using MWE-specific non-terminals. They
</p>
<p>investigate a CFG-based model and a model based
on tree-substitution grammars. Second, Candito
and Constant (2014) compares several architec-
tures for graph-based dependency parsing and
MWE recognition, in which MWE recognition is
conducted before, during, and after parsing. Fi-
nally, Nasr et al. (2015) explores a joint model of
MWE recognition and dependency parsing. They
focus on complex function words. In terms of data
representation, they adopt one similar to ours, in-
sofar as the components of an MWE are linked
by dependency edges whose labels are MWE-
specific.
</p>
<p>7 Conclusion
</p>
<p>We constructed a corpus that ensures consistency
in Ontonotes between dependency structures and
English MWEs, including named entities. Fur-
thermore, we explored models that can predict
both MWE-spans and an MWE-aware depen-
dency structure. Our experiments show that by
using additional MWE-span features, our joint
model achieves an MWE recognition improve-
ment of 1.35 points over the pipeline model.
</p>
<p>Acknowledgments
</p>
<p>This work was partially supported by JST CREST
Grant Number JPMJCR1513 and JSPS KAK-
ENHI Grant Number 15K16053. We are grate-
ful to members of the Computational Linguistics
Laboratory at NAIST, and to the anonymous re-
viewers for their valuable feedback. Regarding the
preparation of a title list from English-language
Wikipedia articles, we are particularly grateful for
the assistance given by Motoki Sato.
</p>
<p>References
Anne Abeillé, Lionel Clément, and François Toussenel.
</p>
<p>2003. Building a Treebank for French, Springer
Netherlands, Dordrecht, pages 165–187.
</p>
<p>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English web treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, Pennsylvania, USA. .
</p>
<p>Marie Candito and Matthieu Constant. 2014. Strate-
gies for contiguous multiword expression analysis
and dependency parsing. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Associ-
ation for Computational Linguistics, pages 743–753.
https://doi.org/10.3115/v1/P14-1070.
</p>
<p>431</p>
<p />
</div>
<div class="page"><p />
<p>Rose Jenny Finkel and D. Christopher Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 326–334. http://aclweb.org/anthology/N09-
1037.
</p>
<p>Spence Green, Marie-Catherine de Marneffe, and
Christopher D Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics 39(1):195–227.
</p>
<p>Matthew Honnibal, Yoav Goldberg, and Mark John-
son. 2013. Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing, Association for Computational Linguistics,
chapter A Non-Monotonic Arc-Eager Transition
System for Dependency Parsing, pages 163–172.
http://aclweb.org/anthology/W13-3518.
</p>
<p>Akihiko Kato, Hiroyuki Shindo, and Yuji Matsumoto.
2016. Construction of an english dependency
corpus incorporating compound function words.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Sara Goggi, Marko Gro-
belnik, Bente Maegaard, Joseph Mariani, Helene
Mazo, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016). European Language Resources
Association (ELRA), Paris, France.
</p>
<p>Ioannis Korkontzelos and Suresh Manandhar. 2010.
Can recognising multiword expressions improve
shallow parsing? In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Los Angeles, California, pages 636–
644. http://www.aclweb.org/anthology/N10-1089.
</p>
<p>John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International
Conf. on Machine Learning. Morgan Kaufmann,
San Francisco, CA, pages 282–289. cite-
seer.ist.psu.edu/lafferty01conditional.html.
</p>
<p>Marie-Catherine Marneffe and D. Christopher
Manning. 2008. Coling 2008: Proceedings
of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, Coling 2008
Organizing Committee, chapter The Stanford
Typed Dependencies Representation, pages 1–8.
http://aclweb.org/anthology/W08-1301.
</p>
<p>Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar Täckström, Claudia Bedini, Núria
</p>
<p>Bertomeu Castelló, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 92–97.
http://aclweb.org/anthology/P13-2017.
</p>
<p>Alexis Nasr, Carlos Ramisch, José Deulofeu, and
André Valli. 2015. Joint dependency parsing and
multiword expression tokenization. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1116–
1126. http://www.aclweb.org/anthology/P15-1108.
</p>
<p>Sameer S. Pradhan, Eduard H. Hovy, Mitchell P.
Marcus, Martha Palmer, Lance A. Ramshaw, and
Ralph M. Weischedel. 2007. Ontonotes: A uni-
fied relational semantic representation. In Proceed-
ings of the First IEEE International Conference on
Semantic Computing (ICSC 2007), September 17-
19, 2007, Irvine, California, USA. pages 517–526.
https://doi.org/10.1109/ICSC.2007.83.
</p>
<p>Nathan Schneider, Spencer Onuffer, Nora Kazour,
Emily Danchik, Michael T. Mordowanec, Henri-
etta Conrad, and Noah A. Smith. 2014. Compre-
hensive annotation of multiword expressions in a
social web corpus. In Nicoletta Calzolari, Khalid
Choukri, Thierry Declerck, Hrafn Loftsson, Bente
Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC’14). European Lan-
guage Resources Association (ELRA), Reykjavik,
Iceland, pages 455–461. ACL Anthology Identifier:
L14-1433.
</p>
<p>Yutaro Shigeto, Ai Azuma, Sorami Hisamoto,
Shuhei Kondo, Tomoya Kouse, Keisuke Sak-
aguchi, Akifumi Yoshimoto, Frances Yung, and
Yuji Matsumoto. 2013. Proceedings of the 9th
Workshop on Multiword Expressions, Associa-
tion for Computational Linguistics, chapter Con-
struction of English MWE Dictionary and its
Application to POS Tagging, pages 139–144.
http://aclweb.org/anthology/W13-1021.
</p>
<p>Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.
http://aclweb.org/anthology/N03-1033.
</p>
<p>Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics, pages 188–193.
http://aclweb.org/anthology/P11-2033.
</p>
<p>432</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 433–440
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2069
</p>
<p>Improving Semantic Composition with Offset Inference
</p>
<p>Thomas Kober, Julie Weeds, Jeremy Reffin and David Weir
TAG laboratory, Department of Informatics, University of Sussex
</p>
<p>Brighton, BN1 9RH, UK
{t.kober, j.e.weeds, j.p.reffin, d.j.weir}@sussex.ac.uk
</p>
<p>Abstract
</p>
<p>Count-based distributional semantic mod-
els suffer from sparsity due to unobserved
but plausible co-occurrences in any text
collection. This problem is amplified
for models like Anchored Packed Trees
(APTs), that take the grammatical type of a
co-occurrence into account. We therefore
introduce a novel form of distributional in-
ference that exploits the rich type struc-
ture in APTs and infers missing data by the
same mechanism that is used for semantic
composition.
</p>
<p>1 Introduction
</p>
<p>Anchored Packed Trees (APTs) is a recently pro-
posed approach to distributional semantics that
takes distributional composition to be a process
of lexeme contextualisation (Weir et al., 2016).
A lexeme’s meaning, characterised as knowledge
concerning co-occurrences involving that lexeme,
is represented with a higher-order dependency-
typed structure (the APT) where paths associated
with higher-order dependencies connect vertices
associated with weighted lexeme multisets. The
central innovation in the compositional theory is
that the APT’s type structure enables the precise
alignment of the semantic representation of each
of the lexemes being composed. Like other count-
based distributional spaces, however, it is prone to
considerable data sparsity, caused by not observ-
ing all plausible co-occurrences in the given data.
Recently, Kober et al. (2016) introduced a sim-
ple unsupervised algorithm to infer missing co-
occurrence information by leveraging the distribu-
tional neighbourhood and ease the sparsity effect
in count-based models.
</p>
<p>In this paper, we generalise distributional in-
ference (DI) in APTs and show how precisely
</p>
<p>the same mechanism that was introduced to sup-
port distributional composition, namely “offset-
ting” APT representations, gives rise to a novel
form of distributional inference, allowing us to in-
fer co-occurrences from neighbours of these rep-
resentations. For example, by transforming a rep-
resentation of white to a representation of “things
that can be white”, inference of unobserved, but
plausible, co-occurrences can be based on find-
ing near neighbours (which will be nouns) of the
“things that can be white” structure. This further-
more exposes an interesting connection between
distributional inference and distributional compo-
sition. Our method is unsupervised and maintains
the intrinsic interpretability of APTs1.
</p>
<p>2 Offset Representations
</p>
<p>The basis of how composition is modelled in the
APT framework is the way that the co-occurrences
are structured. In characterising the distribu-
tional semantics of some lexeme w, rather than
just recording a co-occurrence between w and w′
</p>
<p>within some context window, we follow Padó and
Lapata (2007) and record the dependency path
from w to w′. This syntagmatic structure makes it
possible to appropriately offset the semantic repre-
sentations of each of the lexemes being composed
in some phrase. For example many nouns will
have distributional features starting with the type
amod, which cannot be observed for adjectives or
verbs. Thus, when composing the adjective white
with the noun clothes, the feature spaces of the
two lexemes need to be aligned first. This can
be achieved by offsetting one of the constituents,
which we will explain in more detail in this sec-
tion.
</p>
<p>We will make use of the following nota-
</p>
<p>1We release our code and data at https://github.
com/tttthomasssss/acl2017
</p>
<p>433</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2069">https://doi.org/10.18653/v1/P17-2069</a></div>
</div>
<div class="page"><p />
<p>tion throughout this work. A typed distribu-
tional feature consists of a path and a lexeme
such as in amod:white. Inverse paths are de-
noted by a horizontal bar above the dependency
relation such as in dobj:prefer and higher-
order paths are separated by a dot such as in
amod.compound:dress.
</p>
<p>Offset representations are the central compo-
nent in the composition process in the APT frame-
work. Figure 1 shows the APT representations for
the adjective white (left) and the APT for the noun
clothes (right), as might have been observed in a
text collection. Each node holds a multiset of lex-
emes and the anchor of an APT reflects the current
perspective of a lexeme at the given node. An off-
set representation can be created by shifting the
anchor along a given path. For example the lex-
eme white is at the same node as other adjectives
such as black and clean, whereas nouns such as
shoes or noise are typically reached via the amod
edge.
</p>
<p>Offsetting in APTs only involves a change in
the anchor, the underlying structure remains un-
changed. By offsetting the lexeme white by amod
the anchor is shifted along the amod edge, which
results in creating a noun view for the adjective
white. We denote the offset view of a lexeme for
a given path by superscripting the offset path, for
example the amod offset of the adjective white is
denoted as whiteamod. The offsetting procedure
changes the starting points of the paths as visi-
ble in Figure 1 between the anchors for white and
whiteamod, since paths always begin at the anchor.
The red dashed line in Figure 1 reflects that anchor
shift. The lexeme whiteamod represents a proto-
typical “white thing”, that is, a noun that has been
modified by the adjective white. We note that all
edges in the APT space are bi-directional as ex-
emplified in the coloured amod and amod edges
in the APT for white, however for brevity we only
show uni-directional edges in Figure 1.
</p>
<p>By considering the APT representations for the
lexemes white and clothes in Figure 1, it be-
comes apparent that lexemes with different parts
of speech are located in different areas of the
semantic space. If we want to compose the
adjective-noun phrase white clothes, we need to
offset one of the two constituents to align the fea-
ture spaces in order to leverage their distributional
commonalities. This can be achieved by either
creating a noun offset view of white, by shift-
</p>
<p>ing the anchor along the amod edge, or by cre-
ating an adjective offset representation of clothes
by shifting its anchor along amod. In this work
we follow Weir et al. (2016) and always offset
the dependent in a given relation. Table 1 shows
a subset of the features of Figure 1 as would
be represented in a vectorised APT. Vectorising
the whole APT lexicon results in a very high-
dimensional and sparse typed distributional space.
The features for whiteamod (middle column) high-
light the change in feature space caused by offset-
ting the adjective white. The features of the off-
set view whiteamod, are now aligned with the noun
clothes such that the two can be composed. Com-
position can be performed by either selecting the
union or intersection of the aligned features.
</p>
<p>white whiteamod clothes
:clean amod:clean amod:wet
amod:shoes :shoes :dress
amod.dobj:wear dobj:wear dobj:wear
amod.nsubj.earn nsubj:earn nsubj:admit
</p>
<p>Table 1: Sample of vectorised features for the APTs shown
in Figure 1. Offsetting white by amod creates an offset view,
whiteamod, representing a noun, and has the consequence of
aligning the feature space with clothes.
</p>
<p>2.1 Qualitative Analysis of Offset
Representations
</p>
<p>Any offset view of a lexeme is behaviourally iden-
tical to a “normal” lexeme. It has an associ-
ated part of speech, a distributional representation
which locates it in semantic space, and we can
find neighbours for it in the same way that we find
neighbours for any other lexeme. In this way, a
single APT data structure is able to provide many
different views of any given lexeme. These views
reflect the different ways in which the lexeme is
used. For example lawnsubj is the nsubj off-
set representation of the noun law. This lexeme
is a verb and represents an action carried out by
the law. This contrasts with lawdobj, which is the
dobj offset representation of the noun law. It is
also a verb, however represents actions done to the
law. Table 2 lists the 10 nearest neighbours for
a number of lexemes, offset by amod, dobj and
nsubj respectively.
</p>
<p>For example, the neighbourhood of the lexeme
ancient in Table 2 shows that the offset view for
ancientamod is a prototypical representation of an
“ancient thing”, with neighbours easily associated
with the property ancient. Furthermore, Table 2
</p>
<p>434</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Structured distributional APT space. Different colours reflect different parts of speech. Boxes denote the current
anchor of the APT, circles represent nodes in the APT space, holding lexemes, and edges represent their relationship within the
space.
</p>
<p>illustrates that nearest neighbours of offset views
are often other offset representations. This means
that for example actions carried out by a mother
tend to be similar to actions carried out by a father
or a parent.
</p>
<p>2.2 Offset Inference
</p>
<p>Our approach generalises the unsupervised algo-
rithm proposed by Kober et al. (2016), henceforth
“standard DI”, as a method for inferring missing
knowledge into an APT representation. Rather
than simply inferring potentially plausible, but un-
observed co-occurrences from near distributional
neighbours, inferences can be made involving off-
set APTs. For example, the adjective white can
be offset so that it represents a noun — a proto-
typical “white thing”. This allows inferring plau-
sible co-occurrences from other “things that can
be white”, such as shoes or shirts. Our algorithm
therefore reflects the contextualised use of a word.
This has the advantage of being able to make flex-
ible and fine grained distinctions in the inference
process. For example if the noun law is used as
a subject, our algorithm allows inferring plausi-
ble co-occurrences from “other actions carried out
by the law”. This contrasts the use of law as an
object, where offset inference is able to find co-
occurrences on the basis of “other actions done
to the law”. This is a crucial advantage over the
method of Kober et al. (2016) which only supports
inference on uncontextualised lexemes.
</p>
<p>A sketch of how offset inference for a lexeme
w works is shown in Algorithm 1. Our algorithm
requires a distributional model M , an APT repre-
sentation for the lexeme w for which to perform
offset inference, a dependency path p, describing
the offset for w, and the number of neighbours k.
</p>
<p>The offset representation of w′ is then enriched
with the information from its distributional neigh-
bours by some merge function. We note that if the
offset path p is the empty path, we would recover
the algorithm presented by Kober et al. (2016).
Our algorithm is unsupervised, and agnostic to the
input distributional model and the neighbour re-
trieval function.
</p>
<p>Algorithm 1 Offset Inference
1: procedure OFFSET INFERENCE(M , w, p, k)
2: w′ ← offset(w, p)
3: for all n in neighbours(M,w′, k) do
4: w′′ ← merge(w′′, n)
5: end for
6: return w′′
7: end procedure
</p>
<p>Connection to Distributional Composition
</p>
<p>An interesting observation is the similarity be-
tween distributional inference and distributional
composition, as both operations are realised by the
same mechanism — an offset followed by infer-
ring plausible co-occurrence counts for a single
lexeme in the case of distributional inference, or
for a phrase in the case of composition. The merg-
ing of co-occurrence dimensions for distributional
inference can also be any of the operations com-
monly used for distributional composition such as
pointwise minimum, maximum, addition or multi-
plication.
</p>
<p>This relation creates an interesting dynamic be-
tween distributional inference and composition
when used in a complementary manner as in this
work. The former can be used as a process of co-
occurrence embellishment which is adding miss-
</p>
<p>435</p>
<p />
</div>
<div class="page"><p />
<p>Offset Representation Nearest Neighbours
ancientamod civilzation, mythology, tradition, ruin, monument, trackway, tomb, antiquity, folklore, deity
redamod blueamod, blackamod, greenamod, darkamod, onion, pepper, red, tomato, carrot, garlic
economicamod politicalamod, societalamod, cohabiting, economy, growth, cohabitant, globalisation, competitiveness,
</p>
<p>globalization, prosperity
governmentdobj overthrow, partydobj, authoritydobj, leaderdobj, capitaldobj, forcedobj, statedobj, officialdobj, ministerdobj, oust
problemdobj difficultydobj, solve, coded, issuedobj, injurydobj, overcome, questiondobj, think, lossdobj, relieve
lawdobj violate, ruledobj, enact, repeal, principledobj, unmake, enforce, policydobj, obey, flout
researchernsubj physiciannsubj, writernsubj, theorize, thwart, theorise, hypothesize, surmise, studentnsubj, workernsubj, apprehend
mothernsubj wifensubj, fathernsubj, parentnsubj, womannsubj, re-married, remarry, girlnsubj,breastfeed, familynsubj, disown
lawnsubj rulensubj, principlensubj, policynsubj, criminalize, casensubj, contractnsubj, prohibit, proscribe, enjoin, chargensubj
</p>
<p>Table 2: List of the 10 nearest neighbours of amod, dobj and nsubj offset representations.
</p>
<p>ing information, however with the risk of introduc-
ing some noise. The latter on the other hand can be
used as a process of co-occurrence filtering, that is
leveraging the enriched representations, while also
sieving out the previously introduced noise.
</p>
<p>3 Experiments
</p>
<p>For our experiments we re-implemented the stan-
dard DI method of Kober et al. (2016) for a direct
comparison. We built an order 2 APT space on the
basis of the concatenation of ukWaC, Wackype-
dia and the BNC (Baroni et al., 2009), pre-parsed
with the Malt parser (Nivre et al., 2006). We
PPMI transformed the raw co-occurrence counts
prior to composition, using a negative SPPMI shift
of log 5 (Levy and Goldberg, 2014b). We also
experimented with composing normalised counts
and applying the PPMI transformation after com-
position as done by Weeds et al. (2017), however
found composing PPMI scores to work better for
this task.
</p>
<p>We evaluate our offset inference algorithm on
two popular short phrase composition benchmarks
by Mitchell and Lapata (2008) and Mitchell and
Lapata (2010), henceforth ML08 and ML10 re-
spectively. The ML08 dataset consists of 120 dis-
tinct verb-object (VO) pairs and the ML10 dataset
contains 108 adjective-noun (AN), 108 noun-noun
(NN) and 108 verb-object pairs. The goal is to
compare a model’s similarity estimates to human
provided judgements. For both tasks, each phrase
pair has been rated by multiple human annotators
on a scale between 1 and 7, where 7 indicates max-
imum similarity. Comparison with human judge-
ments is achieved by calculating Spearman’s ρ
between the model’s similarity estimates and the
scores of each human annotator individually. We
performed composition by intersection and tuned
the number of neighbours by a grid search over {0,
10, 30, 50, 100, 500, 1000} on the ML10 develop-
</p>
<p>ment set, selecting 10 neighbours for NNs, 100 for
ANs and 50 for VOs for both DI algorithms. We
calculate statistical significance using the method
of Steiger (1980).
</p>
<p>Effect of the number of neighbours
Figure 2 shows the effect of the number of neigh-
bours for AN, NN and VO phrases, using offset
inference, on the ML10 development set. Interest-
ingly, NN compounds exhibit an early saturation
effect, while VOs and ANs require more neigh-
bours for optimal performance. One explanation
for the observed behaviour is that up to some
threshold, the neighbours being added contribute
actually missing co-occurrence events, whereas
past that threshold distributional inference de-
grades to just generic smoothing that is simply
compensating for sparsity, but overwhelming the
representations with non-plausible co-occurrence
information. A similar effect has also been ob-
served by Erk and Pado (2010) in an exemplar-
based model.
</p>
<p>Figure 2: Effect of the number of neighbours on the ML10
development set.
</p>
<p>Results
Table 3 shows that both forms of distributional in-
ference significantly outperform a baseline with-
out DI. On average, offset inference outperforms
</p>
<p>436</p>
<p />
</div>
<div class="page"><p />
<p>the method of Kober et al. (2016) by a statistically
significant margin on both datasets.
</p>
<p>ML10 ML08
APT configuration AN NN VO Avg VO
None 0.35 0.50 0.39 0.41 0.22
Standard DI 0.48‡ 0.51 0.43‡ 0.47‡ 0.29‡
</p>
<p>Offset Inference 0.49‡ 0.52 0.44‡ 0.48∗‡ 0.31†‡
</p>
<p>Table 3: Comparison of DI algorithms. ‡ denotes statistical
significance at p &lt; 0.01 in comparison to the method without
DI, * denotes statistical significance at p &lt; 0.01 in compar-
ison to standard DI and † denotes statistical significance at
p &lt; 0.05 in comparison to standard DI.
</p>
<p>Table 4 shows that offset inference substantially
outperforms comparable sparse models by Dinu
et al. (2013) on ML08, achieving a new state-of-
the-art, and matches the performance of the state-
of-the-art neural network model of Hashimoto
et al. (2014) on ML10, while being fully inter-
pretable.
</p>
<p>Model ML10 - Average ML08
Our work 0.48 0.31
Blacoe and Lapata (2012) 0.44 -
Hashimoto et al. (2014) 0.48 -
Weir et al. (2016) 0.43 0.26
Dinu et al. (2013) - 0.23− 0.26
Erk and Padó (2008) - 0.27
</p>
<p>Table 4: Comparison with existing methods.
</p>
<p>4 Related Work
</p>
<p>Distributional inference has its roots in the work
of Dagan et al. (1993, 1994), who aim to find
probability estimates for unseen words in bi-
grams, and Schütze (1992, 1998) who leverages
the distributional neighbourhood through cluster-
ing of contexts for word-sense discrimination. Re-
cently Kober et al. (2016) revitalised the idea for
compositional distributional semantic models.
</p>
<p>Composition with distributional semantic mod-
els has become a popular research area in re-
cent years. Simple, yet competitive methods, are
based on pointwise vector addition or multiplica-
tion (Mitchell and Lapata, 2008, 2010). However,
these approaches neglect the structure of the text
defining composition as a commutative operation.
</p>
<p>A number of approaches proposed in the lit-
erature attempt to overcome this shortcoming by
introducing weighted additive variants (Guevara,
2010, 2011; Zanzotto et al., 2010). Another popu-
lar strand of work models semantic composition
on the basis of ideas arising in formal seman-
tics. Composition in such models is usually imple-
mented as operations on higher-order tensors (Ba-
</p>
<p>roni and Zamparelli, 2010; Baroni et al., 2014; Co-
ecke et al., 2011; Grefenstette et al., 2011; Grefen-
stette and Sadrzadeh, 2011; Grefenstette et al.,
2013; Kartsaklis and Sadrzadeh, 2014; Paperno
et al., 2014; Tian et al., 2016; Van de Cruys et al.,
2013). Another widespread approach to seman-
tic composition is to use neural networks (Bow-
man et al., 2016; Hashimoto et al., 2014; Hill
et al., 2016; Mou et al., 2015; Socher et al., 2012,
2014; Wieting et al., 2015; Yu and Dredze, 2015),
or convolutional tree kernels (Croce et al., 2011;
Zanzotto and Dell’Arciprete, 2012; Annesi et al.,
2014) as composition functions.
</p>
<p>The above approaches are applied to untyped
distributional vector space models where untyped
models contrast with typed models (Baroni and
Lenci, 2010) in terms of whether structural in-
formation is encoded in the representation as in
the models of Erk and Padó (2008); Gamallo
and Pereira-Fariña (2017); Levy and Goldberg
(2014a); Padó and Lapata (2007); Thater et al.
(2010, 2011); Weeds et al. (2014).
</p>
<p>The perhaps most popular approach in the lit-
erature to evaluating compositional distributional
semantic models is to compare human word and
phrase similarity judgements with similarity esti-
mates of composed meaning representations, un-
der the assumption that better distributional repre-
sentations will perform better at these tasks (Bla-
coe and Lapata, 2012; Dinu et al., 2013; Erk and
Padó, 2008; Hashimoto et al., 2014; Hermann and
Blunsom, 2013; Kiela et al., 2014; Turney, 2012).
</p>
<p>5 Conclusion
</p>
<p>In this paper we have introduced a novel form
of distributional inference that generalises the
method introduced by Kober et al. (2016). We
have shown its effectiveness for semantic compo-
sition on two benchmark phrase similarity tasks
where we achieved state-of-the-art performance
while retaining the interpretability of our model.
We have furthermore highlighted an interesting
connection between distributional inference and
distributional composition.
</p>
<p>In future work we aim to apply our novel
method to improve modelling selectional prefer-
ences, lexical inference, and scale up to longer
phrases and full sentences.
</p>
<p>437</p>
<p />
</div>
<div class="page"><p />
<p>References
Paolo Annesi, Danilo Croce, and Roberto Basili.
</p>
<p>2014. Semantic compositionality in tree ker-
nels. In Proceedings of the 23rd ACM Inter-
national Conference on Conference on Informa-
tion and Knowledge Management. ACM, New
York, NY, USA, CIKM ’14, pages 1029–1038.
https://doi.org/10.1145/2661829.2661955.
</p>
<p>Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program for compo-
sitional distributional semantics. Linguistic Issues in
Language Technology 9(6):5–110.
</p>
<p>Marco Baroni, Silvia Bernardini, Adriano Fer-
raresi, and Eros Zanchetta. 2009. The wacky
wide web: a collection of very large linguis-
tically processed web-crawled corpora. Lan-
guage Resources and Evaluation 43(3):209–226.
https://doi.org/10.1007/s10579-009-9081-4.
</p>
<p>Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguistics
36(4):673–721.
</p>
<p>Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Repre-
senting adjective-noun constructions in seman-
tic space. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Cambridge, MA, pages 1183–1193.
http://www.aclweb.org/anthology/D10-1115.
</p>
<p>William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning. Association for Computa-
tional Linguistics, Jeju Island, Korea, pages 546–
556. http://www.aclweb.org/anthology/D12-1050.
</p>
<p>Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model
for parsing and sentence understanding. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1466–1477.
http://www.aclweb.org/anthology/P16-1139.
</p>
<p>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2011. Mathematical foundations for a com-
positional distributed model of meaning. Linguistic
Analysis 36(1-4):345–384.
</p>
<p>Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via
convolution kernels on dependency trees. In
Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Process-
ing. Association for Computational Linguistics,
</p>
<p>Edinburgh, Scotland, UK., pages 1034–1046.
http://www.aclweb.org/anthology/D11-1096.
</p>
<p>Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1993. Contextual word similarity and estimation
from sparse data. In Proceedings of the 31st An-
nual Meeting on Association for Computational Lin-
guistics. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’93, pages 164–171.
https://doi.org/10.3115/981574.981596.
</p>
<p>Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
Las Cruces, New Mexico, USA, pages 272–278.
https://doi.org/10.3115/981732.981770.
</p>
<p>Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the Workshop on Continuous Vector Space
Models and their Compositionality. Association for
Computational Linguistics, Sofia, Bulgaria, pages
50–58. http://www.aclweb.org/anthology/W13-
3206.
</p>
<p>Katrin Erk and Sebastian Padó. 2008. A struc-
tured vector space model for word meaning in
context. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Honolulu, Hawaii, pages 897–906.
http://www.aclweb.org/anthology/D08-1094.
</p>
<p>Katrin Erk and Sebastian Pado. 2010. Exemplar-
based models for word meaning in context.
In Proceedings of the ACL 2010 Conference
Short Papers. Association for Computational
Linguistics, Uppsala, Sweden, pages 92–97.
http://www.aclweb.org/anthology/P10-2017.
</p>
<p>Pablo Gamallo and Martı́n Pereira-Fariña. 2017. Com-
positional semantics using feature-based models
from wordnet. In Proceedings of the 1st Work-
shop on Sense, Concept and Entity Representa-
tions and their Applications. Association for Com-
putational Linguistics, Valencia, Spain, pages 1–11.
http://www.aclweb.org/anthology/W17-1901.
</p>
<p>E. Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh,
and M. Baroni. 2013. Multi-step regression
learning for compositional distributional seman-
tics. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS
2013) – Long Papers. Association for Computa-
tional Linguistics, Potsdam, Germany, pages 131–
142. http://www.aclweb.org/anthology/W13-0112.
</p>
<p>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compo-
sitional distributional model of meaning. In
Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Process-
ing. Association for Computational Linguistics,
</p>
<p>438</p>
<p />
</div>
<div class="page"><p />
<p>Edinburgh, Scotland, UK., pages 1394–1404.
http://www.aclweb.org/anthology/D11-1129.
</p>
<p>Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. Proceedings of the 9th
International Conference on Computational Seman-
tics (IWCS 2011) pages 125–134.
</p>
<p>Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distribu-
tional semantics. In Proceedings of the 2010
Workshop on GEometrical Models of Natural
Language Semantics. Association for Computa-
tional Linguistics, Uppsala, Sweden, pages 33–37.
http://www.aclweb.org/anthology/W10-2805.
</p>
<p>Emiliano Guevara. 2011. Computing semantic com-
positionality in distributional semantics. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics. Association for Compu-
tational Linguistics, Stroudsburg, PA, USA, IWCS
’11, pages 135–144.
</p>
<p>Kazuma Hashimoto, Pontus Stenetorp, Makoto
Miwa, and Yoshimasa Tsuruoka. 2014. Jointly
learning word representations and composition
functions using predicate-argument structures.
In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Pro-
cessing (EMNLP). Association for Computational
Linguistics, Doha, Qatar, pages 1544–1555.
http://www.aclweb.org/anthology/D14-1163.
</p>
<p>Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Sofia, Bulgaria, pages
894–904. http://www.aclweb.org/anthology/P13-
1088.
</p>
<p>Felix Hill, KyungHyun Cho, Anna Korho-
nen, and Yoshua Bengio. 2016. Learning
to understand phrases by embedding the
dictionary. Transactions of the Associa-
tion for Computational Linguistics 4:17–30.
http://www.aclweb.org/anthology/Q/Q16/Q16-
1002.pdf.
</p>
<p>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL).
</p>
<p>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 835–
841. http://www.aclweb.org/anthology/P14-2135.
</p>
<p>Thomas Kober, Julie Weeds, Jeremy Reffin, and
David Weir. 2016. Improving sparse word rep-
resentations with distributional inference for se-
mantic composition. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1691–1702.
https://aclweb.org/anthology/D16-1175.
</p>
<p>Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers). Association for Computational Lin-
guistics, Baltimore, Maryland, pages 302–308.
http://www.aclweb.org/anthology/P14-2050.
</p>
<p>Omer Levy and Yoav Goldberg. 2014b. Neural
word embedding as implicit matrix factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N.D.
Lawrence, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, Cur-
ran Associates, Inc., pages 2177–2185.
</p>
<p>Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceed-
ings of ACL-08: HLT . Association for Computa-
tional Linguistics, Columbus, Ohio, pages 236–244.
http://www.aclweb.org/anthology/P/P08/P08-1028.
</p>
<p>Jeff Mitchell and Mirella Lapata. 2010. Com-
position in distributional models of seman-
tics. Cognitive Science 34(8):1388–1429.
https://doi.org/10.1111/j.1551-6709.2010.01106.x.
</p>
<p>Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Discriminative neural sentence mod-
eling by tree-based convolution. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 2315–
2325. http://aclweb.org/anthology/D15-1279.
</p>
<p>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. Technical report, Växjö Univer-
sity.
</p>
<p>Sebastian Padó and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics 33(2):161–199.
https://doi.org/10.1162/coli.2007.33.2.161.
</p>
<p>Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Baltimore, Maryland, pages 90–
99. http://www.aclweb.org/anthology/P14-1009.
</p>
<p>Hinrich Schütze. 1992. Dimensions of mean-
ing. In Proceedings of ACM/IEEE Con-
ference on Supercomputing. IEEE Com-
puter Society Press, pages 787–796.
http://dl.acm.org/citation.cfm?id=147877.148132.
</p>
<p>439</p>
<p />
</div>
<div class="page"><p />
<p>Hinrich Schütze. 1998. Automatic word sense discrim-
ination. Computational Linguistics 24(1):97–123.
</p>
<p>Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic
compositionality through recursive matrix-vector
spaces. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning. Association for Computational
Linguistics, Jeju Island, Korea, pages 1201–1211.
http://www.aclweb.org/anthology/D12-1110.
</p>
<p>Richard Socher, Andrej Karpathy, Quoc Le, Christo-
pher Manning, and Andrew Ng. 2014. Grounded
compositional semantics for finding and describing
images with sentences. Transactions of the Associa-
tion for Computational Linguistics 2:207–218.
</p>
<p>James H Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin 87(2):245.
</p>
<p>Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, Uppsala, Sweden, pages
948–957. http://www.aclweb.org/anthology/P10-
1097.
</p>
<p>Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing. Asian Federation of Natural Language Pro-
cessing, Chiang Mai, Thailand, pages 1134–1143.
http://www.aclweb.org/anthology/I11-1127.
</p>
<p>Ran Tian, Naoaki Okazaki, and Kentaro Inui. 2016.
Learning semantically and additively composi-
tional distributional representations. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1277–1287.
http://www.aclweb.org/anthology/P16-1121.
</p>
<p>Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research
44(1):533–585.
</p>
<p>Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of se-
mantic compositionality. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 1142–
1151. http://www.aclweb.org/anthology/N13-1134.
</p>
<p>Julie Weeds, Thomas Kober, Jeremy Reffin, and David
Weir. 2017. When a red herring in not a red
</p>
<p>herring: Using compositional methods to detect
non-compositional phrases. In Proceedings of the
15th Conference of the European Chapter of the
Association for Computational Linguistics: Vol-
ume 2, Short Papers. Association for Computa-
tional Linguistics, Valencia, Spain, pages 529–534.
http://www.aclweb.org/anthology/E17-2085.
</p>
<p>Julie Weeds, David Weir, and Jeremy Reffin. 2014.
Distributional composition using higher-order de-
pendency vectors. In Proceedings of the 2nd Work-
shop on Continuous Vector Space Models and their
Compositionality (CVSC). Association for Compu-
tational Linguistics, Gothenburg, Sweden, pages
11–20. http://www.aclweb.org/anthology/W14-
1502.
</p>
<p>David Weir, Julie Weeds, Jeremy Reffin, and Thomas
Kober. 2016. Aligning packed dependency trees: a
theory of composition for distributional semantics.
Computational Linguistics, special issue on Formal
Distributional Semantics 42(4):727–761.
</p>
<p>John Wieting, Mohit Bansal, Kevin Gimpel,
and Karen Livescu. 2015. From paraphrase
database to compositional paraphrase model
and back. Transactions of the Association
for Computational Linguistics 3:345–358.
http://www.aclweb.org/anthology/Q/Q15/Q15-
1025.pdf.
</p>
<p>Mo Yu and Mark Dredze. 2015. Learning composition
models for phrase embeddings. Transactions of the
Association for Computational Linguistics 3:227–
242. http://aclweb.org/anthology/Q/Q15/Q15-
1017.pdf.
</p>
<p>Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete.
2012. Distributed tree kernels. In John Lang-
ford and Joelle Pineau, editors, Proceedings of the
29th International Conference on Machine Learning
(ICML-12). Omnipress, New York, NY, USA, ICML
’12, pages 193–200.
</p>
<p>Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of Coling. pages
1263–1271. http://www.aclweb.org/anthology/C10-
1142.
</p>
<p>440</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 441–447
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2070
</p>
<p>Learning Topic-Sensitive Word Representations
</p>
<p>Marzieh Fadaee Arianna Bisazza Christof Monz
Informatics Institute, University of Amsterdam
</p>
<p>Science Park 904, 1098 XH Amsterdam, The Netherlands
{m.fadaee,a.bisazza,c.monz}@uva.nl
</p>
<p>Abstract
</p>
<p>Distributed word representations are
widely used for modeling words in NLP
tasks. Most of the existing models gen-
erate one representation per word and
do not consider different meanings of a
word. We present two approaches to learn
multiple topic-sensitive representations
per word by using Hierarchical Dirichlet
Process. We observe that by modeling
topics and integrating topic distributions
for each document we obtain representa-
tions that are able to distinguish between
different meanings of a given word.
Our models yield statistically significant
improvements for the lexical substitution
task indicating that commonly used
single word representations, even when
combined with contextual information,
are insufficient for this task.
</p>
<p>1 Introduction
</p>
<p>Word representations in the form of dense vec-
tors, or word embeddings, capture semantic and
syntactic information (Mikolov et al., 2013a; Pen-
nington et al., 2014) and are widely used in many
NLP tasks (Zou et al., 2013; Levy and Goldberg,
2014; Tang et al., 2014; Gharbieh et al., 2016).
Most of the existing models generate one repre-
sentation per word and do not distinguish between
different meanings of a word. However, many
tasks can benefit from using multiple representa-
tions per word to capture polysemy (Reisinger and
Mooney, 2010). There have been several attempts
to build repositories for word senses (Miller, 1995;
Navigli and Ponzetto, 2010), but this is laborious
and limited to few languages. Moreover, defin-
ing a universal set of word senses is challenging
as polysemous words can exist at many levels of
</p>
<p>granularity (Kilgarriff, 1997; Navigli, 2012).
In this paper, we introduce a model that uses
</p>
<p>a nonparametric Bayesian model, Hierarchical
Dirichlet Process (HDP), to learn multiple topic-
sensitive representations per word. Yao and van
Durme (2011) show that HDP is effective in learn-
ing topics yielding state-of-the-art performance
for sense induction. However, they assume that
topics and senses are interchangeable, and train
individual models per word making it difficult to
scale to large data. Our approach enables us to use
HDP to model senses effectively using large unan-
notated training data. We investigate to what ex-
tent distributions over word senses can be approxi-
mated by distributions over topics without assum-
ing these concepts to be identical. The contribu-
tions of this paper are: (i) We propose three un-
supervised, language-independent approaches to
approximate senses with topics and learn multi-
ple topic-sensitive embeddings per word. (ii) We
show that in the Lexical Substitution ranking task
our models outperform two competitive baselines.
</p>
<p>2 Topic-Sensitive Representations
</p>
<p>In this section we describe the proposed models.
To learn topics from a corpus we use HDP (Teh
et al., 2006; Lau et al., 2014). The main advantage
of this model compared to non-hierarchical meth-
ods like the Chinese Restaurant Process (CRP) is
that each document in the corpus is modeled us-
ing a mixture model with topics shared between
all documents (Teh et al., 2005; Brody and Lap-
ata, 2009). HDP yields two sets of distributions
that we use in our methods: distributions over top-
ics for words in the vocabulary, and distributions
over topics for documents in the corpus.
</p>
<p>Similarly to Neelakantan et al. (2014), we use
neighboring words to detect the meaning of the
context, however, we also use the two HDP dis-
</p>
<p>441</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2070">https://doi.org/10.18653/v1/P17-2070</a></div>
</div>
<div class="page"><p />
<p>…
</p>
<p>…
</p>
<p>…
</p>
<p>…
</p>
<p>owi+j
</p>
<p>……
</p>
<p>…
</p>
<p>…
</p>
<p>…
</p>
<p>owi+j
</p>
<p>…
</p>
<p>…
</p>
<p>…
</p>
<p>…
</p>
<p>owi+j
</p>
<p>(a) (b) (c)
</p>
<p>p(⌧ |di)
</p>
<p>r00(w⌧
k
</p>
<p>i )
</p>
<p>hHTLEadd hSTLEhHTLE
</p>
<p>r0(wi)r
0(w⌧i )r(w
</p>
<p>⌧
i ) r
</p>
<p>00(w⌧
1
</p>
<p>i ) r
00(w⌧
</p>
<p>2
</p>
<p>i )
</p>
<p>Figure 1: Illustration of our topic-sensitive representation models: (a) hard-topic labeled representations
(HTLE), (b) hard topic-labeled representations plus generic word representation (HTLEadd), (c) soft
topic-labeled representations (STLE).
</p>
<p>tributions. By doing so, we take advantage of
the topic of the document beyond the scope of
the neighboring words, which is helpful when the
immediate context of the target word is not suf-
ficiently informative. We modify the Skipgram
model (Mikolov et al., 2013a) to obtain multiple
topic-sensitive representations per word type using
topic distributions. In addition, we do not cluster
context windows and train for different senses of
the words individually. This reduces the sparsity
problem and provides a better representation esti-
mation for rare words. We assume that meanings
of words can be determined by their contextual in-
formation and use the distribution over topics to
differentiate between occurrences of a word in dif-
ferent contexts, i.e., documents with different top-
ics. We propose three different approaches (see
Figure 1): two methods with hard topic labeling
of words and one with soft labeling.
</p>
<p>2.1 Hard Topic-Labeled Representations
</p>
<p>The trained HDP model can be used to hard-label a
new corpus with one topic per word through sam-
pling. Our first model variant (Figure 1(a)) re-
lies on hard labeling by simply considering each
word-topic pair as a separate vocabulary entry. To
reduce sparsity on the context side and share the
word-level information between similar contexts,
we use topic-sensitive representations for target
words (input to the network) and standard, i.e.,
unlabeled, word representations for context words
(output). Note that this results in different input
and output vocabularies. The training objective
is then to maximize the log-likelihood of context
words wi`j given the target word-topic pair wτi :
</p>
<p>LHardT -SG “ 1
I
</p>
<p>Iÿ
</p>
<p>i“1
</p>
<p>ÿ
</p>
<p>´cďjďc
j‰0
</p>
<p>log ppwi`j |wτi q
</p>
<p>where I is the number of words in the training cor-
pus, c is the context size and τ is the topic assigned
towi by HDP sampling. The embedding of a word
in context hpwiq is obtained by simply extracting
the row of the input lookup table (r) corresponding
to the HDP-labeled word-topic pair:
</p>
<p>hHTLEpwiq “ rpwτi q (1)
A possible shortcoming of the HTLE model is
</p>
<p>that the representations are trained separately and
information is not shared between different topic-
sensitive representations of the same word. To ad-
dress this issue, we introduce a model variant that
learns multiple topic-sensitive word representa-
tions and generic word representations simultane-
ously (Figure 1(b)). In this variant (HTLEadd), the
target word embedding is obtained by adding the
word-topic pair representation (r1) to the generic
representation of the corresponding word (r0):
</p>
<p>hHTLEaddpwiq “ r1pwτi q ` r0pwiq (2)
2.2 Soft Topic-Labeled Representations
The model variants above rely on the hard label-
ing resulting from HDP sampling. As a soft al-
ternative to this, we can directly include the topic
distributions estimated by HDP for each document
(Figure 1(c)). Specifically, for each update, we use
the topic distribution to compute a weighted sum
over the word-topic representations (r2):
</p>
<p>hSTLEpwiq “
Tÿ
</p>
<p>k“1
ppτk|diq r2pwτki q (3)
</p>
<p>where T is the total number of topics, di the doc-
ument containing wi, and ppτk|diq the probability
assigned to topic τk by HDP in document di. The
training objective for this model is:
</p>
<p>LSoftT -SG “ 1
I
</p>
<p>Iÿ
</p>
<p>i“1
</p>
<p>ÿ
</p>
<p>´cďjďc
j‰0
</p>
<p>log ppwi`j |wi, τq
</p>
<p>442</p>
<p />
</div>
<div class="page"><p />
<p>word bat
</p>
<p>Pre-trained w2v bats, batting, Pinch hitter Brayan Pena, batsman, batted, Hawaiian hoary, Batting
Pre-trained GloVe bats, batting, Bat, catcher, fielder, hitter, outfield, hitting, batted, catchers, balls
SGE on Wikipedia uroderma, magnirostrum, sorenseni, miniopterus, promops, luctus, micronycteris
</p>
<p>TSE on Wikipedia τ1 ball, pitchout, batter, toss-for, umpire, batting, bowes, straightened, fielder, flies
τ2 vespertilionidae, heran, hipposideros, sorenseni, luctus, coxi, kerivoula, natterer
</p>
<p>word jaguar
</p>
<p>Pre-trained w2v jaguars, Macho B, panther, lynx, rhino, lizard, tapir, tiger, leopard, Florida panther
Pre-trained GloVe jaguars, panther, mercedes, Jaguar, porsche, volvo, ford, audi, mustang, bmw, biuck
SGE on Wikipedia electramotive, viper, roadster, saleen, siata, chevrolet, camaro, dodge, nissan, escort
</p>
<p>TSE on Wikipedia τ1 ford, bmw, chevrolet, honda, porsche, monza, nissan, dodge, marauder, peugeot, opel
τ2 wiedii, puma, margay, tapirus, jaguarundi, yagouaroundi, vison, concolor, tajacu
</p>
<p>word appeal
</p>
<p>Pre-trained w2v appeals, appealing, appealed, Appeal, rehearing, apeal, Appealing, ruling, Appeals
Pre-trained GloVe appeals, appealed, appealing, Appeal, court, decision, conviction, plea, sought
SGE on Wikipedia court, appeals, appealed, carmody, upheld, verdict, jaruvan, affirmed, appealable
</p>
<p>TSE on Wikipedia τ1 court, case, appeals, appealed, decision, proceedings, disapproves, ruling
τ2 sfa, steadfast, lackadaisical, assertions, lack, symbolize, fans, attempt, unenthusiastic
</p>
<p>Table 1: Nearest neighbors of three examples in different representation spaces using cosine similarity.
w2v and GloVe are pre-trained embeddings from (Mikolov et al., 2013a) and (Pennington et al., 2014)
respectively. SGE is the Skipgram baseline and TSE is our topic-sensitive Skipgram (cf. Equation (1)),
both trained on Wikipedia. τk stands for HDP-inferred topic k.
</p>
<p>where τ is the topic of document di learned by
HDP. The STLE model has the advantage of di-
rectly applying the distribution over topics in the
Skipgram model. In addition, for each instance,
we update all topic representations of a given word
with non-zero probabilities, which has the poten-
tial to reduce the sparsity problem.
</p>
<p>2.3 Embeddings for Polysemous Words
</p>
<p>The representations obtained from our models are
expected to capture the meaning of a word in dif-
ferent topics. We now investigate whether these
representations can distinguish between different
word senses. Table 1 provides examples of near-
est neighbors. For comparison we include our own
baseline, i.e., embeddings learned with Skipgram
on our corpus, as well as Word2Vec (Mikolov
et al., 2013b) and GloVe embeddings (Pennington
et al., 2014) pre-trained on large data.
</p>
<p>In the first example, the word bat has two dif-
ferent meanings: animal or sports device. One
can see that the nearest neighbors of the baseline
and pre-trained word representations either center
around one primary, i.e., most frequent, meaning
of the word, or it is a mixture of different mean-
ings. The topic-sensitive representations, on the
other hand, correctly distinguish between the two
different meanings. A similar pattern is observed
for the word jaguar and its two meanings: car
</p>
<p>or animal. The last example, appeal, illustrates
a case where topic-sensitive embeddings are not
clearly detecting different meanings of the word,
despite having some correct words in the lists.
Here, the meaning attract does not seem to be cap-
tured by any embedding set.
</p>
<p>These observations suggest that topic-sensitive
representations capture different word senses to
some extent. To provide a systematic validation of
our approach, we now investigate whether topic-
sensitive representations can improve tasks where
polysemy is a known issue.
</p>
<p>3 Evaluation
</p>
<p>In this section we present the setup for our exper-
iments and empirically evaluate our approach on
the context-aware word similarity and lexical sub-
stitution tasks.
</p>
<p>3.1 Experimental setup
</p>
<p>All word representations are learned on the En-
glish Wikipedia corpus containing 4.8M docu-
ments (1B tokens). The topics are learned on a
100K-document subset of this corpus using the
HDP implementation of Teh et al. (2006). Once
the topics have been learned, we run HDP on the
whole corpus to obtain the word-topic labeling
(see Section 2.1) and the document-level topic dis-
tributions (Section 2.2). We train each model vari-
</p>
<p>443</p>
<p />
</div>
<div class="page"><p />
<p>Dimension
</p>
<p>Model 100 300 600
</p>
<p>SGE + C (Mikolov et al., 2013a) 0.59 0.59 0.62
MSSG (Neelakantan et al., 2014) 0.60 0.61 0.64
</p>
<p>HTLE 0.63 0.56 0.55
HTLEadd 0.61 0.61 0.58
STLE 0.59 0.58 0.55
</p>
<p>Table 2: Spearman’s rank correlation performance
for the Word Similarity task on SCWS.
</p>
<p>ant with window size c “ 10 and different embed-
ding sizes (100, 300, 600) initialized randomly.
</p>
<p>We compare our models to several baselines:
Skipgram (SGE) and the best-performing multi-
sense embeddings model per word type (MSSG)
(Neelakantan et al., 2014). All model variants are
trained on the same training data with the same
settings, following suggestions by Mikolov et al.
(2013a) and Levy et al. (2015). For MSSG we use
the best performing similarity measure (avgSimC)
as proposed by Neelakantan et al. (2014).
</p>
<p>3.2 Context-Aware Word Similarity Task
Despite its shortcomings (Faruqui et al., 2016),
word similarity remains the most frequently used
method of evaluation in the literature. There are
multiple test sets available but in almost all of
them word pairs are considered out of context. To
the best of our knowledge, the only word simi-
larity data set providing word context is SCWS
(Huang et al., 2012). To evaluate our models on
SCWS, we run HDP on the data treating each
word’s context as a separate document. We com-
pute the similarity of each word pair as follows:
</p>
<p>Simpw1, w2q “ cosphpw1q,hpw2qq
where hpwiq refers to any of the topic-sensitive
representations defined in Section 2. Note that w1
and w2 can refer to the same word.
</p>
<p>Table 2 provides the Spearman’s correlation
scores for different models against the human
ranking. We see that with dimensions 100 and
300, two of our models obtain improvements over
the baseline. The MSSG model of Neelakantan
et al. (2014) performs only slightly better than our
HLTE model by requiring considerably more pa-
rameters (600 vs. 100 embedding size).
</p>
<p>3.3 Lexical Substitution Task
This task requires one to identify the best replace-
ments for a word in a sentential context. The pres-
</p>
<p>LS-SE07 LS-CIC
</p>
<p>Dimension Dimension
Model Infer. 100 300 600 100 300 600
</p>
<p>SGE
n/a
</p>
<p>36.2 40.5 41.1 30.4 32.1 32.3
SGE + C 36.6 40.9 41.6 32.8 36.1 36.8
MSSG 37.8 41.1 42.9 33.9 37.8 39.1
</p>
<p>HTLE
Smp
</p>
<p>39.8N 42.5N 43.0N 32.1 32.7 33.0
HTLEadd 39.4M 41.3N 41.8 30.4 31.5 31.7
STLE 35.2 36.7 39.0 32.9 32.3 33.9
</p>
<p>HTLE
Exp
</p>
<p>40.3N 42.8N 43.4N 36.6N 40.9N 41.3N
HTLEadd 39.9N 41.8N 42.2 35.5M 37.9M 38.6
STLE 38.7M 41.0 41.0 36.8N 36.8 37.1
</p>
<p>Table 3: GAP scores on LS-SE07 and LS-CIC
sets. For SGE + C we use the context embeddings
to disambiguate the substitutions. Improvements
over the best baseline (MSSG) are marked N at
p ă .01 and M at p ă .05.
</p>
<p>ence of many polysemous target words makes this
task more suitable for evaluating sense embed-
ding. Following Melamud et al. (2015) we pool
substitutions from different instances and rank
them by the number of annotators that selected
them for a given context. We use two evaluation
sets: LS-SE07 (McCarthy and Navigli, 2007), and
LS-CIC (Kremer et al., 2014).
</p>
<p>Unlike previous work (Szarvas et al., 2013; Kre-
mer et al., 2014; Melamud et al., 2015) we do
not use any syntactic information, motivated by
the fact that high-quality parsers are not available
for most languages. The evaluation is performed
by computing the Generalized Average Precision
(GAP) score (Kishida, 2005). We run HDP on the
evaluation set and compute the similarity between
target word wt and each substitution ws using two
different inference methods in line with how we
incorporate topics during training:
</p>
<p>Sampled (Smp): SimTSEpws, wtq “
cosphpwτs q,hpwτ
</p>
<p>1
t qq `
</p>
<p>ř
c cosphpwτs q,opwcqq
</p>
<p>C
</p>
<p>Expected (Exp): SimTSEpws, wtq “
ÿ
</p>
<p>τ,τ 1
ppτq ppτ 1q
</p>
<p>cosphpwτs q,hpwτ
1
t qq `
</p>
<p>ř
τ,c cosphpwτs q,opwcqq ppτq
</p>
<p>C
</p>
<p>where hpwτs q and hpwτ 1t q are the representations
for substitution word s with topic τ and target
word t with topic τ 1 respectively (cf. Section 2),
wc are context words of wt taken from a sliding
window of the same size as the embeddings, opwcq
is the context (i.e., output) representation of wc,
and C is the total number of context words. Note
</p>
<p>444</p>
<p />
</div>
<div class="page"><p />
<p>Model Dim = 100 Dim = 300 Dim = 600n. v. adj. adv. All n. v. adj. adv. All n. v. adj. adv. All
</p>
<p>SGE + C 37.2 31.6 37.1 42.2 36.6 39.2 35.0 39.0 55.4 40.9 39.7 35.7 39.9 56.2 41.6
HTLE 42.4 33.9 38.1 49.7 40.3 44.9 37.0 41.0 50.9 42.8 45.2 37.2 42.1 51.9 43.4
</p>
<p>Table 4: GAP scores on the candidate ranking task on LS-SE07 for different part-of-speech categories.
</p>
<p>that these two methods are consistent with how we
train HTLE and STLE.
</p>
<p>The sampled method, similar to HTLE, uses the
HDP model to assign topics to word occurrences
during testing. The expected method, similar to
STLE, uses the HDP model to learn the probabil-
ity distribution of topics of the context sentence
and uses the entire distribution to compute the sim-
ilarity. For the Skipgram baseline we compute the
similarity SimSGE+Cpws, wtq as follows:
</p>
<p>cosphpwsq,hpwtqq `
ř
c cosphpwsq,opwcqq
</p>
<p>C
</p>
<p>which uses the similarity between the substitution
word and all words in the context, as well as the
similarity of target and substitution words.
</p>
<p>Table 3 shows the GAP scores of our models
and baselines.1 One can see that all models us-
ing multiple embeddings per word perform better
than SGE. Our proposed models outperform both
SGE and MSSG in both evaluation sets, with more
pronounced improvements for LS-CIC. We further
observe that our expected method is more robust
and performs better for all embedding sizes.
</p>
<p>Table 4 shows the GAP scores broken down
by the main word classes: noun, verb, adjective,
and adverb. With 100 dimensions our best model
(HTLE) yields improvements across all POS tags,
with the largest improvements for adverbs and
smallest improvements for adjectives. When in-
creasing the dimension size of embeddings the im-
provements hold up for all POS tags apart from
adverbs. It can be inferred that larger dimension
sizes capture semantic similarities for adverbs and
context words better than other parts-of-speech
categories. Additionally, we observe for both eval-
uation sets that the improvements are preserved
when increasing the embedding size.
</p>
<p>4 Related Work
</p>
<p>While the most commonly used approaches learn
one embedding per word type (Mikolov et al.,
</p>
<p>1We use the nonparametric rank-based Mann-Whitney-
Wilcoxon test (Sprent and Smeeton, 2016) to check for sta-
tistically significant differences between runs.
</p>
<p>2013a; Pennington et al., 2014), recent studies
have focused on learning multiple embeddings per
word due to the ambiguous nature of language
(Qiu et al., 2016). Huang et al. (2012) cluster word
contexts and use the average embedding of each
cluster as word sense embeddings, which yields
improvements on a word similarity task. Nee-
lakantan et al. (2014) propose two approaches,
both based on clustering word contexts: In the
first, they fix the number of senses manually, and
in the second, they use an ad-hoc greedy procedure
that allocates a new representation to a word if
existing representations explain the context below
a certain threshold. Li and Jurafsky (2015) used
a CRP model to distinguish between senses of
words and train vectors for senses, where the num-
ber of senses is not fixed. They use two heuris-
tic approaches for assigning senses in a context:
‘greedy’ which assigns the locally optimum sense
label to each word, and ‘expectation’ which com-
putes the expected value for a word in a given con-
text with probabilities for each possible sense.
</p>
<p>5 Conclusions
</p>
<p>We have introduced an approach to learn topic-
sensitive word representations that exploits the
document-level context of words and does not re-
quire annotated data or linguistic resources. Our
evaluation on the lexical substitution task suggests
that topic distributions capture word senses to
some extent. Moreover, we obtain statistically sig-
nificant improvements in the lexical substitution
task while not using any syntactic information.
The best results are achieved by our hard topic-
labeled model which learns topic-sensitive repre-
sentations by assigning topics to target words.
</p>
<p>Acknowledgments
</p>
<p>This research was funded in part by the
Netherlands Organization for Scientific Research
(NWO) under project numbers 639.022.213 and
639.021.646, and a Google Faculty Research
Award. We thank the anonymous reviewers for
their helpful comments.
</p>
<p>445</p>
<p />
</div>
<div class="page"><p />
<p>References
Samuel Brody and Mirella Lapata. 2009. Bayesian
</p>
<p>word sense induction. In Proceedings of the
12th Conference of the European Chapter of
the ACL (EACL 2009). Association for Computa-
tional Linguistics, Athens, Greece, pages 103–111.
http://www.aclweb.org/anthology/E09-1013.
</p>
<p>Manaal Faruqui, Yulia Tsvetkov, Pushpendre Ras-
togi, and Chris Dyer. 2016. Problems with eval-
uation of word embeddings using word similar-
ity tasks. In Proc. of the 1st Workshop on
Evaluating Vector Space Representations for NLP.
http://arxiv.org/pdf/1605.02276v1.pdf.
</p>
<p>Waseem Gharbieh, Virendra C Bhavsar, and Paul
Cook. 2016. A word embedding approach
to identifying verb–noun idiomatic combi-
nations. In Proc. of the 12th Workshop on
Multiword Expressions MWE 2016. page 112.
https://www.aclweb.org/anthology/W/W16/W16-
1817.pdf.
</p>
<p>Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word pro-
totypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Compu-
tational Linguistics, Jeju Island, Korea, pages 873–
882. http://www.aclweb.org/anthology/P12-1092.
</p>
<p>Adam Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities 31(2):91–113.
</p>
<p>Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evalua-
tion indicator for information retrieval experiments.
National Institute of Informatics Tokyo, Japan.
</p>
<p>Gerhard Kremer, Katrin Erk, Sebastian Padó, and Ste-
fan Thater. 2014. What substitutes tell us - anal-
ysis of an “all-words” lexical substitution corpus.
In Proceedings of the 14th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics, Gothenburg, Sweden, pages 540–549.
http://www.aclweb.org/anthology/E14-1057.
</p>
<p>Jey Han Lau, Paul Cook, Diana McCarthy, Spandana
Gella, and Timothy Baldwin. 2014. Learning word
sense distributions, detecting unattested senses and
identifying novel senses using topic models. In
Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Baltimore, Maryland, pages 259–270.
http://www.aclweb.org/anthology/P14-1025.
</p>
<p>Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers). Association for Computational Lin-
guistics, Baltimore, Maryland, pages 302–308.
http://www.aclweb.org/anthology/P14-2050.
</p>
<p>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Asso-
ciation for Computational Linguistics 3:211–225.
https://transacl.org/ojs/index.php/tacl/article/view/570.
</p>
<p>Jiwei Li and Dan Jurafsky. 2015. Do multi-
sense embeddings improve natural language un-
derstanding? In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 1722–1732.
http://aclweb.org/anthology/D15-1200.
</p>
<p>Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task.
In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-
2007). Association for Computational Linguis-
tics, Prague, Czech Republic, pages 48–53.
http://www.aclweb.org/anthology/S/S07/S07-1009.
</p>
<p>Oren Melamud, Omer Levy, and Ido Dagan. 2015.
A simple word embedding model for lexical sub-
stitution. In Proceedings of the 1st Work-
shop on Vector Space Modeling for Natural
Language Processing. Association for Computa-
tional Linguistics, Denver, Colorado, pages 1–7.
http://www.aclweb.org/anthology/W15-1501.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013b. Distributed
representations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26, Curran Associates, Inc.,
pages 3111–3119. http://papers.nips.cc/paper/5021-
distributed-representations-of-words-and-phrases-
and-their-compositionality.pdf.
</p>
<p>George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM 38(11):39–41.
</p>
<p>Roberto Navigli. 2012. SOFSEM 2012: Theory and
Practice of Computer Science: 38th Conference on
Current Trends in Theory and Practice of Computer
Science, Špindlerův Mlýn, Czech Republic, January
21-27, 2012. Proceedings, Springer Berlin Heidel-
berg, Berlin, Heidelberg, chapter A Quick Tour of
Word Sense Disambiguation, Induction and Related
Approaches, pages 115–129.
</p>
<p>Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual se-
mantic network. In Proceedings of the 48th An-
nual Meeting of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics, Uppsala, Sweden, pages 216–225.
http://www.aclweb.org/anthology/P10-1023.
</p>
<p>446</p>
<p />
</div>
<div class="page"><p />
<p>Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 1059–
1069. http://www.aclweb.org/anthology/D14-1113.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 1532–1543.
http://www.aclweb.org/anthology/D14-1162.
</p>
<p>Lin Qiu, Kewei Tu, and Yong Yu. 2016. Context-
dependent sense embedding. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Austin, Texas, pages 183–191.
https://aclweb.org/anthology/D16-1018.
</p>
<p>Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguis-
tics, Los Angeles, California, pages 109–117.
http://www.aclweb.org/anthology/N10-1013.
</p>
<p>Peter Sprent and Nigel C Smeeton. 2016. Applied non-
parametric statistical methods. CRC Press.
</p>
<p>György Szarvas, Róbert Busa-Fekete, and Eyke
Hüllermeier. 2013. Learning to rank lexical sub-
stitutions. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 1926–1932.
http://www.aclweb.org/anthology/D13-1198.
</p>
<p>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for Com-
putational Linguistics, Baltimore, Maryland, pages
1555–1565. http://www.aclweb.org/anthology/P14-
1146.
</p>
<p>Yee W. Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2005. Sharing clusters
among related groups: Hierarchical dirichlet pro-
cesses. In L. K. Saul, Y. Weiss, and L. Bottou,
editors, Advances in Neural Information Process-
ing Systems 17, MIT Press, pages 1385–1392.
http://papers.nips.cc/paper/2698-sharing-clusters-
among-related-groups-hierarchical-dirichlet-
processes.pdf.
</p>
<p>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion 101(476):1566–1581.
</p>
<p>Xuchen Yao and Benjamin van Durme. 2011. Non-
parametric bayesian word sense induction. In
Graph-based Methods for Natural Language Pro-
cessing. The Association for Computer Linguistics,
pages 10–14.
</p>
<p>Will Y. Zou, Richard Socher, Daniel Cer, and
Christopher D. Manning. 2013. Bilingual word
embeddings for phrase-based machine transla-
tion. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 1393–1398.
http://www.aclweb.org/anthology/D13-1141.
</p>
<p>447</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 448–453
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2071
</p>
<p>Temporal Word Analogies: Identifying Lexical Replacement
with Diachronic Word Embeddings
</p>
<p>Terrence Szymanski
Insight Centre for Data Analytics
</p>
<p>University College Dublin
terrence.szymanski@insight-centre.org
</p>
<p>Abstract
</p>
<p>This paper introduces the concept of tem-
poral word analogies: pairs of words
which occupy the same semantic space at
different points in time. One well-known
property of word embeddings is that they
are able to effectively model traditional
word analogies (“word w1 is to word w2
as word w3 is to word w4”) through vector
addition. Here, I show that temporal word
analogies (“wordw1 at time tα is like word
w2 at time tβ”) can effectively be mod-
eled with diachronic word embeddings,
provided that the independent embedding
spaces from each time period are appro-
priately transformed into a common vector
space. When applied to a diachronic cor-
pus of news articles, this method is able to
identify temporal word analogies such as
“Ronald Reagan in 1987 is like Bill Clin-
ton in 1997”, or “Walkman in 1987 is like
iPod in 2007”.
</p>
<p>1 Background
</p>
<p>The meanings of utterances change over time, due
both to changes within the linguistic system and
to changes in the state of the world. For example,
the meaning of the word awful has changed over
the past few centuries from something like “awe-
inspiring” to something more like “very bad”, due
to a process of semantic drift. On the other hand,
the phrase president of the United States has meant
different things at different points in time due to
the fact that different people have occupied that
same position at different times. These are very
different types of changes, and the latter may not
even be considered a linguistic phenomenon, but
both types of change are relevant to the concept of
temporal word analogies.
</p>
<p>I define a temporal word analogy (TWA) as a
pair of words which occupy a similar semantic
space at different points in time. For example, as-
suming that there is a semantic space associated
with “President of the USA”, this space was oc-
cupied by Ronald Reagan in the 1980s, and by
Bill Clinton in the 1990s. So a temporal analogy
holds: “Ronald Reagan in 1987 is like Bill Clinton
in 1997”.
</p>
<p>Distributional semantics methods, particularly
vector-space models of word meanings, have been
employed to study both semantic change and word
analogies, and as such are well-suited for the task
of identifying TWAs. The principle behind these
models, that the meaning of words can be captured
by looking at the contexts in which they appear
(i.e. other words), is not a recent idea, and is gen-
erally attributed to Harris (1954) or Firth (1957).
The modern era of applying this principle algo-
rithmically began with latent semantic analysis
(LSA) (Landauer and Dumais, 1997), and the re-
cent explosion in popularity of word embeddings
is largely due to the very effective word2vec neural
network approach to computing word embeddings
(Mikolov et al., 2013a). In these types of vec-
tor space models (VSMs), the meaning of a word
is represented as a multi-dimensional vector, and
semantically-related words tend to have vectors
that relate to one another in regular ways (e.g. by
occupying nearby points in the vector space). One
factor in word embeddings’ recent popularity is
their eye-catching ability to model word analogies
using vector addition, as in the well-known exam-
ple king + man − woman = queen (Mikolov
et al., 2013b).
</p>
<p>Sagi et al. (2011) were the first to advocate the
use of distributional semantics methods (specifi-
cally LSA) to automate and quantify large-scale
studies of semantic change, in contrast to a more
traditional approach in which a researcher inspects
</p>
<p>448</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2071">https://doi.org/10.18653/v1/P17-2071</a></div>
</div>
<div class="page"><p />
<p>a handful of selected words by hand. While not
using a VSM approach, Mihalcea and Nastase
(2012) used context words as features to perform
“word epoch disambiguation”, effectively captur-
ing changes in word meanings over time. And
several recent papers have combined neural em-
bedding methods with large-scale diachronic cor-
pora (e.g. the Google Books corpus) to study
changes in word meanings over time. Kim et al.
(2014) measured the drift (as cosine similarity) of
a word’s vector over time to identify words like
cell and gay whose meaning changed over the past
100 years. Kulkarni et al. (2015) used a similar
approach, combined with word frequencies and
changepoint detection, to plot a word’s movement
through different lexical neighborhoods over time.
Most recently, Hamilton et al. (2016) employed
this methodology to discover and support two laws
of semantic change, noting that words with higher
frequency or higher levels of polysemy are more
likely to experience semantic changes.
</p>
<p>While the study of word meanings over time
using diachronic text corpora is a relatively niche
subject with little commercial applicability, it has
recently gained attention in the broader compu-
tational linguistics community. A 2015 SemEval
task was dedicated to Diachronic Text Evaluation
(Popescu and Strapparava, 2015); while systems
submitted to the task successfully predicted the
date of a text using traditional machine learning
algorithms (Szymanski and Lynch, 2015), none of
the submissions employed distributional seman-
tics methods. Also in 2015, the president of
the ACL singled out recent work (cited in the
previous paragraph) using word embeddings to
study semantic change as valuable examples of
“more scientific uses of distributed representations
and Deep Learning” in computational linguistics
(Manning, 2015).
</p>
<p>The present work is inspired by this line of re-
search, and is a continuation on the same theme
with a twist: whereas past work has investigated
specific words whose meanings have changed over
time, the present work investigates specific mean-
ings whose words have changed over time.
</p>
<p>2 Method for Discovering Temporal
Word Analogies
</p>
<p>In general, work using diachronic word embed-
dings to study semantic change follows a com-
mon procedure: first, train independent VSMs for
</p>
<p>each time period in the diachronic corpus; second,
transform those VSMs into a common space; and
finally, compare a word’s vectors from different
VSMs to identify patterns of change over time.
This paper follows a similar methodology.
</p>
<p>Algorithm 1 Calculating temporal word analogies
VA ←Word2V ec(CorpusA)
VB ←Word2V ec(CorpusB)
T ← FitTransform(VB, VA)
VB∗ ← ApplyTransform(T, VB)
vec1 ← LookupV ector(VA, word1)
vec2 ← NearestNeighbor(vec1, VB∗)
word2 ← LookupWord(VB∗ , vec2)
</p>
<p>The general process of calculating a TWA is
given in Algorithm 1. For example, if Corpus A
is the 1987 texts, and Corpus B is the 1997 texts,
and word1 is reagan, then word2 will be clinton.
Crucially, it is only by applying the transformation
to the B vector space that it becomes possible to
directly compare the B∗ space with vectors from
theA space. The Python code used in this paper to
implement this method is available to download.1
</p>
<p>1. Training Independent VSMs: The data
used in this analysis is a corpus of New York
Times articles spanning the years 1987 through
2007, containing roughly 50 million words per
year. The texts were lowercased and tokenized,
and common bigrams were re-tokenized by a sin-
gle pass of word2phrase. A separate embedding
model was trained for each year in the corpus us-
ing word2vec with default-like parameters.2 This
resulted in 21 independent vector space models,
each with a vocabulary of roughly 100k words.
</p>
<p>2. Aligning Independent VSMs: Because
training word embeddings typically begins with
a random initial state, and the training itself is
stochastic, the embeddings from different runs
(even on the same dataset) are not comparable
with one another. (Many properties of the embed-
ding space, such as the distances between points,
are consistent, but the actual values of the vectors
are random.) This poses a challenge to work on
diachronic word embeddings, which requires the
ability to compare the vectors of the same word
in different, independently-trained, VSMs. Pre-
vious work has employed different approaches to
this problem:
</p>
<p>1https://github.com/tdszyman/twapy.
2Example parameters: CBOW model, vector size 200,
</p>
<p>window size 8, negative samples 5.
</p>
<p>449</p>
<p />
</div>
<div class="page"><p />
<p>Non-random initialization. In this approach,
used by Kim et al. (2014), the values for the VSM
are not randomly initialized, but instead are ini-
tialized with the values from a previously-trained
model, e.g. training of the 1988 model would be-
gin with values from the 1987 model.
</p>
<p>Local linear regression. This approach, used by
Kulkarni et al. (2015), assumes that two VSMs are
equivalent under a linear transformation, and that
most words’ meanings do not change over time. A
linear regression model is fit to a sample of vectors
from the neighborhood of a word (hence “local”),
minimizing the mean squared error, i.e. minimiz-
ing the distance between the two vectors of a given
word in the two vector spaces. A potential draw-
back of this approach is that it must be applied sep-
arately for each focal word.
</p>
<p>Orthogonal Procrustes. This approach, used
by Hamilton et al. (2016), is similar to linear re-
gression in that it aims to learn a transformation
of one embedding space onto another, minimizing
the distance between pairs of points, but uses a dif-
ferent mathematical method and is applied glob-
ally to the full space.
</p>
<p>A thorough investigation into the relative ben-
efits of the different methods listed above would
be a valuable contribution to future work in this
area. In the present work, I take a global linear
regression approach, broadly similar to that used
by Kulkarni et al. (2015). However, I use a large
sample of points to fit the model, and I apply the
transformation globally to the entire VSM. Exper-
iments showed that the accuracy of the transfor-
mation (measured by the mean Euclidean distance
between pairs of points) increases as the sample
size increases: using 10% of the total vocabulary
shared by the two models (i.e. roughly 10k out of
100k tokens) produces good results, but there is
little reason not to use all of the points (perhaps
excluding the specific words that are the target
of study, although even this does not make much
practical difference in the outputs).
</p>
<p>3. Solving Temporal Word Analogies: Once
the independent VSMs have been transformed, it
is possible to compare vectors from one VSM with
vectors from another VSM. Solving a TWA is then
simply a matter of loading the vector of word w1
from the VSM VA, and then finding the point in
the transformed VSM V ∗2 closest to that vector.
That point corresponds to word w2, the solution to
the analogy. It is also possible to align a series of
</p>
<p>VSMs to a single “root” VSM, and thereby trace
the analogues of a word over time. The results of
applying this method are discussed next.
</p>
<p>3 Example Outputs and Analysis
</p>
<p>Using the New York Times corpus, each of the 20
VSMs from 1998 to 2007 were aligned to the 1987
VSM, making it possible to trace a series of analo-
gies over time (e.g. “Which words in 1988, 1989,
1990 ... are like reagan in 1987?”). A set of il-
lustrative words from 1987 was chosen, and their
vectors from the 1987 VSM were extracted. Then,
for each subsequent year, the nearest word in that
years’ transformed VSM was found. The outputs
are displayed in Table 1.
</p>
<p>One way to interpret Table 1 is to think of each
column as representing a single semantic concept,
and the words in that column illustrate the dif-
ferent words embodying that concept at different
points in time. So column 1 represents “President
of the USA” and column 2 represents “mayor of
New York City”. The outputs of the TWA sys-
tem perfectly reflect the names of the people hold-
ing these offices; likely due to the fact that these
concepts are discussed in the corpus with high fre-
quency and a well-defined lexical neighborhood
(e.g. White House, Oval Office, president).
</p>
<p>While the other columns do not produce quite
as clean analogies, they do tell interesting stories.
The breakup of the Soviet Union is visible in the
transition from soviet to russian in 1992, and later
that concept (something like “geopolitical foe of
the United States”) is taken up in the 2000s by
North Korea and Iran, two members of George
W. Bush’s “Axis of Evil” . Changes in technol-
ogy can be observed, with the 1987 vector for
Walkman (representing something like “portable
listening device”) passing through CD player and
MP3 player ultimately to iPod in 2007. Cultural
changes can also be observed: the TWA “yuppie
in 1987 is like hipster in 2003”, is validated by
reports in the media (Bergstein, 2016).
</p>
<p>It is not easy to pin a precise meaning to each
of these columns, and the “right” answer to any
given TWA is to some degree a subjective judg-
ment. Any given entity may fill multiple roles at
once: which role should be the focus of the anal-
ogy? Each vector in the VSM can be thought of as
combining multiple components: for example, the
vectors for reagan include a component having to
do with Ronald Reagan himself (based on words
</p>
<p>450</p>
<p />
</div>
<div class="page"><p />
<p>1987 reagan koch soviet iran contra navratilova yuppie walkman
1988 reagan koch soviet iran contra sabatini yuppie tape deck
1989 bush koch soviet iran contra navratilova yuppie walkman
1990 bush dinkins soviet iran contra navratilova yuppie headphones
1991 bush dinkins soviet iran contra navratilova yuppie cassette player
1992 bush dinkins russian iran contra sabatini yuppie walkman
1993 clinton dinkins russian iran contra navratilova yuppie cd player
1994 clinton mr giuliani russian iran contra sanchez vicario yuppie walkman
1995 clinton giuliani russian white house graf yuppie cassette player
1996 clinton giuliani russian whitewater graf yuppie walkman
1997 clinton giuliani russian iran contra hingis yuppie headphones
1998 clinton giuliani russian lewinsky hingis yuppie headphones
1999 clinton mayor giuliani russian white house hingis yuppie buttons
2000 clinton giuliani russian white house hingis yuppie headset
2001 bush giuliani russian iran contra capriati yuppie headset
2002 bush bloomberg russian white house hingis gen x mp3 player
2003 bush bloomberg russian white house agassi hipsters walkman
2004 bush bloomberg north korean iran contra federer gen x headphones
2005 bush bloomberg north korean white house roddick geek ear buds
2006 bush bloomberg iranian white house hingis teen headset
2007 bush bloomberg iranian capitol hill federer dads ipod
</p>
<p>Table 1: Examples of words from 1987 and their analogues over time. Each column corresponds to a
single point in vector space, and each row shows the word closest to that point in a given year.
</p>
<p>relating to his personal attributes or names of fam-
ily members) as well as a component having to
do with the presidency (based on words like pres-
ident, veto or White House). The analogies based
on the 1987 reagan vector produce the names of
other presidents over time (as in Table 1); how-
ever, if the 1999 reagan vector is used as the start-
ing point, then 17 of the 20 analogies produced are
either reagan or ronald reagan. This illustrates
how the vector from 1999 contains a stronger com-
ponent of the individual man rather than the pres-
idency, due to the change in how he was writ-
ten about when no longer in office. Similarly,
the Iran Contra vector can be viewed as a mix-
ture of ‘the Iran Contra crisis itself” and a more
generic “White House scandal” concept. This sec-
ond component causes Clinton-era scandals like
Whitewater and Lewinsky to briefly appear in that
space, while the first causes Iran Contra to con-
tinue to appear over time.
</p>
<p>4 Evaluation
</p>
<p>Standard evaluation sets of word analogies exist
and are commonly used as a measure of the qual-
ity of word embeddings (but see Linzen (2016) for
why this can be problematic and misleading). No
data set of manually-verified TWAs currently ex-
ists, so a small evaluation set was assembled by
hand: ten TWA categories were selected which
could be expected to be both newsworthy and un-
ambiguous, and values for each year in the corpus
</p>
<p>were identified using encyclopedic sources. When
all pairs of years are considered, this yields a total
of 4,200 individual TWAs. This data set, including
the prediction outputs, is available online.
</p>
<p>For comparison, a baseline system which al-
ways predicts the output w2 to be the same as the
input w1 was implemented. (A system based on
word co-occurrence counts was also implemented,
but produced no usable output.3) Table 2 shows
the accuracy of the embedding-based system and
the baseline for each category. Accuracy is de-
termined by exact match, so mayor giuliani is in-
correct for giuliani. Some categories are clearly
much more difficult than others: prediction accu-
racy is 96% for “President of the USA”, but less
than 1% for “Best Actress”. The names of Oscar
Best Actress winners change every year with very
little repetition, and it may be that an actress’ role
as an award winner only constitutes a small part of
her overall news coverage.
</p>
<p>Accuracy is a useful metric, but it is not neces-
sarily the best way to evaluate TWAs. Due to the
nature of the data (the U.S. President, for example,
only changes every four or eight years), the base-
line system works quite well when the time depth
of the analogy (δt = |tα − tβ|) is small. However,
</p>
<p>3The co-occurrence matrices were expensive to construct
due to the volume of data, and despite efforts to smooth the
distributions, the analogy outputs were noisy, dominated by
low-frequency tokens with relatively few non-zero compo-
nents and high cosine similarities. But it is possible that with
more careful engineering this approach could be effective.
</p>
<p>451</p>
<p />
</div>
<div class="page"><p />
<p>Baseline Embeddings
CEO of Goldman Sachs 17.6 1.7
Governor of New York 44.8 62.4
Mayor of NYC 24.8 85.0
NFL MVP 1.9 1.4
Oscar Best Actress 1.0 0.5
President of the USA 40.0 96.4
Prime Minister of the UK 37.6 33.6
Secretary of State of USA 11.9 21.9
Super Bowl Champions 5.7 32.6
WTA Top-ranked Player 16.2 26.4
δt &lt;= 5 years 37.8 40.8
δt &gt; 5 years 6.9 32.7
Overall 20.1 36.2
</p>
<p>Table 2: Analogy prediction accuracy.
</p>
<p>as time depth increases, its accuracy drops sharply,
while the embedding-based method remains effec-
tive, as illustrated in Figure 1. And even when the
embedding-based system makes the “wrong” pre-
diction, the output may still be insightful for data
exploration, which is a more likely application for
this method rather than prediction.
</p>
<p>The analogies evaluated here have the benefits
of being easy to compile and evaluate, but they
represent only one specific subset of TWAs. Other,
less-clearly-defined, types of analogies (like the
yuppie and walkman examples) would require a
less rigid (and more expensive), form of evalua-
tion, such as obtaining human acceptability judg-
ments of the automatically-produced analogies.
</p>
<p>5 Conclusion
</p>
<p>In this paper I have presented the concept of tem-
poral word analogies and a method for identi-
fying them using diachronic word embeddings.
The method presented here is effective at solving
TWAs, as shown in the evaluation, but its greater
strength may be as a tool for data exploration. The
small set of examples included in this paper illus-
trate political and social changes that unfold over
time, and in the hands of users with diverse cor-
pora and research questions, many more interest-
ing analogies would likely be discovered using this
same method.
</p>
<p>The method presented in this paper is not the
only way that TWAs could be predicted. If the
VSMs could somehow be trained jointly, rather
than independently, this would eliminate the need
for transformation and the noise it introduces. Or
perhaps it is sufficient to look at lexical neighbor-
hoods, rather the vectors themselves. One lim-
itation of the embedding approach is that it re-
</p>
<p>5 10 15 20
</p>
<p>Time depth δt (years)
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>50
</p>
<p>60
</p>
<p>A
cc
</p>
<p>ur
ac
</p>
<p>y
</p>
<p>Embeddings
Baseline
</p>
<p>Figure 1: Accuracy as function of time depth.
</p>
<p>quires vast amounts of data from each time pe-
riod: tens of millions of words are required to
train a model with word2vec. This makes it im-
practical for many historical corpora, which tend
to be much smaller than the corpus used here. If
a simpler, count-based approach could be made
to work, this might be more applicable to smaller
corpora. A method which incorporates word fre-
quency (Kulkarni et al., 2015) might be effective
at identifying when one word drops from com-
mon usage and another word appears. And as-
signing a measure of confidence to the proposed
TWAs could help automatically identify meaning-
ful analogies from the vast combinations of words
and years that exist.
</p>
<p>In a domain where large quantities of real-time
text are available, this method could potentially
be applied as a form of event detection, identify-
ing new entrants into a semantic space. And the
same method described here could potentially be
applied to other, non-diachronic, types of corpora.
For example, given corpora of British English
and American English, this methodology might be
used to identify dialectal analogies, e.g. “elevator
in US English is like lift in British English.” In-
deed, this general approach of comparing words in
multiple embedding spaces could have many ap-
plications outside of diachronic linguistics.
</p>
<p>Acknowledgments
</p>
<p>I would like to thank all the participants at the
Insight Centre for Data Analytics special interest
group meeting on NLP at NUI Galway for their
encouraging and insightful feedback. Thanks also
to the anonymous ACL reviewers, for encourag-
ing the addition of a quantitative evaluation. This
work was partially supported by Science Founda-
tion Ireland through the Insight Centre for Data
Analytics under grant number SFI/12/RC/2289.
</p>
<p>452</p>
<p />
</div>
<div class="page"><p />
<p>References
Rachelle Bergstein. 2016. Are hipsters the new yup-
</p>
<p>pies? forbes.com October 12, 2016.
</p>
<p>J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis. Philological
Society.
</p>
<p>William L. Hamilton, Jure Leskovec, and Dan Jurafsky.
2016. Diachronic word embeddings reveal histori-
cal laws of semantic change. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).
</p>
<p>Zellig Harris. 1954. Distributional structure. Word
10(23):146–162.
</p>
<p>Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde,
and Slav Petrov. 2014. Temporal analysis of lan-
guage through neural language models. In Proceed-
ings of the ACL 2014 Workshop on Language Tech-
nologies and Computational Social Science. pages
61–65.
</p>
<p>Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and
Steven Skiena. 2015. Statistically significant detec-
tion of linguistic change. In Proceedings of the 24th
International Conference on World Wide Web. pages
625–635.
</p>
<p>Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato’s problem: The latent semantic
analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review
104(2):211–140.
</p>
<p>Tal Linzen. 2016. Issues in evaluating semantic spaces
using word analogies. In Proceedings of the 1st
Workshop on Evaluating Vector-Space Representa-
tions for NLP. pages 13–18.
</p>
<p>Christopher D. Manning. 2015. Computational lin-
guistics and deep learning. Computational Linguis-
tics 41(4).
</p>
<p>Rada Mihalcea and Vivi Nastase. 2012. Word epoch
disambiguation: Finding how words change over
time. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers).
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations Workshop.
</p>
<p>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. pages 746–751.
</p>
<p>Octavian Popescu and Carlo Strapparava. 2015. Se-
meval 2015, task 7: Diachronic text evaluation. In
Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015).
</p>
<p>Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2011.
Tracing semantic change with latent semantic anal-
ysis. In Current Methods in Historical Semantics.
De Gruyter Mouton.
</p>
<p>Terrence Szymanski and Gerard Lynch. 2015. UCD:
Diachronic text classification with character, word,
and syntactic n-grams. In Proceedings of the 9th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2015).
</p>
<p>453</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 454–458
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2072
</p>
<p>Methodical Evaluation of Arabic Word Embeddings
</p>
<p>Mohammed Elrazzaz
Computer Science and Engineering Department
</p>
<p>Qatar University
Doha, Qatar
</p>
<p>mohammed.elrazzaz@qu.edu.qa
</p>
<p>Shady Elbassuoni
Computer Science Department
American University of Beirut
</p>
<p>Beirut, Lebanon
se58@aub.edu.lb
</p>
<p>Khaled Shaban
Computer Science and Engineering Department
</p>
<p>Qatar University
Doha, Qatar
</p>
<p>khaled.shaban@qu.edu.qa
</p>
<p>Chadi Helwe
Computer Science Department
American University of Beirut
</p>
<p>Beirut, Lebanon
cth05@aub.edu.lb
</p>
<p>Abstract
</p>
<p>Many unsupervised learning techniques
have been proposed to obtain meaning-
ful representations of words from text. In
this study, we evaluate these various tech-
niques when used to generate Arabic word
embeddings. We first build a benchmark
for the Arabic language that can be utilized
to perform intrinsic evaluation of different
word embeddings. We then perform addi-
tional extrinsic evaluations of the embed-
dings based on two NLP tasks.
</p>
<p>1 Introduction
</p>
<p>Distributed word representations, commonly re-
ferred to as word embeddings, represent words as
vectors in a low-dimensional space. The goal of
this deep representation of words is to capture syn-
tactic and semantic relationships between words.
These word embeddings have been proven to be
very useful in various NLP applications, particu-
larly those employing deep learning.
</p>
<p>Word embeddings are typically learned using
unsupervised learning techniques on large text
corpora. Many techniques have been proposed to
learn such embeddings (Pennington et al., 2014;
Mikolov et al., 2013; Mnih and Kavukcuoglu,
2013). While most of the work has focused on
English word embeddings, few attempts have been
carried out to learn word embeddings for other lan-
guages, mostly using the above mentioned tech-
niques.
</p>
<p>In this paper, we focus on Arabic word embed-
dings. Particularly, we provide a thorough evalu-
ation of the quality of four Arabic word embed-
dings that have been generated by previous work
</p>
<p>(Zahran et al., 2015; Al-Rfou et al., 2013). We use
both intrinsic and extrinsic evaluation methods to
evaluate the different embeddings. For the intrin-
sic evaluation, we build a benchmark consisting of
over 115,000 word analogy questions for the Ara-
bic language. Unlike previous attempts to evalu-
ate Arabic embeddings, which relied on translat-
ing existing English benchmarks, our benchmark
is the first specifically built for the Arabic lan-
guage and is publicly available for future work in
this area 1. Translating an English benchmark is
not the best strategy to evaluate Arabic embed-
dings for the following reasons. First, the cur-
rently available English benchmarks are specifi-
cally designed for the English language and some
of the questions there are not applicable to Arabic.
Second, Arabic has more relations compared to
English and these should be included in the bench-
mark as well. Third, translating an English bench-
mark is subject to errors since it is usually carried
out in an automatic fashion.
</p>
<p>In addition to the new benchmark, we also ex-
tend the basic analogy reasoning task by taking
into consideration more than two word pairs when
evaluating a relation, and by considering the top-5
words rather than only the top-1 word when an-
swering an analogy question. Finally, we perform
an extrinsic evaluation of the different embeddings
using two different NLP tasks, namely Document
Classification and Named Entity Recognition.
</p>
<p>2 Related Work
</p>
<p>There is a wealth of research on evaluating unsu-
pervised word embeddings, which can be can be
broadly divided into intrinsic and extrinsic evalu-
</p>
<p>1http://oma-project.com/res_home
</p>
<p>454</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2072">https://doi.org/10.18653/v1/P17-2072</a></div>
</div>
<div class="page"><p />
<p>Relation (a, b) (c, d) #pairs #tuples
Capital Egypt QåÓ Cairo �èQëA�®Ë @ Qatar Q¢�̄ Doha �ékðYË@ 124 15252
</p>
<p>Currency Egypt QåÓ Pound éJ
 	Jm.Ì'@ Qatar Q¢�̄ Riyal ÈAK
QË @ 155 23870
Male-Female boy YËð girl �I 	�K. husband h. ð 	P wife
</p>
<p>�ék. ð 	P 101 10100
Opposite male Q» 	X female ú

</p>
<p>�æ 	K

@ woke up iJ.
</p>
<p>
@ slept ú
æÓ
</p>
<p>
@ 110 11990
</p>
<p>Comparative big Q�
J.» bigger Q�.»

@ small Q�
 	ª smaller Q 	ª
</p>
<p>
@ 100 9900
</p>
<p>Nationality Holland @Y 	JËñë Dutch ø
 Y
	JËñë India Y	JêË @ Indian ø
 Y
</p>
<p>	Jë 100 9900
Past Tense travel Q 	® traveled Q 	̄ A fight ÈA�J�̄ fought É�KA�̄ 110 11990
</p>
<p>Plural man Ég. P men ÈAg. P house �I�
K. houses �HñJ
K. 111 12210
Pair man Ég. P 2 men 	àCg. P house �I�
K. 2 houses 	àA�J�
K. 100 9900
ALL 1011 115112
</p>
<p>Table 1: Summary of the Arabic Word Analogy Benchmark
</p>
<p>ations. Intrinsic evaluations mostly rely on word
analogy questions and measure the similarity of
words in the low-dimensional embedding space
(Mikolov et al., 2013; Gao et al., 2014; Schn-
abel et al., 2015). Extrinsic evaluations assess
the quality of the embeddings as features in mod-
els for other tasks, such as semantic role labeling
and part-of-speech tagging (Collobert et al., 2011),
or noun-phrase chunking and sentiment analysis
(Schnabel et al., 2015). However, all of these tasks
and benchmarks are build for English and thus
cannot be used to assess the quality of Arabic word
embeddings, which is the main focus here.
</p>
<p>To the best of our knowledge, only a hand-
ful of recent studies attempted evaluating Ara-
bic word embeddings. Zahran et al. (Zahran
et al., 2015) translated the English benchmark
in (Mikolov et al., 2013) and used it to evalu-
ate different embedding techniques when applied
on a large Arabic corpus. However, as the au-
thors themselves point out, translating an English
benchmark is not the best strategy to evaluate Ara-
bic embeddings. Zahran et al. also consider ex-
trinsic evaluation on two NLP tasks, namely query
expansion for IR and short answer grading.
</p>
<p>Dahou et al. (Dahou et al., 2016) used the anal-
ogy questions from (Zahran et al., 2015) after cor-
recting some Arabic spelling mistakes resulting
from the translation and after adding new analogy
questions to make up for the inadequacy of the
English questions for the Arabic language. They
also performed an extrinsic evaluation using sen-
timent analysis. Finally, Al-Rfou et al. (Al-Rfou
et al., 2013) generated word embeddings for 100
</p>
<p>different languages, including Arabic, and evalu-
ated the embeddings using part-of-speech tagging,
however the evaluation was done only for a hand-
ful of European languages.
</p>
<p>3 Benchmark
</p>
<p>Our benchmark is the first specifically designed
for the Arabic language. It consists of nine re-
lations, each consisting of over 100 word pairs.
An Arabic linguist who was properly introduced
to the word-analogy task provided the list of rela-
tions. Once the nine relations were defined, two
different people collectively generated the word
pairs. The two people are native Arabic speak-
ers, and one of them is a co-author and the other is
not. Table 1 displays the list of all relations in our
benchmark as well as two example word pairs for
each relation. The full benchmark and the evalua-
tion tool can be obtained from the following link:
http://oma-project.com/res_home.
</p>
<p>Translating an English benchmark is not ade-
quate for many reasons. First, the currently avail-
able English benchmarks contain many questions
that are not applicable to Arabic. For example,
comparative and superlative relations are the same
in Arabic, except that the superlatives are usually
prefixed with the Arabic equivalent of ”the”. An-
other example is the opposite relation, where some
words in Arabic do not have antonyms, in which
case the antonym is typically expressed by prefix-
ing the word with ”not”. Second, Arabic has more
relations compared to English. For instance, in
Arabic there is the pair relation (see Table 1 for
an example). Third, translating an English bench-
</p>
<p>455</p>
<p />
</div>
<div class="page"><p />
<p>mark is considerably difficult due to the high am-
biguity of the Arabic language.
</p>
<p>Given our benchmark, we generate a test bank
consisting of over 100,000 tuples. Each tuple con-
sists of two word pairs (a, b) and (c, d) from the
same relation. For each of our nine relations,
we generate a tuple by combining two different
word pairs from the same relation. Once tuples
have been generated, they can be used as word
analogy questions to evaluate different word em-
beddings as defined by Mikolov et al. (Mikolov
et al., 2013). A word analogy question for a tu-
ple consisting of two word pairs (a, b) and (c, d)
can be formulated as follows: ”a to b is like c
to ?”. Each such question will then be answered
by calculating a target vector t = b − a + c.
We then calculate the cosine similarity between
the target vector t and the vector representation of
each word w in a given word embeddings V . Fi-
nally, we retrieve the most similar wordw to t, i.e.,
argmaxw∈V&amp;w/∈{a,b,c}
</p>
<p>w·t
||w||||t|| . If w = d (i.e., the
</p>
<p>same word) then we assume that the word embed-
dings V has answered the question correctly.
</p>
<p>We also use our benchmark to generate addi-
tional analogy questions by using more than two
word pairs per question. This provides a more ac-
curate representation of a relation as mentioned in
(Mikolov et al., 2013). For each relation, we gen-
erate a question per word pair consisting of the
word pair plus 10 random word pairs from the
same relation. Thus, each question would con-
sist of 11 word pairs (ai, bi) where 1 ≤ i ≤ 11.
We then use the average of the first 10 word pairs
to generate the target vector t as follows: t =
1
10
</p>
<p>∑10
i (bi − ai) + a11. Finally we retrieve the
</p>
<p>closest word w to the target vector t using cosine
similarity as in the previous case. The question is
considered to be answered correctly if the answer
word w is the same as b11.
</p>
<p>Moreover, we also extend the traditional word
analogy task by taking into consideration if the
correct answer is among the top-5 closest words in
the embedding space to the target vector t, which
allows us to more leniently evaluate the embed-
dings. This is particularly important in the case of
Arabic since many forms of the same word exist,
usually with additional prefixes or suffixes such
as the equivalent of the article ”the” or possessive
determiners such as ”her”, ”his”, or ”their”. For
example, consider one question which asks ”YËð
to �I 	�K. is like ½ÊÓ to ?”, i.e., ”man to woman is
</p>
<p>like king to ?”, with the answer being ” éºÊÓ” or
”queen”. Now, if we rely only on the top-1 word
and it happens to be ” é�JºÊÓ”, which means ”his
queen” in English, the question would be consid-
ered to be answered wrongly. To relax this and
ensure that different forms of the same word will
not result in a mismatch, we use the top-5 words
for evaluation rather than the top-1.
</p>
<p>4 Evaluation
</p>
<p>We compare four different Arabic word embed-
dings that have been generated by previous work.
The first three are based on a large corpus of
Arabic documents constructed by Zahran et al.
(Zahran et al., 2015), which consists of 2,340,895
words. Using this corpus, the authors generated
three different word embeddings using three dif-
ferent techniques, namely the Continuous Bag-
of-Words (CBOW) model (Mikolov et al., 2013),
the Skip-gram model (Mikolov et al., 2013) and
GloVe (Pennington et al., 2014). The fourth word
embeddings we evaluate in this paper is the Ara-
bic part of the Polyglot word embeddings, which
was trained on the Arabic Wikipedia by Al-Rfou
et al and consists of over 100,000 words (Al-Rfou
et al., 2013). To the best of our knowledge, these
are the only available word embeddings that have
been constructed for the Arabic language.
</p>
<p>4.1 Intrinsic Evaluation
</p>
<p>As we mentioned in the previous section, we use
our word analogy benchmark to evaluate the em-
beddings using four different criteria, namely us-
ing top-1 and top-5 words when representing rela-
tions using two versus 11 word pairs. Tables 2 dis-
plays the accuracy of each embedding technique
for the four evaluation criteria. Note that we con-
sider a question to be answered wrongly if at least
one of the words in the question are not present in
the word embeddings. That is, we take into con-
sideration the coverage of the embeddings as well
(Gao et al., 2014).
</p>
<p>As can be seen in Table 2, the CBOW model
consistently outperforms all other compared mod-
els for all four evaluation criteria. The perfor-
mance of Polyglot is particularly low since the em-
beddings were trained on a much smaller corpus
(Arabic portion of Wikipedia), and thus both its
coverage and the quality of the embeddings are
much lower. As can also be seen from the table,
the accuracies of all the methods are boosted when
</p>
<p>456</p>
<p />
</div>
<div class="page"><p />
<p>Model CBOW Skip-gram GloVe Polyglot CBOW Skip-gram Glove Polyglot
Relation top-1 two pairs top-5 two pairs
Capital 31% 26.6% 31.7% 0.4% 42.9% 40.8% 47% 1.8%
</p>
<p>Currency 3.15% 2% 0.8% 0.4% 4.9%% 3.9% 3.7% 1.6%
Male-Female 29% 24.8% 30.8% 3.8% 45.6% 40.6 52.4% 8.3%
</p>
<p>Opposite 7.6% 4.41% 7.3% 2.3% 15.75% 10.65% 19.8% 5.4%
Comparative 23.9% 15.7% 21.7% 1.4% 39.61% 30.95% 38.3% 4%
Nationality 29% 29.91% 25.8% 0.8% 34.65% 39.6% 32.4% 3%
Past Tense 4.3% 2.7% 4.5% 0.4% 11.4% 9.6% 16.7% 1.5%
</p>
<p>Plural 23.3% 13.28% 19% 2.9% 45.12% 37.9% 41.9% 7.2%
Pair 8.6% 7.6% 1.8% 0.02% 23% 21.3% 5.3% 0.07%
ALL 16.3% 12.8% 14.5% 1.3% 26.6% 23.8% 26.4% 3.4%
</p>
<p>Relation top-1 11 pairs top-5 11 pairs
Capital 28.2% 28.2% 33.8 0% 48.38% 40.3% 50.8% 0.8%
</p>
<p>Currency 3.8% 3.8% 0.64% 0.6% 7% 4.5% 2.5% 0.6%
Male-Female 29.7% 25.7% 26.7% 4.9% 48.5% 39.6% 52.4% 7.9%
</p>
<p>Opposite 5.4% 3.6% 5.4% 2.7% 16.3% 8.1% 15.4% 3.6%
Comparative 31% 23% 25% 1% 49% 36% 39% 2%
Nationality 35% 32% 34% 1% 41% 43% 39% 4%
Past Tense 1.8% 0% 3.6% 1.8% 15.4% 9% 17.2% 3.6%
</p>
<p>Plural 20.7% 11.7% 18% 4.5% 48.6% 39.6% 44.2% 6.3%
Pair 8% 11% 3% 0% 21% 18% 9% 0%
ALL 17.4% 14.8% 16% 1.9% 31.1% 25.4% 28.8% 2.5%
</p>
<p>Table 2: Intrinsic evaluation of the word embeddings using different criteria
</p>
<p>Model Document Classification NER
CBOW 0.948 0.800
</p>
<p>Skip-gram 0.954 0.799
GloVe 0.946 0.816
</p>
<p>Polyglot 0.882 0.649
</p>
<p>Table 3: F-measure for two NLP tasks
</p>
<p>representing a relation using 11 pairs rather just
two pairs. This validates that it is indeed more ap-
propriate to use more than two pairs to represent
relations in word analogy tasks.
</p>
<p>When considering the top-5 matches, the accu-
racies of the embeddings are boosted drastically,
which indeed shows that relying on just the top-1
word to assess the quality of embeddings might be
unduly harsh, particularly in the case of Arabic.
</p>
<p>4.2 Extrinsic Evaluation
</p>
<p>We perform extrinsic evaluation of the four word
embeddings using two NLP tasks, namely: Arabic
Document Classification and Arabic Named En-
tity Recognition (NER). In the Document Clas-
sification task, the goal is to classify Arabic
</p>
<p>Wikipedia articles into four different classes (per-
son (PER), organization (ORG), location (LOC),
or miscellaneous (MISC)). To do this, we re-
lied on a neural network with a Long Short-Term
Memory (LSTM) layer (Hochreiter and Schmid-
huber, 1997), which is fed from the word embed-
dings. The LSTM layer is followed by two fully-
connected layers, which in turn are followed by a
softmax layer that predicts class-assignment prob-
abilities. The model was trained for 150 epochs
on 8,000 articles, validated on 1,000 articles, and
tested on another 1,000 articles.
</p>
<p>In the NER task, the goal is to label each word
in a given sequence using one of the following la-
bels: PER, LOC, ORG, and MISC, which repre-
sent different Named Entity classes. The same ar-
chitecture as in the Document Classification task
was used for this task as well. The model was
trained for 150 epochs on 3,852 sentences and
tested on 963 sentence using Columbia’s Uni-
versity Arabic Named Entity Recognition Corpus
(Columbia University, 2016). We used an LSTM
neural network for both tasks since they flexibly
make use of contextual data and thus are com-
</p>
<p>457</p>
<p />
</div>
<div class="page"><p />
<p>monly used in NLP tasks such as Document Clas-
sification and NER.
</p>
<p>As can be seen in Table 3, the first three meth-
ods CBOW, Skip-gram and GloVe seem to per-
form relatively well for both the Document Clas-
sification task as well as the NER task with very
comparable performance in terms of F-measure.
They also clearly outperform Polyglot when it
comes to both tasks as well.
</p>
<p>4.3 Discussion
</p>
<p>Our experimental results indicate the superiority
of CBOW and SKip-gram as word embeddings
compared to Polyglot. This can be mainly at-
tributed to the fact that the first two embeddings
were trained using a much larger corpus and thus
had both better coverage and higher accuracies
when it comes to the word analogy task. This is
also evident in the case of the extrinsic evaluation.
Thus, when training word embeddings, it is cru-
cial to use large training data to obtain meaningful
embeddings.
</p>
<p>Moreover, when performing the intrinsic eval-
uation of the different embeddings, we observed
that relying on just the top-1 word is unduly harsh
for Arabic. This is mainly attributed to the fact that
for Arabic, and unlike other languages such as En-
glish, different forms of the same word exist and
these must be taken into consideration when eval-
uating the embeddings. Thus, it is advised to use
the top-k matches to perform the evaluation, where
k is 5 for instance. It is also advisable to represent
a relation with multiple word pairs, rather than just
two as is currently done in most similar studies, to
guarantee that the relation is well represented.
</p>
<p>5 Conclusion
</p>
<p>In this paper, we described the first word anal-
ogy benchmark specifically designed for the Ara-
bic language. We used our benchmark to evaluate
available Arabic word embeddings using the basic
analogy reasoning task as well as extensions of it.
In addition, we also evaluated the quality of the
various embeddings using two NLP tasks, namely
Document Classification and NER.
</p>
<p>Acknowledgments
</p>
<p>This work was made possible by NPRP 6-716-1-
138 grant from the Qatar National Research Fund
(a member of Qatar Foundation). The statements
</p>
<p>made herein are solely the responsibilty of the au-
thors.
</p>
<p>References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
</p>
<p>2013. Polyglot: Distributed word represen-
tations for multilingual nlp. arXiv preprint
arXiv:1307.1662 .
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research
12:2493–2537.
</p>
<p>Columbia University. 2016. Arabic Named Entity
Recognition Task. http://www1.ccls.
columbia.edu/˜ybenajiba/downloads.
html.
</p>
<p>Abdelghani Dahou, Shengwu Xiong, Junwei Zhou,
Mohamed Houcine Haddoud, and Pengfei Duan.
2016. Word embeddings and convolutional neural
network for arabic sentiment classification. In Inter-
national Conference on Computational Linguistics.
pages 2418–2427.
</p>
<p>Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014. Wordrep:
A benchmark for research on learning word repre-
sentations. arXiv preprint arXiv:1407.1640 .
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems. pages 2265–2273.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global Vectors for Word
Representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 1532–1543.
</p>
<p>Tobias Schnabel, Igor Labutov, David M Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings. In EMNLP. pages
298–307.
</p>
<p>Mohamed A Zahran, Ahmed Magooda, Ashraf Y Mah-
goub, Hazem Raafat, Mohsen Rashwan, and Amir
Atyia. 2015. Word representations in vector space
and their applications for arabic. In International
Conference on Intelligent Text Processing and Com-
putational Linguistics. Springer, pages 430–443.
</p>
<p>458</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 459–464
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2073
</p>
<p>Multilingual Connotation Frames: A Case Study on Social Media
for Targeted Sentiment Analysis and Forecast
</p>
<p>Hannah Rashkin† Eric Bell‡ Yejin Choi† Svitlana Volkova‡
†Paul G. Allen School of Computer Science &amp; Engineering, University of Washington
</p>
<p>{hrashkin,yejin}@cs.washington.edu
‡Data Sciences and Analytics, Pacific Northwest National Laboratory
</p>
<p>{eric.bell,svitlana.volkova}@pnnl.gov
</p>
<p>Abstract
</p>
<p>People around the globe respond to ma-
jor real world events through social me-
dia. To study targeted public sentiments
across many languages and geographic lo-
cations, we introduce multilingual con-
notation frames: an extension from En-
glish connotation frames of Rashkin et al.
(2016) with 10 additional European lan-
guages, focusing on the implied senti-
ments among event participants engaged
in a frame. As a case study, we present
large scale analysis on targeted public sen-
timents toward salient events and entities
using 1.2 million multilingual connotation
frames extracted from Twitter.
</p>
<p>1 Introduction
</p>
<p>People around the globe use social media to ex-
press their reflections and opinions on major real
world events (Atefeh and Khreich, 2015; Radinsky
and Horvitz, 2013). In order to facilitate multi-
lingual public sentiment tracking on social media,
we introduce multilingual connotation frames,1 a
multilingual extension from English connotation
frames of Rashkin et al. (2016) with 10 additional
European languages, including low-resource lan-
guages such as Polish, Finnish, and Russian.
</p>
<p>Definition 1.1. Connotation Frames: A frame-
work for encoding predicate-specific connotative
relationships implied by a predicate towards its ar-
guments.
</p>
<p>Figure 1 shows a selected subset of the conno-
tation frames that is relevant in our study. See
Rashkin et al. (2016) for the full description of the
connotation frames.
</p>
<p>1Publicly available at homes.cs.washington.
edu/~hrashkin/multicf.html.
</p>
<p>English Verb: survive
Other languages: survivre, sobrevivir, überleben…
</p>
<p>“L'incroyable miraculé des explosions à Brussels: ce Mormon avait 
déjà survécu aux attentats de Boston et de Paris”
</p>
<p>“Este joven ha sobrevivido a los atentados de Boston, 
de París y de Bruselas”
</p>
<p>“US teenager … also survived Boston Marathon bombing”
</p>
<p>“19-jähriger Missionar überlebt drei Terroranschläge”
</p>
<p>Example Tweets
</p>
<p>Writer
</p>
<p>=Reader
Agent Theme
</p>
<p>P(
w
!
</p>
<p>ag
en
</p>
<p>t) P(w !
them
</p>
<p>e)
P(agent! theme)
</p>
<p>theme is 
some type 
</p>
<p>of hardship
</p>
<p>agent is 
sympathetic 
</p>
<p>victim
</p>
<p>———
</p>
<p>Connotation Frame for surviving verbs:
</p>
<p>+
</p>
<p>+
</p>
<p>—
</p>
<p>—
</p>
<p>—
</p>
<p>Figure 1: The connotation frame of “survive” with respect to
directional sentiments among “writer”, “agent”, “theme”,
and “reader”. The tweet examples show automatically in-
duced multilingual connotation frames.
</p>
<p>There are two important benefits to develop
multilingual connotation frames. First, they serve
as a unique lexical resource to enable targeted sen-
timent analysis, which rarely exists for most lan-
guages.
</p>
<p>Definition 1.2. Targeted Sentiment: A sentiment
label indicating how a source entity feels about a
target entity.
</p>
<p>In the example shown in Figure 1, “teenager
survived Boston Marathon bombing”, the conno-
tation frame allows us to correctly interpret (im-
plied) targeted sentiments including:
</p>
<p>1. sentiment(teenager→ bombing) = –
2. sentiment(writer→ bombing) = –
3. sentiment(writer→ teenager) = +
Second, they allow us to study a broad spectrum
</p>
<p>of sentiments including nuanced ones; in the ex-
</p>
<p>459</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2073">https://doi.org/10.18653/v1/P17-2073</a></div>
</div>
<div class="page"><p />
<p>Lang # Tuples Examples
</p>
<p>EN 643,004 (korea, fires, missile)
ES 305,310 (acuerdo, vulnera, derecho)
FR 85,286 (obama, quitte, cuba)
PT 76,849 (valentino, renova, contrato)
RU 28,511 (путин, обсуди, моста)
DE 23,197 (seehofer, lösen, flüchtlingskrise)
NL 14,091 (artiesten, bevestigd, komst)
IT 13,586 (conte, lascia, nazionale)
FI 2,859 (hans, tekemään, valintansa)
SV 2,229 (rubio, avslutar, kampanj)
PL 2,226 (papież, wyrazi, zgodę)
</p>
<p>Table 1: The number and the example of (agent, verb, theme)
tuples extracted from tweets across languages.
</p>
<p>amples discussed above, connotation frames allow
us to infer (1) the likely sentiment among the event
participants (e.g., a surviving teenager is likely to
be negative toward the Boston bombing), and (2)
the likely sentiment of the author towards events
and entities (e.g., the writer is likely to be sympa-
thetic toward the teenager while negative toward
the incident), even though none of these sentiment
implications are overtly expressed.
</p>
<p>To validate the empirical utility of the new mul-
tilingual connotation lexicon, we present a suc-
cessful case study of large scale connotation anal-
ysis (Section 4.1) and forecast (Section 4.2) based
on connotation frames extracted from 1.2 million
tweets in 10 different European languages span-
ning over a 15 day period.
</p>
<p>2 Multilingual Twitter Dataset
</p>
<p>We obtained multilingual geo-located tweets span-
ning Mar 15 – Mar 29, 2016. This 15 day dura-
tion covers Brussels attacks on Mar 22 as well as
one whole week before and after, allowing us to
study the public sentiment dynamics in response
to a major terrorist event. We focus on tweets
that are likely to be about “news-worthy” topics
by selecting tweets that came from trusted sources
such as twitter-verified accounts or known news
accounts, or contained hashtags #breaking or
#news.2 We used SyntaxNet dependency parser
(Andor et al., 2016) and trained additional Syn-
taxNet models for 10 non-English languages us-
ing Universal Dependencies annotations.3 We
extracted 1.2 million agent-verb-theme tuples as
listed in Table 1.
</p>
<p>2We experimented with other automatic ways of find-
ing news sources by identifying popular hashtags, but this
method did not translate well to other languages.
</p>
<p>3http://universaldependencies.org/
</p>
<p>0
1
0
</p>
<p>.3
</p>
<p>.4
</p>
<p>.3
</p>
<p>.3
</p>
<p>.6
</p>
<p>.1
</p>
<p>.3
</p>
<p>.4
</p>
<p>.3
</p>
<p>LSTM
</p>
<p>Fully 
connected  
</p>
<p>softmax 
layer
</p>
<p>Input  
distribution 
</p>
<p>vectors
</p>
<p>+
=
—
</p>
<p>M
ar
</p>
<p> 2
1 
</p>
<p>(U
K 
→
</p>
<p> B
ru
</p>
<p>ss
el
</p>
<p>s)
</p>
<p>M
ar
</p>
<p> 2
2 
</p>
<p>(U
K 
→
</p>
<p> B
ru
</p>
<p>ss
el
</p>
<p>s)
</p>
<p>M
ar
</p>
<p> 2
3 
</p>
<p>(U
K 
→
</p>
<p> B
ru
</p>
<p>ss
el
</p>
<p>s)
</p>
<p>M
ar
</p>
<p> 2
4 
</p>
<p>(U
K 
→
</p>
<p> B
ru
</p>
<p>ss
el
</p>
<p>s)
</p>
<p>Predicted Distrib: Mar 25 
(UK → Brussels)
</p>
<p>}
}
</p>
<p>Figure 2: Diagram of LSTM model for predicting the distri-
bution of perspectives from a location (e.g., UK) towards an
entity (e.g., Brussels) on a given day (e.g., March 25), based
on the previous days.
</p>
<p>3 Methods
</p>
<p>3.1 Multilingual Connotation Frames
</p>
<p>We perform context-based projection of English
connotation frames to 10 additional European lan-
guages using large parallel corpora. Since conno-
tation of a word arises from the context in which
the word is used, we want to ensure the trans-
lated connotation frames are used in similar con-
texts. We use existing parallel corpora with auto-
matic word-alignment: the Opus Corpus (Tiede-
mann, 2012) using Multi-UN parallel data (Eisele
and Chen, 2010) for Russian and EuroParl parallel
data (Koehn, 2005) for all other languages.
</p>
<p>More concretely, for each non-English verb, v′
</p>
<p>(e.g., assassiner in French), we compute the prob-
ability of it being translated to English verb v by
counting the alignments.
</p>
<p>We then define the connotation frame of v′,
F(v′), by transferring the connotation frame of the
English verb v∗, F(v∗), that has the highest trans-
lation probability:
</p>
<p>v∗ = argmaxvp(v|v′)
</p>
<p>F(v′) = F(v∗)
For example, the connotation frame for assassiner
is propagated from murder, the English word that
it is aligned with the most.
</p>
<p>3.2 Extracting Targeted Sentiments
</p>
<p>Using the connotation frame lexicon, we com-
pute the distribution of targeted sentiments to-
wards most-frequently-mentioned named entities.
We also compute sentiments expressed by each
</p>
<p>460</p>
<p />
</div>
<div class="page"><p />
<p>Brussels attacks
+aftermath
</p>
<p>News story about 
Clinton “killing” minesArrest of 
</p>
<p>terror 
suspect 
</p>
<p>Abdeslam
</p>
<p>terror suspect accepts 
arrest and extradition
</p>
<p>most negative 
about terror 
</p>
<p>suspect
</p>
<p>Stories in English tweets from 
Russia about how Apple had 
“assaulted” FBI by refusing to 
</p>
<p>help in their investigations
</p>
<p>Similar to the English tweets 
from Russia, the Russian 
</p>
<p>tweets about Apple are also 
less positive than towards 
most of the other entities.
</p>
<p>+
</p>
<p>-
</p>
<p>-
</p>
<p>-
</p>
<p>-
</p>
<p>-
+
</p>
<p>Figure 3: Heatmap of expected perspectives towards 13 named entities over 2 week period using only English tweets from
European countries. Red is more positive, blue is more negative.
</p>
<p>country by aggregating all sentiments of the writ-
ers located in that country (e.g., the distribution
of positive, neutral, and negative perspectives ex-
pressed towards Obama in British tweets). The
aggregated polarities can be represented as a 3-
dimensional probability vector, p = [p+p=p−], as
will be used in the sentiment forecast task below.
For other analysis, we summarize this polarity dis-
tribution as a scalar score by taking the expected
value of the polarity: E[p] = p+ − p−.
</p>
<p>3.3 Forecasting Sentiment Dynamics
We also study forecasting sentiment dynamics:
predicting the sentiment distribution of the next
day given the sentiment trend of previous days.
For this task, we track the distribution of direc-
tional sentiments from each country towards the
hundred most-frequently-mentioned named enti-
ties. At test-time, each model is given directional
sentiment distributions for the previous 4 days as
input and predicts tomorrow’s distribution (e.g.,
forecasting 1 day ahead). We also train models for
predicting the distribution half a week later (fore-
casting 4 days ahead).
</p>
<p>We performed an additional experiment for En-
glish (ENJ ) where the perspectives of all coun-
tries are aggregated together in order to predict the
global perspective. For all experiments, we use
10-fold cross-validation and measure the symmet-
ric Kullback-Leibler (KL) divergence between the
true distribution and the predicted one.
</p>
<p>We experiment with Long Short-term Memory
</p>
<p>models (LSTMs) (Hochreiter and Schmidhuber,
1997) to integrate the dynamic contextual infor-
mation from the past, as depicted in 2. Hidden
dimension is 16, and we use ADAM optimization
with KL divergence as the objective. For imple-
mentation, we use Keras4 on top of Theano.5
</p>
<p>Baselines We use two baselines. The first is
MEAN, the average distribution seen in the train-
ing data. The second are SVMs with linear ker-
nels, which worked well in predicting influenza
activity in a similar set-up (Santillana et al., 2015).
For the baselines, we encode the distributions from
the 4 previous days as a flattened 12-dim. vector,
and each portion of the distribution is predicted
separately.
</p>
<p>4 Results
</p>
<p>4.1 Connotation Analysis
For the most frequently mentioned named entities,
we compute heatmaps of the expected perspective
being expressed towards that entity.
</p>
<p>In Figure 3A, we use the English tweets from
European countries to plot the change in connota-
tive polarity towards these entities over the course
of the 15 day period. Generally, the changes in
polarity from day-to-day seem to be gradual and
frequently are similar to the day before. There are
a couple of exceptions e.g., the polarity towards
</p>
<p>4https://keras.io
5https://deeplearning.net/software/
</p>
<p>theano
</p>
<p>461</p>
<p />
</div>
<div class="page"><p />
<p>Brussels changes abruptly on March 22 (the day
of the Brussels attacks), reflecting the change in
tone of all tweets related to Brussels at that time.
</p>
<p>Overall, there are mostly positive polarities ex-
pressed. This may reflect people’s tendency to
avoid phrasing stories too harshly, choosing to
be more euphemistic even when they discuss bad
news.
</p>
<p>In Figure 3B, we aggregated the polarities of
these tweets by country of origin. While most
of the polarities are positive-strongly positive, the
tweets about Brussels and Belgium are more neu-
tral or even slightly negative.
</p>
<p>Lastly, in Figure 3C, we used all of the tweets
from European countries to aggregate expected
polarities in 11 different languages. Non-English
languages show a much higher tendency towards
positive scores, particularly the languages with
less tweets (Polish, Finnish, Swedish).
</p>
<p>en es ru f
r
</p>
<p>de p
t it nl pl fi sv
</p>
<p>Mar 29
Mar 28
Mar 27
Mar 26
Mar 25
Mar 24
Mar 23
Mar 22
Mar 21
Mar 20
Mar 19
Mar 18
Mar 17
Mar 16
Mar 15
</p>
<p>0.8
</p>
<p>0.4
</p>
<p>0.0
</p>
<p>0.4
</p>
<p>0.8
</p>
<p>Figure 4: Heatmap of perspectives towards Obama over time
in 11 different languages.
</p>
<p>As a more detailed analysis, Figure 4 shows
a heatmap of how the connotation expressed to-
wards Obama shifts over time across different lan-
guages. Obama was not discussed much in Finnish
or Swedish, whereas he was discussed everyday
in English, Spanish, and Russian. In the middle
of the two week period, the perspective towards
Obama drops slightly, most notably in Spanish,
which overlaps with his controversial trip to Cuba
(March 20 – 22).
</p>
<p>4.2 Sentiment Dynamics
</p>
<p>In Table 2, we summarize the results of our ex-
periments for predicting targetted sentiment dy-
namics. For each language, we report the average
Kullback-Leibler divergence for the baselines and
the LSTM model (higher scores are worse). We
show prediction results in two settings: predicting
</p>
<p>Forecast + 1 day Forecast + 4 days
</p>
<p>Data Mean SVM LSTM Mean SVM LSTM
ENJ 1.96 1.76 1.67 2.03 2.14 2.02
</p>
<p>EN 4.88 1.94 1.79 5.12 3.91 3.91
RU 4.27 1.72 1.34 4.50 3.11 2.74
FR 2.23 1.80 1.76 4.33 3.24 3.12
ES 3.57 2.02 1.82 3.74 3.22 2.98
DE 4.43 2.24 1.77 4.73 3.61 3.55
NL 3.69 2.62 2.05 3.83 3.71 3.41
PT 3.84 1.97 2.04 3.94 3.03 3.11
IT 4.56 2.08 1.97 4.67 3.48 3.28
PL 4.01 1.67 1.46 4.06 3.37 3.32
SV 4.01 1.92 0.76 4.17 2.70 2.23
FI 4.90 2.04 1.84 4.99 4.14 4.20
</p>
<p>Table 2: Average Kullback-Leibler divergence of output of
the LSTM in predicting the distribution of writers’ perspec-
tives per entity for 11 different languages. In the first row, the
perspectives from all countries are aggregated together.
</p>
<p>the distribution one day ahead vs. four days ahead.
The LSTM outperforms the baselines in most
</p>
<p>languages with a few exceptions, such as Por-
tuguese. All models perform worse at forecasting
4 days into the future than one day ahead, demon-
strating how much connotation can vary over time
as news events change, even in a small time period.
On average, the LSTM achieves KL-divergence
of 1.7 when predicting one day ahead and 3.26
when predicting 4 days ahead, lower than any of
the baselines.
</p>
<p>4.3 Error Analysis
For error analysis, we removed entities from Fig-
ure 3 from the training data and used them as a
small test set for an LSTM trained on the remain-
ing data in English with aggregation over all coun-
tries. In Figure 5, we have plotted the predicted
marginal probabilities of four entities with the pos-
itive portion of the distribution (blue line) on the
top half of the y-axis and the negative portion (red
line) flipped onto the negative half of the axis.
</p>
<p>The LSTM follows the general shape of the true
curves, but frequently misses sudden spikes (e.g.,
the spike in negative polarity towards Russia on
March 27th). In Table 3, we also report the KL
divergences on predictions towards these entities.
The model tends to perform less well at predicting
sentiment towards entities where there were sud-
den spikes in sentiment based on news stories.
</p>
<p>5 Related Work
</p>
<p>There have been substantial studies for senti-
ment analysis on twitter (Agarwal et al., 2011;
Kouloumpis et al., 2011; Pak and Paroubek, 2010;
</p>
<p>462</p>
<p />
</div>
<div class="page"><p />
<p>Figure 5: True versus predicted polarity distributions over time on three specific entities (TP: True Positive, PP: Predicted
Positive, TN: True Negative, PN: Predicted Negative).
</p>
<p>Entity KL Entity KL Entity KL
</p>
<p>Trump 0.12 England 0.14 EU 0.22
Obama 0.25 Turkey 0.30 Google 0.32
apple 0.37 Russia 0.76 Belgium 0.95
Clinton 1.13 Brussels 1.14 Abdeslam 1.79
</p>
<p>Table 3: Error analysis on held-out entities.
</p>
<p>Liu and Zhang, 2012), as well as targetted senti-
ment (Deng and Wiebe, 2015), implicit sentiment
(Deng and Wiebe, 2014; Feng et al., 2013; Greene
and Resnik, 2009) and specific aspects of sub-
jective language (Mohammad and Turney, 2010;
Choi and Wiebe, 2014) in other domains. Pre-
vious investigations include using targetted senti-
ment to predict international relations (Chambers
et al., 2015), analyzing stylistic elements to pre-
dict tweet popularity (Tan et al., 2014), and ex-
ploring the re-phrasing of social media posts ref-
erencing specific news articles (Tan et al., 2016).
Compared to most prior studies that focused on
overt sentiment in English-only tweets, our work
aims to study targeted implied sentiments across
temporal, spatial, and linguistic borders.
</p>
<p>Some work (Tsytsarau et al., 2014; O’Connor
et al., 2010; De et al., 2016) has analyzed the tran-
sition of overt sentiment over a period of time and
related the shifts in sentiment to news events. A
body of work has also used predictive signals in
Twitter to track and sense upcoming unrest and
protests in specific countries (Ramakrishnan et al.,
2014; Goode et al., 2015), and the future progres-
sion of flu activity based on multiple text sources
(Santillana et al., 2015). In contrast, we focus on
</p>
<p>predicting the sentiment dynamics in social media
based on previous trends.
</p>
<p>6 Conclusions
</p>
<p>When reporting news, people write with their own
implicit and explicit biases and judgments. An au-
thor’s choice of language reveals connotations to-
wards entities, which can be captured within the
connotation frames that we have extended to 10
European languages.
</p>
<p>This work is one of the first to present a large
scale analysis of multilingual connotation dynam-
ics, and helps explore multiple perspectives on di-
verse issues across languages, time and countries
– a critical piece in understanding journalistic por-
trayal and biases.
</p>
<p>7 Acknowledgments
</p>
<p>This research was conducted at Pacific Northwest
National Laboratory, a multiprogram national lab-
oratory operated by Battelle for the U.S. We would
like to thank Josh Harrison, Jill Schroeder and
Justin Day for their contribution. We would also
like to thank anonymous reviewers for providing
insightful feedback. This material is based upon
work supported by the National Science Founda-
tion Graduate Research Fellowship Program under
Grant No. DGE-1256082, in part by NSF grants
IIS-1408287, IIS-1524371, and gifts by Google
and Facebook.
</p>
<p>463</p>
<p />
</div>
<div class="page"><p />
<p>References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
</p>
<p>bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the work-
shop on languages in social media. ACL, pages 30–
38.
</p>
<p>Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of ACL.
</p>
<p>Farzindar Atefeh and Wael Khreich. 2015. A survey of
techniques for event detection in twitter. Computa-
tional Intelligence 31(1):132–164.
</p>
<p>Nathanael Chambers, Victor Bowen, Ethan Genco,
Xisen Tian, Eric Young, Ganesh Harihara, and Eu-
gene Yang. 2015. Identifying political sentiment be-
tween nation states with social media. In Proceed-
ings of EMNLP. pages 65–75.
</p>
<p>Yoonjung Choi and Janyce Wiebe. 2014. +/-
effectwordnet: Sense-level lexicon acquisition for
opinion inference. In EMNLP. pages 1181–1191.
</p>
<p>Abir De, Isabel Valera, Niloy Ganguly, Sourangshu
Bhattacharya, and Manuel Gomez Rodriguez. 2016.
Learning and forecasting opinion dynamics in social
networks. In Proceedings of NIPS.
</p>
<p>Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In EACL.
pages 377–385.
</p>
<p>Lingjia Deng and Janyce Wiebe. 2015. Mpqa 3.0:
An entity/event-level sentiment corpus. In HLT-
NAACL. pages 1323–1328.
</p>
<p>Andreas Eisele and Yu Chen. 2010. Multiun: A mul-
tilingual corpus from united nation documents. In
Proceedings of LREC. pages 2868–2872.
</p>
<p>Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In ACL.
pages 1774–1784.
</p>
<p>Brian J Goode, Siddharth Krishnan, Michael Roan, and
Naren Ramakrishnan. 2015. Pricing a protest: Fore-
casting the dynamics of civil unrest activity in social
media. PloS one 10(10):e0139911.
</p>
<p>Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of NAACL-HLT . ACL, pages 503–
511.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit. vol-
ume 5, pages 79–86.
</p>
<p>Efthymios Kouloumpis, Theresa Wilson, and Jo-
hanna D Moore. 2011. Twitter sentiment analysis:
The good the bad and the omg! In Proceedings of
ICWSM. AAAI.
</p>
<p>Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining text data,
Springer, pages 415–463.
</p>
<p>Saif M Mohammad and Peter D Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon.
In Proceedings of the NAACL HLT 2010 workshop
on computational approaches to analysis and gen-
eration of emotion in text. Association for Computa-
tional Linguistics, pages 26–34.
</p>
<p>Brendan O’Connor, Ramnath Balasubramanyan, Bryan
Routledge, and Noah Smith. 2010. From tweets to
polls: Linking text sentiment to public opinion time
series. In Proceedings of ICWSM. 122-129, pages
1–2.
</p>
<p>Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of LREC. volume 10, pages 1320–
1326.
</p>
<p>Kira Radinsky and Eric Horvitz. 2013. Mining the web
to predict future events. In Proceedings of WSDM.
ACM, pages 255–264.
</p>
<p>Naren Ramakrishnan, Patrick Butler, Sathappan
Muthiah, Nathan Self, Rupinder Khandpur, Parang
Saraf, Wei Wang, Jose Cadena, Anil Vullikanti,
Gizem Korkmaz, et al. 2014. ’beating the news’
with embers: forecasting civil unrest using open
source indicators. In Proceedings of KDD. ACM,
pages 1799–1808.
</p>
<p>Hannah Rashkin, Sameer Singh, and Yejin Choi. 2016.
Connotation frames: A data-driven investigation. In
Proceedings of ACL.
</p>
<p>Mauricio Santillana, André T Nguyen, Mark Dredze,
Michael J Paul, Elaine O Nsoesie, and John S
Brownstein. 2015. Combining search, social me-
dia, and traditional data sources to improve influenza
surveillance. PLoS Comput Biol 11(10):e1004513.
</p>
<p>Chenhao Tan, Adrien Friggeri, Menlo Park, and Menlo
Park. 2016. Lost in Propagation? Unfolding News
Cycles from the Source. In Proceedings of AAAI.
</p>
<p>Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The
effect of wording on message propagation: Topic-
and author-controlled. In Proceedings of ACL.
</p>
<p>Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In LREC. pages 2214–2218.
</p>
<p>Mikalai Tsytsarau, Themis Palpanas, and Malu Castel-
lanos. 2014. Dynamics of news events and social
media reaction. In Proceedings of KDD. ACM,
pages 901–910.
</p>
<p>464</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 465–470
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2074
</p>
<p>Best–Worst Scaling More Reliable than Rating Scales:
A Case Study on Sentiment Intensity Annotation
</p>
<p>Svetlana Kiritchenko and Saif M. Mohammad
National Research Council Canada
</p>
<p>{svetlana.kiritchenko,saif.mohammad}@nrc-cnrc.gc.ca
</p>
<p>Abstract
</p>
<p>Rating scales are a widely used method
for data annotation; however, they present
several challenges, such as difficulty in
maintaining inter- and intra-annotator con-
sistency. Best–worst scaling (BWS) is
an alternative method of annotation that
is claimed to produce high-quality anno-
tations while keeping the required num-
ber of annotations similar to that of rat-
ing scales. However, the veracity of this
claim has never been systematically estab-
lished. Here for the first time, we set up an
experiment that directly compares the rat-
ing scale method with BWS. We show that
with the same total number of annotations,
BWS produces significantly more reliable
results than the rating scale.
</p>
<p>1 Introduction
</p>
<p>When manually annotating data with quantitative
or qualitative information, researchers in many
disciplines, including social sciences and com-
putational linguistics, often rely on rating scales
(RS). A rating scale provides the annotator with a
choice of categorical or numerical values that rep-
resent the measurable characteristic of the rated
data. For example, when annotating a word for
sentiment, the annotator can be asked to choose
among integer values from 1 to 9, with 1 represent-
ing the strongest negative sentiment, and 9 repre-
senting the strongest positive sentiment (Bradley
and Lang, 1999; Warriner et al., 2013). An-
other example is the Likert scale, which measures
responses on a symmetric agree–disagree scale,
from ‘strongly disagree’ to ‘strongly agree’ (Lik-
ert, 1932). The annotations for an item from mul-
tiple respondents are usually averaged to obtain a
real-valued score for that item. Thus, for an N -
</p>
<p>item set, if each item is to be annotated by five
respondents, then the number of annotations re-
quired is 5N .
</p>
<p>While frequently used in many disciplines, the
rating scale method has a number of limitations
(Presser and Schuman, 1996; Baumgartner and
Steenkamp, 2001). These include:
• Inconsistencies in annotations by different
</p>
<p>annotators: one annotator might assign a
score of 7 to the word good on a 1-to-9 sen-
timent scale, while another annotator can as-
sign a score of 8 to the same word.
• Inconsistencies in annotations by the same
</p>
<p>annotator: an annotator might assign differ-
ent scores to the same item when the annota-
tions are spread over time.
• Scale region bias: annotators often have a
</p>
<p>bias towards a part of the scale, for example,
preference for the middle of the scale.
• Fixed granularity: in some cases, annota-
</p>
<p>tors might feel too restricted with a given rat-
ing scale and may want to place an item in-
between the two points on the scale. On the
other hand, a fine-grained scale may over-
whelm the respondents and lead to even more
inconsistencies in annotation.
</p>
<p>Paired Comparisons (Thurstone, 1927; David,
1963) is a comparative annotation method, where
respondents are presented with pairs of items and
asked which item has more of the property of in-
terest (for example, which is more positive). The
annotations can then be converted into a ranking
of items by the property of interest, and one can
even obtain real-valued scores indicating the de-
gree to which an item is associated with the prop-
erty of interest. The paired comparison method
does not suffer from the problems discussed above
for the rating scale, but it requires a large number
of annotations—orderN2, whereN is the number
of items to be annotated.
</p>
<p>465</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2074">https://doi.org/10.18653/v1/P17-2074</a></div>
</div>
<div class="page"><p />
<p>Best–Worst Scaling (BWS) is a less-known, and
more recently introduced, variant of compara-
tive annotation. It was developed by Louviere
(1991), building on some groundbreaking research
in the 1960s in mathematical psychology and psy-
chophysics by Anthony A. J. Marley and Duncan
Luce. Annotators are presented with n items at a
time (an n-tuple, where n &gt; 1, and typically n =
4). They are asked which item is the best (highest
in terms of the property of interest) and which is
the worst (lowest in terms of the property of in-
terest). When working on 4-tuples, best–worst an-
notations are particularly efficient because by an-
swering these two questions, the results for five out
of six item–item pair-wise comparisons become
known. All items to be rated are organized in a
set of m 4-tuples (m ≥ N , where N is the num-
ber of items) so that each item is evaluated several
times in diverse 4-tuples. Once the m 4-tuples are
annotated, one can compute real-valued scores for
each of the items using a simple counting proce-
dure (Orme, 2009). The scores can be used to rank
items by the property of interest.
</p>
<p>BWS is claimed to produce high-quality anno-
tations while still keeping the number of anno-
tations small (1.5N–2N tuples need to be anno-
tated) (Louviere et al., 2015; Kiritchenko and Mo-
hammad, 2016a). However, the veracity of this
claim has never been systematically established.
In this paper, we pit the widely used rating scale
squarely against BWS in a quantitative experiment
to determine which method provides more reliable
results. We produce real-valued sentiment inten-
sity ratings for 3,207 English terms (words and
phrases) using both methods by aggregating re-
sponses from several independent annotators. We
show that BWS ranks terms more reliably, that
is, when comparing the term rankings obtained
from two groups of annotators for the same set
of terms, the correlation between the two sets of
ranks produced by BWS is significantly higher
than the correlation for the two sets obtained with
RS. The difference in reliability is more marked
when about 5N (or less) total annotations are ob-
tained, which is the case in many NLP annotation
projects (Strapparava and Mihalcea, 2007; Socher
et al., 2013; Mohammad and Turney, 2013). Fur-
thermore, the reliability obtained by rating scale
when using ten annotations per term is matched
by BWS with only 3N total annotations (two an-
notations for each of the 1.5N 4-tuples).
</p>
<p>The sparse prior work in natural language
annotations that uses BWS involves the cre-
ation of datasets for relational similarity (Jurgens
et al., 2012), word-sense disambiguation (Jurgens,
2013), and word–sentiment intensity (Kiritchenko
and Mohammad, 2016a). However, none of these
works has systematically compared BWS with the
rating scale method. We hope that our findings
will encourage the use of BWS more widely to
obtain high-quality NLP annotations. All data
from our experiments as well as scripts to generate
BWS tuples, to generate item scores from BWS
annotations, and for assessing reliability of the an-
notations are made freely available.1
</p>
<p>2 Complexities of Comparative Evaluation
</p>
<p>Both rating scale and BWS are less than perfect
ways to capture the true word–sentiment intensi-
ties in the minds of native speakers of a language.
Since the “true” intensities are not known, deter-
mining which approach is better is non-trivial.2
</p>
<p>A useful measure of quality is reproducibility—
if repeated independent manual annotations from
multiple respondents result in similar sentiment
scores, then one can be confident that the scores
capture the true sentiment intensities. Thus, we
set up an experiment that compares BWS and RS
in terms of how similar the results are on repeated
independent annotations.
</p>
<p>It is expected that reproducibility improves with
the number of annotations for both methods. (Es-
timating a value often stabilizes as the sample size
is increased.) However, in rating scale annota-
tion, each item is annotated individually whereas
in BWS, groups of four items (4-tuples) are anno-
tated together (and each item is present in multi-
ple different 4-tuples). To make the reproducibil-
ity evaluation fair, we ensure that the term scores
are inferred from the same total number of anno-
tations for both methods. For an N -item set, let
krs be the number of times each item is annotated
via a rating scale. Then the total number of rating
scale annotations is krsN . For BWS, let the same
N -item set be converted into m 4-tuples that are
each annotated kbws times. Then the total number
of BWS annotations is kbwsm. In our experiments,
we compare results across BWS and rating scale at
points when krsN = kbwsm.
</p>
<p>1www.saifmohammad.com/WebPages/BestWorst.html
2Existing sentiment lexicons are a result of one or the
</p>
<p>other method and so cannot be treated as the truth.
</p>
<p>466</p>
<p />
</div>
<div class="page"><p />
<p>The cognitive complexity involved in answer-
ing a BWS question is different from that in a rat-
ing scale question. On the one hand, for BWS,
the respondent has to consider four items at a
time simultaneously. On the other hand, even
though a rating scale question explicitly involves
only one item, the respondent must choose a score
that places it appropriately with respect to other
items.3 Quantifying the degree of cognitive load
of a BWS annotation vs. a rating scale annotation
(especially in a crowdsourcing setting) is partic-
ularly challenging, and beyond the scope of this
paper. Here we explore the extent to which the
rating scale method and BWS lead to the same re-
sulting scores when the annotations are repeated,
controlling for the total number of annotations.
</p>
<p>3 Annotating for Sentiment
</p>
<p>We annotated 3,207 terms for sentiment inten-
sity (or degree of positive or negative valence)
with both the rating scale and best–worst scaling.
The annotations were done by crowdsourcing on
CrowdFlower.4 The workers were required to be
native English speakers from the USA.
</p>
<p>3.1 Terms
The term list includes 1,621 positive and negative
single words from Osgood’s valence subset of the
General Inquirer (Stone et al., 1966). It also in-
cluded 1,586 high-frequency short phrases formed
by these words in combination with simple nega-
tors (e.g., no, don’t, and never), modals (e.g., can,
might, and should), or degree adverbs (e.g., very
and fairly). More details on the term selection can
be found in (Kiritchenko and Mohammad, 2016b).
</p>
<p>3.2 Annotating with Rating Scale
The annotators were asked to rate each term on a
9-point scale, ranging from −4 (extremely nega-
tive) to 4 (extremely positive). The middle point
(0) was marked as ‘not at all positive or nega-
tive’. Example words were provided for the two
extremes (−4 and 4) and the middle (0) to give the
annotators a sense of the whole scale.
</p>
<p>Each term was annotated by twenty workers for
the total number of annotations to be 20N (N =
</p>
<p>3A somewhat straightforward example is that good cannot
be given a sentiment score less than what was given to okay,
and it cannot be given a score greater than that given to great.
Often, more complex comparisons need to be considered.
</p>
<p>4The full set of annotations as well as the instruc-
tions to annotators for both methods are available at
http://www.saifmohammad.com/WebPages/BestWorst.html.
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>0 20 40 60 80
</p>
<p>%
 i
</p>
<p>n
c
</p>
<p>o
n
</p>
<p>s
is
</p>
<p>te
n
</p>
<p>c
ie
</p>
<p>s
 
</p>
<p>time span (hours) 
</p>
<p>all terms single words phrases
</p>
<p>Figure 1: The inconsistency rate in repeated anno-
tations by same workers using rating scale.
</p>
<p>3, 207 is the number of terms). A small portion
(5%) of terms were internally annotated by the au-
thors. If a worker’s accuracy on these check ques-
tions fell below 70%, that worker was refused fur-
ther annotation, and all of their responses were dis-
carded. The final score for each term was set to the
mean of all ratings collected for this term.5 On av-
erage, the ratings of a worker correlated well with
the mean ratings of the rest of the workers (average
Pearson’s r = 0.9, min r = 0.8). Also, the Pear-
son correlation between the obtained mean ratings
and the ratings from similar studies by Warriner
et al. (2013) and by Dodds et al. (2011) were 0.94
(on 1,420 common terms) and 0.96 (on 998 com-
mon terms), respectively.6
</p>
<p>To determine how consistent individual annota-
tors are over time, 180 terms (90 single words and
90 phrases) were presented for annotation twice
with intervals ranging from a few minutes to a few
days. For 37% of these instances, the annotations
for the same term by the same worker were differ-
ent. The average rating difference for these incon-
sistent annotations was 1.27 (on a scale from −4
to 4). Fig. 1 shows the inconsistency rate in these
repeated annotations as a function of time inter-
val between the two annotations. The inconsis-
tency rate is averaged over 12-hour periods. One
can observe that intra-annotator inconsistency in-
creases with the increase in time span between the
annotations. Single words tend to be annotated
with higher inconsistency than phrases. However,
when annotated inconsistently, phrases have larger
average difference between the scores (1.28 for
phrases vs. 1.21 for single words). Twelve out
of 90 phrases (13%) have the average difference
greater than or equal to 2 points. This shows that
it is difficult for annotators to remain consistent
when using the rating scale.
</p>
<p>5When evaluated as described in Sections 4 and 5, median
and mode produced worse results than mean.
</p>
<p>6 Warriner et al. (2013) list a correlation of 0.95 on 1029
common terms with the lexicon by Bradley and Lang (1999).
</p>
<p>467</p>
<p />
</div>
<div class="page"><p />
<p>3.3 Annotating with Best–Worst Scaling
The annotators were presented with four terms at
a time (a 4-tuple) and asked to select the most pos-
itive term and the most negative term. The same
quality control mechanism of assessing a worker’s
accuracy on internally annotated check questions
(discussed in the previous section) was employed
here as well. 2N (where N = 3, 207) distinct 4-
tuples were randomly generated in such a manner
that each term was seen in eight different 4-tuples,
and no term appeared more than once in a tuple.7
</p>
<p>Each 4-tuple was annotated by 10 workers. Thus,
the total number of annotations obtained for BWS
was 20N (just as in RS). We used the partial sets
of 1N , 1.5N , and the full set of 2N 4-tuples to
investigate the impact of the number of unique 4-
tuples on the quality of the final scores.
</p>
<p>We applied the counting procedure to ob-
tain real-valued term–sentiment scores from the
BWS annotations (Orme, 2009; Flynn and Marley,
2014): the term’s score was calculated as the per-
centage of times the term was chosen as most pos-
itive minus the percentage of times the term was
chosen as most negative. The scores range from
−1 (most negative) to 1 (most positive). This sim-
ple and efficient procedure has been shown to pro-
duce results similar to ones obtained with more so-
phisticated statistical models, such as multinomial
logistic regression (Louviere et al., 2015).
</p>
<p>In a separate study, we use the resulting dataset
of 3,207 words and phrases annotated with real-
valued sentiment intensity scores by BWS, which
we call Sentiment Composition Lexicon for Nega-
tors, Modals, and Degree Adverbs (SCL-NMA),
to analyze the effect of different modifiers on sen-
timent (Kiritchenko and Mohammad, 2016b).
</p>
<p>4 How different are the results obtained
by rating scale and BWS?
</p>
<p>The difference in final outcomes of BWS and RS
can be determined in two ways: by directly com-
paring term scores or by comparing term ranks.
To compare scores, we first linearly transform the
BWS and rating scale scores to scores in the range
0 to 1. Table 1 shows the differences in scores, dif-
ferences in rank, Spearman rank correlation ρ, and
Pearson correlation r for 3N , 5N , and 20N anno-
tations. Observe that the differences are markedly
larger for commonly used annotation scenarios
</p>
<p>7The script used to generate the 4-tuples is available at
http://www.saifmohammad.com/WebPages/BestWorst.html.
</p>
<p># annotations ∆ score ∆ rank ρ r
3N 0.11 397 0.85 0.85
5N 0.10 363 0.87 0.88
</p>
<p>20N 0.08 264 0.93 0.93
</p>
<p>Table 1: Differences in final outcomes of BWS
and RS, for different total numbers of annotations.
</p>
<p>Term set # terms ρ r
all terms 3,207 .93 .93
single words 1621 .94 .95
all phrases 1586 .92 .91
</p>
<p>negated phrases 444 .74 .79
pos. phrases that have a negator 83 -.05 -.05
neg. phrases that have a negator 326 .46 .46
</p>
<p>modal phrases 418 .75 .82
pos. phrases that have a modal 272 .44 .45
neg. phrases that have a modal 95 .57 .56
</p>
<p>adverb phrases 724 .91 .95
</p>
<p>Table 2: Correlations between sentiment scores
produced by BWS and rating scale.
</p>
<p>where only 3N or 5N total annotations are ob-
tained, but even with 20N annotations, the differ-
ences across RS and BWS are notable.
</p>
<p>Table 2 shows Spearman (ρ) and Pearson (r)
correlation between the ranks and scores produced
by RS and BWS on the full set of 20N annota-
tions. Notice that the scores agree more on single
terms and less so on phrases. The correlation is no-
ticeably lower for phrases involving negations and
modal verbs. Furthermore, the correlation drops
dramatically for positive phrases that have a nega-
tor (e.g., not hurt, nothing wrong).8 The anno-
tators also showed greater inconsistencies while
scoring these phrases on the rating scale (std. dev.
σ = 1.17 compared to σ = 0.81 for the full set).
Thus it seems that the outcomes of rating scale and
BWS diverge to a greater extent when the com-
plexity of the items to be rated increases.
</p>
<p>5 Annotation Reliability
</p>
<p>To assess the reliability of annotations produced
by a method (BWS or rating scale), we calculate
average split-half reliability (SHR) over 100 trials.
SHR is a commonly used approach to determine
consistency in psychological studies, that we em-
ploy as follows. All annotations for a term or a
tuple are randomly split into two halves. Two sets
</p>
<p>8A term was considered positive (negative) if the scores
obtained for the term with rating scale and BWS are both
greater than or equal to zero (less than zero). Some terms
were rated inconsistently by the two methods; therefore, the
number of the positive and negative terms for a category
(negated phrases and modal phrases) does not sum to the total
number of terms in the category.
</p>
<p>468</p>
<p />
</div>
<div class="page"><p />
<p>0.75
</p>
<p>0.8
</p>
<p>0.85
</p>
<p>0.9
</p>
<p>0.95
</p>
<p>1
</p>
<p>0 5000 10000 15000 20000 25000 30000
</p>
<p>ρ 
</p>
<p>number of annotations in each half-set  
</p>
<p>Rating Scale BWS, 1N BWS, 1.5N BWS, 2N
</p>
<p>RS half-sets: 4N (~13K) ann. vs. 4N (~13K) ann. 
</p>
<p>Each half-set includes 4 ann./term for N terms 
</p>
<p>BWS half-sets: 4N (~13K) ann. vs. 4N (~13K) ann. 
</p>
<p>Each half-set includes 2 ann./tuple for 2N tuples 
</p>
<p>Figure 2: SHR for RS and BWS (for N = 3207).
</p>
<p>of scores are produced independently from the two
halves. Then the correlation between the two sets
of scores is calculated. If a method is more reli-
able, then the correlation of the scores produced
by the two halves will be high. Fig. 2 shows
the Spearman rank correlation (ρ) for half-sets ob-
tained from rating scale and best–worst scaling
data as a function of the available annotations in
each half-set. It shows for each annotation set the
split-half reliability using the full set of annota-
tions (10N per half-set) as well as partial sets ob-
tained by choosing krs annotations per term for
rating scale (where krs ranges from 1 to 10) or
kbws annotations per 4-tuple for BWS (where kbws
ranges from 1 to 5). The graph also shows BWS
results obtained using 1N , 1.5N , and 2N unique
4-tuples. In each case, the x-coordinate repre-
sents the total number of annotations in each half-
set. Recall that the total number of annotations for
rating scale equals krsN , and for BWS it equals
kbwsm, where m is the number of 4-tuples. Thus,
for the case where m =2N , the two methods are
compared at points where krs =2kbws.
</p>
<p>There are two important observations we can
make from Fig. 2. First, we can conclude that
the reliability of the BWS annotations is very sim-
ilar on the sets of 1N , 1.5N , and 2N annotated
4-tuples as long as the total number of annotations
is the same. This means that in practice, in order
to improve annotation reliability, one can increase
either the number of unique 4-tuples to annotate
or the number of independent annotations for each
4-tuple. Second, annotations produced with BWS
are more reliable than annotations obtained with
rating scales. The difference in reliability is es-
pecially large when only a small number of an-
notations (≤ 5N ) are available. For the full set
of more than 64K annotations (10N = ∼32K in
</p>
<p>Term set # terms BWS RS
all terms 3,207 .98 .95
single words 1621 .98 .96
all phrases 1586 .98 .94
negated phrases 444 .91 .78
pos. phrases that have a negator 83 .79 .17
neg. phrases that have a negator 326 .81 .49
</p>
<p>modal phrases 418 .96 .80
pos. phrases that have a modal 272 .89 .53
neg. phrases that have a modal 95 .83 .63
</p>
<p>adverb phrases 724 .97 .92
</p>
<p>Table 3: Average SHR for BWS and rating scale
(RS) on different subsets of terms.
</p>
<p>each half-set) available for both methods, the av-
erage split-half reliability for BWS is ρ = 0.98
and for the rating scale method the reliability is
ρ = 0.95 (the difference is statistically significant,
p &lt; .001). One can obtain a reliability of ρ = 0.95
with BWS using just 3N (∼10K) annotations in a
half-set (30% of what is needed for rating scale).9
</p>
<p>Table 3 shows the split-half reliability (SHR)
on different subsets of terms. Observe that posi-
tive phrases that include a negator (the class that
diverged most across BWS and rating scale), is
also the class that has an extremely low SHR
when annotated by rating scale. The drop in SHR
for the same class when annotated with BWS is
much less. Similar pattern is observed for other
phrase classes as well, although to a lesser extent.
All of the results shown in this section, indicate
that BWS surpasses rating scales on the ability
to reliably rank items by sentiment, especially for
phrasal items that are linguistically more complex.
</p>
<p>6 Conclusions
</p>
<p>We presented an experiment that directly com-
pared the rating scale method of annotation with
best–worst scaling. We showed that, controlling
for the total number of annotations, BWS pro-
duced significantly more reliable results. The dif-
ference in reliability was more marked when about
5N (or less) total annotations for an N -item set
were obtained. BWS was also more reliable when
used to annotate linguistically complex items such
as phrases with negations and modals. We hope
that these findings will encourage the use of BWS
more widely to obtain high-quality annotations.
</p>
<p>Acknowledgments
</p>
<p>We thank Eric Joanis and Tara Small for discus-
sions on best–worst scaling and rating scales.
</p>
<p>9Similar trends are observed with Pearson’s coefficient
though the gap between BWS and RS results is smaller.
</p>
<p>469</p>
<p />
</div>
<div class="page"><p />
<p>References
Hans Baumgartner and Jan-Benedict E.M. Steenkamp.
</p>
<p>2001. Response styles in marketing research: A
cross-national investigation. Journal of Marketing
Research 38(2):143–156.
</p>
<p>Margaret M. Bradley and Peter J. Lang. 1999. Affec-
tive norms for English words (ANEW): Instruction
manual and affective ratings. Technical report, The
Center for Research in Psychophysiology, Univer-
sity of Florida.
</p>
<p>Herbert Aron David. 1963. The method of paired com-
parisons. Hafner Publishing Company, New York.
</p>
<p>Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M. Kloumann, Catherine A. Bliss, and Christo-
pher M. Danforth. 2011. Temporal patterns of
happiness and information in a global social net-
work: Hedonometrics and Twitter. PloS One
6(12):e26752.
</p>
<p>T. N. Flynn and A. A. J. Marley. 2014. Best-worst scal-
ing: theory and methods. In Stephane Hess and An-
drew Daly, editors, Handbook of Choice Modelling,
Edward Elgar Publishing, pages 178–201.
</p>
<p>David Jurgens. 2013. Embracing ambiguity: A com-
parison of annotation methodologies for crowd-
sourcing word sense labels. In Proceedings
of the Annual Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies (NAACL). Atlanta, Georgia, pages 556–562.
http://aclweb.org/anthology/N13-1062.
</p>
<p>David Jurgens, Saif M. Mohammad, Peter Turney,
and Keith Holyoak. 2012. SemEval-2012 Task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the International Workshop on Seman-
tic Evaluation (SemEval). Montréal, Canada, pages
356–364. http://www.aclweb.org/anthology/S12-
1047.
</p>
<p>Svetlana Kiritchenko and Saif M. Mohammad. 2016a.
Capturing reliable fine-grained sentiment associa-
tions by crowdsourcing and best–worst scaling. In
Proceedings of the Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL). San Diego, California, pages 811–817.
https://doi.org/10.18653/v1/N16-1095.
</p>
<p>Svetlana Kiritchenko and Saif M. Mohammad. 2016b.
The effect of negators, modals, and degree ad-
verbs on sentiment composition. In Proceed-
ings of the 7th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis. San Diego, California, pages 43–52.
http://www.aclweb.org/anthology/W16-0410.
</p>
<p>Rensis Likert. 1932. A technique for the measurement
of attitudes. Archives of psychology .
</p>
<p>Jordan J. Louviere. 1991. Best-worst scaling: A model
for the largest difference judgments. Working Paper.
</p>
<p>Jordan J. Louviere, Terry N. Flynn, and A. A. J. Marley.
2015. Best–Worst Scaling: Theory, Methods and
Applications. Cambridge University Press.
</p>
<p>Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word–emotion association lexicon.
Computational Intelligence 29(3):436–465.
</p>
<p>Bryan Orme. 2009. Maxdiff analysis: Simple count-
ing, individual-level logit, and HB. Sawtooth Soft-
ware, Inc.
</p>
<p>Stanley Presser and Howard Schuman. 1996. Ques-
tions and Answers in Attitude Surveys: Experiments
on Question Form, Wording, and Context. SAGE
Publications, Inc.
</p>
<p>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Seattle, USA.
</p>
<p>Philip Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and associates. 1966. The Gen-
eral Inquirer: A Computer Approach to Content
Analysis. The MIT Press.
</p>
<p>Carlo Strapparava and Rada Mihalcea. 2007.
Semeval-2007 task 14: Affective text. In
Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-
2007). Association for Computational Linguis-
tics, Prague, Czech Republic, pages 70–74.
http://www.aclweb.org/anthology/S/S07/S07-1013.
</p>
<p>Louis L. Thurstone. 1927. A law of comparative judg-
ment. Psychological review 34(4):273.
</p>
<p>Amy Beth Warriner, Victor Kuperman, and Marc Brys-
baert. 2013. Norms of valence, arousal, and dom-
inance for 13,915 English lemmas. Behavior Re-
search Methods 45(4):1191–1207.
</p>
<p>470</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 471–477
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2075
</p>
<p>Demographic Inference on Twitter using Recursive Neural Networks
</p>
<p>Sunghwan Mac Kim, Qiongkai Xu, Lizhen Qu, Stephen Wan and Cécile Paris
Data61, CSIRO, Australia
</p>
<p>{Mac.Kim, Qiongkai.Xu, Lizhen.Qu, Stephen.Wan, Cecile.Paris}@data61.csiro.au
</p>
<p>Abstract
</p>
<p>In social media, demographic inference is
a critical task in order to gain a better un-
derstanding of a cohort and to facilitate in-
teracting with one’s audience. Most pre-
vious work has made independence as-
sumptions over topological, textual and la-
bel information on social networks. In
this work, we employ recursive neural net-
works to break down these independence
assumptions to obtain inference about de-
mographic characteristics on Twitter. We
show that our model performs better than
existing models including the state-of-the-
art.
</p>
<p>1 Introduction
</p>
<p>Social media is a popular public platform for com-
municating, sharing information and expressing
opinions. Millions of users discuss a variety of
topics such as politics or sports. Valuable insights
can be obtained by analysing social media content
(e.g., mining consumer preferences), and, conse-
quently, social media data is now a valuable re-
source. Accordingly, social media analytics have
received much attention among researchers and
companies (Wan and Paris, 2014; Valdes et al.,
2015; Zubiaga et al., 2016).
</p>
<p>Inferring demographic characteristics from so-
cial media is a useful mechanism to gain a bet-
ter understanding of a cohort and one’s audi-
ence, and to facilitate interacting with that audi-
ence. Many researchers have studied ways to in-
fer demographic attributes of Twitter users, such
as age (Mislove et al., 2011; Mohammady Arde-
haly and Culotta, 2015), gender (Filippova, 2012;
Taniguchi et al., 2015), occupation (Preoţiuc-
Pietro et al., 2015; Kim et al., 2016), location (Jur-
gens et al., 2015; Jayasinghe et al., 2016) or politi-
</p>
<p>cal preferences (Volkova et al., 2014; McCormick
et al., 2015).
</p>
<p>A common approach to infer demographic char-
acteristics is the use of supervised classifiers
trained on textual features. The main limitation
of this approach is that it makes little use of the
network topology. Several network embedding
methods have been proposed to learn distributed
dense representations for vertices in graphs: Deep-
Walk (Perozzi et al., 2014) or LINE (Tang et al.,
2015). While these two models can capture the
topological structure of social networks, their per-
formances are still limited, as the text features
associated with vertices are not considered. For
instance, text messages that Twitter users post,
tweets, can offer great potential to enhance the ver-
tex embeddings. Yang et al. (2015) proposed the
Text-Associated DeepWalk (TADW) to enhance
the discriminative power of the vertex embeddings
by incorporating the text information into the em-
bedding generation process. Although this matrix
factorisation framework is effective on the vertex
classification task, it can produce suboptimal em-
beddings. This is because the label information
is not exploited in the unsupervised framework.
More recently, Pan et al. (2016) proposed the Tri-
Party Deep Network Representation (TriDNR),
a method that incorporates the label information
in addition to the text and topological informa-
tion. However, in TriDNR, the text in a vertex
is assumed to be independent of neighbour ver-
tices. Furthermore, the two optimisation problems
(learning vertex embeddings and training discrim-
inative classifiers) are tackled separately. This is
also true of TADW.
</p>
<p>In this paper, we tackle the problem of infer-
ring demographic characteristics from social net-
works as a vertex classification task on graphs. We
employ recursive neural networks (RNNs) to in-
fer three demographic attributes of Twitter users
</p>
<p>471</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2075">https://doi.org/10.18653/v1/P17-2075</a></div>
</div>
<div class="page"><p />
<p>1
4
</p>
<p>2
</p>
<p>6
</p>
<p>3
</p>
<p>5
</p>
<p>(a) Graph
</p>
<p>1
</p>
<p>4
</p>
<p>2
</p>
<p>6
</p>
<p>3
</p>
<p>5
</p>
<p>(b) Tree
</p>
<p>RNU1
</p>
<p>RNU4
</p>
<p>RNU2
</p>
<p>RNU6
</p>
<p>RNU3
</p>
<p>RNU5
</p>
<p>softmax
</p>
<p>x4
</p>
<p>x1
</p>
<p>h1
</p>
<p>x5x6
</p>
<p>h6
h4 h5
</p>
<p>h3h2
</p>
<p>x2 x3
</p>
<p>(c) GRNN
</p>
<p>Figure 1: An illustration of GRNN construction steps: (a) The graph consisting of six vertices; (b) The
tree converted from the graph using breadth-first search; (c) The GRNN constructed from the tree. The
target vertex is marked in red.
</p>
<p>(age, gender and user type) based on network
topology, text content and label information. Our
model breaks down the independence assumption
by leveraging RNNs on paths in graphs. We show
that our model achieves better performance com-
pared to existing models including the state-of-
the-art. While high performance is achieved using
solely neural network models, more importantly,
we find that different demographic inference tasks
benefit to varying the topological size of RNNs.
To our knowledge, there has been little previous
work applying neural network based methods to
the problem of inferring social media demograph-
ics.
</p>
<p>2 Graph Recursive Neural Networks
</p>
<p>RNNs are deep learning models that recursively
compose the vector of a parent unit from those of
child units over a given structure in topological or-
der (Pollack, 1990). They have shown to be very
effective for various natural language process-
ing (NLP) tasks, capturing syntactic and seman-
tic composition (Socher et al., 2011; Qian et al.,
2015). In this section, we describe the framework
of Graph RNNs (GRNNs) (Xu et al., 2017) to clas-
sify the vertices of a graph. This framework allows
us to infer the demographic characteristics of so-
cial media users.
</p>
<p>We formally define the problem of Twitter ver-
tex classification as follows: A Twitter social net-
work is defined as G = (V,E), where V is the
set of vertices (users) and E is the set of edges
(relationships) between the vertices. Each edge
e ∈ E is an ordered pair e = (vi, vj), where
vi, vj ∈ V , that is unweighted1 (wij = 0) but di-
</p>
<p>1While the edges in the Twitter social network can be
weighted (e.g., based on the importance of relationship), we
only take into account an unweighted social network in this
study.
</p>
<p>rected ((i, j) 6≡ (j, i)). Each vi is associated with
a pair of (xi, yi), where xi ∈ X is a feature vector
and yi ∈ Y is a particular label that depends on xi.
X and Y thus denote a set of feature vectors and a
set of possible predicted labels in G, respectively.
Our goal is then to predict the most likely label
ŷt ∈ Y for vt ∈ V , which is the target vertex to
be classified: ŷt = argmaxyt∈Y Pθ(yt|vt, G,X )
using a RNN with parameters θ.
</p>
<p>GRNNs contain five components that will be
presented in this section: 1) Graph-to-Tree Con-
version, 2) Word Embedding layer, 3) Recursive
Neural Unit layer and 4) Softmax Output layer, as
illustrated in Figure 1.
</p>
<p>2.1 Graph-to-Tree Conversion
A graph is converted to tree structures before con-
structing a RNN for each tree. Specifically, we
construct a tree T = (VT , ET ) of depth d rooted
at vt using a breadth-first search algorithm from
G. VT and ET are the sets of vertices and edges
in the tree. (v, w) ∈ ET denotes an edge from a
parent vertex v to a child vertex w.
</p>
<p>2.2 Word Embeddings
Let Si = {w1, w2, ..., wR} be texts (e.g., tweets)
consisting of R words, which are associated with
a vertex vi. Every word wr is converted into a
real-valued vector er by looking up the embed-
ding matrix E ∈ Rdw|V |, where dw is the size
of word embedding and |V | is a vocabulary size.
The matrix E is initialised using the Skip-gram
model (Mikolov et al., 2013). Si is then fed into
the next layer as a real-valued feature vector xi =
{e1, e2, ..., eR}.
2.3 Recursive Neural Units
Once we construct a tree from a graph, we build
a RNN using one of two types of recursive neu-
ral units (RNUs) for each vertex vk ∈ T : Naive
</p>
<p>472</p>
<p />
</div>
<div class="page"><p />
<p>Recursive Neural Unit (NRNU) and Long Short-
Term Memory Unit (LSTMU).
</p>
<p>2.3.1 Graph Naive Recursive Neural Net
Each NRNU for a vertex vk take its feature vector
xk and a hidden state h̃k as input. Max pooling
produces h̃k from all hidden state vectors hr of the
child vertices vr of vk.2 A hidden state vector hk
of vk is obtained using weight matrices, followed
by a non-linear function tanh:
</p>
<p>h̃k = max
vr∈C(vk)
</p>
<p>{hr}
</p>
<p>hk = tanh(W
(h)xk + U
</p>
<p>(h)h̃k + b
(h))
</p>
<p>where C(vk) is the set of child vertices of vk (vr ∈
C(vk)) and hr is a hidden state of a child vertex vr.
W (h) and U (h) are weight matrices, and b(h) is a
bias vector for model parameters. In this paper, we
refer to Graph Naive Recursive Neural Network as
GNRNN incorporating NRNU as a RNU.
</p>
<p>2.3.2 Graph Long Short-Term Memory Net
LSTMU (Hochreiter and Schmidhuber, 1997) was
originally proposed to tackle a sequential labelling
problem and it is able to model long-range depen-
dencies by incorporating gated memory cells. At
each time step, LSTMU takes the sequential input
vector and the previous hidden state vector to pro-
duce the next hidden state. In this study, LSTMU
is employed as a RNU to represent a vertex in a
tree, and it naturally captures the relationships be-
tween vertices.
</p>
<p>For a vertex vk, LSTMU takes xk and h̃k as in-
put, and generates the input, forget and output gate
signals, denoted as ik, fk and ok respectively. It
produces a memory cell state ck and hidden state
hk with respect to a vertex vk:
</p>
<p>h̃k = max
vr∈C(vk)
</p>
<p>{hr}
</p>
<p>c̃k = tanh(W
(c)xk + U
</p>
<p>(c)h̃k + b
(c))
</p>
<p>ik = σ(W
(i)xk + U
</p>
<p>(i)h̃k + b
(i))
</p>
<p>fkr = σ(W
(f)xk + U
</p>
<p>(f)hr + b
(f))
</p>
<p>ok = σ(W
(o)xk + U
</p>
<p>(o)h̃k + b
(o))
</p>
<p>ck = ik � c̃k +
∑
</p>
<p>vr∈C(vk)
fkr � cr
</p>
<p>hk = ok � tanh(ck)
</p>
<p>where � refers to element-wise product and σ in-
dicates the sigmoid function. W (∗), U (∗) and b(∗)
</p>
<p>2Our preliminary results demonstrated that the max pool-
ing strategy achieved better performance than sum and aver-
age poolings.
</p>
<p>Dataset Gender Age UserType
Users |V | 5,367 6,482 3,017
</p>
<p>Relationships |E| 5,088 6,514 38,785
Labels |Y | 2 2 3
</p>
<p>Avg. degree 1.90 2.01 25.71
Num. texts 383,425 701,889 3,017
Vocab size 12,558 20,946 615
</p>
<p>Table 1: Statistics of the three Twitter social net-
works. Num. texts indicates the number of tweets
for Gender and Age, and the number of profile de-
scriptions for UserType.
</p>
<p>are LSTM parameters. We call a tree-structured
network topology consisting of LSTMUs as Graph
Long Short-Term Memory Net (GLSTMN).
</p>
<p>2.4 Softmax Output
</p>
<p>At the end, the hidden state ht is fed into a softmax
classifier to predict the label yt of the target vertex
vt after calculating the hidden states of all vertices
in T : Pθ(yt|vt, G,X ) = softmax(W (s)ht+b(s))
ŷt = argmaxyt∈Y Pθ(yt|vt, G,X ).
</p>
<p>3 Experimental Setup
</p>
<p>In this section, we provide an overview of datasets
and the models that are evaluated in the experi-
ments.
</p>
<p>3.1 Datasets
</p>
<p>We evaluate the effectiveness of our model on
three types of social networks: gender, age and
user type classification. Twitter users follow oth-
ers or are followed, and two types of relationships
are used to build the social networks: friend and
follower. A user is associated with others via the
following relationship, the user’s friend in Twit-
ter’s terminology. Follower relationships indicate
that a user receives all the tweets from those the
user follows.
</p>
<p>• Gender (Volkova, 2014) is a Twitter social
network encoding friend relationships be-
tween users. The labels of this network are
Male and Female.
</p>
<p>• Age (Volkova, 2014) is a Twitter social net-
work encoding friend relationships between
users. The labels of this network are Young
(18-23 years old) and Old (25-30 years old).
</p>
<p>• UserType (Kim et al., 2017) is a Twitter so-
cial network encoding follower relationships
between users. The labels represent the types
of Twitter users: Individual, Organisation
and other.
</p>
<p>473</p>
<p />
</div>
<div class="page"><p />
<p>To generate text features of vertices, up to 1K
tweets per user are used in Gender and Age,
whereas Twitter user profile descriptions are used
in UserType. All words are stemmed, and then
stop words and words with document frequency
less than 10 are removed. The statistics of the
datasets are summarised in Table 1.
</p>
<p>3.2 Evaluated Models
</p>
<p>We compare the GRNN model with several exist-
ing models to assess vertex classification perfor-
mance.
</p>
<p>• Lexica (LX) (Sap et al., 2014): a lexicon-
based method produced from Twitter to cal-
culate the scores of gender and age. These
scores are used to predict their labels for
users.
</p>
<p>• Logistic Regression (LR) (Hosmer Jr et al.,
2013): a commonly used linear model in the
NLP community, only using textual contents
in vertices. Bag-of-words feature vectors are
generated without incorporating any topolog-
ical information of a network.
</p>
<p>• Label Propagation (LP) (Wang and Zhang,
2006): a graph-based semi-supervised learn-
ing model, where label probabilities are prop-
agated to all unlabelled neighbours. The
probability derivation steps are terminated for
the remaining vertices when all label proba-
bilities converge.
</p>
<p>• Text-Associated DeepWalk (TADW) (Yang
et al., 2015): an unsupervised vertex embed-
ding learning method. Low-dimensional rep-
resentations of vertices are induced both from
their texts and graph relationships based on
inductive matrix factorisation.
</p>
<p>• Tri-Party Deep Network Representation
(TriDNR) (Pan et al., 2016): two neural net-
works incorporating the texts, relationships
and labels of vertices in graphs. As in TADW,
unlabelled vertices are classified using Sup-
port Vector Machines (SVMs) (Cortes and
Vapnik, 1995) trained on learned vertex em-
beddings.
</p>
<p>3.3 Experiment Settings
</p>
<p>In our experiments, we follow the standard ex-
perimental protocol for vertex classification task.
More specifically, we evaluate classification ac-
</p>
<p>Labelled Nodes 20% 40% 60% 80%
LX 50.72 50.35 53.29 57.37
LR 57.37 59.91 61.97 62.37
LP 55.92 60.53 60.39 61.05
</p>
<p>TADW 53.07 50.93 54.04 51.47
TriDNR 50.99 49.56 48.03 56.58
</p>
<p>GNRNN d0 57.37 61.67 63.03 65.79
GNRNN d1 57.76 61.84 63.55 66.05
GNRNN d2 57.76 61.84 63.55 66.05
</p>
<p>GLSTMN d0 58.29 63.68 64.21 68.16
GLSTMN d1 57.37 62.46 63.95 68.16
GLSTMN d2 57.37 62.46 63.95 68.16
</p>
<p>Table 2: Vertex classification results on Gender
(e.g., GNRNN with depth 1 is represented by GN-
RNN d1). Numbers in bold represent the highest
performance in each column in all tables.
</p>
<p>Labelled Nodes 20% 40% 60% 80%
LX 50.10 50.69 46.76 47.95
LR 67.75 71.15 72.97 73.70
LP 67.47 71.61 71.86 73.97
</p>
<p>TADW 66.81 68.80 69.31 68.89
TriDNR 58.13 57.60 49.66 57.53
</p>
<p>GNRNN d0 72.46 74.56 74.62 74.79
GNRNN d1 72.53 71.61 76.69 75.89
GNRNN d2 72.46 71.98 76.14 76.44
</p>
<p>GLSTMN d0 73.49 74.47 75.86 77.53
GLSTMN d1 73.91 75.02 77.93 80.82
GLSTMN d2 73.43 74.65 78.48 80.27
</p>
<p>Table 3: Vertex classification results on Age.
</p>
<p>curacy3 with different training ratios, increasing
from 20% to 80%. For each training ratio, we ran-
domly generate 5 different training datasets. For
each training dataset, we run 10 trials and record
the highest accuracy on each testing dataset. We
then report the average accuracy for the same ratio
of training datasets.
</p>
<p>We test six different model architectures us-
ing NRNU and LSTMU with three different tree
depths (d = 0 , d = 1 , d = 2 ). The tree depth
corresponds to the number of hops between users
in a social network. For all the GRNN models, the
size of the hidden units is set to 200. We use Ada-
grad (Duchi et al., 2011) with a batch size of 20 as
the optimisation method that automatically adapts
the learning rate in training. The initial learning
rate is set to 0.1 for LR and LP, and 0.01 for all
GRNNs.
</p>
<p>4 Results and Analysis
</p>
<p>In this section, we present the experimental results
and analysis on vertex classification for the three
</p>
<p>3We report accuracy following previous work for Age and
Gender datasets, consisting of balanced label distributions.
Although UserType is imbalanced, accuracy is reported in
this paper for simplicity and consistency. Previous work also
reports classification accuracy for this task.
</p>
<p>474</p>
<p />
</div>
<div class="page"><p />
<p>Labelled Nodes 20% 40% 60% 80%
LR 74.76 76.18 76.98 77.91
LP 73.41 74.50 75.99 75.33
</p>
<p>TADW 72.62 75.67 76.43 75.69
TriDNR 74.90 77.91 80.37 81.62
</p>
<p>GNRNN d0 77.02 77.96 78.51 79.21
GNRNN d1 73.97 77.10 78.66 79.37
GNRNN d2 NA NA NA NA
</p>
<p>GLSTMN d0 77.16 78.63 79.40 79.77
GLSTMN d1 72.41 76.51 78.53 80.46
GLSTMN d2 NA NA NA NA
</p>
<p>Table 4: Vertex classification results on UserType.
NA stands for “Not Available”.
</p>
<p>networks. Numbers in bold represent the highest
performance in each column in all tables.
</p>
<p>As shown in Table 2, GNRNN and GLSTMN
perform better than the evaluated models for the
task of gender prediction, with no performance
gain when the depth of the tree is increased.
Namely, GLSTMN d0 is the best performing
model for this task. Table 3 shows that GLSTMN
also achieves the best performance for age pre-
diction. For this task, however, performance in-
creases with the depth of the tree. For the task of
user type classification, Table 4 shows that we at-
tain the best performance with tree depth d = 1
for GLSTMN for the training ratio of 80%. Note
that we could not train the GRNN models with tree
depth 2 (GNRNN d2 and GLSTMN d2) on the
UserType dataset on our Linux server with 64G
memory due to the lack of memory. As shown in
Table 1, the average degree of UserType is roughly
10 times larger than that of Gender or Age. The
degree of a vertex in a graph indicates the num-
ber of edges connected to adjacent vertices. It
means that the GRNN models could have approx-
imately 625 (= 25 × 25) vertices in average for
the tree depth of 2. Our experiments show that a
dense graph consisting of high degree vertices is
intractable under the GRNN model.
</p>
<p>Interestingly, LR and LP are effective meth-
ods for Gender and Age compared to TADW and
TriDNR. In particular, TADW performs poorly
on Gender, and TriDNR marginally outperforms
GLSTMN on the dense social network, UserType.
</p>
<p>To summarise, we observed four findings:
</p>
<p>1. The GRNN models (GNRNN and GLSTMN)
overall outperform the existing models by a
noticeable margin in most cases (except for
UserType), showing the benefit of RNNs for
social network inferences on vertices.
</p>
<p>2. The LSTM unit in GLSTMN gives superior
</p>
<p>performance over the NRN unit in GNRNN
regardless of datasets. Our results are in line
with other findings, showing that the LSTM
works consistently better than the standard
recurrent neural network.
</p>
<p>3. GRNNs have different optimal tree depths for
each demographic inference task. Tree depth
does not improve inference performance for
Gender, indicating that only text informa-
tion is sufficient without incorporating net-
work information. For age, tree depth al-
lows GRNNs to be more effective although
its increase does not consistently lead to bet-
ter performance for GLSTMN. Similarly, tree
depth increases the performance of GRNNs
for UserType. We hypothesise this may be re-
lated to the nature of social interactions (e.g.,
Twitter users in the similar age group are
more likely to interact).
</p>
<p>4. The matrix factorisation-based methods
(TADW and TriDNR) relatively work
well on datasets having high degree vertices,
whereas the GRNN models achieve relatively
good performance on graphs containing low
degree vertices.
</p>
<p>5 Conclusions and Future Work
</p>
<p>In this paper we tackled the demographic infer-
ence problem on Twitter as vertex classification
on a graph using GRNNs, demonstrating their
effectiveness against strong models for selected
datasets. The RNN framework provides an ef-
fective way to incorporate network, text and label
information for Twitter demographic inference.
However, different demographic inference tasks
benefit to varying the tree depth of GRNN mod-
els.
</p>
<p>As our future work, we plan to employ other
state-of-the-art deep learning models (Yang et al.,
2016; Kipf and Welling, 2017) that vary in the
nature of the dependency between network, text
and label information for demographic inference
to confirm the effectiveness of our proposals.
</p>
<p>Acknowledgments
</p>
<p>The authors are grateful to three anonymous ACL
reviewers. We would also like to thank Brian Jin,
who ran the data collection for this study.
</p>
<p>475</p>
<p />
</div>
<div class="page"><p />
<p>References
Corinna Cortes and Vladimir Vapnik. 1995. Support-
</p>
<p>vector networks. Machine Learning 20(3):273–297.
</p>
<p>John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Machine Learning Re-
search 12:2121–2159.
</p>
<p>Katja Filippova. 2012. User demographics and lan-
guage in an implicit social network. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning. Association
for Computational Linguistics, Jeju Island, Korea,
pages 1478–1488.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.
</p>
<p>David W Hosmer Jr, Stanley Lemeshow, and Rodney X
Sturdivant. 2013. Applied logistic regression, vol-
ume 398. John Wiley &amp; Sons.
</p>
<p>Gaya Jayasinghe, Brian Jin, James Mchugh, Bella
Robinson, and Stephen Wan. 2016. CSIRO Data61
at the WNUT geo shared task. In Proceedings of
the 2nd Workshop on Noisy User-generated Text
(WNUT). The COLING 2016 Organizing Commit-
tee, Osaka, Japan, pages 218–226.
</p>
<p>David Jurgens, Tyler Finethy, James McCorriston,
Yi Tian Xu, and Derek Ruths. 2015. Geolocation
prediction in Twitter using social networks: A criti-
cal analysis and review of current practice. In Pro-
ceedings of the Ninth International Conference on
Web and Social Media, ICWSM 2015, University of
Oxford, Oxford, UK, May 26-29, 2015. pages 188–
197.
</p>
<p>Sunghwan Mac Kim, Cécile Paris, Robert Power, and
Stephen Wan. 2017. Distinguishing individuals
from organisations on Twitter. In Proceedings of the
26th International Conference on World Wide Web.
International World Wide Web Conferences Steer-
ing Committee, Perth, Australia, WWW ’17, pages
805–806.
</p>
<p>Sunghwan Mac Kim, Stephen Wan, and Cécile Paris.
2016. Occupational representativeness in Twitter.
In Proceedings of the 21st Australasian Document
Computing Symposium, ADCS 2016, Caulfield, VIC,
Australia, December 5-7, 2016. pages 57–64.
</p>
<p>Thomas N Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. In Proceedings of the International Con-
ference on Learning Representations (ICLR).
</p>
<p>Tyler H. McCormick, Hedwig Lee, Nina Cesare, Ali
Shojaie, and Emma S. Spiro. 2015. Using Twitter
for demographic and social science research: Tools
for data collection and processing. Sociological
Methods &amp; Research .
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.
</p>
<p>Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, and J. Niels Rosenquist. 2011. Un-
derstanding the demographics of Twitter users.
In Proceedings of the Fifth International Confer-
ence on Weblogs and Social Media, ICWSM 2011,
Barcelona, Catalonia, Spain, July 17-21, 2011. The
AAAI Press, pages 554–557.
</p>
<p>Ehsan Mohammady Ardehaly and Aron Culotta. 2015.
Inferring latent attributes of Twitter users with label
regularization. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, Denver, Colorado, pages 185–195.
</p>
<p>Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang,
and Yang Wang. 2016. Tri-party deep network rep-
resentation. In Proceedings of the 25th International
Joint Conference on Artificial Intelligence, IJCAI
2016, New York, NY, USA, 9-15 July 2016. pages
1895–1901.
</p>
<p>Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: Online learning of social rep-
resentations. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining. ACM, New York, NY,
USA, KDD ’14, pages 701–710.
</p>
<p>Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence 46(1-2):77–105.
</p>
<p>Daniel Preoţiuc-Pietro, Vasileios Lampos, and Niko-
laos Aletras. 2015. An analysis of the user occupa-
tional class through Twitter content. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1754–
1764.
</p>
<p>Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan
Zhu, and Xiaoyan Zhu. 2015. Learning tag em-
beddings and tag-specific composition functions in
recursive neural network. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1365–
1374.
</p>
<p>Maarten Sap, Gregory Park, Johannes Eichstaedt, Mar-
garet Kern, David Stillwell, Michal Kosinski, Lyle
Ungar, and Hansen Andrew Schwartz. 2014. Devel-
oping age and gender predictive lexica over social
media. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics, Doha, Qatar, pages 1146–1151.
</p>
<p>476</p>
<p />
</div>
<div class="page"><p />
<p>Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings of
the 24th International Conference on Neural Infor-
mation Processing Systems. Curran Associates Inc.,
USA, NIPS’11, pages 801–809.
</p>
<p>Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang,
Jun Yan, and Qiaozhu Mei. 2015. Line: Large-
scale information network embedding. In Proceed-
ings of the 24th International Conference on World
Wide Web. International World Wide Web Confer-
ences Steering Committee, Republic and Canton of
Geneva, Switzerland, WWW ’15, pages 1067–1077.
</p>
<p>Tomoki Taniguchi, Shigeyuki Sakaki, Ryosuke Shige-
naka, Yukihiro Tsuboshita, and Tomoko Ohkuma.
2015. A weighted combination of text and image
classifiers for user gender inference. In Proceedings
of the Fourth Workshop on Vision and Language.
Association for Computational Linguistics, Lisbon,
Portugal, pages 87–93.
</p>
<p>Jose Manuel Delgado Valdes, Jacob Eisenstein, and
Munmun De Choudhury. 2015. Psychological ef-
fects of urban crime gleaned from social media. In
Proceedings of the 9th International Conference on
Web and Social Media, ICWSM 2015, University of
Oxford, Oxford, UK, May 26-29, 2015. AAAI Press,
Menlo Park, California, pages 598–601.
</p>
<p>Svitlana Volkova. 2014. Twitter data collection:
Crawling users, neighbors and their communica-
tion for personal attribute prediction in social media.
Center for Language and Speech Processing, Johns
Hopkins University .
</p>
<p>Svitlana Volkova, Glen Coppersmith, and Benjamin
Van Durme. 2014. Inferring user political prefer-
ences from streaming communications. In Proceed-
ings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics,
Baltimore, Maryland, pages 186–196.
</p>
<p>Stephen Wan and Cécile Paris. 2014. Improving gov-
ernment services with social media feedback. In
Proceedings of the 19th International Conference on
Intelligent User Interfaces. ACM, New York, NY,
USA, IUI ’14, pages 27–36.
</p>
<p>Fei Wang and Changshui Zhang. 2006. Label propa-
gation through linear neighborhoods. In Proceed-
ings of the 23rd International Conference on Ma-
chine Learning. ACM, New York, NY, USA, ICML
’06, pages 985–992.
</p>
<p>Qiongkai Xu, Qing Wang, Chenchen Xu, and Lizhen
Qu. 2017. Collective vertex classification us-
ing recursive neural network. arXiv preprint
arXiv:1701.06751 .
</p>
<p>Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun,
and Edward Y. Chang. 2015. Network representa-
tion learning with rich text information. In Proceed-
ings of the 24th International Joint Conference on
</p>
<p>Artificial Intelligence. AAAI Press, IJCAI’15, pages
2111–2117.
</p>
<p>Zhilin Yang, William W. Cohen, and Ruslan Salakhut-
dinov. 2016. Revisiting semi-supervised learning
with graph embeddings. In Proceedings of the
33nd International Conference on Machine Learn-
ing, ICML 2016, New York City, NY, USA, June 19-
24, 2016. pages 40–48.
</p>
<p>Arkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-
dine Wong Sak Hoi, and Peter Tolmie. 2016.
Analysing how people orient to and spread rumours
in social media by looking at conversational threads.
PloS one 11(3):e0150989.
</p>
<p>477</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 478–483
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2076
</p>
<p>Twitter Demographic Classification Using Deep Multi-modal Multi-task
Learning
</p>
<p>Prashanth Vijayaraghavan∗
MIT
</p>
<p>Cambridge, MA, USA
pralav@mit.edu
</p>
<p>Soroush Vosoughi∗
MIT
</p>
<p>Cambridge, MA, USA
soroush@mit.edu
</p>
<p>Deb Roy
MIT
</p>
<p>Cambridge, MA, USA
dkroy@media.mit.edu
</p>
<p>Abstract
</p>
<p>Twitter should be an ideal place to get
a fresh read on how different issues are
playing with the public, one that’s poten-
tially more reflective of democracy in this
new media age than traditional polls. Poll-
sters typically ask people a fixed set of
questions, while in social media people
use their own voices to speak about what-
ever is on their minds. However, the de-
mographic distribution of users on Twitter
is not representative of the general pop-
ulation. In this paper, we present a de-
mographic classifier for gender, age, po-
litical orientation and location on Twitter.
We collected and curated a robust Twit-
ter demographic dataset for this task. Our
classifier uses a deep multi-modal multi-
task learning architecture to reach a state-
of-the-art performance, achieving an F1-
score of 0.89, 0.82, 0.86, and 0.68 for gen-
der, age, political orientation, and location
respectively.
</p>
<p>1 Introduction
</p>
<p>While the most ambitious polls are based on stan-
dardized interviews with a few thousand people,
millions are tweeting freely and publicly in their
own voices about issues they care about. This data
offers a vibrant 24/7 snapshot of people’s response
to various events and topics.
</p>
<p>However, the people using Twitter are not rep-
resentative of the general US population (Green-
wood et al., 2016). Therefore, if one is to use Twit-
ter to understand the public’s views on various, it
is essential to understand the demographic of the
users on Twitter. A robust demographic classifica-
tion algorithm can also be utilized for detection of
∗The first two authors contributed equally to this work.
</p>
<p>non-human account, especially in the context of
bots involved in the spread of rumors and false in-
formation on Twitter (Vosoughi, 2015).
</p>
<p>In this paper, we present a state-of-the-art de-
mographic classifier for Twitter. We focus on four
different demographic categories: (a) Gender, (b)
Age, (c) Political Orientation and (d) Location.
We implement different variants of the deep multi-
modal multi-task learning architecture to infer
these demographic categories.
</p>
<p>2 Features
</p>
<p>Our deep multi-modal multi-task learning mod-
els (DMT-Demographic Models) use features ex-
tracted from the users’ Twitter profile (such as
name, profile picture, and description), the users
following network and the users historical tweets
(what they have said in the past). Below, we ex-
plain how these features are extracted and used.
</p>
<p>2.1 Name
</p>
<p>The name specified by users in their profile
is mainly used for gender prediction. We used
three datasets for gender associations of common
names:
</p>
<p>• We used the data from US Census Bureau
data which contains male and female1 first
names along with frequency of names for the
sample male and female population with re-
spect to 1990 census.
</p>
<p>• We obtained yearly data for the 100 most
popular female and male names between
1960 and 2010 and calculate the overall fre-
quency of a name being used in each list.
</p>
<p>• We also have a list of European names and
popular first from other countries associated
</p>
<p>1https://www2.census.gov/topics/genealogy/1990surnames/
</p>
<p>478</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2076">https://doi.org/10.18653/v1/P17-2076</a></div>
</div>
<div class="page"><p />
<p>with gender information2.
</p>
<p>Using the above datasets, we associate each
name with a vector of size 2 representing the prob-
ability that the name occurs in male list and female
list based on frequency available in the datasets.
</p>
<p>2.2 Following Network
Network Features can be a signal in prediction of
some of the demographic parameters. But it is dif-
ficult to utilize the complete list of followers and
following information of each and every user due
to curse of dimensionality. Therefore, we build an
binary vector of size Ndim for each user with each
index of the vector representing a popular Twitter
profiles associated with age, political orientation
or location and the value (1, 0) represents if the
user is following the profile or not. These profiles
are short-listed based on the following techniques:
</p>
<p>• We search for user accounts on Twitter for
task specific keywords like teenager, 80s, 90s
for Age prediction; Democrat, Republican
for political orientation and state names for
location prediction.
</p>
<p>• We take advantage of the data collected from
our earlier work (Vijayaraghavan et al., 2016)
in processing news stories, classifying named
entities into various categories and mapping
them to Twitter handles. We use the political
personalities mapped onto Twitter to the list
of twitter profiles that can potentially act as a
signal for our prediction tasks.
</p>
<p>Sample Twitter handles associated with each of
the tasks are given in Table 1. For gender, the han-
dles were too generic, so we expect that there are
inherent latent features that can contribute towards
gender prediction based on the shortlisted Twit-
ter handles. We experiment with one or two fully-
connected layers and compress the information to
a Nemb-sized vector.
</p>
<p>2.3 Profile Description
The profile description can be really useful to pre-
dict all the demographic parameters. Since, GRU
is computationally less expensive than LSTM and
performs better than standard RNN, we use a gated
recurrent network (GRU) (Cho et al., 2014; Chung
et al., 2014). At each time step t, GRU unit takes a
2https://hackage.haskell.org/package/gender-
0.1.1.0/src/data/nam dict.txt.UTF8
</p>
<p>word embedding xt and a hidden state ht as input.
The internal transition operations of the GRU are
defined as follows:
</p>
<p>zt = σ(W
(z)xt + U
</p>
<p>(z)ht−1 + b(z)) (1)
</p>
<p>rt = σ(W
(r)xt + U
</p>
<p>(r)ht−1 + b(r)) (2)
</p>
<p>h̃t = tanh(Wxt + rt · Uht−1 + b(h) (3)
</p>
<p>ht = zt · ht−1 + (1− zt)h̃t (4)
</p>
<p>where W (z),W (r),W ∈ IRdh×di , U (z), U (r), U ∈
IRdh×dh and · is an element-wise product. The di-
mensions dh and di are hyperparameters repre-
senting the hidden state size and input embedding
size respectively. In our experiments, we represent
the description as a (a) vector using GRU’s final
hidden state i.e. the hidden state representation (re-
ferred as DF ∈ IRdh) at the last time step (b) ma-
trix using all the time steps of hidden state, repre-
sented as DM ∈ IRL×dh , where L is the sequence
length of the user description.
</p>
<p>2.4 Profile Picture
Age and gender prediction can exploit the features
extracted from profile picture. We extract dense
feature representation from the image using the In-
ception architecture (Szegedy et al., 2015). Since
we deal with multiple tasks, we experiment with
two different layers (pool3 and mixed10) repre-
sentations from the Inception architecture. The
output vector sizes are IV = 2048 and IM =
64× 2048 respectively.
</p>
<p>2.5 Tweets
Finally, we also use tweets for our multitask learn-
ing problem. In our experiments, we restrict it to
user’s recent K tweets. The list of tweets, each of
which is word sequence (Si = [wi1, w
</p>
<p>i
2, ..., w
</p>
<p>i
N ]),
</p>
<p>are encoded using a positional encoding scheme
as used in (Sukhbaatar et al., 2015). (For a more
sophisticated encoding of the tweets, one can use
the Tweet2Vec by Vosoughi et al. (Vosoughi et al.,
2016), however the algorithm requires a massive
training dataset, which might not be available to
everyone) For the positional encoding scheme, the
sentence representation is computed by
</p>
<p>Pi =
</p>
<p>N∑
</p>
<p>j=1
</p>
<p>lj · wij (5)
</p>
<p>479</p>
<p />
</div>
<div class="page"><p />
<p>Task Sample Twitter Handles
Age @80s Kidz, @The1980sGirl, @60s70sKids, @IL0VEthe80s, @90syears, @The90sLife
Pol-Orien @realDonaldTrump, @HillaryClinton, @youngdemocrat, @GOP, @NancyPelosi
Location @california, @UWBadgers, @UtahGov, @UMichFootball, @PureMichigan
</p>
<p>Table 1: Sample Twitter handles used to create the network features for each task.
</p>
<p>N is the maximum number of words in a sentence
and lj is a column vector with structure
</p>
<p>lpj = (1− j/N)− (p/q)(1− 2j/N) (6)
</p>
<p>where p is the embedding index and q is the di-
mension of the embedding. The tweet representa-
tion obtained from the positional encoding sum-
marizes the word tokens in the sentence. We ex-
plore tweet features as (1) a vector by summing
up all the K-tweet embeddings Tq ∈ IRq, (2) a ma-
trix by concatenating all the K-tweet embeddings
TKq ∈ IRK×q
</p>
<p>3 DMT-Demographic Models
</p>
<p>Some of the latent information from one task can
be useful to predict another task. Therefore, we
propose three variants of deep multi-modal multi-
task learning demographic models to leverage the
multi-modal nature of data. Figure 1 gives an il-
lustration of our proposed models. In this section,
we explain the single task output layer followed
by the various models.
</p>
<p>3.1 Vanilla DMT-Demographic Model
This model takes vector features extracted from
various user details (explained in section 2) repre-
sented byDF , Tq, Nemb, IV for description, tweet,
network and image features respectively. The fea-
ture vectors are concatenated and passed through
a fully-connected layer. The output of the fully-
connected layer is a compressed latent feature vec-
tor of size h. This shared latent vector is given to
a task-specific output layer explained in Section 4.
For gender prediction task, name features are con-
catenated with latent vector before feeding it to the
output layer.
</p>
<p>3.2 Attention-based DMT-Demographic
Model
</p>
<p>All the modalities do not contribute equally to
each of our tasks. Hence, for each task, we con-
catenate the weighted modal feature representa-
tions obtained through attention mechanism and
then pass it through a fully-connected layer to get
</p>
<p>a latent feature vector. Formally, the extracted fea-
tures vectors represented by DF , Tq, Nemb, IV are
concatenated to get a matrix M ∈ IRd×4 where d
is the dimension of each feature. If the extracted
features are not of the same dimension d, then we
introduce a fully-connected layer and transform it
to a d-sized vector. The attention over different
modal features are computed as follows.
</p>
<p>α = softmax(W (2)tanh(W (1)M + b(1)) + b(2))
(7)
</p>
<p>where α ∈ IR1×d. We multiply each of the fea-
ture vectors by their corresponding α value to get
a weighted feature representation. These weighted
representation are concatenated before passing it
through a fully-connected layer. The latent vec-
tor obtained from the fully connected layer is now
task-specific and not shared between tasks. The la-
tent vector is given to a task-specific output layer.
</p>
<p>3.3 Hierarchical Attention-based DMT -
Demographic Model
</p>
<p>This model is a slight variant of the previous
model. In this model, we introduce another level of
attention mechanism over the extracted features.
The main intuition behind this approach is to have
more attention on individual features based on
their importance for a task. For example, certain
words like ’male’,’husband’ in user’s description
might be more suitable for gender prediction than
any other task. So we weigh such words higher
than the other words in the description during
gender prediction task. However, these weights
might not be applicable for a location prediction
task. Hence, we implement a hierarchical attention
mechanism that has task-specific weighted feature
extraction followed by task-specific attention over
the modalities. The rest of the architecture is sim-
ilar to the attention-based model.
</p>
<p>This model uses the matrix representation as-
sociated with each of the features like descrip-
tion (DM ), tweets (TKq) and profile picture (IM ).
However, the network features (Nemb) remain un-
changed. The attention applied over the extracted
features is similar to Equation 7 where the dimen-
</p>
<p>480</p>
<p />
</div>
<div class="page"><p />
<p>Concatenate
</p>
<p>FC
</p>
<p>Description
DF
</p>
<p>Tweets
Tq
</p>
<p>Profile Picture
IV
</p>
<p>Network Feature
Nemb
</p>
<p>FC
</p>
<p>Description
DF
</p>
<p>Tweets
Tq
</p>
<p>Profile Picture
IV
</p>
<p>Network Feature
Nemb
</p>
<p>X X = {A, L, P, G}
</p>
<p>Description
DM
</p>
<p>Tweets
TKq
</p>
<p>Profile Picture
IM
</p>
<p>Network Feature
Nemb
</p>
<p>Attention Attention Attention
</p>
<p>Attention
</p>
<p>FC
</p>
<p>X X = {A, L, P, G}
</p>
<p>Attention
</p>
<p>X X = {A, L, P, G}
</p>
<p>Figure 1: Illustration of variants of the DMT-Demographic Model. Left: Vanilla DMT-Demographic
Model; Center: Attention-based DMT-Demographic Model; Right: Hierarchical Attention-based DMT-
Demographic Model.
</p>
<p>sions of weight parameters are feature-specific.
For the sake of convenience, let β(F ) be the
weights similar to α associated with a feature F .
For each feature F, we perform a weighted sum
over the extracted representation matrix to obtain
a vector representation. Let M (F ) denote the ma-
trix representation of an extracted feature F, then
the vector representation V (F ) of the feature F can
be computed as follows.
</p>
<p>V (F ) =
</p>
<p>rF∑
</p>
<p>r=1
</p>
<p>β(F )r M
(F )
r (8)
</p>
<p>where rF is the maximum number of rows in the
representation matrix M (F ) associated with fea-
ture F. These vector representations of all the fea-
tures are fed to layers similar to attention-based
DMT model.
It is important to note that all the models incor-
porate name features with the final latent vector
representation for gender prediction task.
</p>
<p>4 Output Layer
</p>
<p>Given a specific task A, we feed the latent feature
vector h(A), obtained after applying any of the ex-
plained models, to a softmax layer depending on
the classification task. So the task-specific repre-
sentations are fed to task-specific output layers.
</p>
<p>ỹ(A) = softmax(W (A)h(A) + b(A)), (9)
</p>
<p>where ỹ(A) is a distribution over various cate-
gories associated with task A.
</p>
<p>For each task A, we minimize the cross-entropy
of the predicted and true distributions.
</p>
<p>L(A)(ỹ(A), y(A)) =
</p>
<p>N∑
</p>
<p>i=1
</p>
<p>C(A)∑
</p>
<p>j=1
</p>
<p>y
j(A)
i log(ỹ
</p>
<p>j(A)
i )
</p>
<p>(10)
where yj(A)i and ỹi
</p>
<p>j(A) are the true label and
prediction probabilities for task A, N denotes the
total number of training samples and C(A) is the
total number of classes associated with the task A.
Thus, the parameters of our network are optimized
for global objective function given by:
</p>
<p>η =
∑
</p>
<p>A∈X
L(A)(ỹ(A), y(A)) (11)
</p>
<p>where X={Age, Location, Political Orientation,
Gender}
</p>
<p>5 Data Collection &amp; Evaluation
</p>
<p>We agglomerated data based on user tweets and
their profile description. With access to Twitter
API, we were able to get the timeline and pro-
file information of a subset of users. We perform
simple analysis of tweets and user description and
those that contain phrases like ”I’m a girl / woman
/ guy / man / husband / wife / mother/ father”, ”I
am a democrat / republican / liberal / conservative”
or ”I support hillary / trump”, ”Happy 30th birth-
day to me”,”I’m 30 years old”, ”Born in 1980” etc.
and their variants are shortlisted. These phrases act
as indicators of gender, political orientation and
age. For location prediction task, we used a com-
bination of two different Twitter fields to collect
</p>
<p>481</p>
<p />
</div>
<div class="page"><p />
<p>Task Test Data Size MajorityClassifier (%)
Gender 9,960 53%
Age 6,580 43%
Pol-Orien 5,255 52%
Location 16,956 9%
</p>
<p>Table 2: Task-specific details of test data.
</p>
<p>data: (a) latitude, longitude from geo-tagged user
tweets, (b) Location field in user profile informa-
tion. The various categories associated with each
of the tasks are: (a) Gender: M,F (b) Age: &lt; 30,
30− 60, &gt; 60 (c) Political Orientation: Democrat,
Republican (d) Location: All states in USA.
</p>
<p>In order to avoid selection bias in the dataset
collected, we introduce some noise in the training
set by randomly removing the terms (from tweet
or description) used for shortlisting the user pro-
file. The total size of the training set is 50,859.
We evaluate our models on task-specific annotated
(mechanical turk) data or data collected based on
different phrase indicators from user’s tweet or de-
scription that was not a part of training set. The de-
tails of the test set are given in Table 2. The macro
F1-score of different DMT-Demographic models
(plus two baseline non-neural network based mod-
els) on the test data can be seen in Table 3.
Hierarchical-Attention model performs well ahead
of the other two models for almost all the tasks.
However, the performance of all the models fall
flat for location prediction task. Location-specific
feature augmentation can be explored to improve
its performance further.
</p>
<p>6 Related Work
</p>
<p>The main distinctions of several of these models
with DMT-Demographic models are that (a) most
previous literature use only tweet content analy-
sis to predict demographic information (Nguyen
et al., 2013) while our model leverages different
modals of user information including profile pic-
ture, (b) though some of the works use interest-
ing network information they do not leverage other
user details as potential signals (Colleoni et al.,
2014; Culotta et al., 2015), (c) many of the models
involve a lot of feature engineering like extracting
location indicative words for geolocation predic-
tion, etc. (Han et al., 2014; Sloan et al., 2015), (d)
our model learns shared and task-specific layer pa-
rameters as we handle the demographic prediction
</p>
<p>Task Model Macro F1
Gender Random Forrest 0.817
</p>
<p>SVM 0.828
Vanilla DMT 0.866
Attention DMT 0.875
Hierarchical-
Attention DMT
</p>
<p>0.890
</p>
<p>Age Random Forrest 0.724
SVM 0.733
Vanilla DMT 0.792
Attention DMT 0.805
Hierarchical-
Attention DMT
</p>
<p>0.819
</p>
<p>Political Random Forrest 0.785
Orientation SVM 0.772
</p>
<p>Vanilla DMT 0.825
Attention DMT 0.847
Hierarchical-
Attention DMT
</p>
<p>0.859
</p>
<p>Location Random Forrest 0.668
SVM 0.665
Vanilla DMT 0.678
Attention DMT 0.674
Hierarchical-
Attention DMT
</p>
<p>0.680
</p>
<p>Table 3: Task-specific Macro F1-score for differ-
ent DMT-Demographic models.
</p>
<p>problem as a multi-task learning problem using
different modalities like image (profile picture),
text (tweets and user description) and network fea-
tures (following).
</p>
<p>7 Conclusion
</p>
<p>In this paper, we presented a state-of-the-art demo-
graphic classifier for identifying the gender, age,
political orientation and the location of users on
Twitter. We also collected and curated a novel
Twitter demographic dataset and explored differ-
ent variants of deep multi-modal multi-task learn-
ing architectures, settling on the Hierarchical-
Attention DMT as the top performing model,
achieving an F1-score of 0.89, 0.82, 0.86, and 0.68
for gender, age, political orientation, and location
respectively.
</p>
<p>In the future, we intend to use the demographic
classifier presented in this paper to study the de-
mographic biases present on Twitter.
</p>
<p>482</p>
<p />
</div>
<div class="page"><p />
<p>References
Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
</p>
<p>danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .
</p>
<p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .
</p>
<p>Elanor Colleoni, Alessandro Rozza, and Adam Arvids-
son. 2014. Echo chamber or public sphere? predict-
ing political orientation and measuring political ho-
mophily in twitter using big data. Journal of Com-
munication 64(2):317–332.
</p>
<p>Aron Culotta, Nirmal Ravi Kumar, and Jennifer Cutler.
2015. Predicting the demographics of twitter users
from website traffic data. In AAAI. pages 72–78.
</p>
<p>Shannon Greenwood, Andrew Perrin, and
Maeve Duggan. 2016. Demographics
of social media users in 2016. http:
//www.pewinternet.org/2016/11/11/
social-media-update-2016/. Accessed:
2017-01-07.
</p>
<p>Bo Han, Paul Cook, and Timothy Baldwin. 2014. Text-
based twitter user geolocation prediction. Journal of
Artificial Intelligence Research 49:451–500.
</p>
<p>Dong-Phuong Nguyen, Rilana Gravel, RB Trieschnigg,
and Theo Meder. 2013. ” how old do you think i
am?” a study of language and age in twitter .
</p>
<p>Luke Sloan, Jeffrey Morgan, Pete Burnap, and
Matthew Williams. 2015. Who tweets? deriving the
demographic characteristics of age, occupation and
social class from twitter user meta-data. PloS one
10(3):e0115545.
</p>
<p>Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems. pages
2440–2448.
</p>
<p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. 2015. Re-
thinking the inception architecture for computer vi-
sion. arXiv preprint arXiv:1512.00567 .
</p>
<p>Prashanth Vijayaraghavan, Soroush Vosoughi, and Deb
Roy. 2016. Automatic detection and categorization
of election-related tweets. In Tenth International
AAAI Conference on Web and Social Media.
</p>
<p>Soroush Vosoughi. 2015. Automatic detection and ver-
ification of rumors on Twitter. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
</p>
<p>Soroush Vosoughi, Prashanth Vijayaraghavan, and Deb
Roy. 2016. Tweet2vec: Learning tweet embeddings
using character-level cnn-lstm encoder-decoder. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval. ACM, pages 1041–1044.
</p>
<p>483</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 484–490
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2077
</p>
<p>A Network Framework for Noisy Label Aggregation in Social Media
</p>
<p>Xueying Zhan1, Yaowei Wang1, Yanghui Rao1,∗,
Haoran Xie2, Qing Li3, Fu Lee Wang4, Tak-Lam Wong2
</p>
<p>1 School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China
2 Department of Mathematics and Information Technology,
The Education University of Hong Kong, Hong Kong SAR
</p>
<p>3 Department of Computer Science, City University of Hong Kong, Hong Kong SAR
4 Caritas Institute of Higher Education, Hong Kong SAR
</p>
<p>{zhanxy5, wangyw7}@mail2.sysu.edu.cn, raoyangh@mail.sysu.edu.cn,
hrxie2@gmail.com, qing.li@cityu.edu.hk, pwang@cihe.edu.hk, tlwong@eduhk.hk
</p>
<p>Abstract
</p>
<p>This paper focuses on the task of noisy
label aggregation in social media, where
users with different social or culture back-
grounds may annotate invalid or malicious
tags for documents. To aggregate noisy la-
bels at a small cost, a network framework
is proposed by calculating the matching
degree of a document’s topics and the
annotators’ meta-data. Unlike using the
back-propagation algorithm, a probabilis-
tic inference approach is adopted to esti-
mate network parameters. Finally, a new
simulation method is designed for vali-
dating the effectiveness of the proposed
framework in aggregating noisy labels.
</p>
<p>1 Introduction
</p>
<p>Social media allows users to share their views,
opinions, emotion tendencies, and other person-
al information online. It is quite valuable to ana-
lyze and predict user opinions from these materials
(Wang and Pal, 2015), in which supervised learn-
ing is one of the effective paradigms (Xu et al.,
2015). However, the performance of a supervised
learning algorithm relies heavily on the quality of
training labels (Song et al., 2015). In social media,
many training data are collected via simple heuris-
tic rules or online crowdsourcing systems, such
as Amazon’s Mechanical Turk (www.mturk.com)
which allows multiple labelers to annotate the
same object (Zhang et al., 2013). Due to the lack
</p>
<p>∗The corresponding author.
</p>
<p>of quality control, it can be hard for a model to
reconcile such noise in training labels.
</p>
<p>This study aims to aggregate noisy labels by
matching annotators and documents. Unlike other
noisy label aggregation and integration tasks (or
algorithms), such as Learning to Rank (LtR) and
integrating crowdsourced labels which rely on ac-
curate instance sources (Ustinovskiy et al., 2016)
or confidence scores (Oyama et al., 2013), we on-
ly need features that can be obtained with a small
cost (i.e., topics). Compared with acquiring accu-
rate instance sources or confidence scores, which
is very hard, extracting topics can be done con-
veniently by many existing topic models. Note
that label noise is not always random, as adver-
sarial noise may occur in real-world environments
when a malicious agent is permitted to select la-
bels for certain instances (Auer and Cesa-Bianchi,
1998). For example, a fake annotator is purchased
to promote defective goods by giving high ratings.
Noisy labels in such a manner are extremely dif-
ficult to be handled (Nicholson et al., 2015). To
validate the effectiveness of aggregating the afore-
mentioned noisy labels, we propose to design a
new simulation method in Section 4.
</p>
<p>2 Related Work
</p>
<p>To aggregate or refine noisy labels, several ap-
proaches have been proposed recently. Whitehill
et al. (Whitehill et al., 2009) explored a proba-
bilistic model to combine labels from both human
labelers and automatic classifiers in image classi-
fication. Raykar et al. (Raykar et al., 2010) used
a Bayesian approach for supervised learning over
</p>
<p>484</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2077">https://doi.org/10.18653/v1/P17-2077</a></div>
</div>
<div class="page"><p />
<p>noisy labels from multiple annotators. Oyama et
al. (Oyama et al., 2013) proposed to integrate la-
bels of crowdsourcing workers using their con-
fidence scores. Song et al. (Song et al., 2015)
developed a single-label refinement algorithm to
adjust noisy and missing labels. Ustinovskiy et
al. (Ustinovskiy et al., 2016) proposed an opti-
mization framework via remapping and reweight-
ing methods to solve the problem of LtR with the
existence of noisy labels.
</p>
<p>Different from the previous study that modeled
the difficulties of instances and the user’s author-
ity (Whitehill et al., 2009), we target at integrat-
ing multiple labels for each instance by estimat-
ing the matching degree of documents and anno-
tators. Consequently, our work is applicable to
aggregating individual sentiment labels in social
media, where users under various scenarios (e.g.,
character and preference) may express invalid or
noisy sentiments to different topics.
</p>
<p>3 Noisy Label Aggregation Framework
</p>
<p>3.1 Problem Definition
</p>
<p>The problem of noisy label aggregation is defined
as follows: Given N documents (instances) anno-
tated by M users (annotators) over C kinds of la-
bels, we generate D topics by existing unsuper-
vised topic models. Let T ∈ RN×D be topics of
all instances, where the i-th row of T (i.e., Ti) is
the topic distribution of document i, and the size
of Ti (i.e., |Ti|) is D. Let F ∈ RM×U be features
(e.g., age and gender) of all annotators, where Fj
is the feature distribution of user j and |Fj | = U .
To model different dimensions of document topics
(D) and annotator features (U ) jointly, we map Ti
and Fj to K latent factors denoted as Si and Aj ,
i.e., |Si| = |Aj | = K.
</p>
<p>words
</p>
<p>f
</p>
<p>topics
</p>
<p>iΖ
</p>
<p>1
A
</p>
<p>2
A jA 1iV 2iV ijV
</p>
<p>iS
</p>
<p>iCZ
</p>
<p>w
ei
</p>
<p>gh
t 
</p>
<p>la
b
</p>
<p>el
s
</p>
<p>o
n
</p>
<p>e-
m
</p>
<p>ax
 
</p>
<p>p
o
</p>
<p>o
li
</p>
<p>n
g
</p>
<p>to
p
</p>
<p>ic 
</p>
<p>extractio
n
</p>
<p>fu
lly
</p>
<p>-
</p>
<p>co
n
</p>
<p>n
ected
</p>
<p>m
atch
</p>
<p>in
g 
</p>
<p>d
egree
</p>
<p>weight transformation with softmax
</p>
<p>Figure 1: Our proposed network framework.
</p>
<p>To estimate the ground truth label Zi, we pro-
pose a novel network framework via aggregating
the observable labels Vi, as shown in Fig. 1. In
our framework, the correctness of Vij depends on
whether annotator j matches document i.
</p>
<p>3.2 Detailed Steps
</p>
<p>Topic Extraction (TE): For document features, it
is rough to use tf or tf-idf since they ignore the
versatility of semantics among various contexts.
Without considering the semantic units called top-
ics, the accurate category of each document may
be hard to access (Song et al., 2016). Short mes-
sages (e.g., tweets) are prevalent in social media,
which differ from normal documents insofar as the
number of words is fewer and most words only oc-
cur once in each instance. To extract topics from
such a sparse word space, we employ the Biterm
Topic Model (BTM) by breaking each document
into biterms and leveraging the information of the
whole corpus (Yan et al., 2013).
</p>
<p>Fully-connected Operation (FcO): There can
be a large difference between dimensions of doc-
ument topics and annotator features, so we need
convert T and F to the same latent space. This
step conducts linear transformation by introduc-
ing fully-connected weights WT ∈ RD×K and
WF ∈ RU×K , as follows: S = TWT and
A = FWF. The values of S and A are propor-
tional to the label correctness probability.
</p>
<p>Since more cohesive topics may indicate that
the document’s category is more concentrated and
can be correctly annotated by more users, the
topic distribution embeds key information on the
document factors S. To map T to S well, we
propose the concept of topic entropy that acts
as the constraint factor, by calculating the cen-
tralization of each document’s topics: H(di) =
−∑Dz=1 p(tz|di) logD
</p>
<p>(
p(tz|di)
</p>
<p>)
, where p(tz|di)
</p>
<p>is the probability of the z-th topic conditioned to
document i, and D constrains the values ranging
from 0 to 1. The lower H(di), the higher the con-
centration of topics and the label correctness for
document i. We thus infer the relationship be-
tween Si and H(di) as ||Si||2 ∝ 1/H(di), where
||Si||2 is the Euclidean norm of Si.
</p>
<p>Matching Degree Calculation (MDC): This
step calculates the matching degree of document
i and annotator j, which is denoted as gij by the
similarity/distance between latent factors Si and
Aj . Intuitively, a basketball enthusiast j matches
</p>
<p>485</p>
<p />
</div>
<div class="page"><p />
<p>close to a document i that contains the “basket-
ball” topic, which indicates that the “matching de-
gree” of i and j is high with a large similarity. The
inner product is used here, and it can be replaced
by distance measures.
</p>
<p>Weight Transformation (WT): We employ
transformation to distinguish different scores ef-
fectively. The activation function is sigmoid (soft-
max) or tanh. Since most document labels are as-
sumed to be discrete independent variables, we en-
code Vij as a binary vector. The higher gij of a
label, the closer it is to the ground truth. Namely,
we should weight these labels in such a way that
if a label has high gij , its weight will be increased;
meanwhile, other labels should be punished. For
sigmoid and tanh, the punishment is 1 − wij and
−wij , respectively. Take four labels, the transfor-
mation weight wij and Vij = (1, 0, 0, 0) as an ex-
ample, the label weight via sigmoid is V newij =
(wij , 1− wij , 1− wij , 1− wij).
</p>
<p>Label Weighting (LW) and One-max Pooling:
The final step is to output by integrating weight-
ed labels, where the multiplicative combination is
used in aggregation, and the output is the maxi-
mum one of aggregated labels ZiC .
</p>
<p>3.3 Parameter Estimation
Since training labels may contain noise, it is in-
accurate to employ the back-propagation method
which uses the error between predicted and train-
ing labels as feedback for parameter estimation.
Thus, we turn the estimation of model parameters
WT and WF into a probabilistic problem. The
graphical representation is illustrated in Fig. 2.
</p>
<p>K
</p>
<p>nm
V
</p>
<p>M
</p>
<p>N
</p>
<p>K
</p>
<p>( )k
T
</p>
<p>W
( )k
n
S
</p>
<p>( )k
F
</p>
<p>W
( )k
m
A
</p>
<p>T
</p>
<p>FZ
</p>
<p>Figure 2: Probabilistic graphical representation.
</p>
<p>Firstly, we define W = {WT,WF} for sim-
plicity. Secondly, the parameter distribution is de-
termined by the Maximum A Posteriori (MAP)
principal: W∗ = arg maxW Pr(W|V,T,F) =
arg maxW
</p>
<p>∑
Z Pr(Z)Pr(W|V,T,F,Z).
</p>
<p>Finally, the following Expectation Maximiza-
tion (EM) algorithm is used to estimate W∗.
</p>
<p>Initialization: We first initialize W randomly.
The prior of ground truth Z can be set to 1/C or
the frequency of each observable label.
</p>
<p>Expectation (E): We then compute the expecta-
tion of the joint log-likelihood of observable and
hidden variables given W (i.e., the Q function),
as follows: Q(W) = E[lnPr(V,Z,T,F|W)] =
E[lnPr(V|Z,T,F,W)]+E[lnPr(Z,T,F|W)].
</p>
<p>Maximization (M): According to the Q func-
tion, the maximum likelihood of hidden variables
is estimated by the gradient ascent method.
</p>
<p>Alternation: The above E and M steps are alter-
nately performed until the likelihood converges.
</p>
<p>4 Experiments
</p>
<p>4.1 Datasets and Baselines
</p>
<p>As sentiment and emotion detection are widely
studied in social media analysis (Wang and Pal,
2015), we test model performance based on the
Stanford Twitter Sentiment (STS) and the Interna-
tional Survey on Emotion Antecedents and Reac-
tions (ISEAR) corpus. The original STS dataset
(Go et al., 2009) contains 1.6 million tweets that
were automatically labeled as positive or negative
using emoticons as labels, in which 80K (5%)
randomly selected tweets were used to speed up
the training process, 16K (1%) randomly select-
ed tweets were used as the validation set, and
359 tweets were manually annotated as the test-
ing set (dos Santos and Gatti, 2014). ISEAR is
composed of 7, 666 sentences annotated by 1, 096
participants with different culture backgrounds
(Scherer and Wallbott, 1994). These participants
completed questionnaires about their 34 kinds of
personal information (e.g., age, gender, city, coun-
try, and religion), as well as their experiences and
reactions over seven emotions. For the ISEAR
corpus, we randomly selected 60% of sentences
as the training set, 20% as the validation set, and
the remaining 20% as the testing set.
</p>
<p>We use the following models for comparison:
Majority Voting (MV) (Sheng et al., 2008), Maxi-
mum Likelihood Estimator (MLE) (Raykar et al.,
2010), and Generative model of Labels, Abilities
and Difficulties (GLAD) (Whitehill et al., 2009).
The baselines of MV and MLE are implement-
ed by following (Sheng et al., 2008; Raykar et al.,
2010), and GLAD is run by the software that is
available in public at (Whitehill et al., 2009). We
</p>
<p>486</p>
<p />
</div>
<div class="page"><p />
<p>also implement the multivariate version of GLAD,
called MGLAD as the baseline for the ISEAR
corpus with seven emotions. Although there are
some more recent models on label aggregation
(Oyama et al., 2013) or refinement (Song et al.,
2015; Ustinovskiy et al., 2016), they either require
additional features like users’ reported confidence
scores, or are only suitable to a corpus with one
label for each document. To compare sentiment
and emotion classification performance using the
aggregated labels for training, we further apply
the above noisy label aggregation models to a lin-
ear Support Vector Machine (SVM) with squared
hinge loss (Chang and Lin, 2011). As shown in
the existing studies with refined labels, the lin-
ear SVM performed well on sentiment classifi-
cation of reviews (Pang et al., 2002) and tweets
(Vo and Zhang, 2015).
</p>
<p>4.2 Experimental Design
</p>
<p>To evaluate the performance of noisy label ag-
gregation models, each instance should be anno-
tated by multiple users. Unlike previous studies
which introduced a parameter to disturb ground
truth labels (Sheng et al., 2008) or employed
online crowdsourcing systems (Whitehill et al.,
2009; Raykar et al., 2010) to generate noisy an-
notations, we design a new simulation approach
by following the process of Profile Injection
Attack in Collaborative Recommender Systems
(Williams and Mobasher, 2006). This is because
the existing methods can not assign multiple labels
to each instance, or are difficult to generate virtual
users and access their information (e.g., age and
gender). In particular, the following steps have
been performed. First, we generate virtual user-
s with different features, making them the neigh-
bors of existing (actual) annotators. For each di-
mension of the actual annotators’ features, we take
the mean value if the attribute is continuous. For
discrete attributes, we randomly select one type
from the existing attribute values. If the dataset
has no user features, we set it as a unit vector.
Second, we generate document annotating vectors
for virtual users. Each annotating vector is com-
posed of three parts: annotating for filler instances
(IF ), which is a set of randomly chosen filler in-
stances drawn from the whole dataset, untagged
instances (I∅), and the target instance (it). The
purpose of setting IF and I∅ is to make the vir-
tual user looks like an ordinary annotator. We
</p>
<p>select three simulation types from Profile Injec-
tion Attack (Williams and Mobasher, 2006), i.e.,
random, average, and love/hate. In the random
method, the label for each instance i ∈ IF is
drawn from a normal distribution around the an-
notations across the whole dataset, and the prob-
ability of labeling correctly to i is 1/C. The cor-
responding probabilities are 0.5 and 1 for the av-
erage and love/hate methods, respectively. In all
these methods, the annotation for it is randomly
selected from wrong labels.
</p>
<p>We tune the number of topics D and annota-
tor features U by performing a grid search over
all D and U values, with D ∈ {2, 3, 4, ..., 10}
on both datasets, U = 34 on ISEAR, and U ∈
{1, 10, 100, 500, 1000} on STS that contains user
ID only. The value of K is set to the maximum of
D and U . Based on the performance on the vali-
dation set, we set D = 6, U = 1000,K = 1000
for STS, and D = 2, U = 34,K = 34 for ISEAR.
For the sum of |IF | and |it| (i.e., attack size) for
each virtual user, we set it as the mean number
of annotations in actual users. The sum of se-
lecting it in each simulation is called the profile
size, and the percentage of the profile size is de-
noted as o. Following the previous criterion of
choosing the noise rate (Auer and Cesa-Bianchi,
1998), we set o ∈ {0.05, 0.1, 0.2, 0.5}. Ac-
cording to (Ustinovskiy et al., 2016), each target
instance except for those in IF is annotated by
three users. Thus, the number of virtual users is
set to 2oN . We set the parameter values of MV,
MLE, and M/GLAD according to (Sheng et al.,
2008; Raykar et al., 2010; Whitehill et al., 2009),
and apply the grid search method to obtain the op-
timal parameters for SVM.
</p>
<p>4.3 Results and Analysis
</p>
<p>Firstly, we evaluate the noisy label aggregation
performance of different models by comparing the
proportion of estimated labels which match the ac-
tual categories (i.e., accuracy). The results are
shown in Fig. 3, which indicates that our model
performs the best under various conditions. From
the aspect of simulation methods, the accuracy of
the random one is the lowest and the Love/Hate
one is the highest, which is consistent to the cor-
rectly labeling probability for each method. The
results of the random and average ones over STS
are similar, because C = 2 on STS.
</p>
<p>Particularly, our model performs better than
</p>
<p>487</p>
<p />
</div>
<div class="page"><p />
<p>(a) Random over STS (b) Random over ISEAR
</p>
<p>(c) Average over STS (d) Average over ISEAR
</p>
<p>(e) Love/Hate over STS (f) Love/Hate over ISEAR
</p>
<p>Figure 3: Label aggregation performance.
</p>
<p>baselines in aggregating noisy labels, especially
when the noise scale becomes large. For instance,
our model achieves 85% and 57% accuracies on
STS and ISEAR when using the random method
and o = 0.5, which indicates that our model has
higher capability of recognizing adversarial noise
(it). In the random method, we can also observe
that the performance differences are more signifi-
cant on ISEAR than STS. This is because ISEAR
has more elaborate, i.e., 34 kinds of observable
user information, which validates the joint influ-
ence of users and documents on noisy label aggre-
gation. To evaluate the performance differences
statistically, we use the 12 groups of results over
all methods and o values based on the convention-
al significance level (i.e., p value) of 0.05. The
p values of t-tests between our model and MV,
M/GLAD, MLE are 0.0087, 0.0009, 0.0067 over
STS, and 0.0535, 0.1037, 0.0007 over ISEAR,
which indicates that the performance differences
between our model and baselines are statistically
significant on both datasets, except for MV and
MGLAD in the love/hate method over ISEAR.
The reason may be that each virtual user annotates
around seven instances on ISEAR, and only one
label is incorrect for the love/hate method, which
makes the simple MV perform competitively.
</p>
<p>Secondly, we compare the classification perfor-
</p>
<p>(a) Random over STS (b) Random over ISEAR
</p>
<p>(c) Average over STS (d) Average over ISEAR
</p>
<p>(e) Love/Hate over STS (f) Love/Hate over ISEAR
</p>
<p>Figure 4: Classification performance.
</p>
<p>mance of SVM using labels from different noisy
label aggregation models for training. The accura-
cies are shown in Fig. 4, in which dotted lines rep-
resent results on benchmark datasets without con-
ducting the Profile Injection Attack process. Com-
pared to other methods, the performance of SVM
based on the aggregated labels from our model is
almost closer to that of SVM using benchmark
datasets. For the average method and o = 0.2
over STS, we can observe that SVM in conjunc-
tion with our model performs even better than that
on the benchmark dataset. This is because emoti-
cons are used as annotations for STS, which may
introduce errors to the original labels.
</p>
<p>5 Conclusions
</p>
<p>In this paper, we proposed a network frame-
work for noisy label aggregation by calculating
the matching degree of documents and annotators.
Experiments using a new simulation method of
generating noisy labels validated the effectiveness
of the proposed framework. As our model is linear
in feature transformation, it is flexible to handle
large-scale datasets. In the future, we plan to com-
pare the model performance using different topic
models, improve our model by exploiting the feed-
back of a small proportion of refined labels, and
recruit actual participants to provide noisy labels.
</p>
<p>488</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgments
</p>
<p>The authors are thankful to the reviewers for their
constructive comments and suggestions on this pa-
per. The work described in this paper was support-
ed by the National Natural Science Foundation
of China (61502545), a grant from the Research
Grants Council of the Hong Kong Special Admin-
istrative Region, China (UGC/FDS11/E03/16), the
Start-Up Research Grant (RG 37/2016-2017R),
and the Internal Research Grant (RG 66/2016-
2017) of The Education University of Hong Kong.
</p>
<p>References
P. Auer and N. Cesa-Bianchi. 1998. On-line learning
</p>
<p>with malicious noise and the closure algorithm. An-
nals of Mathematics and Artificial Intelligence 23(1-
2):83–99.
</p>
<p>C.-C. Chang and C.-J. Lin. 2011. LIBSVM: A li-
brary for support vector machines. Journal of ACM
Transactions on Intelligent Systems and Technology
2(3):27:1–27:27.
</p>
<p>C.N. dos Santos and M. Gatti. 2014. Deep convo-
lutional neural networks for sentiment analysis of
short texts. In Proceedings of the 25th Internation-
al Conference on Computational Linguistics (COL-
ING). pages 69–78.
</p>
<p>A. Go, R. Bhayani, and L. Huang. 2009. Twitter sen-
timent classification using distant supervision. C-
s224n Project Report .
</p>
<p>B. Nicholson, J. Zhang, V.S. Sheng, and Z. Wang.
2015. Label noise correction methods. In IEEE
International Conference on Data Science and Ad-
vanced Analytics (DSAA). pages 1–9.
</p>
<p>S. Oyama, Y. Baba, Y. Sakurai, and H. Kashima. 2013.
Accurate integration of crowdsourced labels using
workers’ self-reported confidence scores. In Pro-
ceedings of the 23rd International Joint Conference
on Artificial Intelligence (IJCAI). pages 2554–2560.
</p>
<p>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumb-
s up? sentiment classification using machine learn-
ing techniques. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP). pages 79–86.
</p>
<p>V.C. Raykar, S. Yu, L.H. Zhao, G.H. Valadez, C. Florin,
L. Bogoni, and L. Moy. 2010. Learning from crowd-
s. Journal of Machine Learning Research 11:1297–
1322.
</p>
<p>K.R. Scherer and H.G. Wallbott. 1994. Evidence for
universality and cultural variation of differential e-
motion response patterning. Journal of Personality
&amp; Social Psychology 66(2):310–328.
</p>
<p>V.S. Sheng, F. Provost, and P.G. Ipeirotis. 2008. Get
another lable? improving data quality and data min-
ing using multiple, noisy labelers. In Proceedings of
the 14th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (SIGKDD).
pages 614–622.
</p>
<p>K. Song, W. Gao, L. Chen, S. Feng, D. Wang, and
C. Zhang. 2016. Build emotion lexicon from the
mood of crowd via topic-assisted joint non-negative
matrix factorization. In Proceedings of the 39th
International ACM SIGIR conference on Research
and Development in Information Retrieval (SIGIR).
pages 773–776.
</p>
<p>Y. Song, C. Wang, M. Zhang, H. Sun, and Q. Yang.
2015. Spectral label refinement for noisy and miss-
ing text labels. In Proceedings of the 29th AAAI
Conference on Artificial Intelligence (AAAI). pages
2972–2978.
</p>
<p>Y. Ustinovskiy, V. Fedorova, G. Gusev, and
P. Serdyukov. 2016. An optimization framework for
remapping and reweighting noisy relevance labels.
In Proceedings of the 39th International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR). pages 105–114.
</p>
<p>D. Vo and Y. Zhang. 2015. Target-dependent twit-
ter sentiment classification with rich automatic fea-
tures. In Proceedings of the 24th International Joint
Conference on Artificial Intelligence (IJCAI). pages
1347–1353.
</p>
<p>Y. Wang and A. Pal. 2015. Detecting emotions in so-
cial media: A constrained optimization approach. In
Proceedings of the 24th International Joint Confer-
ence on Artificial Intelligence (IJCAI). pages 996–
1002.
</p>
<p>J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J.R.
Movellan. 2009. Whose vote should count more:
Optimal integration of labels from labelers of un-
known expertise. In Proceedings of the 23rd Annual
Conference on Neural Information Processing Sys-
tems (NIPS). pages 2035–2043.
</p>
<p>C.A. Williams and B. Mobasher. 2006. Thesis: Profile
injection attack detection for securing collaborative
recommender systems. Service Oriented Computing
&amp; Applications 1(3):157–170.
</p>
<p>R. Xu, T. Chen, Y. Xia, Q. Lu, B. Liu, and X. Wang.
2015. Word embedding composition for data imbal-
ances in sentiment and emotion classification. Cog-
nitive Computation 7(2):226–240.
</p>
<p>X. Yan, J. Guo, Y. Lan, and X. Cheng. 2013. A biterm
topic model for short texts. In Proceedings of the
22nd International Conference on World Wide Web
(WWW). pages 1445–1456.
</p>
<p>J. Zhang, X. Wu, and V.S. Sheng. 2013. Imbalanced
multiple noisy labeling for supervised learning. In
Proceedings of the 27th AAAI Conference on Artifi-
cial Intelligence (AAAI). pages 1080–1085.
</p>
<p>489</p>
<p />
</div>
<div class="page"><p />
<p>A ISEAR’s Annotator Features
</p>
<p>The ISEAR corpus contains 34 kinds of personal
information of participants. For clarity, the total
set of annotator features is given below.
</p>
<p>• Subject’s backgrounds: (1) city, (2) Country,
(3) ID suffix, (4) gender, (5) age, (6) reli-
gion, (7) practising religion, (8) father’s job,
(9) mother’s job, and (10) field of study.
</p>
<p>• Questionnaire: (11) when did the situation or
event happen? (12) how long did you feel the
emotion? (13) how intense was this feeling?
</p>
<p>• Physiological symptoms of participants: (14)
ergotropic arousal, (15) trophotropic arousal,
and (16) felt temperature.
</p>
<p>• Expressive behavior and other features of
participants: (17) movement behavior, (18)
laughing or smiling, (19) crying or sobbing,
(20) nonverbal activity, (21) paralinguistic
activity, (22) verbal activity, (23) moving a-
gainst people or things, aggression, (24) did
you expect the situation or event that caused
your emotion to occur? (25) did you try to
hide or to control your feelings so that no-
body would know how you really felt? (26)
did you find the event itself pleasant or un-
pleasant? (27) would you say that the situ-
ation or event that caused your emotion was
unjust or unfair? (28) did the event help or
hinder you to follow your plans or to achieve
your aims? (29) who do you think was re-
sponsible for the event in the first place? (30)
how did you evaluate your ability to act on or
to cope with the event and its consequences
when you were first confronted with this sit-
uation? (31) if the event was caused by your
own or someone else’s behavior, would this
behavior itself be judged as improper or im-
moral by your acquaintances? (32) how did
this event affect your feelings about yourself,
such as your self-esteem or your self confi-
dence? (33) how did this event change your
relationships with the people involved? and
(34) the “NEUTRO” attribute.
</p>
<p>B Noisy Label Aggregation Algorithm
</p>
<p>In our method of noisy label aggregation as
shown in Algorithm 1, the cost of calculating S
and A by FcO (line 6) is linear to the number of
</p>
<p>Algorithm 1 Noisy Label Aggregation
Input:
</p>
<p>V: Observable labels;
F: Features of users;
ω: Words of documents;
δ: Threshold of convergence.
</p>
<p>Output:
Aggregated labels.
</p>
<p>1: T← TE(ω);
2: Initialize parameter W randomly;
3: Q← 0;
4: repeat
5: lastQ← Q;
6: {S,A} ← FcO(W, T, F);
7: for each i ∈ [1, N ] do
8: for each j ∈ [1,M ] do
9: gij = MDC(Si,Aj);
</p>
<p>10: V newij = WT (gij , Vij , sigmoid);
11: end for
12: ZiC = LW(Vnewi );
13: end for
14: Q← E-Step(ZiC);
15: W←M-Step(Q, W);
16: until |Q - lastQ| &lt; δ;
17: return Zi, i.e., the maximum one of ZiC .
</p>
<p>instances, i.e., O(NDK), and the total number
of users, i.e., O(MUK), respectively. Before the
EM iteration (lines 7 to 13), it takes O(NM(K +
C)) to weigh all labels V. For each iteration of
EM (lines 14 to 15), the optimization with stochas-
tic gradient descent takes O(NMC+NK+MK)
when each user annotates all documents. Assume
that our algorithm converges after t iterations (t &lt;
10 in our experiments), the overall time complex-
ity is O(NM(K + C)t), which is linear to the
numbers of instances and users.
</p>
<p>C Gradient Derivation
</p>
<p>Given the estimated value of ZiC , the Q function
can be calculated by Q(W) =
</p>
<p>∑
ij ZiC lnV
</p>
<p>new
ij +
</p>
<p>const. Since the vector V newij has two possible
values when using sigmoid (i.e., wij and 1 −
wij), the gradient of lnV newij on parameter W
</p>
<p>i,k
T
</p>
<p>is (Vij−wij)Ajk, i.e., [wij(1−wij)]/wijAjk and
[−wij(1−wij)]/(1−wij)Ajk, respectively. Then,
the gradient of Q on parameter W i,kT can be de-
rived as ∂Q/∂W i,kT =
</p>
<p>∑
j ZiC(Vij − wij)Ajk.
</p>
<p>Similarly, the gradient of Q on parameter W j,kF is
given by ∂Q/∂W j,kF =
</p>
<p>∑
i ZiC(Vij − wij)Sik.
</p>
<p>490</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 491–497
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2078
</p>
<p>Parser Adaptation for Social Media by Integrating Normalization
</p>
<p>Rob van der Goot
University of Groningen
</p>
<p>r.van.der.goot@rug.nl
</p>
<p>Gertjan van Noord
University of Groningen
</p>
<p>g.j.m.van.noord@rug.nl
</p>
<p>Abstract
</p>
<p>This work explores normalization for
parser adaptation. Traditionally, normal-
ization is used as separate pre-processing
step. We show that integrating the nor-
malization model into the parsing algo-
rithm is beneficial. This way, multiple nor-
malization candidates can be leveraged,
which improves parsing performance on
social media. We test this hypothesis
by modifying the Berkeley parser; out-of-
the-box it achieves an F1 score of 66.52.
Our integrated approach reaches a signif-
icant improvement with an F1 score of
67.36, while using the best normalization
sequence results in an F1 score of only
66.94.
</p>
<p>1 Introduction
</p>
<p>The non-canonical language use on social media
introduces many difficulties for existing NLP
models. For some NLP tasks, there has already
been an effort to annotate enough data to train
models, e.g. named entity recognition (Baldwin
et al., 2015), sentiment analysis (Nakov et al.,
2016) and paraphrase detection (Xu et al., 2015).
For parsing social media texts, such a resource is
not available yet, although there are some small
treebanks that can be used for development/testing
purposes (Foster et al., 2011; Kong et al., 2014;
Kaljahi et al., 2015; Daiber and van der Goot,
2016). To the best of our knowledge, the only
treebank big enough to train a supervised parser
for user generated content is the English Web
Treebank (Petrov and McDonald, 2012). This
treebank consists of constituency trees from five
different web domains, not including the domain
of social media.
</p>
<p>0 1 2 3
</p>
<p>this (0.5)
</p>
<p>ths (0.3)
</p>
<p>thus (0.2)
</p>
<p>as (0.5)
</p>
<p>is (0.4)
</p>
<p>s (0.1)
</p>
<p>nice (0.7)
</p>
<p>nive (0.2)
</p>
<p>rice (0.1)
</p>
<p>Figure 1: A possible output of the normalization
model for the sentence ‘ths s nice’.
</p>
<p>The magnitude of domain adaptation prob-
lems for the social media domain becomes clear
when training the Berkeley parser on newswire
text, and comparing its in-domain performance
with performance on the Twitter domain. The
Berkeley parser achieves an F1 score above 90
on newswire text (Petrov and Klein, 2007). An
empirical experiment that we carried out on a
Twitter treebank shows that the F1 score drops
below 70 for this domain.
</p>
<p>Annotating a new training treebank for this do-
main would not only be an expensive solution, the
ever-changing nature of social media makes this
approach less effective over time. We propose an
approach in which we integrate normalization into
the parsing model. The normalization model pro-
vides the parser with different normalization can-
didates for each word in the input sentence. Exist-
ing algorithms can then be used to find the optimal
parse tree over this lattice (Bar-Hillel et al., 1961).
A possible normalization lattice for the sentence
‘this is nice’ is shown in Figure 1. In this example
output, the probability of ‘as’ is higher than the
probability of ‘is’, whereas the most fluent word
sequence would be ‘this is nice’. The parser can
disambiguate this word graph because it has ac-
cess to the syntactic context: ‘is’ is usually tagged
as VBZ, while ‘as’ is mostly tagged as IN. This ex-
ample shows the main motivation for using an in-
tegrated approach; the extra information from the
normalization can be useful for parsing.
</p>
<p>491</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2078">https://doi.org/10.18653/v1/P17-2078</a></div>
</div>
<div class="page"><p />
<p>2 Related Work
</p>
<p>SANCL 2012 hosted a shared task on parsing the
English Web Treebank (EWT) (Petrov and Mc-
Donald, 2012). A wide variety of different ap-
proaches were used: ensemble parsers, product
grammars, self/up-training, word clustering, genre
classification and normalization. The teams that
used normalization often used simple rule-based
systems, and the actual effect of normalization on
the final parser performance was not tested. Fos-
ter (2010) experiment with rule-based normaliza-
tion on forum data in isolation and report a perfor-
mance gain of 2% in F1 score.
</p>
<p>A theoretical exploration of the effect of nor-
malization on forum data is done by Kaljahi et al.
(2015). They released the Foreebank, a treebank
consisting of forum texts, annotated with normal-
ization and constituency trees. They show that
parsing manually normalized sentences results in
a 2% increase of F1 score. Baldwin and Li (2015)
evaluate the effect of different normalization ac-
tions on dependency parsing performance for the
social media domain. They conclude that a vari-
ety of different normalization actions is useful for
parsing.
</p>
<p>A more practical exploration of the effect of
normalization for the social media domain is done
by Zhang et al. (2013). They test the effect of
automatic normalization on dependency parsing
by using automatically derived parse trees of the
normalized sentences as reference. Other work
that uses automatic normalization is Daiber and
van der Goot (2016), which compare the effect of
lexical normalization with machine translation on
a manually annotated dependency treebank. All
previous work uses only the best normalization se-
quence; errors in this pre-processing step are di-
rectly propagated to the parser.
</p>
<p>For POS tagging, however, a joint approach is
proposed by Li and Liu (2015). They use the n-
best output of different normalization systems to
generate a Viterbi encoding, based on all possible
pairs of normalization candidates and their possi-
ble POS tags. Using this joint approach, they im-
prove on both POS tagging and normalization.
</p>
<p>3 Method
</p>
<p>We first describe how an existing normalization
model is modified for this specific use. Then we
discuss how we integrate this normalization into
the parsing model.
</p>
<p>3.1 Normalization
</p>
<p>We use an existing normalization model (van der
Goot, 2016). This model generates candidates us-
ing the Aspell spell checker1 and a word embed-
dings model trained on Twitter data (Godin et al.,
2015). Features from this generation are comple-
mented with n-gram probability features of canon-
ical text (Brants and Franz, 2006) and the Twit-
ter domain. A random forest classifier (Breiman,
2001) is exploited for the ranking of the generated
candidates.
</p>
<p>Van der Goot (2016) focused on finding the cor-
rect normalization candidate for erroneous tokens,
gold error detection was assumed. Therefore, the
model was trained only on the words that were
normalized in the training data. Since we do not
know in advance which words should be normal-
ized, we can not use this model. Instead, we train
the model on all words in the training data, includ-
ing words that do not need normalization. Accord-
ingly, we add the original token as a normalization
candidate and add a binary feature to indicate this.
These adaptations enable the model to learn which
words should be normalized.
</p>
<p>We compare the traditional approach of only us-
ing the best normalization sequence with an inte-
grated approach, in which the parsing model has
access to multiple normalization candidates for
each word. Within the integrated approach, we
compare normalizing only the words unknown to
the parser against normalizing all words. We re-
fer to these approaches as ‘UNK’ and ‘ALL’, re-
spectively. Figure 1 shows a possible output when
using ALL. When using UNK, the word ‘nice’
would not have any normalization candidates.
</p>
<p>3.2 Parsing
</p>
<p>We adapt the state-of-the-art PCFG Berkeley
Parser (Petrov and Klein, 2007) to fit our needs.
The main strength of this PCFG-LA parser is that
it automatically learns to split constituents into
finer categories during training, and thus learns a
more refined grammar than a raw treebank gram-
mar. It maintains efficiency by using a coarse-to-
fine parsing setup. Unknown words are clustered
by prefixes, suffixes, the presence of special char-
acters or capitals and their position in the sentence.
</p>
<p>Parsing word lattices is not a new problem.
The parsing as intersection algorithm (Bar-Hillel
et al., 1961) laid the theoretical background for ef-
</p>
<p>1www.aspell.net
</p>
<p>492</p>
<p />
</div>
<div class="page"><p />
<p>ficiently deriving the best parse tree of a word lat-
tice given a context-free grammar. Previous work
on parsing a word lattice in a PCFG-LA setup in-
cludes Constant et al. (2013), and Goldberg and
Elhadad (2011) for the Berkeley Parser. However,
these models do not support probabilities, which
are naturally provided by the normalization in our
setup. Another problem is the handling of word
ambiguities, which is crucial in our model.
</p>
<p>Our adaptations to the Berkeley Parser resem-
ble the adaptations done by Goldberg and Elhadad
(2011). In addition, we allow multiple words on
the same position. For every POS tag in every
position we only keep the highest scoring word.
This suffices, since there is no syntactic ambiguity
possible with only unary rules from POS tags to
words, and therefore it is impossible for the lower
scoring words to end up in the final parse tree.
</p>
<p>To incorporate the probability from the normal-
ization model (Pnorm) into the chart, we combine
it with the probability from the POS tag assigned
by the built-in tagger of the Berkeley parser (Ppos)
using the weighted harmonic mean (Rijsbergen,
1979):
</p>
<p>Pchart = (1 + β
2) ∗ Pnorm ∗ Ppos
</p>
<p>(β2 ∗ Pnorm) + Ppos
(1)
</p>
<p>Here, β is the relative weight we give to the nor-
malization and Pchart is the probability used in the
parsing chart. We use this formula because it al-
lows us to have a weighted average, in which we
reward the model if both probabilities are more
balanced.
</p>
<p>4 Data
</p>
<p>The normalization model we use is supervised, i.e.
it needs annotated training data from the target do-
main. This is readily available for Twitter; we use
2,000 manually normalized Tweets from Li and
Liu (2014) as training data.
</p>
<p>We use the treebank from Foster et al. (2011)
as develop and test data for our parser. It com-
prises 269 trees for developing and 250 trees for
testing, all annotated using the annotation guide-
lines for the Penn Treebank (Bies et al., 1995)
with some small adaptations for the Twitter do-
main (usernames, hashtags and urls are annotated
as an NNP under an NP). For training, we use
the English Web Treebank (EWT) concatenated
with the standard training sections (2-21) of the
Wall Street Journal (WSJ) part of the Penn tree-
bank (Marcus et al., 1993).
</p>
<p>Corpus Sents Words/ Unk%
sent
</p>
<p>WSJ (2-21) 39,832 23.9 4.4
EWT 16,520 15.3 3.7
Foster et al. (2011)* 269 11.1 9.3
Li and Liu (2014) 2,577 15.7 14.1
</p>
<p>Table 1: Some basic statistics for our training and
development corpora. % of unknown words (Unk)
calculated against the Aspell dictionary ignoring
capitalization. *Only the development part.
</p>
<p>Some basic statistics of our training and devel-
opment data can be found in Table 1. Perhaps sur-
prisingly, the percentage of unknown words in the
EWT is lower than in the WSJ. This can be ex-
plained by the fact that the WSJ texts contains lots
of jargon and named entities which are not present
in the Aspell dictionary. The difference in per-
centage of unknown words between the normal-
ization training data and the development treebank
data might be an obstacle at first sight, but this
can be overcome by tuning the weight (β) when
combining the normalization and parse probabili-
ties (Equation 1). Nevertheless, the effect of nor-
malization will be smaller when there is less noise
in the data.
</p>
<p>5 Results
</p>
<p>The parser is evaluated using the F1 score as im-
plemented by EVALB2. All results in this section
are averaged over 10 runs, using different seeds for
the normalization model, unless mentioned other-
wise.
</p>
<p>The performance of our model depends on two
parameters: the number of normalization candi-
dates per word α and the weight given to the
normalization β. We tuned these parameters on
the development data using α ∈ [1-10] and β ∈
[0.125, 0.25, 0.5, 1, 2, 4, 8, 16] to find the optimal
values. The best performance is achieved using
α = 6 and β = 2. From this optimal setting, we
will compare the effects of these variables for both
the UNK and the ALL normalization strategies.
</p>
<p>Figure 2 shows the effect of using different
numbers of candidates and our baseline: the
vanilla Berkeley parser. Using only the single best
normalization sequence (α = 1) we can obtain
an improvement of 1.7% when normalizing all to-
kens. If we only normalize the unknown tokens
</p>
<p>2nlp.cs.nyu.edu/evalb
</p>
<p>493</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: F1 scores on the development data when
using multiple candidates while normalizing ALL
words or only the UNKnown words (beta = 2),
compared to a VANilla Berkeley parser.
</p>
<p>the performance is slightly worse, but it still out-
performs the baseline.
</p>
<p>If we use more normalization candidates, per-
formance increases; it converges around α = 6.
At this optimal setting, the baseline is outper-
formed by 2.2%. However, if more than only the
first candidate is used, it is not beneficial to nor-
malize all words anymore. This is probably an
effect of creating too much distance between the
original sentence and the normalization. The F1
score converges for higher number of candidates,
because lower ranked candidates have very low
normalization probabilities and are thus unlikely
to affect the final parse.
</p>
<p>The normalization model seldomly finds a cor-
rect candidate beyond α &gt; 2, at α = 2 the recall
for unknown words is 89.4% on the LexNorm cor-
pus (Han and Baldwin, 2011), whereas the accu-
racy at α = 6 is 91.7%. Perhaps surprisingly, the
parser performance still improves when increasing
α. Manual evaluation reveals that these improve-
ments are obtained by using incorrect normaliza-
tion candidates. Because these normalization can-
didates share some syntactic properties with the
original word, they can still help in deriving a
better syntactic parse. Figure 3 shows an exam-
ple of this phenomenon; “Bono” is normalized to
Bono’s, and is therefore tagged as an NNS, even
though this tag is still not correct, the head gets
tagged correctly as NP. Combined with the nor-
malization of “NOT”, this results in a much better
parse tree.
</p>
<p>Table 2 shows the results using different
</p>
<p>SBARQ
</p>
<p>.
</p>
<p>!
</p>
<p>SQ
</p>
<p>VP
</p>
<p>VB
</p>
<p>god
</p>
<p>NP
</p>
<p>NNP
</p>
<p>NOT
</p>
<p>VBZ
</p>
<p>is
</p>
<p>WHADVP
</p>
<p>WRB
</p>
<p>Bono
</p>
<p>S
</p>
<p>.
</p>
<p>!
</p>
<p>VP
</p>
<p>NP
</p>
<p>NN
</p>
<p>god
</p>
<p>RB
</p>
<p>not
</p>
<p>VBZ
</p>
<p>is
</p>
<p>NP
</p>
<p>NNS
</p>
<p>Bono’s
</p>
<p>Figure 3: An example parse from the development
corpus; left is the output of the vanilla Berkeley
parser, right is the output with the integrated nor-
malization.
</p>
<p>weights. We compare the non-integrated approach
(α = 1) with the optimal number of candidates
(α = 6). The best results are achieved when β
is 2, meaning that the normalization should get a
higher weight than the POS tagger. The integrated
model scores higher with almost all weights, the
difference between ALL and UNK is similar as in
Figure 2.
</p>
<p>For the test data, we use the parameter settings
that performed best on the development treebank
(UNK, α = 6, β = 2), and the best performing
seed for the normalization model. The results on
the test data are compared to the traditional ap-
proach of only using the best normalization se-
quence, the vanilla Berkeley parser, and the Stan-
ford PCFG parser (Petrov and Klein, 2007) in Ta-
ble 3. The integrated approach significantly out-
performs the Berkeley parser as well as the tradi-
tional approach. It becomes apparent that the test
part of the treebank is more difficult than the de-
velopment part. Although the increase is smaller,
</p>
<p>Cands(α) 1 6
Weight(β) UNK ALL UNK ALL
0.125 70.79 71.12 70.88 69.45
0.25 70.79 71.12 70.99 62.97
0.5 70.86 71.18 71.21 70.51
1.0 71.19 71.52 71.78 71.08
2.0 71.77 72.10 72.21 71.46
4.0 71.73 72.01 72.02 71.43
8.0 71.12 71.33 71.69 71.26
16.0 70.29 70.32 70.50 70.09
</p>
<p>Table 2: F1 scores on the development data using
different weights, comparing only using the best
candidate versus using 6 candidates.
</p>
<p>494</p>
<p />
</div>
<div class="page"><p />
<p>Parser dev test
</p>
<p>Stanford parser 66.05 61.95
Berkeley parser 70.85 66.52
</p>
<p>Best norm. seq. 72.04 66.94
Integrated norm. 72.77 67.36*
</p>
<p>Gold POS tags 74.98 71.80
</p>
<p>Table 3: F1 scores of our proposed models and
previous work on the test set, trained on the EWT
and WSJ. *Statistical significant against Berkeley-
parser at P &lt; 0.01 and at P &lt; 0.05 against the
best normalization sequence using a paired t-test.
</p>
<p>normalization still improves parser performance.
On the development set, 46% of the errors which
can be accounted to mistakes made by the POS
tagger are solved, whereas on the test set, we only
solve 16% of this theoretical upper bound.
</p>
<p>6 Discussion
</p>
<p>The addition of multiple words on one position
in the chart will probably lead to less pruning in
the Berkeley parser, because more constituents in
the tree will have a relatively high probability. To
test if performance improvements are simply an
effect of less pruning, we perform two additional
experiments. Firstly, we use the vanilla Berke-
ley parser with lower pruning thresholds3 on the
Twitter development treebank. This results in a
decrease in F1 score from 70.85 to 70.64, showing
that our normalization model has a different effect.
Secondly, we run our proposed parsing model on
the standard development part of the more canon-
ical WSJ data (section 24). The vanilla Berkeley
parser achieves an F1 score of 89.15, whereas our
best performing model scores 89.12 due to over-
normalization. This shows that our model does not
improve performance across all domains.
</p>
<p>To test the effect of the normalization on the
search space, we simply count the number of sur-
viving constituents in the chart in the middle and
final parse level. Results can be found in Ta-
ble 4. There is a slight increase in the number of
constituents when using normalization. A simi-
lar effect can be found for the parsing time; aver-
aged over 10 runs, the vanilla Berkeley parser took
24.3 seconds on the development set, whereas our
</p>
<p>3Tested by running the parser with --accurate. We
also tried to tune the thresholds even further manually, but
this had similar effects.
</p>
<p>Parse level Berkeley Integrated Norm.
</p>
<p>3 756 765
6 3,086 3,115
</p>
<p>Table 4: The average number of constituents in the
chart per sentence for the middle parsing level (3)
and the final level (6) on our development set.
</p>
<p>model took 24.5 seconds on the same machine.
</p>
<p>7 Conclusion
</p>
<p>We have shown that we can significantly improve
the parsing of out-of domain data by using nor-
malization. If we use normalization as a sim-
ple pre-processing step, we observe a small im-
provement in performance, while higher improve-
ments can be achieved by using an integrated ap-
proach. Improvements in parsing performance are
not only an effect of using correct normalization
candidates, but are also due to wrong normaliza-
tion candidates which share syntactic properties
with the original word. Additionally, we show that
when using only the best normalization sequence,
it is better to normalize all words instead of only
the unknown words. However, when using an in-
tegrated approach it is better to only consider un-
known words for normalization.
</p>
<p>Potential directions for future work include:
allowing multiword replacements, normalization
driven by the parsing model, and using lexical-
ized parsing so that the normalization candidates
are used for more decisions in the parsing process
than just assigning POS tags. To further improve
the F1-score for the parsing of Tweets, comple-
mentary methods can be used: reranking, uptrain-
ing or ensembling parsers and grammars are some
obvious next steps.
</p>
<p>The source code of our experiments has been
made publicly available 4.
</p>
<p>Acknowledgements
We would like to thank our colleagues, especially
Barbara Plank and Antonio Toral , and the anony-
mous reviewers for their valuable feedback. Fur-
thermore we would like to thank Jennifer Foster
for sharing the Twitter treebank. This work is
part of the Parsing Algorithms for Uncertain Input
project, funded by the Nuance Foundation.
</p>
<p>4https://bitbucket.org/robvanderg/
berkeleygraph
</p>
<p>495</p>
<p />
</div>
<div class="page"><p />
<p>References
Timothy Baldwin, Marie-Catherine de Marneffe,
</p>
<p>Bo Han, Young-Bum Kim, Alan Ritter, and
Wei Xu. 2015. Shared tasks of the 2015
workshop on noisy user-generated text: Twitter
lexical normalization and named entity recogni-
tion. In Proceedings of the Workshop on Noisy
User-generated Text. Association for Computa-
tional Linguistics, Beijing, China, pages 126–135.
http://www.aclweb.org/anthology/W15-4319.
</p>
<p>Tyler Baldwin and Yunyao Li. 2015. An in-depth
analysis of the effect of text normalization in so-
cial media. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Denver, Colorado, pages 420–
429. http://www.aclweb.org/anthology/N15-1045.
</p>
<p>Yehoshua Bar-Hillel, Micha Perles, and Eliahu Shamir.
1961. On formal properties of simple phrase struc-
ture grammars. Sprachtypologie und Universalien-
forschung 14:143–172.
</p>
<p>Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for Treebank II style Penn
Treebank project. Technical report, University of
Pennsylvania.
</p>
<p>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Technical report, Google.
</p>
<p>Leo Breiman. 2001. Random forests. Machine learn-
ing 45(1):5–32.
</p>
<p>Matthieu Constant, Joseph Le Roux, and Anthony Si-
gogne. 2013. Combining compound recognition and
pcfg-la parsing with word lattices and conditional
random fields. ACM Transactions on Speech and
Language Processing (TSLP) 10(3):8.
</p>
<p>Joachim Daiber and Rob van der Goot. 2016. The
denoised web treebank: Evaluating dependency
parsing under noisy input conditions. In Pro-
ceedings of the Tenth International Conference
on Language Resources and Evaluation (LREC
2016). European Language Resources Associa-
tion (ELRA), Paris, France. http://www.lrec-
conf.org/proceedings/lrec2016/pdf/86 Paper.pdf.
</p>
<p>Jennifer Foster. 2010. “cba to check the spelling”:
Investigating parser performance on discussion fo-
rum posts. In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, Los Angeles, California, pages 381–384.
http://www.aclweb.org/anthology/N10-1060.
</p>
<p>Jennifer Foster, Özlem Çetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef Van Genabith. 2011.
</p>
<p>#hardtoparse: POS Tagging and parsing the Twit-
terverse. In AAAI 2011 Workshop On Analyzing Mi-
crotext. United States, pages 20–25.
</p>
<p>Fréderic Godin, Baptist Vandersmissen, Wesley
De Neve, and Rik Van de Walle. 2015. Multimedia
Lab @ ACL WNUT NER shared task: Named entity
recognition for Twitter microposts using distributed
word representations. In Proceedings of the Work-
shop on Noisy User-generated Text. Association for
Computational Linguistics, Beijing, China, pages
146–153. http://www.aclweb.org/anthology/W15-
4322.
</p>
<p>Yoav Goldberg and Michael Elhadad. 2011. Joint
hebrew segmentation and parsing using a pcfgla
lattice parser. In Proceedings of the 49th
Annual Meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies. Association for Computational Linguis-
tics, Portland, Oregon, USA, pages 704–709.
http://www.aclweb.org/anthology/P11-2124.
</p>
<p>Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Portland, Oregon, USA, pages
368–378. http://www.aclweb.org/anthology/P11-
1038.
</p>
<p>Rasoul Kaljahi, Jennifer Foster, Johann Roturier,
Corentin Ribeyre, Teresa Lynn, and Joseph Le Roux.
2015. Foreebank: Syntactic analysis of customer
support forums. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 1341–1347.
http://aclweb.org/anthology/D15-1157.
</p>
<p>Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
Tweets. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 1001–1012.
http://www.aclweb.org/anthology/D14-1108.
</p>
<p>Chen Li and Yang Liu. 2014. Improving text normal-
ization via unsupervised model and discriminative
reranking. In Proceedings of the ACL 2014 Student
Research Workshop. Association for Computational
Linguistics, Baltimore, Maryland, USA, pages 86–
93. http://www.aclweb.org/anthology/P14-3012.
</p>
<p>Chen Li and Yang Liu. 2015. Joint POS tagging and
text normalization for informal text. In Proceedings
of the Twenty-Fourth International Joint Conference
on Artificial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015. pages 1263–1269.
http://ijcai.org/Proceedings/15/Papers/182.pdf.
</p>
<p>Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
</p>
<p>496</p>
<p />
</div>
<div class="page"><p />
<p>corpus of English: The Penn Treebank. Computa-
tional linguistics 19(2):313–330.
</p>
<p>Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 task 4: Sentiment analysis in Twitter. In Pro-
ceedings of the 10th International Workshop on Se-
mantic Evaluation (SemEval-2016). Association for
Computational Linguistics, San Diego, California,
pages 1–18. http://www.aclweb.org/anthology/S16-
1001.
</p>
<p>Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference. Association for Computational Lin-
guistics, Rochester, New York, pages 404–411.
http://aclweb.org/anthology/N07-1051.
</p>
<p>Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL). volume 59.
</p>
<p>CJ Rijsbergen. 1979. Information Retrieval, volume 2.
University of Glasgow.
</p>
<p>Rob van der Goot. 2016. Normalizing so-
cial media texts by combining word embed-
dings and edit distances in a random for-
est regressor. In Normalisation and Anal-
ysis of Social Media Texts (NormSoMe).
http://www.let.rug.nl/rob/doc/normsome2016.pdf.
</p>
<p>Wei Xu, Chris Callison-Burch, and Bill Dolan. 2015.
Semeval-2015 task 1: Paraphrase and semantic
similarity in twitter (pit). In Proceedings of the
9th International Workshop on Semantic Evalua-
tion (SemEval 2015). Association for Computa-
tional Linguistics, Denver, Colorado, pages 1–11.
http://www.aclweb.org/anthology/S15-2001.
</p>
<p>Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive
parser-centric text normalization. In Proceed-
ings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Sofia, Bulgaria, pages 1159–1168.
http://www.aclweb.org/anthology/P13-1114.
</p>
<p>497</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 498–503
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2079
</p>
<p>AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine
</p>
<p>Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan Chen,
Weipeng Zhao, Haiqing Chen, Jun Huang, Wei Chu
</p>
<p>Alibaba Group, Hangzhou, China
{minghui.qmh,fenglin.lfl}@alibaba-inc.com
</p>
<p>Abstract
</p>
<p>We propose AliMe Chat, an open-domain
chatbot engine that integrates the joint re-
sults of Information Retrieval (IR) and Se-
quence to Sequence (Seq2Seq) based gen-
eration models. AliMe Chat uses an at-
tentive Seq2Seq based rerank model to
optimize the joint results. Extensive ex-
periments show our engine outperforms
both IR and generation based models. We
launch AliMe Chat for a real-world indus-
trial application and observe better results
than another public chatbot.
</p>
<p>1 Introduction
</p>
<p>Chatbots have boomed during the last few years,
e.g., Microsoft’s XiaoIce, Apple’s Siri, Google’s
Google Assistant. Unlike traditional apps where
users interact with them through simple and struc-
tured language (e.g., “submit”, “cancel”, “order”,
etc.), chatbots allow users to interact with them us-
ing natural language, text or speech (even image).
</p>
<p>We are working on enabling bots to answer cus-
tomer questions in the E-commerce industry. Cur-
rently, our bot serves millions of customer ques-
tions per day (mainly Chinese, also some English).
The majority of them is business-related, but also
around 5% of them is chat-oriented (several hun-
dreds of thousands in number). To offer better
user experience, it is necessary to build an open-
domain chatbot engine.
</p>
<p>Commonly used techniques for building open-
domain chatbots include IR model (Ji et al., 2014;
Yan et al., 2016b) and generation model (Bah-
danau et al., 2015; Sutskever et al., 2014; Vinyals
and Le, 2015). Given a question, the former re-
trieves the nearest question in a Question-Answer
(QA) knowledge base and takes the paired an-
swer, the latter generates an answer based on a
</p>
<p>pre-trained Seq2Seq model. Often, IR models fail
to handle long-tail questions that are not close to
those in a QA base, and generation models may
generate inconsistent or meaningless answers (Li
et al., 2016; Serban et al., 2016).
</p>
<p>To alleviate these problems, we propose a hy-
brid approach that integrates both IR and gener-
ation models. In our approach, we use an atten-
tive Seq2Seq rerank model to optimize the joint
results. Specifically, for a question, we first use an
IR model to retrieve a set of QA pairs and use them
as candidate answers, and then rerank the candi-
date answers using an attentive Seq2Seq model: if
the top candidate has a score higher than a certain
threshold, it will be taken as the answer; otherwise
the answer will be offered by a generation based
model (see Fig. 1 for the detailed process).
</p>
<p>Our paper makes the following contributions:
</p>
<p>• We propose a novel hybrid approach that uses
an attentive Seq2Seq model to optimize the
joint results of IR and generation models.
</p>
<p>• We conducted a set of experiments to assess
the approach. Results show that our approach
outperforms both IR and generation.
</p>
<p>• We compared our chatbot engine with a pub-
lic chatbot. Evidence suggests that our en-
gine has a better performance.
</p>
<p>• We launched AliMe Chat for a real-world in-
dustrial application.
</p>
<p>The rest of the paper is structured as follows:
Section 2 presents our hybrid approach, followed
by experiments in Section 3, related work is in
Section 4, and Section 5 concludes our work.
</p>
<p>2 A Seq2Seq based Rerank Approach
</p>
<p>We present an overview of our approach in Fig. 1.
At first, we construct a QA knowledge base from
the chat log of our online customer service cen-
</p>
<p>498</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2079">https://doi.org/10.18653/v1/P17-2079</a></div>
</div>
<div class="page"><p />
<p>Question 
q 
</p>
<p>IR 
Candidates 
r1, r2, …, rk 
</p>
<p>o(r) = max o(ri) 
</p>
<p>Output 
(Answer) 
</p>
<p>Answer 
Generation 
</p>
<p>r’ 
</p>
<p>o(r) ≥ T 
</p>
<p>Answer 
Rerank 
</p>
<p>q - ri: o(ri) 
</p>
<p>Attentive  
Seq2Seq  
</p>
<p>Model 
</p>
<p>QA  
Knowledge  
</p>
<p>Base 
</p>
<p>r’ 
</p>
<p>r 
</p>
<p>Yes : r 
 
</p>
<p>No : r’ 
</p>
<p>Figure 1: Overview of our hybrid approach.
</p>
<p>ter. Based on this QA knowledge base, we then
develop three models: an IR model, a genera-
tion based model and a rerank model. There are
two points to be noted: (1) all the three mod-
els are based on words (i.e., word segmentation is
needed): the input features of IR model are words,
while those of generation model and rerank model
are word embeddings, which are pre-trained us-
ing fasttext (Bojanowski et al., 2016) and further
fine-tuned in the two models; (2) our generation
based model and rerank model are built on the
same Seq2Seq structure, the former is to generate
an output while the latter is to score the candidate
answers with regarding to an input question.
</p>
<p>Given an input question q and a threshold T , the
procedure of our approach is as follows:
</p>
<p>• First, we use the IR model to retrieve a set of
k candidate QA pairs 〈qkbi , ri〉ki=1 (k = 10).
• Second, we pair q with each candidate an-
</p>
<p>swer ri and calculate a confidence score
o(ri) = s(q, ri) for each pair using the scor-
ing function in Eqn. 2 of the rerank model.
</p>
<p>• Third, we consider the answer r with the
maximal score o(r) = max o(ri): if o(r) ≥
T , take the answer r; otherwise output a reply
r′ from the generation based model.
</p>
<p>Here, the threshold T is obtained through an
empirical study, to be discussed in Section 3.2.
</p>
<p>2.1 QA Knowledge Base
</p>
<p>We use the chat log of our online customer service
center between 2016-01-01 and 2016-06-01 as our
original data source (the conversations are taken
between customers and staff). We construct QA
pairs from conversations by pairing each question
with an adjacent answer. When needed, we flatten
consecutive questions (resp. answers) by concate-
</p>
<p>nating them. After that, we filter out QA pairs that
contain business related keywords. Finally, we ob-
tained 9,164,834 QA pairs.
</p>
<p>2.2 IR Model
</p>
<p>Our retrieval model employs search technique to
find the most similar question for each input and
then obtain the paired answer. With word segmen-
tation, we build an inverted index for the set of all
9,164,834 questions by mapping each word to a set
of questions that contain that word. Given a ques-
tion, we segment it into a set of words, remove
stop words, extend the set with their synonyms,
and use the refined set to call back a set of QA
candidate pairs. We then employ BM25 (Robert-
son et al., 2009) to calculate similarities between
the input question and the retrieved questions, and
take the paired answer of the most similar one as
the answer.
</p>
<p>2.3 Generation based Model
</p>
<p>Our generation based model is built on the atten-
tive Seq2Seq structure (Bahdanau et al., 2015).
</p>
<p>Let θi = {y1, y2, · · · , yi−1, ci}, the probability
of generating a word yi at position i is given by
Eqn. 1, where f is a nonlinear function that com-
putes the probability, si−1 is the hidden state of the
output at position i − 1, ci is a context vector that
depends on (h1, h2, · · · , hm), the hidden states
of the input sequence: ci =
</p>
<p>∑m
j=1 αijhj , αij =
</p>
<p>a(si−1, hj) is given by an alignment model that
scores how well the input at position j matches to
the output at i− 1 (Bahdanau et al., 2015). An ex-
ample is shown in Fig. 2, where i = 3 and m = 4.
</p>
<p>p(yi = wi|θi) = p(yi = wi|y1, y2, . . . , yi−1, ci)
= f(yi−1, si−1, ci) (1)
</p>
<p>499</p>
<p />
</div>
<div class="page"><p />
<p>+ 
</p>
<p>α3,1 α3,2 α3,3 
</p>
<p>Alignment 
Model 
</p>
<p>h1 h2 h3 h4 s1 s2 s3 s4 
</p>
<p>c3 
</p>
<p>不好意思
Sorry
</p>
<p>，
,
</p>
<p>刚 &lt;EOS&gt;有事
(I) have been busy
</p>
<p>亲 ， 没事的
</p>
<p>，
,
</p>
<p>没事的
that’s OK
</p>
<p>&lt;EOS&gt;
α3,4 
</p>
<p>亲
Hey
</p>
<p>Decoding the word “没事的”
</p>
<p>Figure 2: Attentive Seq2Seq model. Our model is mainly for Chinese.
</p>
<p>We choose Gated Recurrent Units (GRU) as our
Recurrent Neural Network (RNN) unit. A few im-
portant implementations are discussed below.
</p>
<p>Bucketing and padding. To handle questions
and answers of different lengths, we employ the
bucket mechanism proposed in Tensorflow 1. We
use five buckets (5, 5), (5, 10), (10, 15), (20, 30),
(45, 60) to accommodate QA pairs of different
length, e.g., a question of length 4 and an an-
swer of length 8 will be put in bucket (5, 10), and
pad questions and answers with a special symbol
“ PAD” when needed.
</p>
<p>Softmax over sampled words. To speed up the
training process, we apply softmax to a set of sam-
pled vocabulary words (the target word and 512
random ones) rather than the whole set. The idea
is similar with the importance sampling strategy in
(Jean et al., 2014).
</p>
<p>Beam search decoder. In the decode phase, we
use beam search, which maintains top-k (k = 10)
output sequences at each moment t, instead of
greedy search, which keeps only one at each time
t, to make our generation more reasonable.
</p>
<p>2.4 Attentive Seq2Seq Rerank Model
</p>
<p>Our rerank model uses the same attentive Seq2Seq
model to score candidate answers with regarding
to an input question. Specifically, we choose mean
probability, denoted as sMean-Prob in Eqn. 2, as our
scoring function (a candidate answer is treated as
a word sequence w1, w2, · · · , wn). We have also
tried inverse of averaged cross-entropy and har-
monic mean, but they had a poorer performance.
</p>
<p>sMean-Prob =
1
</p>
<p>n
</p>
<p>n∑
</p>
<p>i=1
</p>
<p>p(yi = wi|θi) (2)
</p>
<p>1https://www.tensorflow.org/tutorials/seq2seq
</p>
<p>3 Experiments
</p>
<p>In our experiments, we first examined the effec-
tiveness of attentive Seq2Seq model with the scor-
ing criterion mean probability; we then evaluated
the effectiveness of IR, Generation, IR + Rerank,
IR + Rerank + Generation (our approach); we also
conducted an online A/B test on our approach and
a baseline chatbot engine; lastly, we compared our
engine with a publicly available chatbot.
</p>
<p>For evaluation, we have business analysts go
through the answer of each testing question (two
analysts for the experiment comparing with an-
other public chatbot, and one for the other exper-
iments), and mark them with three graded labels:
“0” for unsuitable, “1” means that the answer is
only suitable in certain contexts, “2” indicates that
the answer is suitable. To determine whether an
answer is suitable or not, we define five evaluation
rules, namely “right in grammar”, “semantically
related”, “well-spoken language”, “context inde-
pendent” and “not overly generalized”. An answer
will be labeled as suitable only if it satisfies all the
rules, neutral if it satisfies the first three and breaks
either of the latter two, and unsuitable otherwise.
</p>
<p>We use top-1 accuracy (Ptop1) as the criterion
because the output of some approaches can be
more than one (e.g., IR). This indicator measures
whether the top-1 candidate is suitable or neutral,
and is calculated as follows: Ptop1 = (Nsuitable +
Nneutral)/Ntotal, where Nsuitable means the num-
ber of questions marked as suitable (other symbols
are defined similarly).
</p>
<p>3.1 Evaluating Rerank Models
</p>
<p>We first compared two Seq2Seq models (the ba-
sic one proposed in (Cho et al., 2014), the atten-
tive one presented in Section 2.4), on three scor-
</p>
<p>500</p>
<p />
</div>
<div class="page"><p />
<p>ing criteria (mean probability, inverse of averaged
cross-entropy and harmonic mean) using a set of
randomly sampled 500 questions. We show the
Ptop1 result in Table 1, which suggests that the at-
tentive Seq2Seq model with sMean-Prob has the best
performance. We use it in our rerank model.
</p>
<p>IR+Rerank IR
sMean-Prob sCross-Ent sHM
</p>
<p>Basic 0.48 0.48 0.47 0.47Attentive 0.54 0.53 0.53
</p>
<p>Table 1: Comparison of different rerank models.
</p>
<p>3.2 Evaluating Candidate Approaches
We then evaluated the effectiveness of the follow-
ing four approaches with another set of 600 ques-
tions: IR, Generation, IR + Rerank, IR + Rerank
+ Generation. We present the result in Fig. 3.
Clearly the proposed approach (IR + Rerank +
Generation) has the best top-1 accuracy: with a
confidence score threshold T = 0.19, Ptop1 =
60.01%. Here, questions with a score higher than
0.19 (the left of the dashed line, 535 out of 600),
are answered using rerank, and the rest is handled
by generation. The Ptop1 for the other three alter-
natives are 47.11%, 52.02%, and 56.23%, respec-
tively. Note that a narrowly higher Ptop1 can be
achieved if a higher threshold is used (e.g., 0.48),
or, put differently, rerank less and generate more.
We use the lower threshold because of the uncon-
trollability and poor interpretability of Seq2Seq
generation: with an elegant decrease at the Ptop1 ,
we gain more controllability and interpretability.
</p>
<p>0.46 
</p>
<p>0.48 
</p>
<p>0.50 
</p>
<p>0.52 
</p>
<p>0.54 
</p>
<p>0.56 
</p>
<p>0.58 
</p>
<p>0.60 
</p>
<p>0.62 
</p>
<p>13
.2
</p>
<p>5 
2.
</p>
<p>82
 
</p>
<p>1.
90
</p>
<p> 
1.
</p>
<p>43
 
</p>
<p>1.
13
</p>
<p> 
1.
</p>
<p>03
 
</p>
<p>0.
95
</p>
<p> 
0.
</p>
<p>89
 
</p>
<p>0.
83
</p>
<p> 
0.
</p>
<p>79
 
</p>
<p>0.
73
</p>
<p> 
0.
</p>
<p>69
 
</p>
<p>0.
66
</p>
<p> 
0.
</p>
<p>63
 
</p>
<p>0.
57
</p>
<p> 
0.
</p>
<p>54
 
</p>
<p>0.
50
</p>
<p> 
0.
</p>
<p>48
 
</p>
<p>0.
44
</p>
<p> 
0.
</p>
<p>40
 
</p>
<p>0.
37
</p>
<p> 
0.
</p>
<p>33
 
</p>
<p>0.
31
</p>
<p> 
0.
</p>
<p>28
 
</p>
<p>0.
27
</p>
<p> 
0.
</p>
<p>25
 
</p>
<p>0.
23
</p>
<p> 
0.
</p>
<p>21
 
</p>
<p>0.
19
</p>
<p> 
0.
</p>
<p>18
 
</p>
<p>0.
16
</p>
<p> 
0.
</p>
<p>14
 
</p>
<p>0.
00
</p>
<p> 
</p>
<p>Confidence Score (T)
</p>
<p>IR + Rerank + Generation (T = 0. 9)
</p>
<p>IR
</p>
<p>To
p-
</p>
<p>1 
A
</p>
<p>cc
ur
</p>
<p>ac
y 
</p>
<p> (P
to
</p>
<p>p1
)
</p>
<p>Generation Rerank
</p>
<p>Figure 3: Top-1 accuracy of candidate approaches.
</p>
<p>3.3 Online A/B Test
We implemented the proposed method in AliMe
Chat, our online chatbot engine, and conducted an
A/B test on the new and the existing IR method
(questions are equally distributed to the two ap-
proaches). We randomly sampled 2136 QA pairs,
with 1089 questions answered by IR and 1047
handled by our hybrid approach, and compared
</p>
<p>their top-1 accuracies. As shown in Table 2, the
new approach has a Ptop1 of 60.36%, which is
much higher than that of the IR baseline (40.86%).
</p>
<p>Model Ntotal Nunsuitable Nneutral Nsuitable Ptop1
IR 1089 644 384 61 40.86%
Hybrid 1047 415 504 128 60.36%
</p>
<p>Table 2: Comparison with IR model in A/B test.
</p>
<p>3.4 Comparing with a Public Chatbot
</p>
<p>To further evaluate our approach, we compared it
with a publicly available chatbot 2. We select 878
out of the 1047 testing questions (used in the A/B
test) by removing questions relevant to our chat-
bot, and use it to test the public one. To com-
pare their answers with ours, two business analysts
were asked to choose a better response for each
testing question. Table 3 shows the averaged re-
sults from the two analysts, clearly our chatbot has
a better performance (better on 37.64% of the 878
questions and worse on 18.84%). The Kappa mea-
sure between the analysts is 0.71, which shows a
substantial agreement.
</p>
<p>Win Equal Lose
Number 330 382 165
Percentage 37.64% 43.52% 18.84%
</p>
<p>Table 3: Comparison with another chatbot.
</p>
<p>3.5 Online Serving
</p>
<p>We deployed our approach in our chatbot engine.
For online serving, reranking is of key importance
to run time performance: if k candidate QA pairs
are ranked asynchronously, the engine has to wait
for the last reranker and it will get worse when
QPS (questions per second) is high. Our solution
is to bundle each k QA pairs together and trans-
form it into a k×nmatrix (n is the maximal length
of the concatenation of the k QA pairs, padding is
used when needed), and then make use of parallel
matrix multiplication in the rerank model to ac-
celerate the computation. In our experiments, the
batch approach helps to save 41% of the process-
ing time when comparing with the asynchronous
way. Specifically, more than 75% of questions
take less than 150ms with rerank and less than
200ms with generation. Moreover, our engine is
able to support a peak QPS of 42 on a cluster of 5
service instances, with each reserving 2 cores and
4G memory on an Intel Xeon E5-2430 server. This
makes our approach applicable to industrial bots.
</p>
<p>2http://www.tuling123.com/
</p>
<p>501</p>
<p />
</div>
<div class="page"><p />
<p>We launched AliMe Chat as an online service
and integrated it into AliMe Assist, our intelligent
assistant in the E-commerce field that supports not
only chatting but also customer service (e.g., sales
return), shopping guide and life assistance (e.g.,
book flight). We show an example chat dialog gen-
erated by our chat service 3 in Fig 4
</p>
<p>1
</p>
<p> .  1  &gt; 1 8 &gt; &lt;
</p>
<p>1 . &gt; &lt;
</p>
<p>? &gt;
</p>
<p>&gt; &lt;? &gt; &lt;
</p>
<p>, 1! ? &gt; &lt;
.
</p>
<p>Figure 4: An example chat dialog of AliMe Chat.
</p>
<p>4 Related Work
</p>
<p>Closed-domain dialog systems typically use rule-
or template- based methods (Williams and Zweig,
2016; Wen et al., 2016), and dialog state track-
ing (Henderson, 2015; Wang and Lemon, 2013;
Mrksic et al., 2015). Differently, open-domain
chatbots often adopt data-driven techniques. Com-
monly used include IR and Seq2Seq generation.
</p>
<p>IR based techniques mainly focus on finding the
nearest question(s) from a QA knowledge base for
an input question, e.g., (Isbell et al., 2000), (Ji
et al., 2014), (Yan et al., 2016b). A recent work
(Yan et al., 2016a) has tried a neural network based
method for matching. Usually, IR based models
have difficulty in handling long-tail questions.
</p>
<p>Seq2Seq based generation models are typically
trained on a QA knowledge base or conversation
corpus, and used to generate an answer for each
input. In this direction, RNN based Seq2Seq mod-
els are shown to be effective (Cho et al., 2014;
</p>
<p>3Interested readers can access AliMe Assist through the
Taobao App by following the path “(My Taobao)→(My Al-
iMe)”.
</p>
<p>Sutskever et al., 2014; Ritter et al., 2011; Shang
et al., 2015; Sordoni et al., 2015; Serban et al.,
2016). A basic Seq2Seq model is proposed in
(Sutskever et al., 2014), and enhanced with atten-
tion by (Bahdanau et al., 2015). Further, Sordoni
et al. (2015) considered context information, Li
et al. (2016) tried to let Seq2Seq models gener-
ate diversified answers by attaching a diversity-
promoting objective function. Despite many ef-
forts, Seq2Seq generation models are still likely to
generate inconsistent or meaningless answers.
</p>
<p>Our work combines both IR based and genera-
tion based models. Our work differs from another
recent combinational approach (Song et al., 2016)
in that they use an IR model to rerank the union of
retrieved and generated answers. Furthermore, we
found that our attentive Seq2Seq rerank approach
helps to improve the IR results significantly.
</p>
<p>5 Conclusion
</p>
<p>In this paper, we proposed an attentive Seq2Seq
based rerank approach that combines both IR and
generation based model. We have conducted a se-
ries of evaluations to assess the effectiveness of
our proposed approach. Results show that our hy-
brid approach outperforms both the two models.
We implemented this new method in an industrial
chatbot and released an online service.
</p>
<p>There are many interesting problems to be fur-
ther explored. One is context, which is of key im-
portance to multi-round interaction in dialog sys-
tem. Currently, we use a simple strategy to incor-
porate context: given a question, if less than three
candidates are retrieved by the IR model, we en-
hance it with its previous question and sent the
concatenation to the IR engine again. We have
tried other context-aware techniques, e.g. context
sensitive model (Sordoni et al., 2015), neural con-
versation model (Sutskever et al., 2014), but they
do not scale up well in our scenario. We are still
exploring scalable context-aware methods. Also,
we are working on personification, i.e., empower-
ing our chatbot with characters and emotions.
</p>
<p>Acknowledgments
</p>
<p>The authors would like to thank Juwei Ren, Lanbo
Li, Zhongzhou Zhao, Man Yuan, Qingqing Yu,
Jun Yang and other members of Alibaba Cloud
for helpful discussions and comments. We would
also like to thank reviewers for their valuable com-
ments.
</p>
<p>502</p>
<p />
</div>
<div class="page"><p />
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR 2015 .
</p>
<p>Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of EMNLP. pages 1724–1734.
</p>
<p>Matthew Henderson. 2015. Machine learning for dia-
log state tracking: A review. In Proceedings of The
First International Workshop on Machine Learning
in Spoken Language Processing.
</p>
<p>Charles Lee Isbell, Jr. Michael Kearns, Dave Kormann,
Satinder Singh, and Peter Stone. 2000. Cobot in
lambdamoo: A social statistics agent. In Proceed-
ings of the Seventeenth National Conference on Ar-
tificial Intelligence. AAAI Press, pages 36–41.
</p>
<p>Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2014. On using very large
target vocabulary for neural machine translation.
CoRR abs/1412.2007.
</p>
<p>Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint.
</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting objec-
tive function for neural conversation models. In the
Proceedings of the NAACL HLT 2016. pages 110–
119.
</p>
<p>Nikola Mrksic, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gasic, Pei-hao Su, David Vandyke,
Tsung-Hsien Wen, and Steve J. Young. 2015. Multi-
domain dialog state tracking using recurrent neural
networks. CoRR abs/1506.07190.
</p>
<p>Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of EMNLP 2011. pages 583–593.
</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. FTIR 3(4):333–389.
</p>
<p>Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C. Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using genera-
tive hierarchical neural network models. In the Pro-
ceedings of the Thirtieth AAAI 2016. pages 3776–
3784.
</p>
<p>Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. In the Proceedings of ACL 2015. pages 1577–
1586.
</p>
<p>Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and
Ming Zhang. 2016. Two are better than one: An
ensemble of retrieval- and generation-based dialog
systems. volume abs/1610.07149.
</p>
<p>Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In the Proceed-
ings of NAACL HLT 2015. pages 196–205.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In the Proceedings of NIPS 2014. pages
3104–3112.
</p>
<p>Oriol Vinyals and Quoc V. Le. 2015. A neural con-
versational model. ICML Deep Learning Workshop
2015 CoRR,abs/1506.05869.
</p>
<p>Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the di-
alog state tracking challenge: On the believability
of observed information. In the Proceedings of the
SIGDIAL 2013. pages 423–432.
</p>
<p>Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016. A network-
based end-to-end trainable task-oriented dialogue
system. arXiv preprint.
</p>
<p>Jason Williams and Geoffrey Zweig. 2016. End-to-end
lstm-based dialog control optimized with supervised
and reinforcement learning. Technical report.
</p>
<p>Rui Yan, Yiping Song, and Hua Wu. 2016a. Learning
to respond with deep neural networks for retrieval-
based human-computer conversation system. In
Proceedings of SIGIR ’16, pages 55–64.
</p>
<p>Zhao Yan, Nan Duan, Jun-Wei Bao, Peng Chen, Ming
Zhou, Zhoujun Li, and Jianshe Zhou. 2016b. Doc-
chat: An information retrieval approach for chatbot
engines using unstructured documents. In the Pro-
ceedings of ACL 2016.
</p>
<p>503</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 504–509
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2080
</p>
<p>A Conditional Variational Framework for Dialog Generation
</p>
<p>Xiaoyu Shen1∗, Hui Su2∗, Yanran Li3, Wenjie Li3, Shuzi Niu2, Yang Zhao4
Akiko Aizawa4 and Guoping Long2
</p>
<p>1Saarland University, Saarbrücken, Germany
2Institute of Software, Chinese Academy of Science, China
</p>
<p>3The Hong Kong Polytechnic University, Hong Kong
4National Institute of Informatics, Tokyo, Japan
</p>
<p>Abstract
</p>
<p>Deep latent variable models have been
shown to facilitate the response generation
for open-domain dialog systems. How-
ever, these latent variables are highly ran-
domized, leading to uncontrollable gener-
ated responses. In this paper, we propose a
framework allowing conditional response
generation based on specific attributes.
These attributes can be either manually as-
signed or automatically detected. More-
over, the dialog states for both speakers are
modeled separately in order to reflect per-
sonal features. We validate this framework
on two different scenarios, where the at-
tribute refers to genericness and sentiment
states respectively. The experiment result
testified the potential of our model, where
meaningful responses can be generated in
accordance with the specified attributes.
</p>
<p>1 Introduction
</p>
<p>Seq2seq neural networks, ever since the success-
ful application in machine translation (Sutskever
et al., 2014), have demonstrated impressive re-
sults on dialog generation and spawned a great
deal of variants (Vinyals and Le, 2015; Yao
et al., 2015; Sordoni et al., 2015; Shang et al.,
2015). The vanilla seq2seq models suffer from
the problem of generating too many generic re-
sponses (generic denotes safe, commonplace re-
sponses like ”I don’t know”). One major rea-
son is that the element-wise prediction models
stochastical variations only at the token level, se-
ducing the system to gain immediate short re-
wards and neglect the long-term structure. To
</p>
<p>∗Authors contributed equally. Correspondence should
be sent to H. Su (suhui15@iscas.ac.cn) and X. Shen
(xshen@lsv.uni-saarland.de)
</p>
<p>cope with this problem, (Serban et al., 2017) pro-
posed a variational hierarchical encoder-decoder
model (VHRED) that brought the idea of varia-
tional auto-encoders (VAE) (Kingma and Welling,
2013; Rezende et al., 2014) into dialog genera-
tion. For each utterance, VHRED samples a la-
tent variable as a holistic representation so that the
generative process will learn to maintain a coher-
ent global sentence structure. However, the latent
variable is learned purely in an unsupervised way
and can only be explained vaguely as higher level
decisions like topic or sentiment. Though effec-
tive in generating utterances with more informa-
tion content, it lacks the ability of explicitly con-
trolling the generating process.
</p>
<p>This paper presents a conditional variational
framework for generating specific responses, in-
spired by the semi-supervised deep generative
model (Kingma et al., 2014). The principle idea is
to generate the next response based on the dialog
context, a stochastical latent variable and an exter-
nal label. Furthermore, the dialog context for both
speakers is modeled separately because they have
different talking styles, personality and sentiment.
The whole network structure functions like a con-
ditional VAE (Sohn et al., 2015; Yan et al., 2016).
We test our framework on two scenarios. For the
first scenario, the label serves as a signal to indi-
cate whether the response is generic or not. By as-
signing different values to the label, either generic
or non-generic responses can be generated. For
the second scenario, the label represents an imi-
tated sentiment tag. Before generating the next re-
sponse, the appropriate sentiment tag is predicted
to direct the generating process.
</p>
<p>Our framework is expressive and extendable.
The generated responses agree with the predefined
labels while maintaining meaningful. By chang-
ing the definition of the label, our framework can
</p>
<p>504</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2080">https://doi.org/10.18653/v1/P17-2080</a></div>
</div>
<div class="page"><p />
<p>Figure 1: Computational graph for SPHRED
structure. The status vector for Speaker A and
Speaker B is modeled by separate RNNs then con-
catenated to represent the dialog context.
</p>
<p>be easily applied to other specific areas.
</p>
<p>2 Models
</p>
<p>To provide a better dialog context, we build a hier-
archical recurrent encoder-decoder with separated
context models (SPHRED). This section first in-
troduces the concept of SPHRED, then explains
the conditional variational framework and two ap-
plication scenarios.
</p>
<p>2.1 SPHRED
</p>
<p>We decomposes a dialog into two levels: se-
quences of utterances and sub-sequences of words,
as in (Serban et al., 2016). Let w1, . . . ,wN
be a dialog with N utterances, where wn =
(wn,1, . . . , wn,Mn) is the n-th utterance. The prob-
ability distribution of the utterance sequence fac-
torizes as:
</p>
<p>N∏
</p>
<p>n=1
</p>
<p>Mn∏
</p>
<p>m=1
</p>
<p>Pθ(wm,n|wm,&lt;n,w&lt;n) (1)
</p>
<p>where θ represents the model parameters and w&lt;n
encodes the dialog context until step n.
</p>
<p>If we model the dialog context through a single
recurrent neural network (RNN), it can only rep-
resent a general dialog state in common but fail to
capture the respective status for different speakers.
This is inapplicable when we want to infer implicit
personal attributes from it and use them to influ-
ence the sampling process of the latent variable,
as we will see in Section 2.4. Therefore, we model
the dialog status for both speakers separately. As
displayed in Figure 1, SPHRED contains an en-
coder RNN of tokens and two status RNNs of ut-
terances, each for one speaker. When modeling
turn k in a dialog, each status RNN takes as in-
put the last encoder RNN state of turn k − 2. The
</p>
<p>Figure 2: Graphical model for the conditional
variational framework. Solid lines denote gen-
erative model Pθ(zn|yn,wn−11 ) and Pθ(wn |
yn, zn,w
</p>
<p>n−1
1 ). When y
</p>
<p>t+1 is known, there exists
an additional link from yt+1 to z (dashed line). Ct
</p>
<p>encodes context information up to time t. Dotted
lines are posterior approximation Qφ(zn|yn,wn1 ).
</p>
<p>higher-level context vector is the concatenation of
both status vectors.
</p>
<p>We will show later that SPHRED not only well
keeps individual features, but also provides a bet-
ter holistic representation for the response decoder
than normal HRED.
</p>
<p>2.2 Conditional Variational Framework
VAEs have been used for text generation in (Bow-
man et al., 2015; Semeniuta et al., 2017), where
texts are synthesized from latent variables. Start-
ing from this idea, we assume every utterance
wn comes with a corresponding label yn and la-
tent variable zn. The generation of zn and wn
are conditioned on the dialog context provided by
SPHRED, and this additional class label yn. This
includes 2 situations, where the label of the next
sequence is known (like for Scenario 1 in Section
2.3) or not (Section 2.4). For each utterance, the
latent variable zn is first sampled from a prior dis-
tribution. The whole dialog can be explained by
the generative process:
</p>
<p>Pθ(zn|yn,wn−11 ) = N (µprior,Σprior) (2)
</p>
<p>Pθ(wn | yn, zn,wn−11 ) =
Mn∏
</p>
<p>m=1
</p>
<p>Pθ(wn,m | yn, zn,wn−11 , wn,m−1n,1 )
(3)
</p>
<p>When the label yn is unknown, a suitable classi-
fier is implemented to first predict it from the con-
text vector. This classifier can be designed as, but
not restricted to, multilayer perceptrons (MLP) or
support vector machines (SVM).
</p>
<p>Similarly, the posterior distribution of zn is ap-
proximated as in Equation 4, where the context
</p>
<p>505</p>
<p />
</div>
<div class="page"><p />
<p>and label of the next utterance is provided. The
graphical model is depicted in Figure 2.
</p>
<p>Qφ(zn|yn,wn1 ) = N (µposterior,Σposterior) (4)
</p>
<p>The training objective is derived as in For-
mula 5, which is a lower bound of the logarithm
of the sequence probability. When the label is
to be predicted (ȳn), an additional classification
loss (first term) is added such that the distribution
qφ(yn|wn−11 ) can be learned together with other
parameters.
</p>
<p>logPθ(w1, . . . ,wN ) ≥ Ep(wn,yn)
[
qφ(yn|wn−11 )
</p>
<p>]
</p>
<p>−
N∑
</p>
<p>n=1
</p>
<p>KL
[
Qψ(zn | wn1 ,yn)||Pθ(zn | wn−11 , ȳn)
</p>
<p>]
</p>
<p>+ EQψ(zn|wn1 ,yn)[logPθ(wn | zn,w
n−1
1 ,yn)]
</p>
<p>(5)
</p>
<p>2.3 Scenario 1
A major focus in the current research is to avoid
generating generic responses, so in the first sce-
nario, we let the label y indicate whether the cor-
responding sequence is a generic response, where
y = 1 if the sequence is generic and y = 0 oth-
erwise. To acquire these labels, we manually con-
structed a list of generic phrases like “I have no
idea”, “I don’t know”, etc. Sequences containing
any one of such phrases are defined as generic,
which in total constitute around 2 percent of the
whole corpus. At test time, if the label is fixed as
0, we expect the generated response should mostly
belong to the non-generic class.
</p>
<p>No prediction is needed, thus the training cost
does not contain the first item in Formula 5. This
scenario is designed to demonstrate our frame-
work can explicitly control which class of re-
sponses to generate by assigning corresponding
values to the label.
</p>
<p>2.4 Scenario 2
In the second scenario, we experiment with as-
signing imitated sentiment tags to generated re-
sponses. The personal sentiment is simulated by
appending :), :( or :P at the end of each utter-
ance, representing positive, negative or neutral
sentiment respectively. For example, if we ap-
pend ”:)” to the original ”OK”, the resulting ”OK
:)” becomes positive. The initial utterance of ev-
ery speaker is randomly tagged. We consider two
rules for the tags of next utterances. Rule 1 con-
fines the sentiment tag to stay constant for both
</p>
<p>speakers. Rule 2 assigns the sentiment tag of next
utterance as the average of the preceding two ones.
Namely, if one is positive and the other is negative,
the next response would be neutral.
</p>
<p>The label y represents the sentiment tag, which
is unknown at test time and needs to be predicted
from the context. The probability qφ(yn|wn−11 )
is modeled by feedforward neural networks. This
scenario is designed to demonstrate our frame-
work can successfully learn the manually defined
rules to predict the proper label and decode re-
sponses conforming to this label.
</p>
<p>3 Experiments
</p>
<p>We conducted our experiments on the Ubuntu di-
alog Corpus (Lowe et al., 2015), which contains
about 500,000 multi-turn dialogs. The vocabulary
was set as the most frequent 20,000 words. All the
letters are transferred to lowercase and the Out-
of-Vocabulary (OOV) words were preprocessed as
&lt;unk&gt; tokens.
</p>
<p>3.1 Training Procedures
</p>
<p>Model hyperparameters were set the same as in
VHRED model except that we reduced by half
the context RNN dimension. The encoder, con-
text and decoder RNNs all make use of the Gated
Recurrent Unit (GRU) structure (Cho et al., 2014).
Labels were mapped to embeddings with size 100
and word vectors were initialized with the pu-
bic Word2Vec embeddings trained on the Google
News Corpus1. Following (Bowman et al., 2015),
25% of the words in the decoder were randomly
dropped. We multiplied the KL divergence and
classification error by a scalar which starts from
zero and gradually increases so that the training
would initially focus on the stochastic latent vari-
ables. At test time, we outputted responses us-
ing beam search with beam size set to 5 (Graves,
2012) and &lt;unk&gt; tokens were prevented from
being generated. We implemented all the mod-
els with the open-sourced Python library Tensor-
flow (Abadi et al., 2016) and optimized using the
Adam optimizer (Kingma and Ba, 2014). Dialogs
are cut into set of slices with each slice containing
80 words then fed into the GPU memory. All mod-
els were trained with batch size 128. We use the
learning rate 0.0001 for our framework and 0.0002
for other models. Every model is tested on the val-
</p>
<p>1https://code.google.com/archive/p/
word2vec/
</p>
<p>506</p>
<p />
</div>
<div class="page"><p />
<p>idation dataset once every epoch and stops until it
gains nothing more within 5 more epochs.
</p>
<p>3.2 Evaluation
</p>
<p>Accurate automatic evaluation of dialog gener-
ation is difficult (Galley et al., 2015; Pietquin
and Hastie, 2013). In our experiment, we con-
ducted three embedding-based evaluations (aver-
age, greedy and extrema) (Liu et al., 2016) on
all our models, which map responses into vector
space and compute the cosine similarity. Though
not necessarily accurate, the embedding-based
metrics can to a large extent measure the semantic
similarity and test the ability of successfully gen-
erating a response sharing a similar topic with the
golden answer. The results of a GRU language
model (LM), HRED and VHRED were also pro-
vided for comparison. For the two scenarios of our
framework, we further measured the percentage
of generated responses matching the correct labels
(accuracy). In (Liu et al., 2016), current popular
metrics are shown to be not well correlated with
human judgements. Therefore, we also carried out
a human evaluation. 100 examples were randomly
sampled from the test dataset. The generated re-
sponses from the models were shuffled and ran-
domly distributed to 5 volunteers2. People were
requested to give a binary score to the response
from 3 aspects, grammaticality, coherence with
history context and diversity. Every response was
evaluated 3 times and the result agreed by most
people was adopted.
</p>
<p>3.3 Results of Metric-based Evaluation
</p>
<p>As can be seen from Table 1, SPHRED outper-
forms both HRED and LM over all the three
embedding-based metrics. This implies separating
the single-line context RNN into two independent
parts can actually lead to a better context represen-
tation. It is worth mentioning the size of context
RNN hidden states in SPHRED is only half of that
in HRED, but it still behaves better with fewer pa-
rameters. Hence it is reasonable to apply this con-
text information to our framework.
</p>
<p>The last 4 rows in Table 1 display the results
of our framework applied in two scenarios men-
tioned in Section 2.3 and 2.4. SCENE1-A and
SCENE1-B correspond to Scenario 1 with the la-
bel fixed as 1 and 0. 90.9% of generated responses
</p>
<p>2All volunteers are well-educated students who have re-
ceived a Bachelor’s degree on computer science or above.
</p>
<p>in SCENE1-A are generic and 86.9% in SCENE1-
B are non-generic according to the manually-built
rule, which verified the proper effect of the label.
SCENE2-A and SCENE2-B correspond to rule 1
and 2 in Scenario 2. Both successfully predict
the sentiment with very minor mismatches (0.2%
and 0.8%). The high accuracy further demon-
strated SPHRED’s capability of maintaining indi-
vidual context information. We also experimented
by substituting the encoder with a normal HRED,
the resulting model cannot predict the correct sen-
timent at all because the context information is
highly mingled for both speakers. The embedding
based scores of our framework are still compara-
ble with SPHRED and even better than VHRED.
Imposing an external label didn’t bring any signif-
icant quality decline.
</p>
<p>Model Average Greedy Extrema Accuracy
LM 0.360 0.348 0.310 -
HRED 0.429 0.466 0.383 -
SPHRED 0.468 0.478 0.434 -
VHRED 0.403 0.432 0.374 -
SCENE1-A - - - 90.9%
SCENE1-B 0.426 0.432 0.396 86.9%
SCENE2-A 0.465 0.440 0.428 99.8%
SCENE2-B 0.463 0.437 0.420 99.2%
</p>
<p>Table 1: Metric-based Evaluation. SCENE1-A is
set to generate generic responses, so it makes no
sense to measure it with embedding-based metrics
</p>
<p>3.4 Results of Human Evaluation
</p>
<p>We conducted human evaluations on VHRED and
our framework (Table 3). All models share similar
scores, except SCENE1-A receiving lower scores
with respect to coherence. This can be explained
by the fact that SCENE1-A is trained to generate
only generic responses, which limits its power of
taking coherence into account. VHRED and Sce-
nario 2 perform close to each other. Scenario 1,
due to the effect of the label, receives extreme
scores for diversity.
</p>
<p>In general, the statistical results of human eval-
uations on sentence quality are very similar be-
tween the VHRED model and our framework.
This agrees with the metric-based results and sup-
ports the conclusion drawn in Section 3.3. Though
the sample size is relatively small and human
judgements can be inevitably disturbed by subjec-
tive factors, we believe these results can shed some
light on the understanding of our framework.
</p>
<p>A snippet of the generated responses can be
</p>
<p>507</p>
<p />
</div>
<div class="page"><p />
<p>Context Response
anybody in the house ????? eou how to change the default
ubuntu wall paper ? eou how to change the default ubuntu
wallpaper ? eou eot Is there an echo in your head ? Is
there an echo in your head ? eou eot what do you mean
? eou eot Repeating = Bad . eou eot no body is
answering me eou eot
</p>
<p>.
LM: What do you want to do with it ?
HRED: I don’t know .
SPHRED: If you want to change the
default wallpaper , you can change the
default theme
</p>
<p>How can I install seamonkey ? eou To save me the pastebin
eou I am looking to install seamonkey , anyone ? eou
eot http://www.seamonkey-project.org/ eou eot It i
</p>
<p>snot in the ubuntu repository any more eou eot
</p>
<p>.
SCENE1-A: sorry i have no idea .
SCENE1-B: you need to find the
package that you can use .
</p>
<p>hey guys , how can I add an extra xsession to ubuntu 10.04 ? :)
eou that is , I dont want GNOME :) eou eot try this
</p>
<p>: https://wiki.ubuntu. com/CustomXSession :( eou eot
</p>
<p>.
SCENE2-A: ok thanks :)
</p>
<p>hey guys , how can I add an extra xsession to ubuntu 10.04 ? :(
eou that is , I dont want GNOME :( eou eot try this
</p>
<p>: https://wiki.ubuntu. com/CustomXSession :) eou eot
</p>
<p>.
SCENE2-B: thank you for the help !
:P
</p>
<p>Table 2: Examples of context-response pairs for the neural network models. eou denotes end-of-
utterance and eot denotes end-of-turn token
</p>
<p>Model G CD C¬D ¬CD ¬C¬D
VHRED 97% 41% 12 % 24% 23%
SCENE1-A 96% 3% 37% 1% 59%
SCENE1-B 96% 47% 9% 40% 4%
SCENE2-A 97% 40% 14 % 23% 23%
SCENE2-B 95% 38% 20% 31% 11%
</p>
<p>Table 3: Human Judgements, G refers to Gram-
maticality and the last four columns is the confu-
sion matrix with respect to coherence and diversity
</p>
<p>seen in Table 2. Generally speaking, SPHRED
better captures the intentions of both speakers,
while HRED updates the common context state
and the main topic might gradually vanish for the
different talking styles of speakers. SCENE1-A
and SCENE1-B are designed to reply to a given
context in two different ways. We can see both re-
sponses are reasonable and fit into the right class.
The third and fourth rows are the same context
with different appended sentiment tags and rules,
both generate a suitable response and append the
correct tag at the end.
</p>
<p>4 Discussion and future work
</p>
<p>In this work, we propose a conditional varia-
tional framework for dialog generation and ver-
ify it on two scenarios. To model the dialog state
for both speakers separately, we first devised the
SPHRED structure to provide the context vec-
tor for our framework. Our evaluation results
</p>
<p>show that SPHRED can itself provide a better con-
text representation than HRED and help generate
higher-quality responses. In both scenarios, our
framework can successfully learn to generate re-
sponses in accordance with the predefined labels.
Though with the restriction of an external label,
the score of generated responses didn’t signifi-
cantly decreased, meaning that we can constrain
the generation within a specific class while still
maintaining the quality.
</p>
<p>The manually-defined rules, though primitive,
represent two most common sentiment shift con-
ditions in reality. The results demonstrated the
potential of our model. To apply to real-world
scenarios, we only need to adapt the classifier
to detect more complex sentiments, which we
leave for future research. External models can be
used for detecting generic responses or classify-
ing sentiment categories instead of rule or symbol-
based approximations. We focused on the con-
trolling ability of our framework, future research
can also experiment with bringing external knowl-
edge to improve the overall quality of generated
responses.
</p>
<p>5 Acknowledgement
</p>
<p>This work was supported by the National Natu-
ral Science of China under Grant No. 61602451,
61672445 and JSPS KAKENHI Grant Numbers
15H02754, 16K12546.
</p>
<p>508</p>
<p />
</div>
<div class="page"><p />
<p>References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene
</p>
<p>Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
2016. Tensorflow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint
arXiv:1603.04467 .
</p>
<p>Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .
</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .
</p>
<p>Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Mar-
garet Mitchell, Jianfeng Gao, and Bill Dolan. 2015.
deltableu: A discriminative metric for genera-
tion tasks with intrinsically diverse targets. arXiv
preprint arXiv:1506.06863 .
</p>
<p>Alex Graves. 2012. Sequence transduction with
recurrent neural networks. arXiv preprint
arXiv:1211.3711 .
</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
</p>
<p>Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in Neural Information Processing Systems. pages
3581–3589.
</p>
<p>Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114 .
</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. arXiv preprint
arXiv:1603.08023 .
</p>
<p>Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. arXiv preprint arXiv:1506.08909 .
</p>
<p>Olivier Pietquin and Helen Hastie. 2013. A survey on
metrics for the evaluation of user simulations. The
knowledge engineering review 28(01):59–73.
</p>
<p>Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .
</p>
<p>Stanislau Semeniuta, Aliaksei Severyn, and Erhardt
Barth. 2017. A hybrid convolutional variational
autoencoder for text generation. arXiv preprint
arXiv:1702.02390 .
</p>
<p>Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. AAAI .
</p>
<p>Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. AAAI .
</p>
<p>Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. arXiv preprint arXiv:1503.02364 .
</p>
<p>Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In Advances
in Neural Information Processing Systems. pages
3483–3491.
</p>
<p>Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714 .
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.
</p>
<p>Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .
</p>
<p>Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2016. Attribute2image: Conditional image
generation from visual attributes. In European Con-
ference on Computer Vision. Springer, pages 776–
791.
</p>
<p>Kaisheng Yao, Geoffrey Zweig, and Baolin Peng.
2015. Attention with intention for a neu-
ral network conversation model. arXiv preprint
arXiv:1510.08565 .
</p>
<p>509</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 510–517
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2081
</p>
<p>Question Answering through Transfer Learning
from Large Fine-grained Supervision Data
</p>
<p>Sewon Min∗
Seoul National University
shmsw25@snu.ac.kr
</p>
<p>Minjoon Seo
University of Washington
minjoon@uw.edu
</p>
<p>Hannaneh Hajishirzi
University of Washington
hannaneh@uw.edu
</p>
<p>Abstract
</p>
<p>We show that the task of question answer-
ing (QA) can significantly benefit from the
transfer learning of models trained on a
different large, fine-grained QA dataset.
We achieve the state of the art in two
well-studied QA datasets, WikiQA and
SemEval-2016 (Task 3A), through a basic
transfer learning technique from SQuAD.
For WikiQA, our model outperforms the
previous best model by more than 8%.
We demonstrate that finer supervision pro-
vides better guidance for learning lexical
and syntactic information than coarser su-
pervision, through quantitative results and
visual analysis. We also show that a sim-
ilar transfer learning procedure achieves
the state of the art on an entailment task.
</p>
<p>1 Introduction
</p>
<p>Question answering (QA) is a long-standing chal-
lenge in NLP, and the community has introduced
several paradigms and datasets for the task over
the past few years. These paradigms differ from
each other in the type of questions and answers
and the size of the training data, from a few hun-
dreds to millions of examples.
</p>
<p>We are particularly interested in the context-
aware QA paradigm, where the answer to each
question can be obtained by referring to its accom-
panying context (paragraph or a list of sentences).
Under this setting, the two most notable types of
supervisions are coarse sentence-level and fine-
grained span-level. In sentence-level QA, the task
is to pick sentences that are most relevant to the
question among a list of candidates (Yang et al.,
2015). In span-level QA, the task is to locate the
</p>
<p>∗ All work was done while the author was an exchange
student at University of Washington.
</p>
<p>smallest span in the given paragraph that answers
the question (Rajpurkar et al., 2016).
</p>
<p>In this paper, we address coarser, sentence-
level QA through a standard transfer learning1
</p>
<p>technique of a model trained on a large, span-
supervised QA dataset. We demonstrate that the
target task not only benefits from the scale of the
source dataset but also the capability of the fine-
grained span supervision to better learn syntactic
and lexical information.
</p>
<p>For the source dataset, we pretrain on
SQuAD (Rajpurkar et al., 2016), a recently-
released, span-supervised QA dataset. For the
source and target models, we adopt BiDAF (Seo
et al., 2017), one of the top-performing mod-
els in the dataset’s leaderboard. For the tar-
get datasets, we evaluate on two recent QA
datasets, WikiQA (Yang et al., 2015) and Se-
mEval 2016 (Task 3A) (Nakov et al., 2016), which
possess sufficiently different characteristics from
that of SQuAD. Our results show 8% improve-
ment in WikiQA and 1% improevement in Se-
mEval. In addition, we report state-of-the-art re-
sults on recognizing textual entailment (RTE) in
SICK (Marelli et al., 2014) with a similar transfer
learning procedure.
</p>
<p>2 Background and Data
</p>
<p>Modern machine learning models, especially deep
neural networks, often significantly benefit from
transfer learning. In computer vision, deep con-
volutional neural networks trained on a large im-
age classification dataset such as ImageNet (Deng
et al., 2009) have proved to be useful for initial-
izing models on other vision tasks, such as ob-
ject detection (Zeiler and Fergus, 2014). In nat-
</p>
<p>1 The borderline between transfer learning and domain
adaptation is often ambiguous (Mou et al., 2016). We choose
the term “transfer learning” because we also adapt the pre-
trained QA model to an entirely different task, RTE.
</p>
<p>510</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2081">https://doi.org/10.18653/v1/P17-2081</a></div>
</div>
<div class="page"><p />
<p>Span-level QA Sentence-level QA RTE
SQuAD WikiQA SemEval-2016 Task 3A SICK
</p>
<p>Q
Which company made
</p>
<p>Q
I saw an ad, data entry jobs online. It req- Four kids are
</p>
<p>Spectre? Who made airbus uired we give a fee and they promise fixed P doing backbends
amount every month. Is this a scam? in the park.
</p>
<p>C
</p>
<p>Spectre (2015) is the C1 Airbus SAS is an aircraft manufacturing subsi- well probably is so i be more careful if i Four girls are
24th James Bond film diary of EADS, a European aerospace company. were u. Why you looking for online jobs doing backbends
produced by Eon C2 Airbus began as an union of aircraft companies. SCAM!!!!!!!!!!!!!!!!!!!!!! H and playing
Productions. It C3 Aerospace companies allowed the establishment Bcoz i got a baby and iam nt intrested to outdoors.
features (...) of a joint-stock company, owned by EADS. sent him in a day care. thats y iam (...)
</p>
<p>A “Eon Productions” A C1(Yes), C2(No), C3(No) C1(Good), C2(Good), C3(Bad) A Entailment
</p>
<p>Table 1: Examples of question-context pairs from QA datasets and premise-hypothesis pair from RTE dataset. Q indicates
question, C indicates context, A indicates answer, P indicates premise and H indicates hypothesis.
</p>
<p>ural language processing, domain adaptation has
traditionally been an important topic for syntactic
parsing (McClosky et al., 2010) and named entity
recognition (Chiticariu et al., 2010), among oth-
ers. With the popularity of distributed represen-
tation, pre-trained word embedding models such
as word2vec (Mikolov et al., 2013b,a) and glove
(Pennington et al., 2014) are also widely used
for natural language tasks (Karpathy and Fei-Fei,
2015; Kumar et al., 2016). Instead of these, we
initialize our models from a QA dataset and show
how standard transfer learning can achieve state-
of-the-art in target QA datasets.
</p>
<p>There have been several QA paradigms in NLP,
which can be categorized by the context and su-
pervision used to answer questions. This context
can range from structured and confined knowl-
edge bases (Berant et al., 2013) to unstructured
and unbounded natural language form (e.g., doc-
uments on the web (Voorhees and Tice, 2000))
and unstructured, but restricted in size (e.g., a
paragraph or multiple sentences (Hermann et al.,
2015)). The recent advances in neural question an-
swering lead to numerous datasets and successful
models in these paradigms (Rajpurkar et al., 2016;
Yang et al., 2015; Nguyen et al., 2016; Trischler
et al., 2016). The answer types in these datasets
are largely divided into three categories: sentence-
level, in-context span, and generation. In this
paper, we specifically focus on the former two
and show that span-supervised models can better
learn syntactic and lexical features. Among these
datasets, we briefly describe three QA datasets to
be used for the experiments in this paper. We also
give the description of an RTE dataset for an ex-
ample of a non-QA task. Refer to Table 1 to see
the examples of the datasets.
</p>
<p>SQuAD (Rajpurkar et al., 2016) is a recent span-
based QA dataset, containing 100k/10k train/dev
examples. Each example is a pair of context para-
</p>
<p>graph from Wikipedia and a question created by a
human, and the answer is a span in the context.
</p>
<p>SQUAD-T is our modification of SQuAD
dataset to allow for sentence selection QA. (‘T’
for senTence). We split the context paragraph
into sentences and formulate the task as classi-
fying whether each sentence contains the answer.
This enables us to make a fair comparison between
pretraining with span-supervised and sentence-
supervised QA datasets.
</p>
<p>WikiQA (Yang et al., 2015) is a sentence-level
QA dataset, containing 1.9k/0.3k train/dev an-
swerable examples. Each example consists of a
real user’s Bing query and a snippet of a Wikipedia
article retrieved by Bing, containing 18.6 sen-
tences on average. The task is to classify whether
each sentence provides the answer to the query.
</p>
<p>SemEval 2016 (Task 3A) (Nakov et al., 2016)
is a sentence-level QA dataset, containing
1.8k/0.2k/0.3k train/dev/test examples. Each ex-
ample consists of a community question by a user
and 10 comments. The task is to classify whether
each comment is relevant to the question.
</p>
<p>SICK (Marelli et al., 2014) is a dataset for
recognizing textual entailment (RTE), containing
4.5K/0.5K/5.0K train/dev/test examples. Each ex-
ample consists of a hypothesis and a premise, and
the goal is to determine if the premise is entailed
by, contradicts, or is neutral to the hypothesis
(hence classification problem). We also report re-
sults on SICK to show that span-supervised QA
dataset can be also useful for non-QA datasets.
</p>
<p>3 Model
</p>
<p>Among numerous models proposed for span-
level QA tasks (Xiong et al., 2017; Wang and
Jiang, 2017b), we adopt an open-sourced model,
BiDAF2 (Seo et al., 2017).
</p>
<p>2https://allenai.github.io/bi-att-flow
</p>
<p>511</p>
<p />
</div>
<div class="page"><p />
<p>BiDAF. The inputs to the model are a ques-
tion q, and a context paragraph x. Then the
model selects the best answer span, which is
argmax(i,j) y
</p>
<p>start
i y
</p>
<p>end
j , where i &lt;= j. Here,
</p>
<p>ystarti and y
end
i are start and end position proba-
</p>
<p>bilities of i-th element, respectively.
Here, we briefly describe the answer mod-
</p>
<p>ule which is important for transfer learning to
sentence-level QA. The input to the answer mod-
ule is a sequence of vectors {hi} each of which
encodes enough information about the i-th con-
text word and its relationship with its surrounding
words and the question words. Then the role of the
answer module is to map each vector hi to its start
and end position probabilities, ystarti and y
</p>
<p>end
i .
</p>
<p>BiDAF-T refers to the modified version of
BiDAF to make it compatible with sentence-level
QA. (‘T’ for senTence). In this task, the inputs are
a question q and a list of sentences, x1, . . . ,xT ,
where T is the number of the sentences. Note that,
unlike BiDAF, which outputs single answer per
example, Here we need to output a C-way clas-
sification for each k-th sentence.
</p>
<p>Since BiDAF is a span-selection model, it can-
not be directly used for sentence-level classifica-
tion. Hence we replace the original answer mod-
ule of BiDAF with a different answer module,
and keep the other modules identical to those of
BiDAF. Given the input to the new answer mod-
ule, {hk1, . . . ,hkN}, where the superscript is the
sentence index (1 ≤ k ≤ T ), we obtain the
C–way classification scores for the k-th sentence,
ỹk ∈ [0, 1]C via max-pooling method:
</p>
<p>ỹk = softmax(Wmax(hk1, . . . ,h
k
N ) + b) (1)
</p>
<p>where W ∈ RC×d,b ∈ RC are trainable weight
matrix and bias, respectively, and max() function
is applied elementwise.
</p>
<p>For WikiQA and SemEval 2016, the number of
classes (C) is 2, i.e. each sentence (or comment) is
either relevant or not relevant. Since some of the
metrics used for these datasets require full rank-
ing, we use the predicted probability for “relevant”
label to rank the sentences.
</p>
<p>Note that BiDAF-T can be also used for the RTE
dataset, where we can consider the hypothesis as
a question and the premise as a context sentence
(T = 1), and classify each example into ‘entail-
ment’, ‘neutral’, or ‘contradiction’ (C = 3).
</p>
<p>Transfer Learning. Transfer learning between
the same model architectures3 is straightforward:
we first initialize the weights of the target model
with the weights of the source model pretrained on
the source dataset, and then we further train (fine-
tune) on the target model with the target dataset.
To transfer from BiDAF (on SQuAD) to BiDAF-
T, we transfer all the weights of the identical
modules, and initialize the new answer module in
BiDAF-T with random values. For more training
details, refer to Appendix A.
</p>
<p>4 Experiments
</p>
<p>Pretrained Fine- WikiQA SemEval-2016
dataset tuned MAP MRR P@1 MAP MRR AvgR
</p>
<p>- - 62.96 64.47 49.38 76.40 82.20 86.51
SQuAD-T No 75.22 76.40 62.96 47.23 49.31 60.01
SQuAD No 75.19 76.31 62.55 57.80 66.10 71.13
</p>
<p>SQuAD-T Yes 76.44 77.85 64.61 76.30 82.51 86.64
SQuAD Yes 79.90 82.01 70.37 78.37 85.58 87.68
</p>
<p>SQuAD* Yes 83.20 84.58 75.31 80.20 86.44 89.14
Rank 1 74.33 75,45 - 79.19 86.42 88.82
Rank 2 74.17 75.88 64.61 77.66 84.93 88.05
Rank 3 70.69 72.65 - 77.58 85.21 88.14
</p>
<p>Table 2: Results on WikiQA and SemEval-2016 (Task 3A).
The first row is a result from non-pretrained model, and * in-
dicates ensemble method. Metrics used are Mean Average
Precision (MAP), Mean Reciprocal Rank (MRR), Precision
at rank 1 (P@1), and Average Recall (AvgR). Rank 1,2,3 indi-
cate the results by previous works, ordered by MAP. For Wik-
iQA, they are from Wang and Jiang (2017a); Tymoshenko
et al. (2016); Miller et al. (2016), respectively. For SemEval-
2016, they are from Filice et al. (2016); Joty et al. (2016);
Mihaylov and Nakov (2016). SQuAD*&amp;Yes sets the new
state of the art on both datasets.
</p>
<p>Question Answering Results. Table 2 reports
the state-of-the-art results of our transfer learn-
ing on WikiQA and SemEval-2016 and the per-
formance of previous models as well as several
ablations that use no pretraining or no finetuning.
There are multiple interesting observations from
Table 2 as follows:
(a) If we only train the BiDAF-T model on the
target datasets with no pretraining (first row of Ta-
ble 2), the results are poor. This shows the impor-
tance of both pretraining and finetuning.
(b) Pretraining on SQuAD and SQuAD-T with
no finetuning (second and third row) achieves re-
sults close to the state-of-the-art in the WikiQA
dataset, but not in SemEval-2016. Interestingly,
our result on SemEval-2016 is not better than
only training without transfer learning. We con-
jecture that this is due to the significant differ-
ence between the domain of SemEval-2016 and
</p>
<p>3 Strictly speaking, this is a domain adaptation scenario.
</p>
<p>512</p>
<p />
</div>
<div class="page"><p />
<p>that of SQuAD, which are from community and
Wikipedia, respectively.
</p>
<p>(c) Pretraining on SQuAD and SQuAD-T with
finetuning (fourth and fifth row) significantly out-
performs (by more than 5%) the highest-rank sys-
tems on WikiQA. It also outperforms the second
ranking system in SemEval-2016 and is only 1%
behind the first ranking system.
</p>
<p>(d) Transfer learning models achieve better re-
sults with pretraining on span-level supervision
(SQuAD) than coarser sentence-level supervision
(SQuAD-T).4
</p>
<p>Finally, we also use the ensemble of 12 differ-
ent training runs on the same BiDAF architecture,
which obtains the state of the art in both datasets.
This system outperforms the highest-ranking sys-
tem in WikiQA by more than 8% and the best sys-
tem in SemEval-2016 by 1% in every metric. It is
important to note that, while we definitely benefit
from the scale of SQuAD for transfer learning to
smaller WikiQA, given the gap between SQuAD-
T and SQuAD (&gt; 3%), we see a clear sign that
span-supervision plays a significant role well.
</p>
<p>Varying the size of pretraining dataset. We
vary the size of SQuAD dataset used during pre-
training, and test on WikiQA with finetuning. Re-
sults are shown in Table 3. As expected, MAP on
WikiQA drops as the size of SQuAD decreases. It
is worth noting that pretraining on SQuAD-T (Ta-
ble 2) yields 0.5 point lower MAP than pretraining
on 50% of SQuAD. In other words, roughly speak-
ing, span-level supervision data is worth more than
twice the size of sentence-level supervision data
for the purpose of pretraining. Also, even a small
size of fine-grained supervision data helps; pre-
training with 12.5% of SQuAD gives an advantage
of more than 7 points than no pretraining.
</p>
<p>Analysis. Figure 1 shows the latently-learned at-
tention maps between the question and one of the
context sentences from a WikiQA example in Ta-
ble 1. The top map is pretrained on SQuAD-
T (corresponding to SQuAD-T&amp;Yes in Table 2)
and the bottom map is pretrained on SQuAD
(SQuAD&amp;Yes). The more red the color, the higher
</p>
<p>4We additionally perform Mann-Whitney U Test and Mc-
Nemars Test to show the statistical significance of the advan-
tage of span-level pretraining over sentence-level pretraining.
For WikiQA, the advantage is statistically significant with the
confidence levels of 97.1% and 99.6%, respectively. For Se-
mEval, we obtain the confidence levels of 97.8% and 99.9%,
respectively.
</p>
<p>Percentage of used SQuAD dataset MAP
100% 79.90
50% 76.94
25% 74.39
</p>
<p>12.5% 70.76
</p>
<p>Table 3: Results with varying sizes of SQuAD dataset used
during pretraining. All of them are finetuned and tested on
WikiQA.
</p>
<p>who
</p>
<p>made
</p>
<p>airbus
</p>
<p>who
</p>
<p>made
</p>
<p>airbus
</p>
<p>Figure 1: Attention maps showing correspondence between
the words of a question (vertical) and the subset of its context
(horizontal) in WikiQA for (top) SQuAD-T-pretrained model
and (bottom) SQuAD-pretrained model. The more red, the
higher the correspondence.
</p>
<p>the relevance between the words. There are two
interesting observations here.
</p>
<p>First, in SQuAD-pretrained model (bottom),
we see a high correspondence between ques-
tion’s airbus and context’s aircraft and
aerospace, but the SQuAD-T-pretrained model
fails to learn such correspondence.
</p>
<p>Second, we see that the attention map of the
SQuAD-pretrained model is more sparse, indicat-
ing that it is able to more precisely localize cor-
respondence between question and context words.
In fact, we compare the sparsity of WikiQA test
examples in SQuAD&amp;Y and SQuAD-T&amp;Y. Fol-
lowing Hurley and Rickard (2009), the sparsity of
an attention map is defined by
</p>
<p>sparsity =
| {x ∈ V|x ≤ �} |
</p>
<p>|V| (2)
</p>
<p>where V is a set of values between 0 and 1 in at-
tention map, and � is a small value which we de-
fine 0.01 for here. A histogram of the sparsity is
shown in Figure 2. There is a large gap in the av-
erage sparsity of WikiQA test examples between
SQuAD&amp;Yes and SQuAD-T&amp;Yes, which are 0.84
and 0.56, respectively.
</p>
<p>More analyses including error analysis and
more visualizations are shown in Appendix B.
</p>
<p>Entailment Results. In addition to QA experi-
ments, we also show that the models trained on
span-supervised QA can be useful for textual en-
tailment task (RTE). Table 4 shows the trans-
</p>
<p>513</p>
<p />
</div>
<div class="page"><p />
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>0
</p>
<p>0
.1
</p>
<p>0
.2
</p>
<p>0
.3
</p>
<p>0
.4
</p>
<p>0
.5
</p>
<p>0
.6
</p>
<p>0
.7
</p>
<p>0
.8
</p>
<p>0
.9
</p>
<p># 
o
</p>
<p>f 
ex
</p>
<p>am
p
</p>
<p>le
s
</p>
<p>sparsity
</p>
<p>Figure 2: Histogram of the sparsity (Equation 2) of
the attention maps of SQuAD-T-pretrained model (SQuAD-
T&amp;Yes, blue) and SQuAD-pretrained model (SQuAD&amp;Yes,
red). Mean sparsity of SQuAD-pretrained model (0.84)
is clearly higher than that of SQuAD-T-pretrained model
(0.56).
</p>
<p>Pretrained dataset / Previous work Accuracy
- 77.96
</p>
<p>SQuAD-T 81.49
SQuAD 82.86
SQuAD* 84.38
</p>
<p>SNLI 83.20
SQuAD-T + SNLI 85.00
SQuAD + SNLI 86.63
SQuAD + SNLI* 88.22
Yin et al. (2016) 86.2
</p>
<p>Lai and Hockenmaier (2014) 84.57
Zhao et al. (2014) 83.64
</p>
<p>Jimenez et al. (2014) 83.05
Mou et al. (2016) 70.9
</p>
<p>Mou et al. (2016) (pretrained on SNLI) 77.6
</p>
<p>Table 4: Results on SICK after finetuning. The first row is
only trained on SICK. * indicates ensemble method.
</p>
<p>fer learning results of BiDAF-T on SICK dataset
(Marelli et al., 2014), with various pretraining rou-
tines. Note that SNLI (Bowman et al., 2015) is
a similar task to SICK and is significantly larger
(150K/10K/10K train/dev/test examples). Here
we highlight three observations:
(a) BiDAF-T pretrained on SQuAD outperforms
that without any pretraining by 6% and that pre-
trained on SQuAD-T by 2%, which demonstrates
that the transfer learning from large span-based
QA gives a clear improvement.
(b) Pretraining on SQuAD+SNLI outperforms
pretraining on SNLI only. Given that SNLI is
larger than SQuAD, the difference in their perfor-
mance is a strong indicator that we are benefiting
from not only the scale of SQuAD, but also the
fine-grained supervision that it provides.
(c) We outperform the previous state of the art
by 2% with the ensemble of SQuAD+SNLI pre-
training routine.
</p>
<p>It is worth noting that Mou et al. (2016) also
shows improvement on SICK by pretraining on
SNLI.
</p>
<p>5 Conclusion
</p>
<p>In this paper, we show state-of-the-art results on
WikiQA and SemEval-2016 (Task 3A) as well as
an entailment task, SICK, outperforming previous
results by 8%, 1%, and 2%, respectively. We show
that question answering with sentence-level super-
vision can greatly benefit from standard transfer
learning of a question answering model trained on
a large, span-level supervision. We additionally
show that such transfer learning can be applicable
in other NLP tasks such as textual entailment.
</p>
<p>Acknowledgments
</p>
<p>This research was supported by the NSF (IIS
1616112), Allen Institute for AI (66-9175), Allen
Distinguished Investigator Award, and Google Re-
search Faculty Award. We thank the anonymous
reviewers for their helpful comments.
</p>
<p>References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
</p>
<p>Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP.
</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP.
</p>
<p>Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP.
</p>
<p>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. Imagenet: A large-scale
hierarchical image database. In CVPR.
</p>
<p>Simone Filice, Danilo Croce, Alessandro Moschitti,
and Roberto Basili. 2016. Kelp at semeval-2016
task 3: Learning semantic relations between ques-
tions and answers. SemEval 16:1116–1123.
</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In NIPS.
</p>
<p>Niall Hurley and Scott Rickard. 2009. Comparing
measures of sparsity. IEEE Transactions on Infor-
mation Theory 55(10):4723–4741.
</p>
<p>Sergio Jimenez, George Duenas, Julia Baquero,
Alexander Gelbukh, Av Juan Dios Bátiz, and
Av Mendizábal. 2014. Unal-nlp: Combining soft
cardinality features for semantic textual similarity,
relatedness and entailment. In SemEval Workshop.
</p>
<p>514</p>
<p />
</div>
<div class="page"><p />
<p>Shafiq Joty, Alessandro Moschitti, Fahad A Al Obaidli,
Salvatore Romeo, Kateryna Tymoshenko, and Anto-
nio Uva. 2016. Convkn at semeval-2016 task 3: An-
swer and question selection for question answering
on arabic and english fora. SemEval pages 896–903.
</p>
<p>Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In CVPR.
</p>
<p>Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. 2016. Ask
me anything: Dynamic memory networks for natu-
ral language processing. In ICML.
</p>
<p>Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. SemEval .
</p>
<p>Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of com-
positional distributional semantic models. In LREC.
</p>
<p>David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In NAACL-HLT .
</p>
<p>Todor Mihaylov and Preslav Nakov. 2016. Semanticz
at semeval-2016 task 3: Ranking relevant answers in
community question answering using semantic sim-
ilarity based on fine-tuned word embeddings. Se-
mEval pages 879–886.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In ICLR.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS.
</p>
<p>Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for directly
reading documents. In EMNLP.
</p>
<p>Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? In EMNLP.
</p>
<p>Preslav Nakov, Llus Mrquez, Alessandro Moschitti,
Walid Magdy Mubarak Hamdy Hamdy, abed Al-
hakim Freihat, Jim Glass, and Bilal Randeree. 2016.
Semeval-2016 task 3: Community question answer-
ing. SemEval pages 525–545.
</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine read-
ing comprehension dataset. In NIPS Workshop.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.
</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In EMNLP.
</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In ICLR.
</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .
</p>
<p>Kateryna Tymoshenko, Daniele Bonadiman, and
Alessandro Moschitti. 2016. Convolutional neural
networks vs. convolution kernels: Feature engineer-
ing for answer sentence reranking. In NAACL-HLT .
</p>
<p>Ellen M Voorhees and Dawn M Tice. 2000. Building a
question answering test collection. In ACM SIGIR.
</p>
<p>Shuohang Wang and Jing Jiang. 2017a. A compare-
aggregate model for matching text sequences. In
ICLR.
</p>
<p>Shuohang Wang and Jing Jiang. 2017b. Machine com-
prehension using match-lstm and answer pointer. In
ICLR.
</p>
<p>Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. arXiv preprint arXiv:1702.03814
.
</p>
<p>Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In ICLR.
</p>
<p>Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In EMNLP.
</p>
<p>Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2016. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
TACL .
</p>
<p>Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .
</p>
<p>Matthew D Zeiler and Rob Fergus. 2014. Visualiz-
ing and understanding convolutional networks. In
ECCV .
</p>
<p>Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. Ecnu:
One stone two birds: Ensemble of heterogenous
measures for semantic relatedness and textual entail-
ment. SemEval pages 271–277.
</p>
<p>515</p>
<p />
</div>
<div class="page"><p />
<p>A Training details
Parameters. For pretraining BiDAF on
SQuAD, we follow the exact same procedure in
Seo et al. (2017). For pretraining BiDAF-T on
SQuAD-T, we use the same hyperparameters for
all modules except the answer module, for which
we use the hidden state size of 200. The learning
rate is controlled by AdaDelta (Zeiler, 2012) with
the initial learning rate of 0.5 and minibatch size
of 50. We maintain the moving averages of all
weights of the model with the exponential decay
rate of 0.999 during training and use them at test.
The loss function is the cross entropy between ỹk
</p>
<p>and the one-hot vector of the correct classification.
</p>
<p>Convergence. For all settings, we train models
until performance on development set continue to
decrease for 5k steps. Table 5 shows the median
selected step on each setting.
</p>
<p>Dataset Pretrained selected step
SQuAD - 18k
</p>
<p>SQuAD-T - 50k
WikiQA - 6k
WikiQA SQuAD-T 6k
WikiQA SQuAD 3k
</p>
<p>SemEval-2016 - 9k
SemEval-2016 SQuAD-T 4k
SemEval-2016 SQuAD 3k
</p>
<p>SICK - 13k
SNLI - 55k
SICK SQuAD-T 9k
SNLI SQuAD-T 31k
SICK SQuAD 18k
SNLI SQuAD 49k
SICK SQuAD-T + SNLI 7k
SICK SQuAD + SNLI 7k
</p>
<p>Table 5: Median global step, which has the best perfor-
mance on development set.
</p>
<p>B More Analysis
Attention maps. We show some more examples
of attention maps in Figure 3. (Top) We see high
correspondence between same word from ques-
tion and context such as senator and john,
in SQuAD-pretrained model, but the SQuAD-T-
pretrained model fails to learn such correspon-
dence. (Bottom) We see high correspondence
between stems from question and stem from
context (left) as well as plant from question
and plants from context (right), in SQuAD-
pretrained model, but the SQuAD-T-pretrained
model fails to learn such correspondence.
</p>
<p>Error Analysis. Table 7 shows the comparison
between answers by SQuAD-T-pretrained model
and SQuAD-pretrained model on the example
</p>
<p>what
</p>
<p>are
</p>
<p>the
</p>
<p>parts
</p>
<p>of
</p>
<p>plant
</p>
<p>stems
</p>
<p>what
</p>
<p>are
</p>
<p>the
</p>
<p>parts
</p>
<p>of
</p>
<p>plant
</p>
<p>stems
</p>
<p>what
</p>
<p>state
</p>
<p>was
</p>
<p>john
</p>
<p>mccain
</p>
<p>a
</p>
<p>senator
</p>
<p>in
</p>
<p>during
</p>
<p>the
</p>
<p>2008
</p>
<p>election
</p>
<p>what
</p>
<p>state
</p>
<p>was
</p>
<p>john
</p>
<p>mccain
</p>
<p>a
</p>
<p>senator
</p>
<p>in
</p>
<p>during
</p>
<p>the
</p>
<p>2008
</p>
<p>election
</p>
<p>Figure 3: More attention maps showing correspondence be-
tween the words of a question (vertical) and one of candidate
sentences (horizontal) in WikiQA for (top in each subfigure)
SQuAD-MC-pretrained model and (bottom in each subfig-
ure) SQuAD-pretrained model. The more red, the higher the
correspondence.
</p>
<p>of WikiQA and SemEval-2016 from Table 1.
On WikiQA, SQuAD-T-pretrained model selects
C2 instead of the groundtruth answer C1. On
SemEval-2016, SQuAD-pretrained model ranks
C3 (bad comment) higher than C2 (good com-
ment).
</p>
<p>In addition, we sampled 100 example randomly
</p>
<p>516</p>
<p />
</div>
<div class="page"><p />
<p>Category Id Category Example Question Relevant Sentence
</p>
<p>1 Exact Match When did SpongeBob first air?
The pilot episode of SpongeBob SquarePants first aired in the
United States on May 1, 1999, following the ...
</p>
<p>2 Paraphrase When was How the West Was Won filmed? How the West Was Won is a 1962 American epic Western film.
3 No Clear Clue When was Mary Anderson born? Mary Anderson (1866-1953) was a real estate developer, ....
4 Need prior sentence (pronoun) When did Texas become a state? In 1845, it joined the United States as the 28th state.
</p>
<p>5 Need prior sentence (context)
How do you play spades?
</p>
<p>Its major difference as compared to other Whist variants is that,
instead of trump being decided by the highest bidder or at random,
the Spade suit is always trump, hence the name.
</p>
<p>6 Hard to answer How kimberlite pipes form? Volcanic pipes are relatively rare.
</p>
<p>Category Id Category Example Question Id Example Question
1 Asking information Q347 R25 hi all is there any IKEA showroom in and around DOHA? Kindly reply thank you
</p>
<p>2
Asking opinion or
</p>
<p>Q326 R90
Salam I am mechanical Eng. 15 years experience i got a job for Rasgas co. direct
</p>
<p>recommendation in hire I am married and i have 2 kids 5 and 3 years old.my life style
specific situation is average. Is 8.000 QR enough as a basic salary? (...)
</p>
<p>3
Asking feelings in Q348 R67 oh i wish they will build Disneyland in Qatar : ) how do you think guys? It will be
specific situation perfect for Qatar : )
</p>
<p>4 Asking abstract thing
Q341 R11 id like to get to know more about Al Jazeera International from anyone on QATAR
</p>
<p>LIVING who works at Al Jazeera.
</p>
<p>5 Not Asking Q337 R21
I just stumbled across this news article about the the American university campuses
at Education City and thought some of you may also find it interesting.
</p>
<p>6 Asking a lot of things at once
Q337 R16
</p>
<p>How good are Karwa services? Are they : 1. Courteous/Rude? 2. Taking the correct
route/Longer route? (...) 7. A
pleasure/displeasure to ride?
</p>
<p>Table 6: Examples from each category on (top) WikiQA and (bottom) SemEval-2016 (Task 3A).
</p>
<p>WikiQA SemEval-2016
SQuAD-T&amp;Yes C2 &gt; C1 &gt; C3 C1 &gt; C3 &gt; C2
SQuAD&amp;Yes C1 &gt; C2 &gt; C3 C2 &gt; C1 &gt; C3
Groundtruth C1(Y), C2(N), C3(N) C1(Good), C2(Good), C3(Bad)
</p>
<p>Table 7: Comparison of ranked answers by SQuAD-T-
pretrained model (SQuAD-T&amp;Yes) and SQuAD-pretrained
model (SQuAD&amp;Yes) of examples from WikiQA and
SemEval-2016 (Task 3A) in Table 1.
</p>
<p>Pretrained dataset
total
</p>
<p>Category Id
SQuAD-T-Y SQuAD-Y 1 2 3 4 5 6
</p>
<p>total 100 37 38 6 15 2 2
Correct Correct 49 28 14 3 4 0 0
Wrong Correct 26 8 14 1 3 0 0
Correct Wrong 9 1 4 1 3 0 0
Wrong Wrong 16 0 6 1 5 2 2
</p>
<p>Table 8: Comparison of performance of SQuAD-T-
pretrained model (SQuAD-T-Y) and SQuAD-pretrained
model (SQuAD-Y) on WikiQA.
</p>
<p>Pretrained dataset
total
</p>
<p>Category Id
No Pretrain SQuAD-Y 1 2 3 4 5 6
</p>
<p>total 100 29 38 7 12 9 5
Correct Correct 30 12 11 2 5 0 0
Wrong Correct 22 6 10 0 2 2 2
Correct Wrong 5 0 1 2 1 0 1
Wrong Wrong 43 11 16 3 4 7 2
</p>
<p>Table 9: Comparison of performance of model with-
out pretraining (No Pretrain) and SQuAD-pretrained model
(SQuAD-Y) on SemEval-2016 (Task 3A).
</p>
<p>from WikiQA and SemEval-2016, and classified
them into 6 categories(Table 6). In Table 8, we
compare the performance on these WikiQA exam-
ples by SQuAD-T-pretrained model and SQuAD-
pretrained model. It shows that span supervision
clearly helps answering questions on Category 1
and 2, which are easier to answer, with answering
</p>
<p>correctly on most of the questions in Category 1.
Similarly, we show the comparison of the perfor-
mance on classified examples of the model with-
out pretraining and SQuAD-pretrained model on
SemEval-2016. It also shows that span supervi-
sion helps answering questions asking information
or opinion/recommendation.
</p>
<p>C More Results
SQuAD-T. To better understand SQuAD-T
dataset, we show the performance BiDAF-T with
different training routines. We get MAP 89.46 and
accuracy 85.34% with SQuAD-trained BiDAF
model, and MAP 90.18 and accuracy 84.69% with
SQuAD-T-trained BiDAF-T model. There is no
large gap between the two models, as each para-
graph of SQuAD-T has 5 sentences on average,
which makes the classification problem easier than
WikiQA.
</p>
<p>SNLI. Other larger RTE datasets such as SNLI
also benefit from transfer learning, although the
improvement is smaller. We confirm the improve-
ment by showing that the result on SNLI when
pretraining on SQuAD with BiDAF is 82.6%,
which is slightly higher than that of the model
pretrained on SQuAD-T (81.6%). This, however,
did not outperform the state of the art (88.8%) by
Wang et al. (2017). This is mostly because BiDAF
(or BiDAF-T) is a QA model, which is not de-
signed for RTE tasks.
</p>
<p>517</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 518–523
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2082
</p>
<p>Self-Crowdsourcing Training for Relation Extraction
</p>
<p>Azad Abad†, Moin Nabi†, Alessandro Moschitti
†DISI, University of Trento, 38123 Povo (TN), Italy
</p>
<p>Qatar Computing Research Institute, HBKU, 34110, Doha, Qatar
{azad.abad,moin.nabi}@unitn.it
</p>
<p>amoschitti@gmail.com
</p>
<p>Abstract
</p>
<p>One expensive step when defining crowd-
sourcing tasks is to define the examples
and control questions for instructing the
crowd workers. In this paper, we intro-
duce a self-training strategy for crowd-
sourcing. The main idea is to use an au-
tomatic classifier, trained on weakly su-
pervised data, to select examples associ-
ated with high confidence. These are used
by our automatic agent to explain the task
to crowd workers with a question answer-
ing approach. We compared our relation
extraction system trained with data anno-
tated (i) with distant supervision and (ii)
by workers instructed with our approach.
The analysis shows that our method rela-
tively improves the relation extraction sys-
tem by about 11% in F1.
</p>
<p>1 Introduction
</p>
<p>Recently, the Relation Extraction (RE) task has at-
tracted the attention of many researchers due to
its wide range of applications such as question an-
swering, text summarization and bio-medical text
mining. The aim of this task is to identify the type
of relation between two entities in a given text.
Most work on RE has mainly regarded the applica-
tion of supervised methods, which require costly
annotation, especially for large-scale datasets.
</p>
<p>To overcome the annotation problem, Craven et
al. (1999) firstly proposed to collect automatic
annotation through Distant Supervision (DS). In
the DS setting, the training data for RE is of-
ten automatically annotated utilizing an external
Knowledge-Base (KB) such as Wikipedia or Free-
base (Hoffmann et al., 2010; Riedel et al., 2010;
Nguyen and Moschitti, 2011). Although DS has
</p>
<p>shown to be promising for RE, it also produces
many noisy labels in the automatic annotated data,
which deteriorate the performance of the system
trained on it.
</p>
<p>Hoffmann et al. (2011) showed that by simply
adding a small set of high quality labeled instances
(i.e., human-annotated training data) to a larger
set of instances annotated by DS, makes the over-
all precision of the system significantly increases.
Such level of quality of the labels usually can be
obtained at low cost via crowdsourcing.
</p>
<p>However, this finding does not hold for more
complex tasks, where the annotators1 need to have
some expertise on them. For instance in RE, sev-
eral works have shown that only a marginal im-
provement can be achieved via crowdsourcing the
data (Angeli et al., 2014; Zhang et al., 2012; Per-
shina et al., 2014). In such papers, the well-
known Gold Standard quality control mechanism
was used without annotators being trained.
</p>
<p>Very recently, despite the previous results, Liu
et al. (2016) showed a larger improvement for the
RE task when training crowd workers in an in-
teractive tutorial procedure called “Gated Instruc-
tion”. This approach, however, requires a set of
high-quality labeled data (i.e., the Gold Standard)
for providing the instruction and feedback to the
crowd workers. However, acquiring such data re-
quires a considerable amount of human effort.
</p>
<p>In this paper, we propose to alternatively
use Silver Standard, i.e., a high-quality auto-
matic annotated data, to train the crowd workers.
Specifically, we introduce a self-training strategy
for crowd-sourcing, where the workers are first
trained with simpler examples (which we assume
to be less noisy) and then gradually presented with
more difficult ones. This is biologically inspired
by the common human process of gradual learn-
</p>
<p>1From now, the both entities annotators and crowd work-
ers refer to the same concept.
</p>
<p>518</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2082">https://doi.org/10.18653/v1/P17-2082</a></div>
</div>
<div class="page"><p />
<p>Figure 1: User Interface of crowd worker training: instruction phase
</p>
<p>ing, starting from the simplest concepts.
Moreover, we propose an iterative human-
</p>
<p>machine co-training framework for the task of RE.
The main idea is (i) to automatically select a subset
of less-noisy examples applying an automatic clas-
sifier, (ii) training the annotators with such subset,
and (iii) iterating this process after retraining the
classifiers using the annotated data. That is, the
educated crowd workers can provide higher qual-
ity annotations, which can be used by the system
in the next iteration to improve the quality of its
classification. In other words, this cycle gradually
improves both system and human annotators. This
is in line with the studies in human-based compu-
tational approaches, which showed that the crowd
intelligence can effectively alleviate the drifting
problem in auto-annotation systems (Sun et al.,
2014; Russakovsky et al., 2015).
</p>
<p>Our study shows that even without using any
gold standard, we can still train workers and their
annotations can achieve results comparable with
the more costly state-of-the-art methods. In sum-
mary our contributions are the following:
</p>
<p>• we introduce a self-training strategy for
crowdsourcing;
</p>
<p>• we propose an iterative human-machine co-
training framework for the task of RE; and
</p>
<p>• we test our approach on a standard bench-
mark, obtaining a slightly lower perfor-
mance compared to the state-of-the-art meth-
ods based on Gold Standard data.
</p>
<p>This study opens up avenues for exploiting in-
expensive crowdsourcing solutions similar to ours
to achieve performance gain in NLP tasks.
</p>
<p>2 Background Work
</p>
<p>There is a large body of work on DS for RE, but we
only discuss the most related to our work and re-
fer the reader to other recent work (Wu and Weld,
2007; Mintz et al., 2009; Bunescu, 2007; Hoff-
mann et al., 2010; Riedel et al., 2010; Surdeanu
et al., 2012; Nguyen and Moschitti, 2011).
</p>
<p>Many researchers have exploited the techniques
of combining the DS data with small human anno-
tated data collected via crowdsourcing, to improve
the relation extractor accuracy (Liu et al., 2016;
Angeli et al., 2014; Zhang et al., 2012). Angeli
et al. (2014) reported a minor improvement using
active learning methods to select the best instances
to be crowdsourced.
</p>
<p>In the same direction, Zhang et al. (2012) stud-
ied the effect of providing human feedback in
crowdsourcing tasks and observed a minor im-
provement in terms of F1. At high level, our work
may be viewed as employing crowdsourcing for
RE. In that spirit, we are similar to these works,
but with the main difference of training crowd
workers to obtain higher quality annotations.
</p>
<p>The most related paper to our work is by Liu
et al. (2016), who trained the crowd workers via
“Gated Instruction”. They also showed that col-
lecting higher-quality annotations can be achieved
through training the workers. The produced data
also improved the performance of the RE systems
trained on it. Our study confirms their finding.
However, unlike them, we do not employ any Gold
Standard (annotated by experts) for training the
annotators and instead we propose a self-training
strategy to select a set of high-quality automatic
annotated data (namely, Silver Standard).
</p>
<p>519</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: User Interface of crowd worker training: interactive QA phase
</p>
<p>3 Self-Crowdsourcing Training
</p>
<p>In this section we first explain, our proposed
method for automatically identifying high-quality
examples (i.e., Silver Standard) to train the crowd
workers and collect annotations for the lower-
quality examples. We then explain the scheme de-
signed for crowd worker training and annotation
collection.
</p>
<p>3.1 Silver Standard Mining
</p>
<p>The main idea of our approach to Self-
Crowdsourcing training is to use the classifier’s
score for gradually training the crowd workers,
such that the examples and labels associated with
the highest prediction values (i.e., the most reli-
able) will be used as Silver Standard.
</p>
<p>More in detail, our approach is based on a
noisy-label dataset, DS, whose labels are ex-
tracted in a distant supervision fashion and CS a
dataset to be labeled by the crowd. The first step is
to divide CS into three parts: CSI , which is used
to create the instructions for the crowd workers;
CSQ, which is used for asking questions about
sentence annotations; and CSA, which is used to
collect the labels from annotators, after they have
been trained.
</p>
<p>To select CSI , we train a classifier C on DS,
and then used it to label CS examples. In partic-
ular, we used MultiR framework (Hoffmann et al.,
2011) to train C, as it is a widely used framework
for RE. Then, we sort CS in a descending order
according to the classifier prediction scores and se-
lect the first Ni elements, obtaining CSI .
</p>
<p>Next, we select the Nq examples of CS \ CSI
with the highest score to create the set CSQ. Note
that the latter contains highly-reliable classifier an-
notations but since the scores are lower than for
</p>
<p>CSI examples, we conjecture that they may be
more difficult to be annotated by the crowd work-
ers.
</p>
<p>Finally, CSA is assigned with the remaining ex-
amples, i.e., CS \ CSI \ CSQ. These have the
lowest confidence and should therefore be anno-
tated by crowd workers. Ni and Nq can be tuned
on the task, we set both to 10% of the data.
</p>
<p>3.2 Training Schema
</p>
<p>We conducted crowd worker training and annota-
tion collection using the well-known Crowdflower
platform2. Given CSI and CSQ (see Section 3.1),
we train the annotators in two steps:
</p>
<p>(i) User Instruction: first, a definition of each
relation type (borrowed from TAC-KBP official
guideline) is shown to the annotators. This ini-
tial training step provides the crowd workers with
a big picture of the task. We then train the anno-
tators showing them a set of examples from CSI
(see Fig. 1). The latter are presented in the reverse
order of difficulty. The ranked list of examples
provided by our self-training strategy facilitates
the gradual education of the annotators (Nosof-
sky, 2011). This gives us the benefit of training
the annotators with any level of expertise, which
is a crucial property of crowdsourcing when there
is no clue about the workers’ expertise in advance.
</p>
<p>(ii) Interactive QA: after the initial step, we
challenge the workers in an interactive QA task
with multiple-choice questions over the sentence
annotation (see Fig. 2). To accomplish that, we
designed an artificial agent that interacts with the
crowd workers: it corrects their mistakes and
makes them reasoning on why their answer was
wrong. Note that, to have a better control of the
</p>
<p>2www.crowdflower.com
</p>
<p>520</p>
<p />
</div>
<div class="page"><p />
<p>Precision Recall F1
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.38
</p>
<p>0.53
</p>
<p>0.44
</p>
<p>0.37
</p>
<p>0.47
</p>
<p>0.41
</p>
<p>0.35
</p>
<p>0.48
</p>
<p>0.4
</p>
<p>CSI CSQ CSA
</p>
<p>Figure 3: Accuracy of different CS partitions
</p>
<p>worker training, we perform a selection of the sen-
tences in CSQ to be used for questioning in a
category-wise fashion. Meaning that, we select the
subsets of examples for each class of relation sep-
arately. We observed in practice that initially a lot
of examples are classified as “No Relation”. This
is due to a difficulty of the task for the DS-based
model. Thus, we used them in CSA.
</p>
<p>4 Experimental Setup
</p>
<p>In this section, we first introduce the details of
the used corpora, then explain the feature extrac-
tion and RE pipeline and finally present the exper-
iments and discuss the results in detail.
</p>
<p>4.1 Corpora
We used TAC-KBP newswires, one of the most
well-known corpora for RE task. As DS, we se-
lected 700K sentences automatically annotated us-
ing Freebase as an external KB. We used the ac-
tive learning framework proposed by Angeli et al.
(2014) to select CS. This allowed us to select the
best sentences to be annotated by humans (sam-
pleJS). As a result, we obtained 4,388 sentences.
We divided the CS sentences in CSI , CSQ and
CSA, with 10%, 10% and 80% split, respectively.
We requested at least 5 annotations for each sen-
tence.
</p>
<p>Similarly to Liu et al. (2016), we restricted our
attention to 5 relations between person and loca-
tion3. For both DS and CS, we used the publicly
available data provided by Liu et al. (2016). Ulti-
mately, 221 crowd workers participated to the task
with minimum 2 and maximum 400 annotations
per crowd worker. To evaluate our model, we ran-
domly selected 200 sentences as test set and had
</p>
<p>3Nationality, Place-of-birth, Place-of-resident, Place-of-
death, Traveled-to
</p>
<p>Model Pr. Rec. F1
DS-only 0.43 0.52 0.47
Our Method 0.50 0.54 0.52
Gated Instruction 0.53 0.57 0.55
</p>
<p>Table 1: Evaluation of the impact of theCSA label
quality in the RE task.
</p>
<p>a domain expert to manually tag them using the
TAC-KBP annotation guidelines.
</p>
<p>4.2 Relation Extraction Pipeline
</p>
<p>We used the relation extractor, MultiR (Hoffmann
et al., 2010) along with lexical and syntactic fea-
tures proposed by Mintz et al. (2009) such as: (i)
Part of Speech (POS); (ii) windows of k words
around the matched entities; (iii) the sequences of
words between them; and (iv) finally, dependency
structure patterns between entity pairs. These
yield low-Recall as they appear in conjunctive
forms but at the same time they produce a high
Precision.
</p>
<p>4.3 Experimental Results
</p>
<p>In the first set of experiments, we verified the qual-
ity of our Silver Standard set used in our self-
training methods. For this purpose, we trained
MultiR on CSI , CSQ and CSA and evaluate them
on our test set. Figure 3 illustrates the results in
terms of Precision, Recall and F1 for each parti-
tion separately. They suggest that, the extractors
trained on CSI and CSQ are significantly better
than the extractor trained on the lower part of CS,
i.e., CSA, even if the latter is much larger than the
other two (80% vs. 10%).
</p>
<p>In the next set of experiments, we evaluated the
impact of adding a small set of crowdsourced data
to a large set of instances annotated by Distant Su-
pervision. We conducted the RE experiments in
this setting, as this allowed us to directly compare
with Liu et al. (2016). Thus, we used CSA anno-
tated by our proposed method along with the noisy
annotated DS to train the extractor.
</p>
<p>We compared our method with (i) the DS-only
baseline and (ii) the state of the art, Gated Instruc-
tion (GI) strategy (Liu et al., 2016). We empha-
size that the same set of examples (both DS and
CS) are used in this experiment and just replaced
the GI annotations with the annotations collected
using our proposed framework.
</p>
<p>521</p>
<p />
</div>
<div class="page"><p />
<p>Models DS-only Our Model GI
Accuracy 56% 82% 91%
</p>
<p>Table 2: Annotation Accuracy of crowd workers
</p>
<p>The results are shown in Table 1. Our method
improves the DS-only baseline by 7%, 5% and
2% (absolute) in Precision, Recall and F1, re-
spectively. This improvement clearly confirms the
benefit of our fully automatic approach to crowd-
sourcing in RE task.
</p>
<p>Additionally, our model is just 3% lower than
the GI method in terms of F1. In both our method
and GI, the crowd workers are trained before en-
rolling in the main task. However, GI trains an-
notators using Gold Standard data, which involves
a higher level of supervision with respect to our
method. Thus our self-training method is poten-
tially effective and an inexpensive alternative to
GI.
</p>
<p>We also analyzed the accuracy of the crowd
workers in terms of the quality of their annota-
tions. For this purpose, we randomly selected 100
sentences from CSA and then had them manually
annotated by an expert. We compared the accuracy
of the annotations collected with our proposed ap-
proach with those provided by DS-only baseline
and the GI method. Table 2 shows the results:
the annotations performed by workers trained with
our method are just slightly less accurate than the
annotations produced by annotators trained with
GI. This outcome is inline with the positive impact
of our good quality annotation on the RE perfor-
mance.
</p>
<p>5 Conclusion
</p>
<p>In this paper, we have proposed a self-training
strategy for crowdsourcing as an effective alterna-
tive to train annotators with Gold Standard. Our
experimental results show that the annotations of
workers trained with our method are accurate and
produce a good performance when used in learn-
ing algorithms for RE. Our study suggests that
automatically training annotators can replace the
popular consensus-based filtering scheme. Our
method achieves this goal through an inexpensive
training procedure.
</p>
<p>In the future, it would be interesting to study if
our method generalizes to other difficult or even
simpler tasks. In particular, our approach opens
up many research directions on how to best train
</p>
<p>workers or best select data for them, similarly to
what active learning methods have been doing for
training machines.
</p>
<p>Acknowledgement
</p>
<p>This work has been partially supported by the EC
project CogNet, 671625 (H2020-ICT-2014-2, Re-
search and Innovation action). Many thanks to
the anonymous reviewers for their valuable sug-
gestions.
</p>
<p>References
Gabor Angeli, Julie Tibshirani, Jean Y. Wu, and
</p>
<p>Christopher D. Manning. 2014. Combining distant
and partial supervision for relation extraction. In In
Proceedings of EMNLP. pages 1556–1567.
</p>
<p>Razvan C. Bunescu. 2007. Learning to extract rela-
tions from the web using minimal supervision. In In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL07).
</p>
<p>Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology. AAAI Press, pages 77–86.
http://dl.acm.org/citation.cfm?id=645634.663209.
</p>
<p>Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for infor-
mation extraction of overlapping relations. In
Proceedings of the 49th Annual Meeting of
the Association for Computational Linguis-
tics: Human Language Technologies - Volume
1. Association for Computational Linguistics,
Stroudsburg, PA, USA, HLT ’11, pages 541–550.
http://dl.acm.org/citation.cfm?id=2002472.2002541.
</p>
<p>Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extrac-
tors. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’10, pages 286–295.
http://dl.acm.org/citation.cfm?id=1858681.1858711.
</p>
<p>Angli Liu, Jonathan Bragg Xiao Ling Stephen Soder-
land, and Daniel S Weld. 2016. Effective crowd an-
notation for relation extraction. In Association for
Computational Linguistics. NAACL-HLT 2016.
</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Dan
Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2 - Volume
</p>
<p>522</p>
<p />
</div>
<div class="page"><p />
<p>2. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’09, pages 1003–1011.
http://dl.acm.org/citation.cfm?id=1690219.1690287.
</p>
<p>Truc-Vien T. Nguyen and Alessandro Moschitti.
2011. End-to-end relation extraction using distant
supervision from external semantic repositories.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume
2. Association for Computational Linguistics,
Stroudsburg, PA, USA, HLT ’11, pages 277–282.
http://dl.acm.org/citation.cfm?id=2002736.2002794.
</p>
<p>Robert M Nosofsky. 2011. The generalized context
model: An exemplar model of classification. For-
mal approaches in categorization pages 18–39.
</p>
<p>Maria Pershina, Bonan Min, Wei Xu, and Ralph Gr-
ishman. 2014. Infusion of labeled data into distant
supervision for relation extraction. In Proceedings
of the 2014 Conference of the Association for Com-
putational Linguistics (ACL 2014). Association for
Computational Linguistics, Baltimore, US.
</p>
<p>Sebastian Riedel, Limin Yao, and Andrew Mc-
Callum. 2010. Modeling relations and their
mentions without labeled text. In Proceedings
of the 2010 European Conference on Ma-
chine Learning and Knowledge Discovery in
Databases: Part III. Springer-Verlag, Berlin,
Heidelberg, ECML PKDD’10, pages 148–163.
http://dl.acm.org/citation.cfm?id=1889788.1889799.
</p>
<p>Olga Russakovsky, Li-Jia Li, and Li Fei-Fei. 2015.
Best of both worlds: Human-machine collaboration
for object annotation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
</p>
<p>Chong Sun, Narasimhan Rampalli, Frank Yang, and
AnHai Doan. 2014. Chimera: Large-scale classi-
fication using machine learning, rules, and crowd-
sourcing. Proc. VLDB Endow. 7(13):1529–1540.
https://doi.org/10.14778/2733004.2733024.
</p>
<p>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, EMNLP-CoNLL ’12, pages 455–465.
http://dl.acm.org/citation.cfm?id=2390948.2391003.
</p>
<p>Fei Wu and Daniel S. Weld. 2007. Autonomously
semantifying wikipedia. In Proceedings of the
Sixteenth ACM Conference on Conference on In-
formation and Knowledge Management. ACM,
New York, NY, USA, CIKM ’07, pages 41–50.
https://doi.org/10.1145/1321440.1321449.
</p>
<p>Ce Zhang, Feng Niu, Christopher Ré, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In Proceedings
</p>
<p>of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers - Volume
1. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’12, pages 825–834.
http://dl.acm.org/citation.cfm?id=2390524.2390640.
</p>
<p>523</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–529
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2083
</p>
<p>A Generative Attentional Neural Network Model for Dialogue Act
Classification
</p>
<p>Quan Hung Tran and Ingrid Zukerman and Gholamreza Haffari
Faculty of Information Technology
</p>
<p>Monash University, Australia
hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu
</p>
<p>Abstract
</p>
<p>We propose a novel generative neural net-
work architecture for Dialogue Act clas-
sification. Building upon the Recurrent
Neural Network framework, our model in-
corporates a new attentional technique and
a label-to-label connection for sequence
learning, akin to Hidden Markov Mod-
els. Our experiments show that both of
these innovations enable our model to out-
perform strong baselines for dialogue-act
classification on the MapTask and Switch-
board corpora. In addition, we analyse
empirically the effectiveness of each of
these innovations.
</p>
<p>1 Introduction
</p>
<p>Dialogue Act (DA) classification is a sequence-
to-sequence learning task where a sequence of
utterances is mapped into a sequence of DAs.
Some works in DA classification treat each ut-
terance as an independent instance (Julia et al.,
2010; Gambäck et al., 2011), which leads to ig-
noring important long-range dependencies in the
dialogue history. Other works have captured
inter-utterance relationships using models such as
Hidden Markov Models (HMMs) (Stolcke et al.,
2000; Surendran and Levow, 2006) or Recur-
rent Neural Networks (RNNs) (Kalchbrenner and
Blunsom, 2013; Ji et al., 2016), where RNNs have
been particularly successful.
</p>
<p>In this paper, we present a generative model of
utterances and dialogue acts which conditions on
the relevant part of the dialogue history. To this
effect, we use the attention mechanism (Bahdanau
et al., 2014) developed originally for sequence-to-
sequence models, which has proven effective in
Machine Translation (Bahdanau et al., 2014; Lu-
ong et al., 2015) and DA classification (Shen and
</p>
<p>Lee, 2016). The intuition is that different parts of
an input sequence have different levels of impor-
tance with respect to the objective, and this mech-
anism enables the selection of the important parts.
However, the traditional attention mechanism suf-
fers from the attention-bias problem (Wang et al.,
2016), where the attention mechanism tends to fa-
vor the inputs at the end of a sequence. To address
this problem, we propose a gated attention mech-
anism, where the attention signal is represented as
a gate over the input vector.
</p>
<p>In addition, when generating a dialogue act, we
capture its direct dependence on the previous di-
alogue act — a reasonable source of information,
which, surprisingly, has not been explored in the
RNN literature for DA classification.
</p>
<p>Our experiments show that our model signifi-
cantly outperforms variants that do not have our
innovations, i.e., the gated attention mechanism
and direct label-to-label dependency.
</p>
<p>2 Model Description
</p>
<p>Assume that we have a training dataset D com-
prising a collection of dialogues, where each dia-
logue consists of a sequence of utterances {yt}Tt=1
and the corresponding sequence of dialogue acts
{zt}Tt=1. Each utterance yt is a sequence of to-
kens, and its n-th token is denoted yt,n.
</p>
<p>We propose a generative neural model for
dialogue PΘΘΘ(y1:T , z1:T ), which specifies a
joint probability distribution over a sequence
of utterances y1:T and the corresponding
sequence of dialogue acts z1:T . This gener-
ative model is then trained discriminatively
by maximising the conditional log-likelihood∑
</p>
<p>(z1:T ,y1:T )∈D logPΘΘΘ(z1:T |y1:T ):
</p>
<p>arg max
ΘΘΘ
</p>
<p>∑
</p>
<p>(y1:T ,z1:T )∈D
log
</p>
<p>PΘΘΘ(y1:T , z1:T )∑
z′1:T
</p>
<p>PΘΘΘ(y1:T , z
′
1:T )
</p>
<p>524</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2083">https://doi.org/10.18653/v1/P17-2083</a></div>
</div>
<div class="page"><p />
<p>Figure 1: Graphical model representation of our model. Red connections depict dialogue-act genera-
tion (1); purple connections (dashed and continuous) depict utterance generation (2).
</p>
<p>where ΘΘΘ represents all neural network param-
eters. Discriminative training is employed in
order to match the use of the model for pre-
dicting dialogue acts during test time, using
arg maxz′1:T PΘΘΘ(z
</p>
<p>′
1:T |y1:T ).
</p>
<p>The generative story of our model is as follows:
(1) generate the dialogue act of the current dia-
logue turn conditioned on the previous dialogue
act and the previous utterance PΘΘΘ(zt|zt−1,yt−1);
and (2) generate the current utterance condi-
tioned on the previous utterance and the current
dialogue act PΘΘΘ(yt|zt,yt−1). In other words,
PΘΘΘ(z1:T ,y1:T ) is decomposed as:
</p>
<p>T∏
</p>
<p>t=1
</p>
<p>PΘΘΘ(zt|zt−1,yt−1)PΘΘΘ(yt|zt,yt−1). (1)
</p>
<p>Furthermore, each utterance is generated by a
sequential process whereby each token yt,n is con-
ditioned on all the previously generated tokens
yt,&lt;n, as well as the external conditioning context
consisting of the dialogue act zt and the previous
turn’s utterance yt−1, i.e.,
</p>
<p>PΘΘΘ(yt|zt,yt−1) =
|yt|∏
</p>
<p>n=1
</p>
<p>PΘΘΘ(yt,n|yt,&lt;n, zt,yt−1).
</p>
<p>(2)
</p>
<p>Importantly, the decomposition of the joint dis-
tribution in Equation 1 allows dynamic program-
ming for exact decoding (§2.2). One possible
extension of our framework is to investigate a
higher-order Markov model, although one needs
to be conscious about the trade-off between the in-
crease in the computational complexity of train-
ing/decoding with higher-order Markov models
versus the potential gain in classification quality.
</p>
<p>We now turn our attention to the neural architec-
ture used to realise the components of our prob-
abilistic model (Figure 1). We define the neural
</p>
<p>model for the conditional probability of the next
dialogue act as follows:
</p>
<p>PΘΘΘ(zt|zt−1,yt−1) =
softmax(W (zt−1)cz ct + b
</p>
<p>(zt−1)
z ),
</p>
<p>(3)
</p>
<p>where ct is the context vector summarising the in-
formation from the previous utterance yt−1, and
W
</p>
<p>(zt−1)
cz and b
</p>
<p>(zt−1)
z are the softmax parameter
</p>
<p>gated on the previous dialogue act zt−1. Due to
gating, the number of parameters of the model
may increase significantly; therefore, we have also
explored a variant where only the bias term b(zt−1)z
is gated. We define the neural model for generat-
ing the tokens of the current utterance as follows:
</p>
<p>PΘΘΘ(yt,n|yt,&lt;n, zt,yt−1) =
softmax(W (zt)hy ht,n−1 + Wcct + by),
</p>
<p>(4)
</p>
<p>where the weight matrix W (zt)hy is gated based
on zt, ct summarises the previous utterance, and
ht,n−1 is the state of an utterance-level RNN sum-
marising all the previously generated tokens:
</p>
<p>ht,n−1 = fff(ht,n−2,EEEyt,n−1), (5)
</p>
<p>where EEEyt,n−1 provides the embedding of the to-
ken yt,n−1 from the embedding table EEE, and fff
can be any non-linear function, i.e., the sim-
ple sigmoid applied to elements of a vec-
tor, or the more complex Long-Short-Term-
Memory unit (LSTM) (Graves, 2013; Hochreiter
and Schmidhuber, 1997), or the Gated-Recurrent-
Unit (GRU) (Chung et al., 2014; Cho et al., 2014).
</p>
<p>In what follows, we elaborate on how to best
summarise the information from the previous ut-
terance in ct, and how to decode for the best se-
quence of dialogue acts given a trend model.
</p>
<p>525</p>
<p />
</div>
<div class="page"><p />
<p>2.1 The Gated Attention Mechanism
Given a sequence of words in an utterance
{y1, . . . , yn}, we would like to compress its infor-
mation in c, which is then used in the conditioning
contexts of other components of the model. Typ-
ically, the last hidden state of the utterance-level
RNN is taken to be the summary vector: c = hn.
However, it has been shown that attending to all
RNN states is more effective.
</p>
<p>The traditional attention mechanism (Bahdanau
et al., 2014) employs a probability vector a over
the words of the input utterance to summarise it.
The attention elements in a are typically calcu-
lated from the current input yn, and the previous
hidden state hn−1:
</p>
<p>αn = g(hn−1,EEEyn) , an =
eαn∑n
</p>
<p>n′=1 e
αn′
</p>
<p>,
</p>
<p>where g is a non-linear function. Once the atten-
tion is defined, the representation of the input is
constructed as
</p>
<p>c =
∑
</p>
<p>n
</p>
<p>anhhhn. (6)
</p>
<p>The problem with this traditional attention
model is that the final hidden state is a function
of all the inputs, hence it is usually more “infor-
mative” than the earlier hidden states due to se-
mantic accumulation (Wang et al., 2016). Thus,
most of the attention signal is assigned to the hid-
den states toward the end of a sequence. In DA
classification, this may not be desirable, since an
important token with respect to a dialogue act can
appear anywhere in an utterance. We call this the
attention bias problem.
</p>
<p>We propose a novel gated attention mechanism,
which is inspired by the gating mechanism in
LSTMs, to fix the attention bias problem. Simi-
lar to the forget gate of LSTMs, we use the avail-
able information to calculate an attention gate that
learns whether to allow the whole input signal to
pass through or to forget all or a part of the input
signal:
</p>
<p>an = ggg(hn−1,EEEyn) (7)
</p>
<p>xn = an �EEEyn (8)
hn = fff(hn−1,xn) (9)
</p>
<p>where � represents element-wise multiplication.
After filtering the important signal from the in-
</p>
<p>put token, the information from our tokens is accu-
mulated in the last hidden state of the RNN, which
</p>
<p>we take as the summary vector c = hhhn. Note that
since the gated attention is applied to the input be-
fore the RNN calculations, it is not affected by the
attention bias.
</p>
<p>2.2 Inference: Viterbi Decoding
</p>
<p>For prediction, we choose the sequence of dia-
logue acts with the highest posterior probability:
</p>
<p>arg max
z′1:T
</p>
<p>PΘΘΘ(z
′
1:T |y1:T )=arg max
</p>
<p>z′1:T
PΘΘΘ(z
</p>
<p>′
1:T ,y1:T )
</p>
<p>Since the joint probability is decomposed fur-
ther according to Equation 1, we can make use of
dynamic programming to find the highest prob-
ability sequence of dialogue acts. Specifically,
the model endows each latent variable zt with a
unary potential PΘΘΘ(yt|zt,yt−1) and binary poten-
tial PΘΘΘ(zt|zt−1,yt−1) functions. PΘΘΘ(yt|zt,yt−1)
and PΘΘΘ(zt|zt−1,yt−1) are akin to the emission and
transition functions of an HMM, and are calcu-
lated using Equations 2 and 3 respectively. Fur-
thermore, the model has been carefully designed
so that the hidden states in the RNNs encod-
ing the utterances to form the context vector ct
(the representation of the previous utterance) are
not affected by the sequence of dialogue acts,
which is crucial to making the inference amenable
to dynamic programming. The resulting infer-
ence algorithm is akin to the Viterbi algorithm for
HMMs.
</p>
<p>3 Experiments
</p>
<p>Datasets. We conduct our experiments on the
MapTask and Switchboard corpora. The MapTask
Dialog Act corpus (Anderson et al., 1991) con-
sists of 128 conversations and more than 27000
utterances in an instruction-giving scenario. There
are 13 DA types in this corpus. For the experi-
ments, the available data is split into three parts,
train/test/validation with 103, 13 and 12 conversa-
tions respectively.
</p>
<p>The Switchboard Dialog Act corpus (Jurafsky
et al., 1997) consists of 1155 transcribed telephone
conversations with around 205000 utterances. In
contrast with the MapTask conversations, which
are task-oriented, the Switchboard corpus con-
sists mostly of general topic conversations. The
Switchboard tag set has 42 DAs.1
</p>
<p>1The original size of the tag set for Switchboard is 226,
which was then collapsed into 42
</p>
<p>526</p>
<p />
</div>
<div class="page"><p />
<p>without gate bias gate all
HMM HMM HMM
</p>
<p>no attn. 60.97% 64.60% 63.55%
traditional 61.72% 64.73% 65.19%
gated attn. 62.21% 65.94% 65.94%
</p>
<p>Table 1: Comparison of our model variants on the
MapTask corpus.
</p>
<p>Baselines. On MapTask, to the best of our
knowledge, there is no standard data split, thus, we
make the comparison against our implementation
of strong baselines such as HMM-trigram (Stol-
cke et al., 2000) and instance-based random forest
classifier (1/2/3-gram features). Ji et al.’s (2016)
results for this corpus are obtained by running
their publicly available code with the same hyper
parameters as those used by our models. We also
report the results of Julia et al. (2010)2 and Suren-
dran et al. (2006). However, the experimental
setup of these two works differs from ours, hence
their results are not directly comparable to ours.
</p>
<p>On Switchboard, we compare our results with
strong baselines using the experimental setup from
Kalchbrenner and Blunsom (2013) and Stolcke et
al. (2000).3
</p>
<p>Our Model Configurations. We experiment
with several variants of our model to explore
the effectiveness of our two improvements: the
HMM-like connection and the gated attention
mechanism. For the HMM connection, we con-
sider three choices: gating all parameters (Equa-
tion 3), gating only the bias, and no connection.
For the attention, we consider three choices: our
new gated attention mechanism, the traditional at-
tention, and no attention. Thus, in total, we ex-
plore nine model variants.
</p>
<p>All the model variants are implemented with the
CNN package4 and trained with Adagrad (Duchi
et al., 2011) using dropout (Srivastava et al., 2014).
They share the same word-embedding size (128)
and hidden vector size (64).5
</p>
<p>2Julia et al. (2010) employed both text transcription and
audio signal. Here, we report the results obtained with the
transcription.
</p>
<p>3There have been other works with different experimental
setups (Gambäck et al., 2011; Webb and Ferguson, 2010) that
obtained accuracies ranging from 77.85% to 80.72%. How-
ever, these results are not directly comparable to ours.
</p>
<p>4https://github.com/clab/cnn-v1.
5The experiments were executed on an Intel Xeon E5-
</p>
<p>2667 CPU with 16GB of RAM. The training time for each
MapTask model is less than a day, the training time for each
Switchboard model takes up to four weeks.
</p>
<p>Models Accuracy
Julia et al. (2010) 55.40%
Surendran et al. (2006) 59.10%
HMM (Stolcke et al. (2000)) 51.40%
Random Forest (n-gram) 55.72%
Ji et al. (2016) 60.97%
Our model
</p>
<p>gated attn. + gated HMM bias 65.94%
gated attn. + gated HMM all 65.94%
</p>
<p>Table 2: Results on MapTask data.
</p>
<p>Models Accuracy
Stolcke et al. (2000) 71.0%
Kalchbrenner and Blunsom (2013) 73.9%
Ji et al. (2016) 72.5%
Shen and Lee (2016) 72.6%
our model
</p>
<p>gated attn. + gated HMM bias 74.2%
gated attn. + gated HMM all 74.0%
</p>
<p>Table 3: Results on Switchboard data.
</p>
<p>Results and Analysis. Table 1 shows the classi-
fication accuracy of the nine variants of our model
on the MapTask corpus. The classification accu-
racy of the two best variants of our model and the
baselines appears in Tables 2 and 3 for MapTask
and Switchboard respectively. The bold numbers
in each table show the best accuracy achieved by
the systems. As seen in these tables, our best mod-
els outperform strong baselines for both corpora.6
</p>
<p>Table 1 shows that adding the attention mecha-
nism is beneficial, as the traditional attention mod-
els always outperform their non-attention coun-
terparts. The gated attention configurations, in
turn, outperform those with the traditional atten-
tion mechanism by 0.49%-1.21%. Interestingly,
the accuracy of Shen and Lee’s (2016) classifier,
which employs an attention mechanism, is lower
than that obtained by Kalchbrenner and Blun-
som (2013), whose mechanism does not use atten-
tion. We believe that the difference in performance
is not due to the attention mechanism being inef-
fective, but because Shen and Lee (2016) treat the
classification of each utterance independently. In
contrast, Kalchbrenner and Blunsom (2013) take
</p>
<p>6Ji et al. (2016) reported an accuracy of 77.0% on the
Switchboard corpus, but their paper does not provide enough
information about the experimental setup to replicate this re-
sult (hyper-parameters, train/test/development split). Thus,
we ran the paper’s publicly available code with our experi-
mental settings, and report the result in our comparison.
</p>
<p>527</p>
<p />
</div>
<div class="page"><p />
<p>the sequential nature of dialog acts into account,
and run an RNN across the conversation, which
conditions the generation of a dialogue act on the
dialogue acts and utterances in all the previous di-
alogue turns.
</p>
<p>As seen in Table 1, the performance gain from
the HMM connection is larger than the gain from
the attention mechanism. Without the attention
mechanism, the HMM connection brings an in-
crease of 3.63% with the gated bias HMM config-
uration and 2.58% with the fully gated HMM con-
figuration. With the use of traditional attention,
the improvement is 3.01% for the bias HMM con-
figuration and 3.47% for the gated HMM config-
uration. Finally with the gated attention in place,
the two HMM configurations improve the accu-
racy by 3.73%.
</p>
<p>We used McNemar’s test to determine the statis-
tical significance between the predictions of differ-
ent models, and found that our model with both in-
novations (HMM connections and gated attention)
is statistically significantly better than the variant
without these innovations with α &lt; 0.01.
</p>
<p>4 Conclusions
</p>
<p>In this work, we have proposed a new gated at-
tention mechanism and a novel HMM-like con-
nection in a generative model of utterances and
dialogue acts. Our experiments show that these
two innovations significantly improve the accu-
racy of DA classification on the MapTask and
Switchboard corpora. In the future, we plan to
apply these two innovations to other sequence-to-
sequence learning tasks. Furthermore, DA classi-
fication itself can be seen as a preprocessing step
in a dialogue system’s pipeline. Thus, we also plan
to investigate the effect of improvements in DA
classification on the downstream components of a
dialogue system.
</p>
<p>References
</p>
<p>Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, et al. 1991. The HCRC map task corpus.
Language and speech 34(4):351–366.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .
</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .
</p>
<p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .
</p>
<p>John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research 12(Jul):2121–2159.
</p>
<p>Björn Gambäck, Fredrik Olsson, and Oscar Täckström.
2011. Active learning for dialogue act classification.
In Interspeech 2011 – Proceedings of the Interna-
tional Conference on Spoken Language Processing.
pages 1329–1332.
</p>
<p>Alex Graves. 2013. Generating sequences with
recurrent neural networks. arXiv preprint
arXiv:1308.0850 .
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen-
stein. 2016. A latent variable recurrent neural net-
work for discourse-driven language models. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
332–342. http://www.aclweb.org/anthology/N16-
1037.
</p>
<p>Fatema N Julia, Khan M Iftekharuddin, and ATIQ U
ISLAM. 2010. Dialog act classification using
acoustic and discourse information of maptask data.
International Journal of Computational Intelligence
and Applications 9(04):289–311.
</p>
<p>Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual,
Draft 13. Technical report, University of Colorado.
</p>
<p>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. arXiv preprint arXiv:1306.3584 .
</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing. pages 1412–1421.
http://aclweb.org/anthology/D15-1166.
</p>
<p>Sheng-syun Shen and Hung-yi Lee. 2016. Neural at-
tention models for sequence classification: Analysis
and application to key term extraction and dialogue
act detection. arXiv preprint arXiv:1604.00077 .
</p>
<p>528</p>
<p />
</div>
<div class="page"><p />
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov.
2014. Dropout: A simple way to prevent
neural networks from overfitting. Journal of
Machine Learning Research 15(1):1929–1958.
http://dl.acm.org/citation.cfm?id=2627435.2670313.
</p>
<p>Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.
</p>
<p>Dinoj Surendran and Gina-Anne Levow. 2006. Dia-
log act tagging with support vector machines and
hidden markov models. In Interspeech 2006 – Pro-
ceedings of the International Conference on Spoken
Language Processing. pages 1950–1953.
</p>
<p>Bingning Wang, Kang Liu, and Jun Zhao. 2016. In-
ner attention based recurrent neural networks for an-
swer selection. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). pages 1288–1297.
http://www.aclweb.org/anthology/P16-1122.
</p>
<p>Nick Webb and Michael Ferguson. 2010. Automatic
extraction of cue phrases for cross-corpus dialogue
act classification. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters. pages 1310–1317.
</p>
<p>529</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 530–535
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2084
</p>
<p>Salience Rank: Efficient Keyphrase Extraction with Topic Modeling
</p>
<p>Nedelina Teneva∗
The University of Chicago
</p>
<p>nteneva@uchicago.edu
</p>
<p>Weiwei Cheng
Amazon
</p>
<p>weiweic@amazon.de
</p>
<p>Abstract
</p>
<p>Topical PageRank (TPR) uses latent topic
distribution inferred by Latent Dirichlet
Allocation (LDA) to perform ranking of
noun phrases extracted from documents.
The ranking procedure consists of running
PageRank K times, where K is the num-
ber of topics used in the LDA model. In
this paper, we propose a modification of
TPR, called Salience Rank. Salience Rank
only needs to run PageRank once and ex-
tracts comparable or better keyphrases on
benchmark datasets. In addition to quality
and efficiency benefits, our method has the
flexibility to extract keyphrases with vary-
ing tradeoffs between topic specificity and
corpus specificity.
</p>
<p>1 Introduction
</p>
<p>Automatic keyphrase extraction consists of find-
ing a set of terms in a document that provides a
concise summary of the text content (Hasan and
Ng, 2014). In this paper we consider unsuper-
vised keyphrase extraction, where no human la-
beled corpus of documents is used for training
a classifier (Grineva et al., 2009; Pasquier, 2010;
Liu et al., 2009b; Zhao et al., 2011; Liu et al.,
2009a). This is a scenario often arising in practical
applications as human annotation and tagging is
both time and resource consuming. Unsupervised
keyphrase extraction is typically casted as a rank-
ing problem – first, candidate phrases are extracted
from documents, typically noun phrases identi-
fied by part-of-speech tagging; then these candi-
dates are ranked. The performance of unsuper-
vised keyphrase extraction algorithms is evaluated
by comparing the most highly ranked keyphrases
with keyphrases assigned by annotators.
</p>
<p>∗ Work done as an intern at Amazon.
</p>
<p>This paper proposes Salience Rank, a modifica-
tion of Topical PageRank algorithm by Liu et al.
(2010). Our method is close in spirit to Single
Topical PageRank by Sterckx et al. (2015) and
includes it as a special case. The advantages of
Salience Rank are twofold:
Performance: The algorithm extracts high-
quality keyphrases that are comparable to, and
sometimes better than, the ones extracted by Top-
ical PageRank. Salience Rank is more efficient
than Topical PageRank as it runs PageRank once,
rather than multiple times.
Configurability: The algorithm is based on the
concept of “word salience” (hence its name),
which is described in Section 3 and can be used
to balance topic specificity and corpus specificity
of the extracted keyphrases. Depending on the use
case, the output of the Salience Rank algorithm
can be tuned accordingly.
</p>
<p>2 Review of Related Models
</p>
<p>Below we introduce some notation and discuss ap-
proaches that are most related to ours.
</p>
<p>Let W = {w1, w2, . . . , wN} be the set of all
the words present in a corpus of documents. Let
G = (W,E) denote a word graph, whose vertices
represent words and an edge e(wi, wj) ∈ E in-
dicates the relatedness between words wi and wj
in a document (measured, e.g., by co-occurrence
or number of co-occurrences between the two
words). The outdegree of vertex wi is given by
Out(wi) =
</p>
<p>∑
i:wi→wj e(wi, wj).
</p>
<p>2.1 Topical PageRank
The main idea behind Topical PageRank (TPR)
(Liu et al., 2010) is to incorporate topical infor-
mation by performing Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) on a corpus of docu-
ments. TPR constructs a word graph G = (W,E)
based on the word co-occurrences within docu-
ments. It uses LDA to find the latent topics of the
</p>
<p>530</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2084">https://doi.org/10.18653/v1/P17-2084</a></div>
</div>
<div class="page"><p />
<p>document, reweighs the word graph according to
each latent topic, and runs PageRank (Page et al.,
1998) once per topic.
</p>
<p>In LDA each word w of a document d is as-
sumed to be generated by first sampling a topic
t ∈ T (where T is a set ofK topics) from d’s topic
distribution θd and then sampling a word from the
distribution over words φt of topic t. Both θd and
φt are drawn from conjugate Dirichlet priors α and
β, respectively. Thus, the probability of word w,
given document d and the priors α and β, is
</p>
<p>p(w | d, α, β) =
∑
</p>
<p>t∈T
p(w | t, β) p(t | d, α) . (1)
</p>
<p>After running LDA, TPR ranks each word wi ∈
W of G by
</p>
<p>Rt(wi) = λ
∑
</p>
<p>j:wj→wi
</p>
<p>e(wi, wj)
</p>
<p>Out(wj)
Rt(wj) + (1−λ)p(t |wi) ,
</p>
<p>(2)
</p>
<p>for t ∈ T , where p(t |w) is estimated via LDA.
TPR assigns a topic specific preference value
</p>
<p>p(t |w) to each w ∈ W as the jump probability
at each vertex depending on the underlying topic.
Intuitively, p(t |w) indicates how much the word
w focuses on topic t.1
</p>
<p>At the next step of TPR, the word scores (2) are
accumulated into keyphrase scores. In particular,
for each topic t, a candidate keyphrase is ranked
by the sum of the word scores
</p>
<p>Rt(phrase) =
∑
</p>
<p>wi∈phrase
Rt(wi) . (3)
</p>
<p>By combining the topic specific keyphrase scores
Rt(phrase) with the probability p(t | d) derived
from the LDA we can compute the final keyphrase
scores across all K topics:
</p>
<p>R(phrase) =
∑
</p>
<p>t∈T
Rt(phrase) p(t | d) . (4)
</p>
<p>2.2 Single Topical PageRank
Single Topical PageRank (STPR) was recently
proposed by Sterckx et al. (2015). It aims
to reduce the runtime complexity of TPR and
at the same time maintain its predictive perfor-
mance. Similar to Salience Rank, it runs PageR-
ank once. STPR is based on the idea of “top-
ical word importance” TWI (w), which is de-
fined as the cosine similarity between the vector of
</p>
<p>1Liu et al. (2010) proposed two other quantities to bias
the random walk, p(w | t) and p(w | t)p(t |w), and showed
that p(t |w) achieves the best empirical result. We therefore
adopt the use of p(t |w) here.
</p>
<p>word-topic probabilities [p(w | t1), . . . , p(w | tK)]
and the vector of document-topic probabilities
[p(t1 | d), . . . , p(tK | d)], for each word w given
the document d. STPR then uses PageRank to rank
each word wi ∈ W by replacing p(t |wi) in (2)
with TWI (wi)∑
</p>
<p>wk∈W TWI (wk)
.
</p>
<p>STPR can be seen as a special case of Salience
Rank, where topic specificity of a word is consid-
ered when constructing the random walk, but cor-
pus specificity is neglected. In practice, however,
balancing these two concepts is important. It may
explain why Salience Rank outperforms STPR in
our experiments.
</p>
<p>3 Salience Rank
</p>
<p>In order to achieve performance and configurabil-
ity, the Salience Rank (SR) algorithm combines
the K latent topics estimated by LDA into a word
metric, called word salience, and uses it as a pref-
erence value for each wi ∈ W . Thus, SR needs
to perform only a single run of PageRank on the
word graph G in order to obtain a ranking of the
words in each document.
</p>
<p>3.1 Word Salience
In the following we provide quantitative measures
for topic specificity and corpus specificity, and de-
fine word salience.
</p>
<p>Definition 3.1 The topic specificity of a word w is
</p>
<p>TS (w) =
∑
</p>
<p>t∈T
p(t |w) log p(t |w)
</p>
<p>p(t)
</p>
<p>= KL (p(t |w) ‖ p(t)) .
(5)
</p>
<p>The definition of topic specificity of a word w
is equivalent to Chuang et al. (2012)’s proposal
of the distinctiveness of a word w, which is in
turn equivalent to the Kullback-Leibler (KL) di-
vergence from the marginal probability p(t), i.e.,
the likelihood that any randomly selected word is
generated by topic t, to the conditional probability
p(t |w), i.e., the likelihood that an observed word
w is generated by a latent topic t. Intuitively, topic
specificity measures how much a word is shared
across topics: The less w is shared across topics,
the higher its topic specificity TS (w).
</p>
<p>As TS (w) is non-negative and unbounded, we
can empirically normalize it to [0, 1] by
</p>
<p>TS (w)−minuTS (u)
maxuTS (u)−minuTS (u)
</p>
<p>531</p>
<p />
</div>
<div class="page"><p />
<p>with the minimum and maximum topic specificity
values in the corpus. In what follows, we always
use normalized topic specificity values, unless ex-
plicitly stated otherwise.
</p>
<p>We apply a straightforward definition for corpus
specificity.
</p>
<p>Definition 3.2 The corpus specificity of a word w
is
</p>
<p>CS (w) = p(w | corpus) . (6)
</p>
<p>The corpus specificity CS (w) of a word w can be
estimated by counting word frequencies in the cor-
pus of interest. Finally, a word’s salience is de-
fined as a linear combination of its topic specificity
and corpus specificity.
</p>
<p>Definition 3.3 The salience of a word w is
</p>
<p>S(w) = (1− α)CS (w) + αTS (w) , (7)
</p>
<p>where α ∈ [0, 1] is a parameter controlling the
tradeoff between the corpus specificity and the
topic specificity of w.
</p>
<p>On one hand, we aim to extract keyphrases that
are relevant to one or more topics while, on the
other hand, the extracted keyphrases as a whole
should have a good coverage of the topics in the
document. Depending on the downstream appli-
cations, it is often useful to be able to control the
balance between these two competing principles.
In other words, sometimes keyphrases with high
topic specificity (i.e., phrases that are representa-
tive exclusively for certain topics) are more appro-
priate, while other times keyphrases with high cor-
pus specificity (i.e., phrases that are representative
of the corpus as a whole) are more appropriate. In-
tuitively, it is advantageous for a keyphrase extrac-
tion algorithm to have an internal “switch” tun-
ing the extent to which extracted keyphrases are
skewed towards particular topics and, conversely,
the extent to which keyphrases generalize across
different topics.
</p>
<p>It needs to be emphasized that the choice of
quantitative measures for topic specificity and cor-
pus specificity used above is just one among many
possibilities. For example, for topic specificity,
one can make use of the topical word impor-
tance by Sterckx et al. (2015), or the several
other alternatives mentioned in Section 2.1 pro-
posed by Liu et al. (2010). For corpus speci-
ficity, alternatives besides vanilla term frequen-
cies, such as augmented frequency (to discount
</p>
<p>longer documents) and logarithmically scaled fre-
quency, quickly come into mind.
</p>
<p>Taking word salience into account, we modify
(2) as follow:
</p>
<p>R(wi) = λ
∑
</p>
<p>j:wj→wi
</p>
<p>e(wj , wi)
</p>
<p>Out(wj)
R(wj) + (1− λ)S(wi) .
</p>
<p>(8)
</p>
<p>The substantial efficiency boost of SR comparing
to TPR lies in the fact that in (2) K PageRanks are
required to calculate Rt(wi), t = 1 . . .K before
obtaining R(wi), while in (8) R(wi) is obtained
with a single PageRank.
</p>
<p>3.2 Algorithm Description
</p>
<p>First, SR performs LDA to estimate the latent top-
ics p(t) presented in the corpus and the probabil-
ity p(t |w), which are used to calculate the topic
specificity and the salience of each word w.
</p>
<p>Similarly to TPR, SR is performed on the word
co-occurrence graph G = (W,E). We use undi-
rected graphs: When sliding a window of size s
through the document, a link between two vertices
is added if these two words appear within the win-
dow. It was our observation that the edge direction
does not affect the keyphrase extraction perfor-
mance much. The same observation was noted by
Mihalcea and Tarau (2004) and Liu et al. (2010).
</p>
<p>We then run the updated version of PageRank
derived in (8) and compute the scores of the can-
didate keyphrases similarly to the way TPR does
using (4). For a fair comparison, noun phrases
with the pattern (adjective)*(noun)+ are
chosen as candidate keyphrases, which represents
zero or more adjectives followed by one or more
nouns. It is the same pattern suggested by Liu et al.
(2010) in the original TPR paper. SR combines the
K PageRank runs in TPR into a single one using
salience as a preference value in the word graph.
</p>
<p>4 Results
</p>
<p>Our experiments are conducted on two widely
used datasets in the keyphrase extraction litera-
ture, 500N-KPCrowd (Marujo et al., 2013) and In-
spec (Hulth, 2003). The 500N-KPCrowd dataset
consists of 500 news articles, 50 stories for
each of 10 categories, manually annotated with
keyphrases by 20 Amazon Mechanical Turk work-
ers. The Inspec dataset is a collection of 2000
paper abstracts of Computer Science &amp; Informa-
tion Technology journal with manually assigned
</p>
<p>532</p>
<p />
</div>
<div class="page"><p />
<p>dataset algorithm precision recall F measure
</p>
<p>500N-KPCrowd
TPR 0.254 0.222 0.229 (±0.010)
STPR 0.252 0.221 0.228 (±0.011)
SR 0.253 0.222 0.229 (±0.010)
</p>
<p>Inspec
TPR 0.225 0.255 0.227 (±0.007)
STPR 0.222 0.254 0.224 (±0.007)
SR 0.265 0.298 0.266 (±0.007)
</p>
<p>Table 1: Comparison of the algorithms on 500N-KPCrowd and Inspec. On both datasets, TPR, STPR and
SR were run with 50 LDA topics. In all experiments we used a damping factor λ = 0.85 in PageRank,
as in the original PageRank algorithm, and a window size s = 2 to construct the word graphs. Changing
the window size s from 2 to 20 does not influence the results much, as also observed in Liu et al. (2010).
The convergence of PageRank is achieved when the l2 norm of the vector containing R(wi) changes
smaller than 10−6. The tradeoff parameter α in SR is fixed at 0.4. The 95% confidence interval for the F
measure is shown in the last column.
</p>
<p># topics precision recall F measure
5 0.249 0.218 0.225 (±0.011)
</p>
<p>50 0.253 0.222 0.229 (±0.010)
250 0.247 0.216 0.223 (±0.011)
500 0.247 0.216 0.223 (±0.011)
</p>
<p>Table 2: Effect of the number of LDA topics when
the top 50 keyphrases were used for evaluating SR
on 500N-KPCrowd. The 95% confidence interval
for the F measure is shown in the last column.
</p>
<p>keyphrases by the authors. Following the eval-
uation process described in Mihalcea and Tarau
(2004), we use only the uncontrolled set of anno-
tated keyphrases for our analysis. Since our ap-
proach is completely unsupervised, we combine
the training, testing, and validation datasets. Top
50 and 10 keyphrases were used for evaluation on
500N-KPCrowd and Inspec, respectively.2
</p>
<p>We compare the performance of Salience Rank
(SR), Topical PageRank (TPR), and Single Top-
ical PageRank (STPR) in terms of precision, re-
call and F measure on 500N-KPCrowd and Inspec.
The results are summarized in Table 1. Details on
parametrization are given in the caption. In terms
of the F measure, SR achieves the best results on
both datasets. It ties TPR and outperforms STPR
on 500N-KPCrowd, and outperforms both TPR
and STPR on Inspec. The source code is avail-
able at https://github.com/methanet/
saliencerank.git.
</p>
<p>We further experiment with varying the num-
</p>
<p>2There are two common ways to set the number of output
keyphrases: using a fixed value a priori as we do (Turney,
1999) or deciding a value with heuristics at runtime (Mihal-
cea and Tarau, 2004).
</p>
<p>α precision recall F measure
1.0 0.247 0.216 0.223 (±0.011)
0.7 0.248 0.216 0.223 (±0.011)
0.4 0.248 0.217 0.224 (±0.011)
0.1 0.254 0.222 0.229 (±0.010)
0.0 0.248 0.217 0.224 (±0.011)
</p>
<p>Table 3: Effect of the α parameter in SR on 500N-
KPCrowd. SR was run with 50 LDA topics and
the top 50 keyphrases were used for the evaluation.
The 95% confidence interval for the F measure is
shown in the last column.
</p>
<p>ber of topics K used for fitting the LDA model
in SR. Table 2 shows how the F measures change
on 500N-KPCrowd as the number of topics varies.
Overall, the impact of topic size is mild, with
K = 50 being the optimal value. The impact ofK
on TPR can be found in Liu et al. (2010). In our
approach, the random walk derived in (8) depends
on the word salience, which in turn depends onK;
In TPR, not only the individual random walk (2)
depends on K, but the final aggregation of rank-
ings of keyphrases also depends on K.
</p>
<p>We also experiment with varying the tradeoff
parameter α of SR. With 500N-KPCrowd, Table 3
illustrates that different α can have a considerable
impact on various performance measures. To com-
plement the quantitative results in Table 3, Table 4
presents a concrete example, showing that vary-
ing α can lead to qualitative changes in the top
ranked keyphrases. In particular, when α = 0
the corpus specificity of the keyphrases SR ex-
tracts is high. This is demonstrated by the fact that
words such as “theory” and “function” are among
</p>
<p>533</p>
<p />
</div>
<div class="page"><p />
<p>Input: Individual rationality, or doing what is best for oneself, is a standard model used to ex-
plain and predict human behavior, and von Neumann-Morgenstern game theory is the classi-
cal mathematical formalization of this theory in multiple-agent settings. Individual rationality,
however, is an inadequate model for the synthesis of artificial social systems where cooper-
ation is essential, since it does not permit the accommodation of group interests other than
as aggregations of individual interests. Satisficing game theory is based upon a well-defined
notion of being good enough, and does accommodate group as well as individual interests
through the use of conditional preference relationships, whereby a decision maker is able to
adjust its preferences as a function of the preferences, and not just the options, of others. This
new theory is offered as an alternative paradigm to construct artificial societies that are capable
of complex behavior that goes beyond exclusive self interest.
</p>
<p>Unique top keyphrases with α = 0 :α = 0 :α = 0 : Unique top keyphrases with α = 1 :α = 1 :α = 1 :
classical mathematical formalization individual interests
preferences group interests
theory artificial social systems
options individual rationality
function conditional preference relationships
multiple agent settings standard model
</p>
<p>Table 4: An example of running SR on an Inspec abstract with a minimum and maximum value of α.
Unique keyphrases among the top 10 are shown.
</p>
<p>the top keyphrases SR selects, which are highly
common words in scientific papers. On the other
hand, when α = 1 these keyphrases are not pre-
sented among the top. This toy example illustrates
the relevance of balancing topic and corpus speci-
ficity in practice: When presenting the keyphrases
to a layman, high corpus specificity is suitable as
it conveys more high-level information; when pre-
senting to an expert in the area, high topic speci-
ficity is suitable as it dives deeper into topic spe-
cific details.
</p>
<p>5 Conclusions &amp; Remarks
</p>
<p>In this paper, we propose a new keyphrase extrac-
tion method, called Salience Rank. It improves
upon the Topical PageRank algorithm by Liu et al.
(2010) and the Single Topical PageRank algorithm
by Sterckx et al. (2015). The key advantages of
this new method are twofold: (i) While maintain-
ing and sometimes improving the quality of ex-
tracted keyphrases, it only runs PageRank once
instead of K times as in Topical PageRank, there-
fore leads to lower runtime; (ii) By constructing
the underlying word graph with newly proposed
word salience, it allows the user to balance topic
and corpus specificity of the extracted keyphrases.
</p>
<p>These three methods rely only on the input cor-
</p>
<p>pus. They can be benefited by external resources
like Wikipedia and WordNet, as indicated by, e.g.,
Medelyan et al. (2009), Grineva et al. (2009),
Martinez-Romo et al. (2016).
</p>
<p>In the keyphrase extraction literature, LDA is
the most commonly used topic modeling method.
Other methods, such as probabilistic latent seman-
tic indexing (Hofmann, 1999), nonnegative matrix
factorization (Sra and Inderjit, 2006), are viable
alternatives. However, it is hard to tell in general
if the keyphrase quality improves with these alter-
natives. We suspect that strongly depends on the
domain of the dataset and a choice may be made
depending on other practical considerations.
</p>
<p>We have fixed the tradeoff parameter α through-
out the experiments for a straightforward compar-
ison to other methods. In practice, one should
search the optimal value of α for the task at hand.
An open question is how to theoretically quan-
tify the relationship between α and various per-
formance measures, such as the F measure.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank Matthias Seeger, Cedric
Archambeau, Jan Gasthaus, Alex Klementiev,
Ralf Herbrich, and the anonymous ACL reviewers
for their valuable inputs.
</p>
<p>534</p>
<p />
</div>
<div class="page"><p />
<p>References
David M Blei, Andrew Y Ng, and Michael I Jordan.
</p>
<p>2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research 3(1):993–1022.
</p>
<p>Jason Chuang, Christopher D Manning, and Jeffrey
Heer. 2012. Termite: Visualization techniques for
assessing textual topic models. In Proceedings of
the International Working Conference on Advanced
Visual Interfaces. pages 74–77.
</p>
<p>Maria Grineva, Maxim Grinev, and Dmitry Lizorkin.
2009. Extracting key terms from noisy and multi-
theme documents. In Proceedings of the 18th In-
ternational Conference on World Wide Web. WWW,
pages 661–670.
</p>
<p>Kazi Saidul Hasan and Vincent Ng. 2014. Automatic
keyphrase extraction: A survey of the state of the
art. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics.
ACL, pages 1262–1273.
</p>
<p>Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval. SIGIR, pages
50–57.
</p>
<p>Anette Hulth. 2003. Improved automatic keyword
extraction given more linguistic knowledge. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing. EMNLP,
pages 216–223.
</p>
<p>Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of the 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics. NAACL, pages 620–
628.
</p>
<p>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing. EMNLP, pages 366–376.
</p>
<p>Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009b. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage. EMNLP, pages 257–266.
</p>
<p>Juan Martinez-Romo, Lourdes Araujo, and An-
dres Duque Fernandez. 2016. Semgraph: Extracting
keyphrases following a novel semantic graph-based
approach. Journal of the Association for Informa-
tion Science and Technology 67(1):71–82.
</p>
<p>Luı́s Marujo, Anatole Gershman, Jaime Carbonell,
Robert Frederking, and João P Neto. 2013. Super-
vised topical key phrase extraction of news stories
using crowdsourcing, light filtering and co-reference
normalization. arXiv preprint arXiv:1306.4886 .
</p>
<p>Olena Medelyan, Eibe Frank, and Ian Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing. EMNLP, pages 1318–1327.
</p>
<p>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing. EMNLP, pages 404–411.
</p>
<p>Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The PageRank citation rank-
ing: Bringing order to the web. Technical report,
Stanford University.
</p>
<p>Claude Pasquier. 2010. Single document keyphrase ex-
traction using sentence clustering and latent Dirich-
let allocation. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation. pages
154–157.
</p>
<p>Suvrit Sra and Dhillon Inderjit. 2006. Generalized non-
negative matrix approximations with Bregman di-
vergences. In Advances in Neural Information Pro-
cessing Systems 18. NIPS, pages 283–290.
</p>
<p>Lucas Sterckx, Thomas Demeester, Johannes Deleu,
and Chris Develder. 2015. Topical word importance
for fast keyphrase extraction. In Proceedings of the
24th International Conference on World Wide Web.
WWW, pages 121–122.
</p>
<p>Peter Turney. 1999. Learning to extract keyphrases
from text. Technical report, National Research
Council Canada, Institute for Information Technol-
ogy.
</p>
<p>Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song,
Palakorn Achananuparp, Ee-Peng Lim, and Xiaom-
ing Li. 2011. Topical keyphrase extraction from
Twitter. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics. ACL, pages 379–388.
</p>
<p>535</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 536–541
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2085
</p>
<p>List-only Entity Linking
</p>
<p>Ying Lin1 ∗, Chin-Yew Lin2, Heng Ji1
1 Computer Science Department,
</p>
<p>Rensselaer Polytechnic Institute, Troy, NY, USA
{liny9,jih}@rpi.edu
</p>
<p>2 Microsoft Research, Beijing, China
cyl@microsoft.com
</p>
<p>Abstract
</p>
<p>Traditional Entity Linking (EL) technolo-
gies rely on rich structures and proper-
ties in the target knowledge base (KB).
However, in many applications, the KB
may be as simple and sparse as lists of
names of the same type (e.g., lists of prod-
ucts). We call it as List-only Entity Link-
ing problem. Fortunately, some mentions
may have more cues for linking, which
can be used as seed mentions to bridge
other mentions and the uninformative en-
tities. In this work, we select the most
linkable mentions as seed mentions and
disambiguate other mentions by compar-
ing them with the seed mentions rather
than directly with the entities. Our exper-
iments on linking mentions to seven auto-
matically mined lists show promising re-
sults and demonstrate the effectiveness of
our approach.1
</p>
<p>1 Introduction
</p>
<p>Traditional Entity Linking (EL) methods usually
rely on rich structures and properties in the tar-
get knowledge base (KB). These methods may not
be effective in applications where detailed descrip-
tions and properties of target entities are absent in
the KB. Consider the following situations:
</p>
<p>Disaster Response and Recovery. When a
disaster strikes, people rush to the web and post
tweets about the damage and casualties. Perform-
ing EL to extract key information, such as devas-
tated towns and donor agencies, can help us mon-
itor the situation and coordinate rescue and recov-
ery efforts. Although many involved entities are
</p>
<p>∗Part of this work was done when the first author was on
an internship at Microsoft Research Asia.
</p>
<p>1The data set is available at: http://nlp.cs.rpi.edu/data/
link-only-entity-linking.html
</p>
<p>not well-known and usually absent in general KBs,
we may be able to acquire lists of these entities
from the local government as the target KB.
</p>
<p>Voice of the Customer. EL also plays
an important role in mining customer opinions
from data generated on social platforms and e-
commerce websites, thereby helping companies
better understand the needs and expectations of
their customers. However, the target products are
often not covered by general KBs. For example,
(Cao et al., 2015) tested 32 names of General Mo-
tors car models and only found 4 in Wikipedia.
Although some companies may choose to main-
tain a comprehensive product KB, it will be much
more practical and less costly to provide only lists
of product names.
</p>
<p>Figure 1: Link mentions to target entities in differ-
ent entity lists.
</p>
<p>Under such circumstances, we need the ability
to perform EL to ad-hoc name lists instead of a
comprehensive KB, namely List-only Entity Link-
ing. Take Figure 1 as an example. For a human
reader, it is not difficult to figure out the referent
entities of mentions in each document based on
</p>
<p>536</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2085">https://doi.org/10.18653/v1/P17-2085</a></div>
</div>
<div class="page"><p />
<p>clues such as “basketball” and “LDA”, whereas we
will not be able to make such inference without the
knowledge of the target entities. However, even if
we lack the minimal knowledge (e.g., Jordan is a
country), we are more confident to link mentions
in d1, d4, and d5 because they co-occur with other
entities in the same list. We consider such men-
tions that we are confident to link as seed men-
tions, and use them to construct contextual and
non-contextual information of the target entities to
enhance entity disambiguation.
</p>
<p>Therefore, in this work, we propose to tackle the
problem of List-only Entity Linking through seed
mentions. We automatically identify seed men-
tions for each list using a two-step method based
on the occurrence of entities and similarity be-
tween mentions. After that, in the entity disam-
biguation phase, we utilize the selected mentions
as a bridge between uninformative entities and
other mentions. Specifically, we comparing fea-
tures of a non-seed mention to those of seed men-
tions of its entity candidates to determine which
entity it should be linked to.
</p>
<p>2 Problem Definition
</p>
<p>Given a mention m and the entity e that it refers
to, we call e the referent entity of m and m the
referential mention of e. In Figure 1, for exam-
ple, Michael Jordan1 is the referent entity of
“Jordan” in document d6, while “Jordan” in docu-
ment d2 is an non-referential mention for Michael
Jordan1.
</p>
<p>As Figure 1 shows, in the setting of List-
only Entity Linking, there are a set of manu-
ally or automatically generated entity lists E =
{E1, E2, ..., El} and a set of documents D =
{d1, d2, ..., dn}. Entities in the same list are ho-
mogeneous and share some common properties.
In our experiment, each document di contains a
mention mi to link. Our goal is to link mi to its
referent entity ei,j ∈ Ej or returns NIL if it is un-
linkable to any entities.
</p>
<p>3 Approach
</p>
<p>Our framework has two modules, entity candi-
date retrieval and entity disambiguation as Fig-
ure 2 shows. For a mention “Jordan,” we retrieve
two candidate entities, Michael Jordan1 and
Michael Jordan2, from the entity lists. Next,
we select a set of seed mentions for each entity
from all documents. To determine the referent en-
</p>
<p>tity of “Jordan”, we compare it with seed mentions
of each candidate instead of the entity itself.
</p>
<p>Figure 2: List-only Entity Linking Framework.
</p>
<p>3.1 Entity Candidate Retrieval
</p>
<p>For each mention mi, we first locate a set of en-
tity candidates Ci = {ei,j |ei,j ∈ Ej} that it pos-
sibly refers to. A mention and its referent entity
may have different surface forms (e.g., “BMW”
and Bayerische Motoren Werke). For this rea-
son, we design a set of matching rules to improve
the recall as shown in Table 1.
</p>
<p>Category Rule Examples
</p>
<p>Abbreviation
</p>
<p>Acronym USDOD/USDD (United States
Department of Defense),
</p>
<p>Initial
Letters
</p>
<p>corp. (corporation), univ. (uni-
versity)
</p>
<p>First and
Last Let-
ters
</p>
<p>Dr. (Doctor), PA (Pennsylva-
nia)
</p>
<p>Omission Address a person by his/her
given name or surname rather
than full name
</p>
<p>Substitution
</p>
<p>Numeral 7-11 (7-Eleven )
Symbol AT&amp;T (American Telephone
</p>
<p>and Telegraph Company)
Accent
Mark
</p>
<p>hermes.com (Hermès)
</p>
<p>Table 1: Alternative form matching rules.
</p>
<p>3.2 Entity Disambiguation
</p>
<p>Next, we proceed to score each candidate ei,j
and determine which one mi should be linked to.
However, we have no knowledge of the target enti-
ties except for names and thus can’t directly com-
pare mi with them. Rather, we propose to bridge
the gap between mentions and entities through
seed mentions.
</p>
<p>537</p>
<p />
</div>
<div class="page"><p />
<p>Figure 3: Bridging the gap between uncertain
mentions and target entities using seed mentions.
</p>
<p>We illustrate the idea in Figure 3. University
of Pennsylvania is retrieved as an entity can-
didate for mentions “University of Pennsylvania”,
“Penn”, and “Pennsylvania.” We are more confi-
dent to link “University of Pennsylvania” in dI and
“Penn” in dII to University of Pennsylvania
because other entities in the University list, such
as “Harvard” and “MIT,” also appear in the same
document. Thus, we select mentions in dI and dII
as seed mentions. From dI and dII , we can extract
both contextual features (e.g., “academic” and “re-
search”) and non-contextual features (e.g., the en-
tity type is ORG). After that, we compare mentions
in other documents with the seeds. We link “Penn”
in dIII to University of Pennsylvania be-
cause its entity type and context are consistent
with the seeds. “Pennsylvania” in dIV , however,
is not linked because it is recognized as a loca-
tion. To capture richer contextual information and
minimize the effect of noise, we select more than
one seed mention using a two-step approach as fol-
lows.
</p>
<p>Figure 4: Seed selection.
</p>
<p>1. Subset Selection. We assume that if multi-
ple names in the same list co-occur within a docu-
</p>
<p>ment, they are all likely to be referential mentions
of this list, such as “Michael Jordan” and “LeBron
James” in d1. Hence, to identify seed mentions
of list Ej , we first narrow the scope down to a
subset D(n)j of documents containing more than
n mentions matching names in Ej . We gradually
increase the n until θ−size ≤ |D
</p>
<p>(n)
j | ≤ θ+size. θ−size
</p>
<p>and θ+size are set to 50 and 300 in our experiments.
2. Clustering. We expect most mentions in the
</p>
<p>selected subset are referential of list Ej , while in
fact the subset is likely to contain a small number
of non-referential mentions. We need to eliminate
them from the subset, otherwise they will intro-
duce misleading features differing from the real
seed mentions, hence hurting the performance of
entity disambiguation. To separate referential and
non-referential mentions in the selected subset, we
make two assumptions: (1) Most mentions in the
subset are referential, and (2) Referential mentions
should be similar to each other while dissimilar
from non-referential ones. Due to the lack of an-
notated data, we approach this problem by per-
forming clustering, which works in an unsuper-
vised fashion. Specifically, we represent features
(described later in this section) of each mention
as a vector and measure the distance between two
mentions using cosine distance. After that, we run
the K-means++ algorithm on the subset to separate
referential and non-referential mentions, and pick
mentions in the largest cluster as seed mentions.
</p>
<p>To determine the referent entity of mention mi,
we calculate the confidence score of linking mi
to ei,j ∈ Ej using the average cosine similarity
between mi and seed mentions of list Ej :
</p>
<p>c(mi, ei,j) =
1
</p>
<p>|Sj |
</p>
<p>|Sj |∑
</p>
<p>p=1
</p>
<p>sim(mi,ms),ms ∈ Sj
</p>
<p>where Sj is the seed set of Ej . Lastly, we link mi
to the candidate with the highest confidence score.
</p>
<p>In this work, we use the following features.
Entity Type. The entity type of a mention
</p>
<p>can be inferred from the text and used for dis-
ambiguation. For example, if most seed men-
tions for the University list are recognized as ORG,
while “Harvard” in the sentence “Harvard was
born and raised in Southwark, Surrey, England”
is tagged as PER, it is unlikely to refer to Harvard
University.
</p>
<p>Textual Context. We also assume that refer-
ential mentions of the same entity should share
</p>
<p>538</p>
<p />
</div>
<div class="page"><p />
<p>similar local contexts. We represent textual con-
text using the average embedding of words within
a window around the mention.
</p>
<p>Punctuation. Punctuations preceding or fol-
lowing a mention may help resolve ambiguity. For
example, “MA” preceded by a comma is possible
to refer to a state, since states are usually the last
component of an address, such as “Boston, MA”.
</p>
<p>4 Experiments
</p>
<p>4.1 Data set
</p>
<p>In our experiment, the construction of data set
consists of two steps: collecting name lists from
NeedleSeek2 (Shi et al., 2010) and extracting doc-
uments from Wikipedia. NeedleSeek is a project
aiming to mine semantic concepts from tera-scale
data (ClueWeb09) and classify them into a wide
range of semantic categories. For example, “KFC”
is mined as a concept in the restaurant category,
along with key sentences and attributes, such as
employee number and founder.
</p>
<p>To obtain target name lists, we select 7 se-
mantic categories (see Table 2) generated by
NeedleSeek as target domains, and take the
top concepts in each category as target entities.
We manually map each name to its pertinent
Wikipedia page as a target entity (e.g., Starbucks
→ enwiki:Starbucks3). Thus, we collect lists
containing 139 target entities in total. Note that
category names are only for result presentation
purpose and not taken as input to our model.
</p>
<p>Category Name Examples
President Barack Obama, Ronald Reagan
Company Microsoft, Apple, Adobe, IBM
University Harvard University, Yale University
State Washington, Florida, California, Texas
Character Gandalf, Aragorn, Legolas, Gimli, Frodo
Brand Prada, Chanel, Burberry, Gucci, Cartier
Restaurant Subway, McDonald’s, KFC, Starbucks
</p>
<p>Table 2: Semantic categories from NeedleSeek.
</p>
<p>Next, we derive a data set from Wikipedia
articles through wikilinks4, which are links to
pages within English Wikipedia. For example,
a wikilink [[Harvard University|Harvard]]
appears as “Harvard” in text and links to the
page enwiki:Harvard_University. Thus, we
can consider “Harvard” as a name mention and
</p>
<p>2http://needleseek.msra.cn
3enwiki: is short for https://en.wikipedia.org/wiki/
4https://en.wikipedia.org/wiki/Help:Link
</p>
<p>enwiki:Harvard_University as its referent en-
tity. Consider the following sentences:
∗ ... then left toattend graduate school on a scholarship at
[[Harvard University|Harvard University]]...
</p>
<p>∗ On October 6, 2012, [[Allison Harvard|Harvard]]
made an appearance in an episode of...
</p>
<p>Because enwiki:Harvard_University is in
the University list, the first mention will be con-
sidered as referential, whereas the second one is
non-referential. We also apply matching rules in
Table 1 to obtain more non-referential mentions.
After that, we extract sentences around wikilinks
as a document.
</p>
<p>Category #Referential #Referential
(balanced)
</p>
<p>#Non-
referential
</p>
<p>President 51, 412 14, 722 14, 818
Company 13, 312 3, 604 3, 642
University 79, 285 30, 101 30, 187
State 86, 743 9, 602 9, 106
Character 729 483 476
Brand 5, 138 1, 739 1, 781
Restaurant 4, 261 4, 261 4, 850
Total 240, 588 64, 512 61, 632
</p>
<p>Table 3: Data set stats.
</p>
<p>From Table 3, we can see that referential en-
tities overwhelm non-referential ones in the ex-
tracted corpus. In order to evaluate our model
fairly, we perform downsampling to balance refer-
ential and non-referential mentions, otherwise we
can achieve high scores even if we link all mention
to the target entities. In the balanced data set, there
are 11, 065 unique entities.
</p>
<p>4.2 Entity Linking Results
</p>
<p>Category Complete Balanced SubsetR P F R P F
President 94.6 89.9 92.2 87.2 80.4 83.7
Company 86.6 95.8 91.0 90.8 85.1 87.9
University 96.7 96.4 96.5 96.9 92.0 94.4
State 96.2 92.1 94.1 95.0 58.6 72.5
Character 92.5 61.3 73.7 92.8 52.2 66.8
Brand 89.6 90.2 89.9 86.7 83.2 84.9
Restaurant 87.0 81.4 84.1 86.9 88.1 87.5
Overall 95.2 93.4 94.3 93.1 81.6 87.0
</p>
<p>Table 4: Overall performance (%). R, P, and F rep-
resent recall, precision, and F1 score, respectively.
</p>
<p>As Table 4 demonstrates, our method shows
promising results (87.0 F1 score) on the balanced
data set. Nevertheless, we notice the low linking
precisions for entities in the Character and State
lists, which are caused by different reasons. For
the Character list, mentions do not suffice to select
</p>
<p>539</p>
<p />
</div>
<div class="page"><p />
<p>high-quality seeds, whereas for the State list, fea-
tures of referential and non-referential mentions
are usually similar. Consider the following sen-
tence:
∗ She witnessed his fatal shooting when they were together
in the President’s Box at Ford’s Theatre on Tenth Street in
</p>
<p>Washington.
</p>
<p>The mention “Washington” refers to
“Washington, D.C.”, which has the same
entity type, LOCATION, as our target entity
“Washington (state)”. In addition, we see
no obvious textual clue that indicates whether
it refers to the State of Washington or not.
Traditional EL approaches usually disambiguate
such mentions through collective inference.
They link “Ford’s Theatre” and “Washington”
to the KB simultaneously. Since there exists an
explicit relation between “Ford’s Theatre” and
“Washington, D.C.”, these two entities receive
high confidence scores and thus are determined as
the referents. Unfortunately, we cannot employ
the knowledge-rich approach in the List-only
Entity Linking scenario.
</p>
<p>5 Related Work
</p>
<p>In this paper, we define and study the List-only
Entity Linking problem based on previous stud-
ies on Target Entity Disambiguation (Wang et al.,
2012; Cao et al., 2015). The key difference is that
they target at the disambiguation of a single list of
entities, whereas we focus on entity linking to an
arbitrary number of lists. Another similar prob-
lem is Named Entity Disambiguation with Link-
less Knowledge Bases (LNED) (Li et al., 2016). It
assumes that entities are isolated in the “linkless”
KB, while each entity still has a description.
</p>
<p>Our idea of selecting seed mentions based on
co-occurrence is similar to collective inference.
Most state-of-the-art EL methods utilize collec-
tive inference to link a set of coherent men-
tions simultaneously by selecting the most coher-
ent set of entity candidates on the KB side (Pan
et al., 2015; Huang et al., 2014; Cheng and Roth,
2013; Cassidy et al., 2012; Xu et al., 2012).
In this work, without explicit relations between
entities in different lists, we only take the co-
occurrence of mentions in the same list into con-
sideration. Therefore, our method is unable to
benefit from the co-occurrence of John Lennon
and Give Peace a Chance although they are ac-
tually strongly connected.
</p>
<p>6 Conclusions and Future Work
</p>
<p>In this paper, we proposed a novel framework to
tackle the problem of List-only Entity Linking.
The core of this framework is selecting seed men-
tions for each entity list to bridge the gap between
mentions and non-informative target entities. Our
results show this EL framework works well for this
task. At present, in the seed selection step, we
simply consider all co-occurring mentions of enti-
ties in the same list. In the future, we will employ
more precise approaches to choose co-occurring
mentions and mine relations between entities in
separate lists to improve seed selection and entity
disambiguation.
</p>
<p>Acknowledgments
</p>
<p>This work was supported by the DARPA DEFT
No.FA8750-13-2-0041, U.S. DARPA LORELEI
Program No. HR0011-15-C-0115, U.S. ARL NS-
CTA No. W911NF-09-2-0053, and NSF IIS-
1523198. The views and conclusions contained in
this document are those of the authors and should
not be interpreted as representing the official poli-
cies, either expressed or implied, of the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on.
</p>
<p>References
</p>
<p>Yixin Cao, Juanzi Li, Xiaofei Guo, Shuanhu Bai, Heng
Ji, and Jie Tang. 2015. Name list only? target
entity disambiguation in short texts. In EMNLP.
https://doi.org/10.18653/v1/D15-1077.
</p>
<p>Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz
Zubiaga, and Hongzhao Huang. 2012. Anal-
ysis and enhancement of wikification for mi-
croblogs with context expansion. In COLING.
http://aclweb.org/anthology/C12-1028.
</p>
<p>Xiao Cheng and Dan Roth. 2013. Relational
inference for wikification. In EMNLP.
http://aclweb.org/anthology/D13-1184.
</p>
<p>Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng
Ji, and Chin-Yew Lin. 2014. Collective tweet wiki-
fication based on semi-supervised graph regulariza-
tion. In ACL. https://doi.org/10.3115/v1/P14-1036.
</p>
<p>Yang Li, Shulong Tan, Huan Sun, Jiawei Han, Dan
Roth, and Xifeng Yan. 2016. Entity disambiguation
with linkless knowledge bases. In WWW.
</p>
<p>540</p>
<p />
</div>
<div class="page"><p />
<p>Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, Heng
Ji, and Kevin Knight. 2015. Unsupervised entity
linking with abstract meaning representation. In
NAACL. https://doi.org/10.3115/v1/N15-1119.
</p>
<p>Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class min-
ing: Distributional vs. pattern-based approaches. In
COLING. http://aclweb.org/anthology/C10-1112.
</p>
<p>Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Sura-
jit Chaudhuri. 2012. Targeted disambiguation of ad-
hoc, homogeneous sets of named entities. In WWW.
https://doi.org/10.1145/2187836.2187934.
</p>
<p>Jian Xu, Qin Lu, Jie Liu, and Ruifeng
Xu. 2012. NLP-comp in TAC 2012 en-
tity linking and slot-filling. In TAC.
https://tac.nist.gov//publications/2012/papers.html.
</p>
<p>541</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 542–546
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2086
</p>
<p>Improving Native Language Identification by Using Spelling Errors
</p>
<p>Lingzhen Chen
DISI
</p>
<p>University of Trento
Trento, Italy
</p>
<p>lzchen.cs@gmail.com
</p>
<p>Carlo Strapparava
HLT
</p>
<p>Fondazione Bruno Kessler
Trento, Italy
</p>
<p>strappa@fbk.eu
</p>
<p>Vivi Nastase
Inst. for Computational Linguistics
</p>
<p>University of Heidelberg
Heidelberg, Germany
</p>
<p>nastase@cl.uni-heidelberg.de
</p>
<p>Abstract
</p>
<p>In this paper, we explore spelling errors as
a source of information for detecting the
native language of a writer, a previously
under-explored area. We note that char-
acter n-grams from misspelled words are
very indicative of the native language of
the author. In combination with other lexi-
cal features, spelling error features lead to
1.2% improvement in accuracy on classi-
fying texts in the TOEFL11 corpus by the
author’s native language, compared to sys-
tems participating in the NLI shared task1.
</p>
<p>1 Introduction
</p>
<p>Native Language Identification (NLI) aims to de-
termine the native language (L1) of a writer based
on his or her writings in a second language (L2).
Though initially motivated by the study of cross-
linguistics influence, the value of NLI is not lim-
ited to education. Potentially, it is also very valu-
able in academic, marketing, security and law en-
forcement fields. Identifying the native language
is based on the assumption that the L1 of an indi-
vidual impacts his or her writing in L2 due to the
language transfer effect.
</p>
<p>We focus here on the influences from L1 that
surface as spelling errors in L2. Crossley and Mc-
Namara (2011) showed that syntactic patterns and
lexical preferences from L1 appear in L2 system-
atically, and are very informative for identifying
the writer’s native language. Texts written by au-
thors with the same L1 also exhibit similarities
with respect to the errors within.
</p>
<p>In terms of spelling, in particular, both the
sound of the words in different languages and the
mapping from sounds to letters in L2 vs. L1, as
well as the particular conventions of writing can
</p>
<p>1https://sites.google.com/site/nlisharedtask2013/home
</p>
<p>have a visible impact. In Italian, for example, each
vowel has only one pronunciation, so it is very
common for Italian writers to confuse the use of
vowels in English: the English e can correspond
to the sounds written either as i or e in Italian. In
Arabic, on the other hand, vowels are rarely writ-
ten, and this could cause writers to miss vowels
when writing in English. In Chinese, since it uses
a completely different writing system compared to
English, there might be a higher probability for
authors to make spelling errors when it comes to
complicated words, because the mapping from En-
glish sounds to letters of the Roman alphabet is not
one-to-one. We test whether we are able to cap-
ture some of these phenomena by going below the
word level to character level and testing their use-
fulness as features for identifying the native lan-
guage of the author.
</p>
<p>Spelling errors have been used as features for
NLI since Koppel et al. (2005). They considered
syntax errors and eight types of spelling errors
such as repeated letters, missing letters, and inver-
sion of letters. The relative frequency of each error
type with regard to the length of the document is
used as feature values. By combining these with
common features such as function words, they ob-
tained a classification accuracy of 80.2% on a sub-
corpora of ICLEv1 that consists of five languages.
More recently, Nicolai et al. (2013) focused on the
misspelled part of a word rather than the type of
spelling errors. They used pairs of correct and
misspelled parts in a word as features. Lavergne
et al. (2013) adopted a similar approach to rep-
resent the spelling errors by the inner-most mis-
spelled substring compared to the correct word.
Combined with other features, they obtained a test
accuracy of 75.29% on the TOEFL11 dataset.
</p>
<p>Character n-grams have been explored, but
not particularly for representing spelling errors.
Brooke and Hirst (2012) showed that using char-
</p>
<p>542</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2086">https://doi.org/10.18653/v1/P17-2086</a></div>
</div>
<div class="page"><p />
<p>acter unigrams, bigrams, and trigrams, the test ac-
curacy can reach 37.4% for 7-class NLI on a sub-
set of ICLEv2 corpus. For the NLI 2013 shared
task, Lahiri and Mihalcea (2013) used as features
character trigrams represented by their raw fre-
quencies. It leads to a test accuracy of 57.77%,
which shows that how often character combina-
tions are used is indicative of the L1 of an author.
</p>
<p>Using complete words to represent spelling er-
rors would not capture regularities that go beyond
a single misspelled instance – like the preference
of using i instead of e by Italian writers. We inves-
tigate the representation of spelling errors through
character n-grams with size up to 3. We assess
the effectiveness of using such feature representa-
tion for NLI and its contribution when combined
with word and lemma n-grams, whose effective-
ness has already been established (Gyawali et al.,
2013; Jarvis et al., 2013). We report high classifi-
cation results when using only spelling errors, and
an improvement of 1.2 percentage points in accu-
racy, compared to the best results obtained in NLI
shared task, when using spelling errors in combi-
nation with word and lemma features.
</p>
<p>2 Data
</p>
<p>The experiments are performed on the TOEFL11
corpus (Blanchard et al., 2013) of English es-
says written by non-native English learners as
part of the Test of English as a Foreign Lan-
guage (TOEFL). We also use the ICLEv2 cor-
pus (Granger et al., 2009) for extracting additional
spelling errors. The TOEFL11 corpus is not the
most recent corpus for the NLI task, but is by far
one of the largest learner corpora that is balanced
in terms of both topics and L1 languages.
</p>
<p>The TOEFL11 corpus contains 4 million to-
kens in 12,100 essays written by authors whose
native language (L1) is one of: Arabic (ARA),
Chinese (ZHO), French (FRA), German (DEU),
Hindi (HIN), Italian (ITA), Japanese (JPN), Ko-
rean (KOR), Spanish (SPA), Telugu (TEL) or
Turkish (TUR). For each target L1, the number of
essays is equal (900 in the train set, 100 in the de-
velopment set and 100 in the test set). The dis-
tribution of the number of essays per topic is not
perfectly balanced across different L1s, but they
are rather close: the average number of essays per
topic is 1,513 and the standard deviation is 229.
</p>
<p>3 Methods
</p>
<p>We aim to analyze the impact of spelling errors on
identifying the native language of the author. We
will analyze them separately and in combination
with commonly used features.
</p>
<p>3.1 Features
</p>
<p>Word n-grams Previous work on NLI avoided
lexical features due to the topic bias in the dataset.
This is not an issue with the TOEFL11 corpus,
since the topic distribution over the different L1s
is quite balanced, as described in the previous sec-
tion. Hence we can use as features the word uni-
grams, bigrams and trigrams. We filter out those
that do not appear in at least two texts in the cor-
pus. In the extraction of word n-grams, we ig-
nore the use of all punctuations. Word n-gram
features have as value their weighted frequencies
using the Log-Entropy weighting technique. (Du-
mais, 1991)
Lemma n-grams This feature represents the lex-
ical choice of a writer, regardless of the inflected
form of a word. We use the n-grams of lemma
produced by the TreeTagger tool (Schmid, 1994)
with size up to 3. The feature extraction rule and
the feature weighting scheme is the same as for
word n-grams.
Character n-grams The frequency of character
or character sequence usage captures information
about the preference of an author in using certain
sounds, combinations of sounds, prefixes or suf-
fixes. We represent texts using character n-grams
and assign them the value vi – their relative fre-
quency with respect to the set of n-grams (uni-
gram, bigram, trigram) that they belong to:
</p>
<p>vi =
count(i)∑
j∈Si
</p>
<p>count(j)
</p>
<p>where Si is the collection of n-grams that have the
same gram size as n-gram i.
Spelling errors The errors made by a writer while
writing in their L2 may result from sound-to-
character mappings in L1, writing preferences or
other biases. These could be strong indicators for
an author’s L1. We extract spelling errors us-
ing the spell shell command. There are 34,233
unique spelling errors in the TOEFL11 corpus.
We looked for additional sources of spelling er-
rors, and extracted a list of 12,488 unique mis-
spelled words from the ICLEv2 corpus that are
produced by writers with the common L1 as in
the TOEFL11 corpus (namely Spanish, French,
</p>
<p>543</p>
<p />
</div>
<div class="page"><p />
<p>Italian, German, Turkish, Chinese and Japanese).
They are referred to here as Spelling error ICLE
and Spelling error TOEFL in the later part of this
paper. Spelling errors are binary features.
Spelling errors as character n-grams Every mis-
spelled word in a text will be represented as char-
acter n-grams, where n = 1..3. Special characters
marking the start and end of a word will be part of
the n-grams. The value of these features is their
relative frequencies, as for character n-grams.
</p>
<p>3.2 Classifiers
</p>
<p>Following the proven effectiveness of Support
Vector Machine (SVM) by numerous experiments
on text classification tasks, we adopt the use of
linear SVM for NLI. In particular, we use linear
SVM implemented by scikit-learn package
(Pedregosa et al., 2011) to perform the multi-class
classification.
</p>
<p>3.3 Experiment Setup
</p>
<p>The data is pre-processed by lower casing the to-
kenized version of the corpus. Each text is rep-
resented through the sets of features described
above. The feature size of word n-grams up to size
3 are over 500,000 and that of lemma n-grams up
to size 3 are over 400,000. The combination of
the two is over 600,000. The hyper-parameter C
of the linear SVM is set to 100, an optimal set-
ting obtained by cross-validation on the train set.
The performance is evaluated by classification ac-
curacy, as was done in the NLI shared task. We test
the performance of the used feature sets through a
10-fold cross-validation on the train+development
set before the final run on the test set.
</p>
<p>4 Results
</p>
<p>The classification accuracies obtained by us-
ing different features and feature combina-
tions are presented in Table 1. The feature
sets include the word, lemma, character n-
grams up to size 3 (denoted as word ngrams,
lemma ngrams, char ngrams respectively),
the misspelled words in Spelling error ICLE
or in Spelling error TOEFL (denoted as
word error icle or word error toefl), and the
character n-grams up to size 3 extracted from
Spelling error ICLE or Spelling error TOEFL
(denoted as char error icle or char error toefl).
</p>
<p>In terms of classifying by a single type of fea-
ture, word n-grams are the most indicative one,
</p>
<p>Type of Feature 10 Fold Test
(1) word ngrams 83.63 (±1.38) 84.16
(2) lemma ngrams 83.18 (±1.46) 84.00
(3) word error toefl 35.05 (±1.42) 32.55
(4) word error icle 24.42 (±1.12) 26.45
(5) char ngrams 66.27 (±0.93) 67.27
(6) char error icle 65.03 (±1.14) 65.73
(7) char error toefl 65.27 (±1.21) 66.45
(1) + (2) 83.91 (±1.50) 84.32
(1) + (2) + (3) 83.82 (±1.53) 84.27
(1) + (2) + (4) 83.90 (±1.49) 84.73
(1) + (2) + (5) 83.92 (±1.31) 84.64
(1) + (2) + (6) 83.85 (±1.26) 84.82
(1) + (2) + (7) 83.81 (±1.26) 84.82
Jarvis et al. (2013) 84.50 83.60
Nicolai et al. (2013) 58.50 81.70
</p>
<p>Table 1: Classification accuracy of using dif-
ferent features by 10-fold cross-validation on the
train+development set and test on the test set, the
accuracy scores are in %. The values in bracket
are the standard deviation of accuracy scores in
10-fold cross-validation.
</p>
<p>which is consistent with the results reported by
other researchers (Jarvis et al., 2013; Nicolai et al.,
2013). Using the combination of word n-grams
and lemma n-grams improves the performance of
using only word n-grams by less than 0.2%. It
is not a significant improvement, mainly because
there is a big overlap in the features in these
two categories. Word-level spelling errors when
used on their own do not perform well, with one
of the causes being sparseness. When combin-
ing them with other features (word n-grams and
lemma n-grams), word error toefl does not seem
to provide any additional information for improv-
ing classification accuracy. By combining the
word error icle with lemma n-grams and word n-
grams, however, we observe a small increase in
classification accuracy of 0.4%. Main reason for
this is that by using word error icle, we incor-
porate errors that are more common (since they
occurred in two different corpora – ICLEv2 and
TOEFL11 corpus).
</p>
<p>From the classification results obtained by us-
ing feature (5), (6) and (7), it is also worth noting
that by using only the character n-grams extracted
from the spelling errors in Spelling error TOEFL
(feature size: 5797), or even filtered by those that
also appear in Spelling error ICLE (feature size:
</p>
<p>544</p>
<p />
</div>
<div class="page"><p />
<p>3907), the accuracy is almost as the same as the
one obtained by using all character n-grams (fea-
ture size: 12601). It shows that the character n-
grams in the spelling errors are a most indicative
part in all the character n-grams when it comes to
identifying the L1 of an author.
</p>
<p>Using character n-grams extracted from
spelling errors works better than directly using
misspelled words. It further implies that while
the misspelled words might differ from one
another in the text, the misspelled parts share
similarities. The classifier trained by combining
word n-grams, lemma n-grams and character
n-grams extracted from Spelling error ICLE or
from Spelling error TOEFL both reach the best
test accuracy in our experiments - 84.82%, which
is 1.2% better than the best result reported by
Jarvis et al. (2013) in the NLI 2013 shared task.
We note that including only the character n-grams
extracted from the spelling errors ((1)+(2)+(6) or
(1)+(2)+(7)) leads to better results than including
all the character n-grams ((1)+(2)+(5)). It sup-
ports the hypothesis that spelling errors capture
relevant information about the writer’s native
language at a character level.
</p>
<p>Table 2 includes some of the most informative
spelling errors made by writers with different L1s.
They were selected based on their weights after
training the SVM with word n-grams, lemma n-
grams and spelling errors (as word). They seem to
confirm our starting hypothesis regarding the vari-
ous phenomena of language – and script – transfer
that can influence spelling errors.
</p>
<p>As shown in the table, the informative errors for
each target language are quite different. French
writers tend to double the character in the word,
for example, they misspell “personally” as “per-
sonnaly” and “developed” as “developped”. It
is also apparent that Japanese writers and Italian
writers tend to misspell the vowels in a word. This
may be the result of rules of word pronunciations
in their own languages, and sound to letter map-
pings in their L1. Arabic writers tend to omit vow-
els. It could be due to the fact that vowels are
rarely written in Arabic language and the writers
carry this habit in their writing in English.
</p>
<p>The results confirm the usefulness of features
representing spelling errors, particularly at a sub-
word level. On their own, they perform on a par
with character level representation of the docu-
ment. They also bring improvement in perfor-
</p>
<p>L1 Word
ARA evry, experince, diffrent, advertisment, statment
</p>
<p>DEU knowlegde, advertisment, successfull, freetime,neccessary
</p>
<p>FRA generaly, personnaly, litterature, independant,developped
</p>
<p>HIN theoritical, sucess, enviornment, sucessful,gandhi
</p>
<p>ITA indipendent, specialistic, tecnology, studing,istance
JPN actualy, youg, shoud, peple, convinient
</p>
<p>KOR poors, newpaper, eventhough, becaus,thesedays
</p>
<p>SPA conclution, consecuences, succesful,responsabilities, enviroment
</p>
<p>TEL oppurtunities, hardwork, intrested, atleast,donot
TUR altough, spesific, easly, succesful, turkish
ZHO sociaty, knowlege, easiler, sucessful, improtant
</p>
<p>Table 2: Most informative spelling errors made
by writers with different L1
</p>
<p>mance when combined with word and lemma n-
grams, indicating that they provide at least partly
complementary information to the frequently used
word n-grams or lemma n-grams, which on their
own have high performance.
</p>
<p>5 Conclusion
</p>
<p>In this work, we investigate the usefulness of
spelling errors for the native language identifica-
tion task. The experiments show that represent-
ing spelling errors through character n-grams cap-
tures interesting phenomena of language transfer.
Both on their own and combined with customarily
used word n-grams, they have high performance in
terms of accuracy, when tested on the TOEFL11
corpus and compared to participating systems in
the NLI shared task. In future work, it would be
interesting to characterize the spelling errors with
respect to similarity to specific L1s, and further
explore the hints that they provide with respect to
the author’s native language.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank EST for making the
TOEFL11 dataset public and Fondazione Bruno
Kessler for providing the facilities to conduct the
related experiments. We also thank the anony-
mous reviewers for their detailed and insightful
comments.
</p>
<p>545</p>
<p />
</div>
<div class="page"><p />
<p>References
Daniel Blanchard, Joel Tetreault, Derrick Hig-
</p>
<p>gins, Aoife Cahill, and Martin Chodorow.
2013. Toefl11: A corpus of non-native
english. ETS Research Report Series
2013(2):i–15. https://doi.org/10.1002/j.2333-
8504.2013.tb02331.x.
</p>
<p>Julian Brooke and Graeme Hirst. 2012. Ro-
bust, lexicalized native language identification.
In COLING 2012, 24th International Confer-
ence on Computational Linguistics, Proceedings
of the Conference: Technical Papers, 8-15 De-
cember 2012, Mumbai, India. pages 391–408.
http://aclweb.org/anthology/C/C12/C12-1025.pdf.
</p>
<p>Scott A. Crossley and Danielle S. McNamara.
2011. Shared features of l2 writing: Inter-
group homogeneity and text classification. Jour-
nal of Second Language Writing 20(4):271–285.
https://doi.org/10.1016/j.jslw.2011.05.007.
</p>
<p>Susan T. Dumais. 1991. Improving the retrieval of in-
formation from external sources. Behavior Research
Methods, Instruments, &amp; Computers 23(2):229–236.
https://doi.org/10.3758/BF03203370.
</p>
<p>Sylviane Granger, Estelle Dagneaux, Fanny Meu-
nier, and Magali Paquot. 2009. The Interna-
tional Corpus of Learner English: Handbook
and CD-ROM, version 2. Presses universi-
taires de Louvain, Louvain-la-Neuve, Belgium.
https://doi.org/10.1017/S0272263110000641.
</p>
<p>Binod Gyawali, Gabriela Ramı́rez-de-la-Rosa, and
Thamar Solorio. 2013. Native language iden-
tification: a simple n-gram based approach.
In Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educa-
tional Applications, BEA@NAACL-HLT 2013, June
13, 2013, Atlanta, Georgia, USA. pages 224–
231. http://aclweb.org/anthology/W/W13/W13-
1729.pdf.
</p>
<p>Scott Jarvis, Yves Bestgen, and Steve Pepper.
2013. Maximizing classification accuracy in
native language identification. In Proceed-
ings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Ap-
plications, BEA@NAACL-HLT 2013, June
13, 2013, Atlanta, Georgia, USA. pages 111–
118. http://aclweb.org/anthology/W/W13/W13-
1714.pdf.
</p>
<p>Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author’s native language by
mining a text for errors. In Proceedings of the
Eleventh ACM SIGKDD International Conference
on Knowledge Discovery in Data Mining. ACM,
New York, NY, USA, KDD ’05, pages 624–628.
https://doi.org/10.1145/1081870.1081947.
</p>
<p>Shibamouli Lahiri and Rada Mihalcea. 2013. Using n-
gram and word network features for native language
identification. In Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Ed-
ucational Applications, BEA@NAACL-HLT 2013,
June 13, 2013, Atlanta, Georgia, USA. pages 251–
259. http://aclweb.org/anthology/W/W13/W13-
1732.pdf.
</p>
<p>Thomas Lavergne, Gabriel Illouz, Aurélien Max, and
Ryo Nagata. 2013. Limsi’s participation to the
2013 shared task on native language identifica-
tion. In Proceedings of the Eighth Workshop
on Innovative Use of NLP for Building Educa-
tional Applications, BEA@NAACL-HLT 2013, June
13, 2013, Atlanta, Georgia, USA. pages 260–
265. http://aclweb.org/anthology/W/W13/W13-
1733.pdf.
</p>
<p>Garrett Nicolai, Bradley Hauer, Mohammad Salameh,
Lei Yao, and Grzegorz Kondrak. 2013. Cognate
and misspelling features for natural language iden-
tification. In Proceedings of the Eighth Work-
shop on Innovative Use of NLP for Building Ed-
ucational Applications, BEA@NAACL-HLT 2013,
June 13, 2013, Atlanta, Georgia, USA. pages 140–
145. http://aclweb.org/anthology/W/W13/W13-
1718.pdf.
</p>
<p>Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Édouard Duchesnay. 2011.
Scikit-learn: Machine learning in python. Jour-
nal of Machine Learning Research 12:2825–2830.
http://dl.acm.org/citation.cfm?id=1953048.2078195.
</p>
<p>Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing. http://www.cis.uni-
muenchen.de/ schmid/tools/TreeTagger/data/tree-
tagger1.pdf.
</p>
<p>546</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 547–553
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2087
</p>
<p>Disfluency Detection using a Noisy Channel Model and a Deep Neural
Language Model
</p>
<p>Paria Jamshid Lou
Department of Computing
</p>
<p>Macquarie University
Sydney, Australia
</p>
<p>paria.jamshid-lou@hdr.mq.edu.au
</p>
<p>Mark Johnson
Department of Computing
</p>
<p>Macquarie University
Sydney, Australia
</p>
<p>mark.johnson@mq.edu.au
</p>
<p>Abstract
</p>
<p>This paper presents a model for disflu-
ency detection in spontaneous speech tran-
scripts called LSTM Noisy Channel Model.
The model uses a Noisy Channel Model
(NCM) to generate n-best candidate dis-
fluency analyses and a Long Short-Term
Memory (LSTM) language model to score
the underlying fluent sentences of each
analysis. The LSTM language model
scores, along with other features, are used
in a MaxEnt reranker to identify the most
plausible analysis. We show that using
an LSTM language model in the reranking
process of noisy channel disfluency model
improves the state-of-the-art in disfluency
detection.
</p>
<p>1 Introduction
</p>
<p>Disfluency is a characteristic of spontaneous
speech which is not present in written text.
Disfluencies are informally defined as interrup-
tions in the normal flow of speech that occur
in different forms, including false starts, correc-
tions, repetitions and filled pauses. According to
Shriberg’s (1994) definition, the basic pattern of
speech disfluencies contains three parts: reparan-
dum1, interregnum and repair. Example 1 illus-
trates a disfluent structure, where the reparandum
to Boston is the part of the utterance that is re-
placed, the interregnum uh, I mean is an optional
part of a disfluent structure that consists of a filled
pause uh and a discourse marker I mean and the re-
pair to Denver replaces the reparandum. The flu-
ent version of Example 1 is obtained by deleting
</p>
<p>1Reparandum is sometimes called edit.
</p>
<p>reparandum and interregnum words.
</p>
<p>I want a flight
</p>
<p>reparandum︷ ︸︸ ︷
to Boston,
</p>
<p>uh, I mean︸ ︷︷ ︸
interregnum
</p>
<p>to Denver︸ ︷︷ ︸
repair
</p>
<p>on Friday (1)
</p>
<p>While disfluency rate varies with the context,
age and gender of speaker, Bortfeld et al. (2001)
reported disfluencies once in every 17 words.
Such frequency is high enough to reduce the read-
ability of speech transcripts. Moreover, disfluen-
cies pose a major challenge to natural language
processing tasks, such as dialogue systems, that
rely on speech transcripts (Ostendorf et al., 2008).
Since such systems are usually trained on fluent,
clean corpora, it is important to apply a speech
disfluency detection system as a pre-processor to
find and remove disfluencies from input data. By
disfluency detection, we usually mean identifying
and deleting reparandum words. Filled pauses and
discourse markers belong to a closed set of words,
so they are trivial to detect (Johnson and Charniak,
2004).
</p>
<p>In this paper, we introduce a new model for de-
tecting restart and repair disfluencies in sponta-
neous speech transcripts called LSTM Noisy Chan-
nel Model (LSTM-NCM). The model uses a Noisy
Channel Model (NCM) to generate n-best candi-
date disfluency analyses, and a Long Short-Term
Memory (LSTM) language model to rescore the
NCM analyses. The language model scores are
used as features in a MaxEnt reranker to select the
most plausible analysis. We show that this novel
approach improves the current state-of-the-art.
</p>
<p>2 Related Work
</p>
<p>Approaches to disfluency detection task fall into
three main categories: sequence tagging, parsing-
based and noisy channel model. The sequence
</p>
<p>547</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2087">https://doi.org/10.18653/v1/P17-2087</a></div>
</div>
<div class="page"><p />
<p>tagging models label words as fluent or disfluent
using a variety of different techniques, including
conditional random fields (Ostendorf and Hahn,
2013; Zayats et al., 2014; Ferguson et al., 2015),
hidden Markov models (Liu et al., 2006; Schuler
et al., 2010) or recurrent neural networks (Hough
and Schlangen, 2015; Zayats et al., 2016). Al-
though sequence tagging models can be easily
generalized to a wide range of domains, they
require a specific state space for disfluency de-
tection, such as begin-inside-outside (BIO) style
states that label words as being inside or outside of
a reparandum word sequence. The parsing-based
approaches refer to parsers that detect disfluen-
cies, as well as identifying the syntactic structure
of the sentence (Rasooli and Tetreault, 2013; Hon-
nibal and Johnson, 2014; Yoshikawa et al., 2016).
Training a parsing-based model requires large an-
notated tree-banks that contain both disfluencies
and syntactic structures. Noisy channel models
(NCMs) use the similarity between reparandum
and repair as an indicator of disfluency. However,
applying an effective language model (LM) inside
an NCM is computationally complex. To allevi-
ate this problem, some researchers use more effec-
tive LMs to rescore the NCM disfluency analyses.
Johnson and Charniak (2004) applied a syntactic
parsing-based LM trained on the fluent version of
the Switchboard corpus to rescore the disfluency
analyses. Zwarts and Johnson (2011) trained ex-
ternal n-gram LMs on a variety of large speech
and non-speech corpora to rank the analyses. Us-
ing the external LM probabilities as features into
the reranker improved the baseline NCM (Johnson
and Charniak, 2004). The idea of applying exter-
nal language models in the reranking process of
the NCM motivates our model in this work.
</p>
<p>3 LSTM Noisy Channel Model
</p>
<p>We follow Johnson and Charniak (2004) in us-
ing an NCM to find the n-best candidate disflu-
ency analyses for each sentence. The NCM, how-
ever, lacks an effective language model to capture
more complicated language structures. To over-
come this problem, our idea is to use different
LSTM language models to score the underlying
fluent sentences of the analyses proposed by the
NCM and use the language model scores as fea-
tures to a MaxEnt reranker to select the best anal-
ysis. In the following, we describe our model and
its components in detail.
</p>
<p>In the NCM of speech disfluency, we assume
that there is a well-formed source utterance X to
which some noise is added and generates a disflu-
ent utterance Y as follows.
</p>
<p>X = a flight to Denver
Y = a flight to Boston uh I mean to Denver
</p>
<p>Given Y , the goal of the NCM is to find the most
likely source sentence X̂ such that:
</p>
<p>X̂ = argmax
X
</p>
<p>P(Y |X)P(X) (2)
</p>
<p>As shown in Equation 2, the NCM contains two
components: the channel model P(Y |X) and the
language model P(X). Calculating the channel
model and language model probabilities, the NCM
generates 25-best candidate disfluency analyses as
follows.
</p>
<p>(3)
</p>
<p>Example 3 shows sample outputs of the NCM,
where potential reparandum words are specified
with strikethrough text. The MaxEnt reranker is
applied on the candidate analyses of the NCM to
select the most plausible one.
</p>
<p>3.1 Channel Model
We assume that X is a substring of Y , so the source
sentence X is obtained by deleting words from
Y . For each sentence Y , there are only a finite
number of potential source sentences. However,
with the increase in the length of Y , the number
of possible source sentences X grows exponen-
tially, so it is not feasible to do exhaustive search.
Moreover, since disfluent utterances may contain
an unbounded number of crossed dependencies,
a context-free grammar is not suitable for finding
the alignments. The crossed dependencies refer to
the relation between repair and reparandum words
which are usually the same or very similar words
in roughly the same order as in Example 4.
</p>
<p>(4)
</p>
<p>We apply a Tree Adjoining Grammar (TAG)
based transducer (Johnson and Charniak, 2004)
</p>
<p>548</p>
<p />
</div>
<div class="page"><p />
<p>which is a more expressive formalism and pro-
vides a systematic way of formalising the chan-
nel model. The TAG channel model encodes the
crossed dependencies of speech disfluency, rather
than reflecting the syntactic structure of the sen-
tence. The TAG transducer is effectively a simple
first-order Markov model which generates each
word in the reparandum conditioned on the pre-
ceding word in the reparandum and the corre-
sponding word in the repair. Further detail about
the TAG channel model can be found in (Johnson
and Charniak, 2004).
</p>
<p>3.2 Language Model
</p>
<p>The language model of the NCM evaluates the
fluency of the sentence with disfluency removed.
The language model is expected to assign a very
high probability to a fluent sentence X (e.g. a
flight to Denver) and a lower probability to a sen-
tence Y which still contains disfluency (e.g. a
flight to Boston uh I mean to Denver). However,
it is computationally complex to use an effective
language model within the NCM. The reason is
the polynomial-time dynamic programming pars-
ing algorithms of TAG can be used to search for
likely repairs if they are used with simple language
models such as a bigram LM (Johnson and Char-
niak, 2004). The bigram LM within the NCM
is too simple to capture more complicated lan-
guage structure. In order to alleviate this problem,
we follow Zwarts and Johnson (2011) by training
LMs on different corpora, but we apply state-of-
the-art recurrent neural network (RNN) language
models.
</p>
<p>LSTM
We use a long short-term memory (LSTM) neu-
ral network for training language models. LSTM
is a particular type of recurrent neural net-
works which has achieved state-of-the-art perfor-
mance in many tasks including language mod-
elling (Mikolov et al., 2010; Jozefowicz et al.,
2016). LSTM is able to learn long dependencies
between words, which can be highly beneficial for
the speech disfluency detection task. Moreover, it
allows for adopting a distributed representation of
words by constructing word embedding (Mikolov
et al., 2013).
</p>
<p>We train forward and backward (i.e. input sen-
tences are given in reverse order) LSTM language
models using truncated backpropagation through
time algorithm (Rumelhart et al., 1986) with mini-
</p>
<p>batch size 20 and total number of epochs 13.
The LSTM model has two layers and 200 hidden
units. The initial learning rate for stochastic gra-
dient optimizer is chosen to 1 which is decayed by
0.5 for each epoch after maximum epoch 4. We
limit the maximum sentence length for training
our model due to the high computational complex-
ity of longer histories in the LSTM. In our exper-
iments, considering maximum 50 words for each
sentence leads to good results. The size of word
embedding is 200 and it is randomly initialized for
all LSTM LMs2.
</p>
<p>Using each forward and backward LSTM lan-
guage model, we assign a probability to the under-
lying fluent parts of each candidate analysis.
</p>
<p>3.3 Reranker
</p>
<p>In order to rank the the 25-best candidate disflu-
ency analyses of the NCM and select the most
suitable one, we apply the MaxEnt reranker pro-
posed by Johnson et al. (2004). We use the fea-
ture set introduced by Zwarts and Johnson (2011),
but instead of n-gram scores, we apply the LSTM
language model probabilities. The features are so
good that the reranker without any external lan-
guage model is already a state-of-the-art system,
providing a very strong baseline for our work.
The reranker uses both model-based scores (in-
cluding NCM scores and LM probabilities) and
surface pattern features (which are boolean in-
dicators) as described in Table 1. Our reranker
optimizes the expected f-score approximation de-
scribed in Zwarts and Johnson (2011) with L2 reg-
ularisation.
</p>
<p>4 Corpora for Language Modelling
</p>
<p>In this work, we train forward and backward
LSTM language models on Switchboard (Godfrey
and Holliman, 1993) and Fisher (Cieri et al., 2004)
corpora. Fisher consists of 2.2× 107 tokens of
transcribed text, but disfluencies are not annotated
in it. Switchboard is the largest available cor-
pus (1.2× 106 tokens) in which disfluencies are
annotated according to Shriberg’s (1994) scheme.
Since the bigram language model of the NCM
is trained on this corpus, we cannot directly use
Switchboard to build LSTM LMs. The reason is
that if the training data of Switchboard is used both
for predicting language fluency and optimizing the
loss function, the reranker will overestimate the
</p>
<p>2All code is written in TensorFlow (Abadi et al., 2015)
</p>
<p>549</p>
<p />
</div>
<div class="page"><p />
<p>model-based features
1-2. forward &amp; backward LSTM LM scores
3-7. log probability of the entire NCM
8. sum of the log LM probability &amp; the log
channel model probability plus number of ed-
its in the sentence
9. channel model probability
surface pattern features
10. CopyFlags X Y: if there is an exact copy in
the input text of length X (1 ≤ X ≤ 3) and the
gap between the copies is Y (0≤ Y ≤ 3)
11. WordsFlags L n R: number of flags to the
left (L) and to the right (R) of a 3-gram area
(0≤ L,R≤ 1)
12. SentenceEdgeFlags B L: it captures the lo-
cation and length of disfluency. The Boolean B
sentence initial or sentence final disfluency, L
(1≤ L≤ 3) records the length of the flags.
</p>
<p>Table 1: The features used in the reranker. They,
except for the first and second one, were applied
by Zwarts and Johnson (2011).
</p>
<p>weight related to the LM features extracted from
Switchboard. This is because the fluent sentence
itself is part of the language model (Zwarts and
Johnson, 2011). As a solution, we apply a k-fold
cross-validation (k = 20) to train the LSTM lan-
guage models when using Switchboard corpus.
</p>
<p>We follow Charniak and Johnson (2001) in
splitting Switchboard corpus into training, devel-
opment and test set. The training data consists of
all sw[23]∗.dps files, development training con-
sists of all sw4[5-9]∗.dps files and test data con-
sists of all sw4[0-1]∗.dps files. Following Johnson
and Charniak (2004), we remove all partial words
and punctuation from the training data. Although
partial words are very strong indicators of disflu-
ency, standard speech recognizers never produce
them in their outputs, so this makes our evaluation
both harder and more realistic.
</p>
<p>5 Results and Discussion
</p>
<p>We assess the proposed model for disfluency de-
tection with all MaxEnt features described in Ta-
ble 1 against the baseline model. The noisy chan-
nel model with exactly the same reranker features
except the LSTM LMs forms the baseline model.
</p>
<p>To evaluate our system, we use two metrics
f-score and error rate. Charniak and John-
son (2001) used the f-score of labelling reparanda
</p>
<p>or “edited” words, while Fiscus et al (2004) de-
fined an “error rate” measure, which is the number
of words falsely labelled divided by the number
of reparanda words. Since only 6% of words are
disfluent in Switchboard corpus, accuracy is not a
good measure of system performance. F-score, on
the other hand, focuses more on detecting “edited”
words, so it is a decent metric for highly skewed
data.
</p>
<p>According to Tables 2 and 3, the LSTM noisy
channel model outperforms the baseline. The
experiment on Switchboard and Fisher corpora
demonstrates that the LSTM LMs provide infor-
mation about the global fluency of an analysis that
the local features of the reranker do not capture.
The LSTM language model trained on Switch-
board corpus results in the greatest improvement.
Switchboard is in the same domain as the test
data and it is also disfluency annotated. Either
or both of these might be the reason why Switch-
board seems to be better in comparison with Fisher
which is a larger corpus and might be expected
to make a better language model. Moreover,
the backward LSTMs have better performance in
comparison with the forward ones. It seems when
sentences are fed in reverse order, the model can
more easily detect the unexpected word order as-
sociated with the reparandum to detect disfluen-
cies. In other words, that the disfluency is ob-
served “after” the fluent repair in a backward lan-
guage model is helpful for recognizing disfluen-
cies.
</p>
<p>baseline 85.3
corpus forward backward both
Switchboard 86.1 86.6 86.8
Fisher 86.2 86.5 86.3
</p>
<p>Table 2: F-scores on the dev set for a variety of
LSTM language models.
</p>
<p>baseline 27.0
corpus forward backward both
Switchboard 25.5 24.8 24.3
Fisher 25.6 25.0 25.3
</p>
<p>Table 3: Expected error rates on the dev set for a
variey of LSTM language models.
</p>
<p>We compare the performance of Kneser-Ney
</p>
<p>550</p>
<p />
</div>
<div class="page"><p />
<p>smoothed 4-gram language models with the
LSTM corresponding on the reranking process of
the noisy channel model. We estimate the 4-
gram models and assign probabilities to the flu-
ent parts of disflueny analyses using the SRILM
toolkit (Stolcke, 2002). As Tables 4 and 5 show
including scores from a conventional 4-gram lan-
guage model does not improve the model’s abil-
ity to find disfluencies, suggesting that the LSTM
model contains all the useful information that the
4-gram model does. In order to give a more gen-
eral idea on the performance of LSTM over stan-
dard LM, we evaluate our model when the lan-
guage model scores are used as the only features
of the reranker. The f-score for the NCM alone
without applying the reranker is 78.7, while using
4-gram language model scores in the reranker in-
creases the f-score to 81.0. Replacing the 4-gram
scores with LSTM language model probabilities
leads to further improvement, resulting an f-score
82.3.
</p>
<p>baseline 85.3
corpus 4-gram LSTM both
Switchboard 85.1 86.8 86.1
Fisher 85.6 86.3 86
</p>
<p>Table 4: F-score for 4-gram, LSTM and combina-
tion of both language models.
</p>
<p>baseline 27.0
corpus 4-gram LSTM both
Switchboard 27.5 24.3 26
Fisher 26.6 25.3 26
</p>
<p>Table 5: Expected error rates for 4-gram, LSTM
and combination of both language models.
</p>
<p>We also compare our best model on the develop-
ment set to the state-of-the-art methods in the liter-
ature. As shown in Table 6, the LSTM noisy chan-
nel model outperforms the results of prior work,
achieving a state-of-the-art performance of 86.8.
It also has better performance in comparison with
Ferguson et al. (2015) and Zayat et al.’s (2016)
models, even though they use richer input that in-
cludes prosodic features or partial words.
</p>
<p>6 Conclusion and Future Work
</p>
<p>In this paper, we present a new model for dis-
fluency detection from spontaneous speech tran-
</p>
<p>Model f-score
Yoshikawa et al. (2016) 62.5
Johnson and Charniak (2004) 79.7
Johnson et al. (2004) 81.0
Rasooli and Tetreault (2013) 81.4
Qian and Liu (2013) 82.1
Honnibal and Johnson (2014) 84.1
Ferguson et al. (2015) * 85.4
Zwarts and Johnson (2011) 85.7
Zayats et al. (2016) * 85.9
LSTM-NCM 86.8
</p>
<p>Table 6: Comparison of the LSTM-NCM to state-
of-the-art methods on the dev set. *Models have
used richer input.
</p>
<p>scripts. It uses a long short-term memory neural
network language model to rescore the candidate
disfluency analyses produced by a noisy channel
model. The LSTM language model scores as fea-
tures in a MaxEnt reranker improves the model’s
ability to detect restart and repair disfluencies. The
model outperforms other models reported in the
literature, including models that exploit richer in-
formation from the input. As future work, we ap-
ply more complex LSTM language models such
as sequence-to-sequence on the reranking process
of the noisy channel model. We also intend to in-
vestigate the effect of integrating LSTM language
models into other kinds of disfluency detection
models, such as sequence labelling and parsing-
based models.
</p>
<p>Acknowledgements
</p>
<p>We would like to thank the anonymous review-
ers for their insightful comments and sugges-
tions. This research was supported by a Google
award through the Natural Language Understand-
ing Focused Program, and under the Australian
Research Councils Discovery Projects funding
scheme (project number DP160102156).
</p>
<p>References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene
</p>
<p>Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mane, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
</p>
<p>551</p>
<p />
</div>
<div class="page"><p />
<p>Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasude-
van, Fernanda Viegas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-scale ma-
chine learning on heterogeneous systems. Software
available from tensorflow.org.
</p>
<p>Heather Bortfeld, Silvia Leon, Jonathan Bloom,
Michael Schober, and Susan Brennan. 2001. Disflu-
ency rates in conversation: Effects of age, relation-
ship, topic, role, and gender. Language and Speech
44(2):123–147.
</p>
<p>Eugene Charniak and Mark Johnson. 2001. Edit
detection and parsing for transcribed speech. In
Proceedings of the 2nd Meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language Technologies.
Stroudsburg, USA, NAACL’01, pages 118–126.
http://aclweb.org/anthology/N01-1016.
</p>
<p>Christopher Cieri, David Miller, and Kevin Walker.
2004. Fisher English training speech part 1 tran-
scripts LDC2004T19. Published by: Linguistic
Data Consortium, Philadelphia, USA.
</p>
<p>James Ferguson, Greg Durrett, and Dan Klein. 2015.
Disfluency detection with a semi-Markov model and
prosodic features. In Proceedings of the Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Denver, USA, NAACL’15,
pages 257–262.
</p>
<p>Jonathan Fiscus, John Garofolo, Audrey Le, Alvin
Martin, David Pallet, Mark Przybocki, and Greg
Sanders. 2004. Results of the fall 2004 STT and
MDE evaluation. In Proceedings of Rich Transcrip-
tion Fall Workshop.
</p>
<p>John Godfrey and Edward Holliman. 1993.
Switchboard-1 release 2 LDC97S62. Published by:
Linguistic Data Consortium, Philadelphia, USA.
</p>
<p>Matthew Honnibal and Mark Johnson. 2014. Joint
incremental disfluency detection and depen-
dency parsing. Transactions of the Association
for Computational Linguistics 2(1):131–142.
http://www.aclweb.org/anthology/Q14-1011.
</p>
<p>Julian Hough and David Schlangen. 2015. Recurrent
neural networks for incremental disfluency detec-
tion. In Proceedings of the 16th Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH). Dresden, Germany, pages
845–853.
</p>
<p>Mark Johnson and Eugene Charniak. 2004. A
TAG-based noisy channel model of speech re-
pairs. In Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguis-
tics. Barcelona, Spain, ACL’04, pages 33–39.
http://aclweb.org/anthology/P04-1005.
</p>
<p>Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disflu-
encies in conversational speech. In Proceedings of
Rich Transcription Workshop.
</p>
<p>Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of language modeling. CoRR abs/1602.02410.
</p>
<p>Yang Liu, Elizabeth Shriberg, Andreas Stolckeand,
Dustin Hillard, Mari Ostendorf, and Mary Harper.
2006. Enriching speech recognition with auto-
matic detection of sentence boundaries and disflu-
encies. IEEE/ACM Transactions on Audio, Speech,
and Language Processing 14(5):1526–1540.
</p>
<p>Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH). Makuhari, Japan, pages 1045–1048.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 27th Annual Conference
on Neural Information Processing Systems (NIPS).
Curran Associates Inc., pages 3111–3119.
</p>
<p>Mari Ostendorf, Benoit Favre, Ralph Grishman, Dilek
Hakkani-Tur, Mary Harper, Dustin Hillard, Julia
Hirschberg, Heng Ji, Jeremy G. Kahn, Yang Liu,
Sameer Maskey, Evgeny Matusov, Hermann Ney,
Andrew Rosenberg, Elizabeth Shriberg, Wen Wang,
and Chuck Wooters. 2008. Speech segmentation
and its impact on spoken document processing.
IEEE Signal Processing Magazine 25(3):59–69.
</p>
<p>Mari Ostendorf and Sangyun Hahn. 2013. A sequen-
tial repetition model for improved disfluency detec-
tion. In Proceedings of the 14th Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH). Lyon, France, pages 2624–
2628.
</p>
<p>Xian Qian and Yang Liu. 2013. Disfluency detec-
tion using multi-step stacked learning. In Pro-
ceedings of the Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies. Atlanta, USA, NAACL’13, pages 820–825.
http://aclweb.org/anthology/N13-1102.
</p>
<p>Mohammad Sadegh Rasooli and Joel Tetreault. 2013.
Joint parsing and disfluency detection in linear time.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Seattle, USA,
pages 124–129. http://aclweb.org/anthology/D13-
1013.
</p>
<p>David Rumelhart, James McClelland, and PDP Re-
search Group. 1986. Parallel Distributed Process-
ing: Explorations in the Microstructure of Cogni-
tion, volume 1. MIT Press.
</p>
<p>552</p>
<p />
</div>
<div class="page"><p />
<p>William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage parsing us-
ing human-like memory constraints. Computational
Linguistics 36(1):1–30.
</p>
<p>Elizabeth Shriberg. 1994. Preliminaries to a theory of
speech disfluencies. Ph.D. thesis, University of Cal-
ifornia, Berkeley, USA.
</p>
<p>Andreas Stolcke. 2002. SRILM: An extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
Association for Computational Linguistics, Denver,
Colorado, USA, volume 2, pages 901–904.
</p>
<p>Masashi Yoshikawa, Hiroyuki Shindo, and Yuji Mat-
sumoto. 2016. Joint transition-based dependency
parsing and disfluency detection for automatic
speech recognition texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). pages 1036–1041.
http://aclweb.org/anthology/D16-1109.
</p>
<p>Victoria Zayats, Mari Ostendorf, and Hannaneh Ha-
jishirzi. 2014. Multi-domain disfluency and re-
pair detection. In Proceedings of the 15th Annual
Conference of the International Speech Commu-
nication Association (INTERSPEECH). Singapore,
pages 2907–2911.
</p>
<p>Victoria Zayats, Mari Ostendorf, and Hannaneh Ha-
jishirzi. 2016. Disfluency detection using a bidirec-
tional LSTM. In Proceedings of the 16th Annual
Conference of the International Speech Communica-
tion Association (INTERSPEECH). San Francisco,
USA, pages 2523–2527.
</p>
<p>Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair
disfluency detection. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics, Port-
land, USA, volume 1 of HLT’11, pages 703–711.
http://aclweb.org/anthology/P11-1071.
</p>
<p>553</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 554–559
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2088
</p>
<p>On the Equivalence of Holographic and Complex
Embeddings for Link Prediction
</p>
<p>Katsuhiko Hayashi
NTT Communication Science Laboratories, Seika-cho, Kyoto 619 0237, Japan
</p>
<p>hayashi.katsuhiko@lab.ntt.co.jp
</p>
<p>Masashi Shimbo
Nara Institute of Science and Technology, Ikoma, Nara 630 0192, Japan
</p>
<p>shimbo@is.naist.jp
</p>
<p>Abstract
</p>
<p>We show the equivalence of two state-
of-the-art models for link prediction/
knowledge graph completion: Nickel et
al’s holographic embeddings and Trouil-
lon et al.’s complex embeddings. We first
consider a spectral version of the holo-
graphic embeddings, exploiting the fre-
quency domain in the Fourier transform
for efficient computation. The analysis of
the resulting model reveals that it can be
viewed as an instance of the complex em-
beddings with a certain constraint imposed
on the initial vectors upon training. Con-
versely, any set of complex embeddings
can be converted to a set of equivalent
holographic embeddings.
</p>
<p>1 Introduction
</p>
<p>Recently, there have been efforts to build and
maintain large-scale knowledge bases represented
in the form of a graph (knowledge graph) (Auer
et al., 2007; Bollacker et al., 2008; Suchanek et al.,
2007). Although these knowledge graphs con-
tain billions of relational facts, they are known
to be incomplete. Knowledge graph completion
(KGC) (Nickel et al., 2015) aims at augmenting
missing knowledge in an incomplete knowledge
graph automatically. It can be viewed as a task
of link prediction (Liben-Nowell and Kleinberg,
2003; Hasan and Zaki, 2011) studied in the field of
statistical relational learning (Getoor and Taskar,
2007). In recent years, methods based on vec-
tor embeddings of graphs have been actively pur-
sued as a scalable approach to KGC (Bordes et al.,
2011; Socher et al., 2013; Guu et al., 2015; Yang
et al., 2015; Nickel et al., 2016; Trouillon et al.,
2016b).
</p>
<p>In this paper, we investigate the connection
</p>
<p>between two models of graph embeddings that
have emerged along this line of research: The
holographic embeddings (Nickel et al., 2016) and
the complex embeddings (Trouillon et al., 2016b).
These models are simple yet achieve the current
state-of-the-art performance in KGC.
</p>
<p>We begin by showing that holographic embed-
dings can be trained entirely in the frequency do-
main induced by the Fourier transform, thereby
reducing the time needed to compute the scoring
function from O(n log n) to O(n), where n is the
dimension of the embeddings.
</p>
<p>The analysis of the resulting training method
reveals that the Fourier transform of holographic
embeddings can be regarded as an instance of the
complex embeddings, with a specific constraint
(viz. conjugate symmetry property) cast on on the
initial values.
</p>
<p>Conversely, we also show that every set of com-
plex embeddings has a set of holographic embed-
dings (with real vectors) that is equivalent, in the
sense that their scoring functions are equal up to
scaling.
</p>
<p>2 Preliminaries
</p>
<p>Let i denote the imaginary unit, R be the set of
real values, and C the set of complex values. We
write [v] j to denote the jth component of vector v.
A superscript T (e.g., vT) represents vector/matrix
transpose. For a complex scalar z, vector z, and
matrix Z, z, z, and Z represent their complex con-
jugate, and Re(z), Re(z), and Re(Z) denote their
real parts, respectively.
</p>
<p>Let x = [x0 · · · xn−1]T ∈ Rn and y =
[y0 · · · yn−1]T ∈ Rn. Note that the vector indices
start from 0 for notational convenience. The cir-
cular convolution of x and y, denoted by x ∗ y, is
</p>
<p>554</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2088">https://doi.org/10.18653/v1/P17-2088</a></div>
</div>
<div class="page"><p />
<p>defined by
</p>
<p>[x ∗ y] j =
n−1∑
</p>
<p>k=0
</p>
<p>x[( j−k) mod n]yk, (1)
</p>
<p>where mod denotes modulus. Likewise, circular
correlation x ? y is defined by
</p>
<p>[x ? y] j =
n−1∑
</p>
<p>k=0
</p>
<p>x[(k− j) mod n]yk. (2)
</p>
<p>While circular convolution is commutative, cir-
cular correlation is not; i.e., x ∗ y = y ∗ x, but
x ? y , y ? x in general. As it can be verified
with Eqs. (1) and (2), x ? y = flip(x) ∗ y, where
flip(x) = [xn−1 · · · x0]T is a vector obtained by
arranging the components of x in reverse.
</p>
<p>For n-dimensional vectors, naively computing
circular convolution/correlation by Eqs. (1) and
(2) requires O(n2) multiplications. However, we
can take advantage of the Fast Fourier Transform
(FFT) algorithm to accelerate the computation:
For circular convolution, first compute the discrete
Fourier transform (DFT) of x and y, and then com-
pute the inverse DFT of their elementwise vector
product, i.e.,
</p>
<p>x ∗ y = F−1(F(x) � F(y)),
</p>
<p>where F : Rn → Cn and F−1 : Cn → Rn re-
spectively denote the DFT and inverse DFT, and
� denotes the elementwise product. DFT and in-
verse DFT can be computed in O(n log n) time
with the FFT algorithm, and thus the computation
time for circular convolution is also O(n log n).
The same can be said of circular correlation. Since
F(flip(x)) = F(x), we have
</p>
<p>x ? y = F−1(F(x) � F(y)). (3)
</p>
<p>By analogy to how the Fourier transform is used
in signal processing, the original real space Rn is
called the “time” domain, and the complex space
Cn where DFT vectors reside is called the “fre-
quency” domain.
</p>
<p>3 Holographic embeddings for
knowledge graph completion
</p>
<p>3.1 Knowledge graph completion
Let E and R be the finite sets of entities and (bi-
nary) relations over entities, respectively. For each
relation r ∈ R and each pair s, o ∈ E of entities,
</p>
<p>Table 1: Correspondence between operations in
time and frequency domains. r ↔ ρ indicates
ρ = F(r) (and also r = F−1(ρ)).
</p>
<p>operation time frequency
</p>
<p>scalar mult. αx ←→ αF(x)
summation x + y ←→ F(x) + F(y)
</p>
<p>flip flip(x) ←→ F(x)
convolution x ∗ y ←→ F(x) � F(y)
correlation x ? y ←→ F(x) � F(y)
</p>
<p>dot product x · y = 1n F(x) · F(y)
</p>
<p>we are interested in whether r(s, o) holds1 or not;
we write r(s, o) = +1 if it holds, and r(s, o) = −1
if not. To be precise, given a training set D =
R × E × E × {−1,+1} such that (r, s, o, y) ∈ D
indicates y = r(s, o), our task is to design a scor-
ing function f : R × E × E → R such that for
each of the triples (r, s, o) not observed inD, func-
tion f should give a higher value if r(s, o) = +1 is
more likely, and a smaller value for those that are
less likely. If necessary, f (r, s, o) can be converted
to probability by P[r(s, o) = +1] = σ( f (r, s, o)),
where σ : R→ (0, 1) is a sigmoid function.
</p>
<p>DatasetD can be regarded as a directed graph in
which nodes represent entities E and edges are la-
beled by relations R. Thus, the task is essentially
that of link prediction (Liben-Nowell and Klein-
berg, 2003; Hasan and Zaki, 2011). Often, it is
also called knowledge graph completion.
</p>
<p>3.2 Holographic embeddings (HolE)
Nickel et al. (2016) proposed holographic em-
beddings (HolE) for knowledge graph completion.
Using training data D, this method learns the vec-
tor embeddings ek ∈ Rn of entities k ∈ E and the
embeddings wr ∈ Rn of relations r ∈ R. The score
for triple (r, s, o) is then given by
</p>
<p>fHolE(r, s, o) = wr · (es ? eo). (4)
</p>
<p>Eq. (4) can be evaluated in time O(n log n) if es?eo
is computed by Eq. (3).
</p>
<p>4 Spectral training of HolE
</p>
<p>To compute the circular correlation in the scoring
function of Eq. (4) efficiently, Nickel et al. (2016)
used Eq. (3) in Section 2 and FFT. In this sec-
tion, we extend this technique further, and con-
</p>
<p>1Depending on the context, letter r is used either as an
index to an element inR or the binary relation it signifies.
</p>
<p>555</p>
<p />
</div>
<div class="page"><p />
<p>sider training HolE solely in the frequency do-
main. That is, real-valued embeddings ek,wr ∈
Rn in the original “time” domain are abolished,
and instead we train their DFT counterparts εk =
F(ek) ∈ Cn and ωk = F(wr) ∈ Cn in the frequency
domain. This formulation eliminates the need of
FFT and inverse FFT, which are the major com-
putational bottleneck in HolE. As a result, Eq. (4)
can be computed in time O(n) directly from εk and
ωk.
</p>
<p>Indeed, equivalent counterparts in the frequency
domain exist for not only convolution/correlation
but all other computations needed for HolE: scalar
multiplication, summation (needed when vectors
are updated by stochastic gradient descent), and
dot product (used in Eq. (4)). The frequency-
domain equivalents for these operations are sum-
marized in Table 1. All of these can be performed
efficiently (in linear time) in the frequency do-
main.
</p>
<p>In particular, the following relation holds for the
dot product between any “time” vectors x, y ∈ Rn.
</p>
<p>x · y = 1
n
F(x) · F(y), (5)
</p>
<p>where the dot product on the right-hand side is the
complex inner product defined by a · b = aTb.
Eq. (5) is known as Parseval’s theorem (also called
the power theorem in (Smith, 2007)), and it states
that dot products in two domains are equal up to
scaling.
</p>
<p>After embeddings εk,ωr ∈ Cn are learned in
the frequency domain, their time-domain coun-
terparts ek = F−1(εk) and wr = F−1(ωr) can be
recovered if needed, but this is not required as
far as computation of the scoring function is con-
cerned. Thanks to Parseval’s theorem, Eq. (4) can
be directly computed from the frequency vectors
εk,ωr ∈ Cn by
</p>
<p>fHolE(r, s, o) =
1
n
ωr · (εs � εo). (6)
</p>
<p>4.1 Conjugate symmetry of spectral
components
</p>
<p>A complex vector ξ = [ξ0 · · · ξn−1]T ∈ Cn is
said to be conjugate symmetric (or Hermitian) if
ξ j = ξ[(n− j) mod n] for j = 0, . . . , n − 1, or, in other
words, if it can be written in the form
</p>
<p>ξ =
</p>
<p>
</p>
<p>[
ξ0 γ flip(γ)
</p>
<p>]T
, if n is odd,
</p>
<p>[
ξ0 γ ξn/2 flip(γ)
</p>
<p>]T
, if n is even,
</p>
<p>for some γ ∈ Cdn/2e−1 and ξ0, ξn/2 ∈ R.
The DFT F(x) is conjugate symmetric if and
</p>
<p>only if x is a real vector. Thus, maintaining con-
jugate symmetry of “frequency” vectors is the key
to ensure their “time” counterparts remain in real
space. Below, we verify that this property is in-
deed preserved with stochastic gradient descent.
Moreover, conjugate symmetry provides a suffi-
cient condition under which dot product takes a
real value. It also has implications on space re-
quirement. These topics are covered in the rest of
this section.
</p>
<p>4.2 Vector initialization and update in
frequency domain
</p>
<p>Typically, at the beginning of training HolE, each
individual embedding is initialized by a random
vector. When we train HolE in the frequency do-
main, we could first generate a random real vector,
regard them as a HolE vector in the time domain,
and compute its DFT as the initial value in the fre-
quency domain. An alternative, easier approach
is to directly generate a random complex vector
that is conjugate symmetric, and use it as the ini-
tial frequency vector. This guarantees the inverse
DFT to be a real vector, i.e., there exists a valid
corresponding image in the time domain.
</p>
<p>Given a training set D (see Section 3.1),
HolE is trained by minimizing the following
objective function over parameter matrix Θ =
[e1 · · · e|E | w1 · · ·w|R|] ∈ Rn×(|E |+|R|):
</p>
<p>∑
</p>
<p>(r,s,o,y)∈D
log{1+exp(−y fHolE(r, s, o))}+λ||Θ||2F (7)
</p>
<p>where λ ∈ R is the hyperparameter controlling
the degree of regularization, and ‖ · ‖F denotes the
Frobenius norm.
</p>
<p>In our version of spectral training of HolE,
the parameters matrix consists of frequency vec-
tors εk and ωr instead of ek and wr, i.e., Θ =
[ε1 · · · ε|E | ω1 · · ·ω|R|] ∈ Cn×(|E |+|R|). Let us dis-
cuss the stochastic gradient descent (SGD) update
with respect to these frequency vectors. In partic-
ular, we are interested in whether conjugate sym-
metry of vectors is preserved by the update.
</p>
<p>Suppose vectors ωr, εs, εo are conjugate sym-
metric. Neglecting the contribution from the
regularization term2 in Eq. (7), we see that in
</p>
<p>2It can be easily verified that the contribution from the
regularization term to SGD update do not violate conjugate
symmetry.
</p>
<p>556</p>
<p />
</div>
<div class="page"><p />
<p>an SGD update step, α∂ fHoLE/∂ωr, α∂ fHoLE/∂εs,
and α∂ fHoLE/∂εo are respectively subtracted from
ωr, εs, εo, where α ∈ R is a factor not depending
on these parameters. Noting the equalities
</p>
<p>wr · (es ? eo) = es · (wr ? eo) = eo · (wr ∗ es)
(see (Nickel et al., 2016, Eq. (12), p. 1958)) and
their frequency counterparts
</p>
<p>ωr · (εs � εo) = εs · (ωr � εo) = εo · (ωr � εs),
obtained through the translation of Table 1, we can
derive
</p>
<p>∂ fHolE
∂ωr
</p>
<p>= εs � εo,
∂ fHolE
∂εs
</p>
<p>= ωr � εo,
∂ fHolE
∂εo
</p>
<p>= ωr � εs.
</p>
<p>As seen from above, conjugation, scalar multipli-
cation, summation, and elementwise product are
used in the SGD update. And it is straightforward
to verify that all these operations preserve con-
jugate symmetry. It follows that if ωr, εs, εo are
initially conjugate symmetric, they will remain so
during the course of training, which assures that
the inverse DFTs of the learned embeddings are
real vectors.
</p>
<p>4.3 Real-valued dot product
In the scoring function of HolE (Eq. (4)), dot prod-
uct is used for generating a real-valued “score”
out of two vectors, wr and es � eo. Likewise,
in Eq. (6), the dot product is applied to ωr and
εs � εo, which are complex-valued. However, pro-
vided that the conjugate symmetry of these vec-
tors is maintained, their dot product is always real.
This follows from Parseval’s theorem; the inverse
DFTs of these frequency vectors are real, and thus
their dot product is also real. Therefore, the dot
product of the corresponding frequency vectors is
real as well, according to Eq. (5).
</p>
<p>4.4 Space requirement
A general complex vector ξ ∈ Cn can be stored
in memory as 2n floating-point numbers, i.e., one
each for the real and imaginary part of a compo-
nent. In our spectral representation of HolE, how-
ever, the first bn/2c components suffice to specify
the frequency vector ξ, since the vector is con-
jugate symmetric. Moreover, ξ0 (and ξn/2 if n
</p>
<p>is even) are real values. Thus, a spectral repre-
sentation of HolE can be specified with exactly n
floating-point numbers, which can be stored in the
same amount of memory as needed by the original
HolE.
</p>
<p>5 Relation to Trouillon et al.’s complex
embeddings
</p>
<p>5.1 Complex embeddings (ComplEx)
Trouillon et al. (2016b) proposed a model of
embedding-based knowledge graph completion,
called complex embeddings (ComplEx). The ob-
jective is similar to Nickel et al.’s; the embed-
dings ek of entities and wr of relations are to be
learned. In their model, however, these vectors
are complex-valued, and are based on the eigen-
decomposition of complex matrix Xr = EWrE
</p>
<p>T
</p>
<p>that encodes relation r ∈ R over pairs of entities,
where Xr ∈ C|E |×|E |, E = [e1, . . . , e|E |]T ∈ C|E |×n,
and Wr = diag(wr) ∈ Cn×n is a diagonal matrix
(with diagonal elements wr ∈ Cn). In practice,
Xr needs to be a real matrix, because its (r, s)-
component must define the score for r(s, o). To
this end, Trouillon et al. simply extracted the
real part; i.e., Xr = Re(EWrE
</p>
<p>T
). Trouillon et al.
</p>
<p>(2016a) advocated this approach, by showing that
any real matrix Xr can be expressed in this form.
</p>
<p>With this formulation, the score for triple
(r, s, o) is given by
</p>
<p>fComplEx(r, s, o) = Re
</p>
<p>
n−1∑
</p>
<p>j=0
</p>
<p>[wr] j[es] j[eo] j
</p>
<p> . (8)
</p>
<p>5.2 Equivalence of holographic and complex
embeddings
</p>
<p>Now let us rewrite Eq. (8). Noting the definition
of complex dot product, i.e., a · b = aTb, we have
</p>
<p>n−1∑
</p>
<p>j=0
</p>
<p>[wr] j[es] j[eo] j = (es � eo)Twr
</p>
<p>= (es � eo) · wr (∵ a · b = aTb)
= (es � eo) · wr
= wr · (es � eo) (∵ a · b = b · a)
</p>
<p>and since Re(z) = Re(z),
</p>
<p>Re(wr · (es � eo)) = Re(wr · (es � eo)).
Thus, Eq. (8) can be written as
</p>
<p>fComplEx(r, s, o) = Re
(
wr · (es � eo)) . (9)
</p>
<p>557</p>
<p />
</div>
<div class="page"><p />
<p>Here, a marked similarity is noticeable between
Eq. (9) and Eq. (6), the scoring function of our
spectral version of HolE (spectral HolE); Com-
plEx extracts the real part of complex dot prod-
uct, whereas in the spectral HolE, dot product is
guaranteed to be real because all embeddings sat-
isfy conjugate symmetry. Indeed, Eq. (6) can be
equally written as
</p>
<p>fHolE(r, s, o) =
1
n
</p>
<p>Re
(
ωr · (εs � εo)) . (10)
</p>
<p>although the operator Re(·) in this formula is re-
dundant, since the inner product is guaranteed to
be real-valued. Nevertheless, Eq. (10) elucidates
the fact that the spectral HolE can be viewed as an
instance of ComplEx, with the embeddings con-
strained to be conjugate symmetric to make the
inner product in Eq. (10) real-valued.
</p>
<p>Conversely, given a set of complex embeddings
for entities and relations, we can construct their
equivalent holographic embeddings, in the sense
that fComplEx(r, s, o) = c fHolE(r, s, o) for every
r, s, o, where c &gt; 0 is a constant. For each n-
dimensional complex embeddings x ∈ {ek}k∈E ∪
{wr}r∈R ⊂ Cn computed by ComplEx, we make a
corresponding HolE h(x) ∈ R2n+1 as follows: For a
given complex embedding x = [x0 · · · xn−1] ∈ Cn,
first compute s(x) ∈ C2n+1 by
</p>
<p>s(x) =
[
0 x0 · · · xn−1 xn−1 · · · x0
</p>
<p>]T
</p>
<p>=
[
0 x flip(x)
</p>
<p>]T
(11)
</p>
<p>and then define h(x) = F−1(s(x)). Since s(x) is
conjugate symmetric, h(x) is a real vector.
</p>
<p>To verify if this conversion defines an equiva-
lent scoring function for any triple (r, s, o), let us
now suppose complex embeddings wr ∈ Cn and
es, eo ∈ Cn are given. Since we regard real vec-
tors h(wr), h(es), h(eo) ∈ R2n+1 as the holographic
embeddings of r, s and o, respectively, the HolE
score for the triple (r, s, o) is given as
</p>
<p>fHolE(r, s, o)
</p>
<p>= h(wr) · (h(es) ? h(eo))
=
</p>
<p>1
n
s(wr) · (s(es) � s(eo)) (∵ Eq. (6))
</p>
<p>=
1
n
s(wr) ·
</p>
<p>[
0 es � eo flip(es � eo)
</p>
<p>]T
(∵ Eq. (11))
</p>
<p>=
1
n
</p>
<p>[
0 wr flip(wr)
</p>
<p>]T ·
[
0 es � eo flip(es � eo)
</p>
<p>]T
</p>
<p>=
1
n
</p>
<p>(
wr · (es � eo) + flip(wr) · flip(es � eo)
</p>
<p>)
</p>
<p>=
1
n
</p>
<p>(
wr · (es � eo) + wr · (es � eo)
</p>
<p>)
</p>
<p>=
1
n
</p>
<p>(
wr · (es � eo) + wr · (es � eo)
</p>
<p>)
</p>
<p>=
2
n
</p>
<p>Re
(
wr · (es � eo))
</p>
<p>=
2
n
</p>
<p>fComplEx(r, s, o),
</p>
<p>which shows that h(·) (or s(·)) gives the desired
conversion from ComplEx to HolE.
</p>
<p>6 Conclusion
</p>
<p>In this paper, we have shown that the holographic
embeddings (HolE) can be trained entirely in the
frequency domain. If stochastic gradient descent
is used for training, the conjugate symmetry of
frequency vectors is preserved, which ensures the
existence of the corresponding holographic em-
bedding in the original real space (time domain).
Also, this training method eliminates the need of
FFT and inverse FFT, thereby reducing the compu-
tation time of the scoring function from O(n log n)
to O(n).
</p>
<p>Moreover, we have established the equivalence
of HolE and the complex embeddings (ComplEx):
The spectral version of HolE is subsumed by Com-
plEx as a special case in which conjugate symme-
try is imposed on the embeddings. Conversely, ev-
ery set of complex embeddings can be converted
to equivalent holographic embeddings.
</p>
<p>Many systems for natural language process-
ing, such as those for semantic parsing and ques-
tion answering, benefit from access to information
stored in knowledge graphs. We plan to further in-
vestigate the property of spectral HolE and Com-
plEx in these applications.
</p>
<p>Acknowledgments
</p>
<p>We thank the anonymous reviewers for helpful
comments. This work was partially supported by
JSPS Kakenhi Grants 26730126 and 15H02749.
</p>
<p>References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
</p>
<p>Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A nucleus for a web of open data.
In The Semantic Web: Proceedings of the 6th In-
ternational Semantic Web Conference and the 2nd
Asian Semantic Web Conference (ISWC ’07/ASWC
’07). Springer, Lecture Notes in Computer Sci-
ence 4825, pages 722–735.
</p>
<p>558</p>
<p />
</div>
<div class="page"><p />
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data (SIGMOD ’08). pages 1247–1250.
</p>
<p>Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured em-
beddings of knowledge bases. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI ’11). pages 301–306.
</p>
<p>Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning. Adaptive Computation
and Machine Learning. The MIT Press.
</p>
<p>Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP
’15). pages 318–327.
</p>
<p>Mohammad Al Hasan and Mohammed J. Zaki. 2011.
A survey of link prediction in social networks. In
Charu C. Aggarwal, editor, Social Network Data An-
alytics, Springer, chapter 9, pages 243–275.
</p>
<p>David Liben-Nowell and Jon Kleinberg. 2003. The
link prediction problem for social networks. In
Proceedings of the 12nd Annual ACM International
Conference on Information and Knowledge Man-
agement (CIKM ’03). pages 556–559.
</p>
<p>Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2015. A review of relational
machine learning for knowledge graphs. Proceed-
ings of the IEEE pages 1–18.
</p>
<p>Maximilian Nickel, Lorenzo Rosasco, and Tomaso
Poggio. 2016. Holographic embeddings of knowl-
edge graphs. In Proceedings of the 30th AAAI Con-
ference on Artificial Intelligence (AAAI ’16). pages
1955–1961.
</p>
<p>III Julius O. Smith. 2007. Mathematics of the Discrete
Fourier Transform (DFT): with Audio Applications.
W3K Publishing, 2nd edition.
</p>
<p>Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with neural
tensor networks for knowledge base completion. In
Advances in Neural Information Processing Systems
26 (NIPS ’13). pages 926–934.
</p>
<p>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying Wordnet and Wikipedia. In Proceed-
ings of the 16th International Conference on World
Wide Web (WWW ’07). pages 697–706.
</p>
<p>Théo Trouillon, Christopher R. Dance, Éric Gaussier,
and Guillaume Bouchard. 2016a. Decomposing
real square matrices via unitary diagonalization.
arXiv.math eprint 1605.07103, arXiv.
</p>
<p>Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016b. Com-
plex embeddings for simple link prediction. In Pro-
ceedings of the 33rd International Conference on
Machine Learning (ICML ’16). pages 2071–2080.
</p>
<p>Bishan Yang, Wen tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge
bases. In Proceedings of the 3rd International Con-
ference on Learning Representations (ICLR ’15).
</p>
<p>559</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2089
</p>
<p>Sentence Embedding for Neural Machine Translation Domain Adaptation
</p>
<p>Rui Wang, Andrew Finch, Masao Utiyama and Eiichiro Sumita
National Institute of Information and Communications Technology (NICT)
</p>
<p>3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{wangrui,andrew.finch,mutiyama,eiichiro.sumita}@nict.go.jp
</p>
<p>Abstract
</p>
<p>Although new corpora are becoming
increasingly available for machine
translation, only those that belong to the
same or similar domains are typically
able to improve translation performance.
Recently Neural Machine Translation
(NMT) has become prominent in the
field. However, most of the existing
domain adaptation methods only focus
on phrase-based machine translation. In
this paper, we exploit the NMT’s internal
embedding of the source sentence and
use the sentence embedding similarity to
select the sentences which are close to
in-domain data. The empirical adaptation
results on the IWSLT English-French and
NIST Chinese-English tasks show that
the proposed methods can substantially
improve NMT performance by 2.4-9.0
BLEU points, outperforming the existing
state-of-the-art baseline by 2.3-4.5 BLEU
points.
</p>
<p>1 Introduction
</p>
<p>Recently, Neural Machine Translation (NMT)
has set new state-of-the-art benchmarks on many
translation tasks (Cho et al., 2014; Bahdanau et al.,
2015; Jean et al., 2015; Tu et al., 2016; Mi et al.,
2016; Zhang et al., 2016). An ever increasing
amount of data is becoming available for NMT
training. However, only the in-domain or related-
domain corpora tend to have a positive impact
on NMT performance. Unrelated additional
corpora, known as out-of-domain corpora, have
been shown not to benefit some domains and tasks
for NMT, such as TED-talks and IWSLT tasks
(Luong and Manning, 2015).
</p>
<p>To the best of our knowledge, there are only
</p>
<p>a few works concerning NMT adaptation (Luong
and Manning, 2015; Freitag and Al-Onaizan,
2016). Most traditional adaptation methods focus
on Phrase-Based Statistical Machine Translation
(PBSMT) and they can be broken down broadly
into two main categories namely model adaptation
and data selection (Joty et al., 2015) as follows.
</p>
<p>For model adaptation, several PBSMT models,
such as language models, translation models and
reordering models, individually corresponding to
each corpus, are trained. These models are
then combined to achieve the best performance
(Sennrich, 2012; Sennrich et al., 2013; Durrani
et al., 2015). Since these methods focus on the
internal models within a PBSMT system, they are
not applicable to NMT adaptation. Recently, an
NMT adaptation method (Luong and Manning,
2015) was proposed. The training is performed
in two steps: first the NMT system is trained
using out-of-domain data, and then further trained
using in-domain data. Empirical results show their
method can improve NMT performance, and this
approach provides a natural baseline.
</p>
<p>For adaptation through data selection, the main
idea is to score the out-domain data using models
trained from the in-domain and out-of-domain
data and select training data from the out-of-
domain data using a cut-off threshold on the
resulting scores. A language model can be
used to score sentences (Moore and Lewis, 2010;
Axelrod et al., 2011; Duh et al., 2013; Wang
et al., 2015), as well as joint models (Hoang and
Sima’an, 2014a,b; Durrani et al., 2015), and more
recently Convolutional Neural Network (CNN)
models (Chen et al., 2016). These methods select
useful sentences from the whole corpus, so they
can be directly applied to NMT. However, these
methods are specifically designed for PBSMT and
nearly all of them use the models or criteria which
do not have a direct relationship with the neural
</p>
<p>560</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2089">https://doi.org/10.18653/v1/P17-2089</a></div>
</div>
<div class="page"><p />
<p>translation process.
For NMT sentences selection, our hypothesis is
</p>
<p>that the NMT system itself can be used to score
each sentence in the training data. Specifically,
an NMT system embeds the source sentence into
a vector representation1 and we can use these
vectors to measure a sentence pair’s similarity
to the in-domain corpus. In comparison with
the CNN or other sentence embedding methods,
this method can directly make use of information
induced by the NMT system information itself. In
addition, the proposed sentence selection method
can be used in conjunction with the NMT further
training method (Luong and Manning, 2015).
</p>
<p>2 NMT Background
</p>
<p>An attention-based NMT system uses a
Bidirectional RNN (BiRNN) as an encoder
and a decoder that emulates searching through a
source sentence during decoding (Bahdanau et al.,
2015). The encoder’s BiRNN consists of forward
and backward RNNs. Each word xi is represented
by concatenating the forward hidden state
</p>
<p>−→
hi and
</p>
<p>the backward one
←−
hi as hi = [
</p>
<p>−→
hi ;
←−
hi ]
&gt;. In this
</p>
<p>way, the source sentence X = {x1, ..., xTx} can
be represented as annotations H = {h1, ..., hTx}.
In the decoder, an RNN hidden state sj for time j
is computed by:
</p>
<p>sj = f(sj−1, yj−1, cj). (1)
</p>
<p>The context vector cj is then, computed as
a weighted sum of these annotations H =
{h1, ..., hTx}, by using alignment weight αji:
</p>
<p>cj =
</p>
<p>Tx∑
</p>
<p>j=1
</p>
<p>αjihi. (2)
</p>
<p>3 Sentence Embedding and Selection
</p>
<p>3.1 Sentence Embedding
</p>
<p>A source sentence can be represented as the
annotations H. However the length of H depends
on the sentence length Tx. To represent a sentence
as a fixed-length vector, we adopt the initial hidden
</p>
<p>1Li et al. (2016)’s fine-tuned NMT systems apply a similar
sentence representation. In comparison, we adopt a transition
layer between the source and target layers and don’t use test
data.
</p>
<p>layer state sinit for the decoder as this vector:
</p>
<p>sinit(X) = tanh(W
</p>
<p>∑Tx
i=1 hi
Tx
</p>
<p>+ b), hi ∈ H,
(3)
</p>
<p>where an average pooling layer averages the
annotation hi for each source word into a fixed-
length source sentence vector, and a nonlinear
transition layer (weights W and bias b are jointly
trained with all the other components of NMT
system) transforms this embedded source sentence
vector into the initial hidden state sinit for the
decoder (Bahdanau et al., 2015).
</p>
<p>3.2 Sentence Selection
</p>
<p>We employ the data selection method, which is
inspired by (Moore and Lewis, 2010; Axelrod
et al., 2011; Duh et al., 2013). As Axelrod
et al. (2011) mentioned, there are some pseudo
in-domain data in out-of-domain data, which are
close to in-domain data. Our intuition is to select
the sentences whose embeddings are similar to the
average in-domain ones, while being dis-similar to
the average out-of-domain ones:
</p>
<p>• 1) We train a French-to-English NMT system
NFE using the in-domain and out-of-domain
data together as training data.2
</p>
<p>• 2) Each sentence f in the training data F
(both in-domain Fin and out-of-domain Fout)
is embedded as a vector vf = sinit(f) by
using NFE.
</p>
<p>• 3) The sentence pairs (f, e) in the out-
of-domain corpus Fout are classified into
two sets: the sentences close to in-domain
sentences, and those that are distant.
</p>
<p>That is, we firstly calculate the vector centers of
in-domain CFin and out-of-domain CFout corpora,
respectively.
</p>
<p>CFin =
</p>
<p>∑
f∈Fin vf
|Fin|
</p>
<p>,
</p>
<p>CFout =
</p>
<p>∑
f∈Fout vf
|Fout|
</p>
<p>.
</p>
<p>(4)
</p>
<p>Then we measure the Euclidean distance d
between each sentence vector vf and in-domain
</p>
<p>2It is possible to use a sample of the out-of-domain data.
In this paper, we use all of them.
</p>
<p>561</p>
<p />
</div>
<div class="page"><p />
<p>vector center CFin as d(vf , CFin) and out-of-
domain vector center CFout as d(vf , CFout),
respectively. We use the difference δ of these two
distances to classify each sentence:
</p>
<p>δf = d(vf , CFin)− d(vf , CFout). (5)
</p>
<p>By using an English-to-French NMT system
NEF, we can obtain a target sentence embedding
ve, in-domain target vector center CEin and
out-of-domain target vector center CEout .
Corresponding distance difference δe is,
</p>
<p>δe = d(ve, CEin)− d(ve, CEout). (6)
</p>
<p>δf , δe and δfe = δf + δe can be used to select
sentences. That is, the sentence pairs (f, e) with
δf (or δe, δfe) less than a threshold are the new
selected in-domain corpus. This threshold is tuned
by using the development data.
</p>
<p>4 Experiments
</p>
<p>4.1 Data sets
</p>
<p>The proposed methods were evaluated on two data
sets as shown in Table 1.
</p>
<p>• IWSLT 2014 English (EN) to French (FR)
corpus3 was used as in-domain training data
and dev2010 and test2010/2011 (Cettolo
et al., 2014), were selected as development
(dev) and test data, respectively. Out-
of-domain corpora contained Common
Crawl, Europarl v7, News Commentary v10
and United Nation (UN) EN-FR parallel
corpora.4
</p>
<p>• NIST 2006 Chinese (ZH) to English corpus5
was used as the in-domain training corpus,
following the settings of (Wang et al.,
2014). Chinese-to-English UN data set
(LDC2013T06) and NTCIR-9 (Goto et al.,
2011) patent data set were used as out-of-
domain data. NIST MT 2002-2004 and NIST
MT 2005/2006 were used as the development
and test data, respectively. We are aware of
that there are additional NIST corpora in a
similar domain, but because this task was for
domain adaptation, we only selected a small
subset, which is mainly focused on news and
</p>
<p>3https://wit3.fbk.eu/mt.php?release=2014-01
4http://statmt.org/wmt15/translation-task.html
5http://www.itl.nist.gov/iad/mig/tests/mt/2006/
</p>
<p>blog texts. The statistics on data sets were
shown in Table 1.
</p>
<p>These adaptation corpora settings were nearly
the same as that used in (Wang et al., 2016). The
differences were:
</p>
<p>• For IWSLT, they chose FR-EN translation
task, which is popular in PBSMT. We chose
EN-FR, which is more popular in NMT;
</p>
<p>• For NIST, they chose 02-05 as dev set, and
we chose 02-04. Because we would report
results on two test sets (MT05 and MT06) in
comparison with only one (MT06).
</p>
<p>IWSLT EN-FR Sentences Tokens
TED training (in-domain) 178.1K 3.5M
WMT training (out-of-domain) 17.8M 450.0M
TED dev2010 0.9K 20.1K
TED test2010 1.6K 31.9K
TED test2011 0.8K 15.6K
NIST ZH-EN Sentences Tokens
NIST in-domain training 430.8K 12.6M
out-of-domain training 8.8M 249.4M
dev (MT02-04) 3.4K 106.4K
test (MT05) 1.0K 34.7K
test (MT06) 1.6K 46.7K
</p>
<p>Table 1: Statistics on data sets.
</p>
<p>4.2 NMT System
</p>
<p>We implemented the proposed method in
Groundhog6 (Bahdanau et al., 2015), which is
one of the state-of-the-art NMT frameworks.
The default settings of Groundhog were applied
for all NMT systems: the word embedding
dimension was 620 and the size of a hidden
layer was 1000, the batch size was 64, the source
and target side vocabulary sizes were 30K, the
maximum sequence length were 50, and the
beam size for decoding was 10. Default dropout
were applied. We used a mini-batch Stochastic
Gradient Descent (SGD) algorithm together with
ADADELTA optimizer (Zeiler, 2012). Training
was conducted on a single Tesla K80 GPU.
Each NMT model was trained for 500K batches,
taking 7-10 days. For sentence embedding and
selection, it only took several hours to process
all of sentences in the training data, because
decoding was not necessary.
</p>
<p>6https://github.com/lisa-groundhog/
GroundHog
</p>
<p>562</p>
<p />
</div>
<div class="page"><p />
<p>4.3 Baselines
Along with the standard NMT baseline system,
we also compared the proposed methods to the
recent state-of-the-art NMT adaptation method
of Luong and Manning (2015)7 as described in
Section 1. Two typical sentence selection methods
for PBSMT were also used as baselines: Axelrod
et al. (2011) used language model-based cross-
entropy difference as criterion; Chen et al. (2016)
used a CNN to classify the sentences as either
in-domain or out-of-domain. In addition, we
randomly sampled out-of-domain data to create
a corpus the same size as that used for the best
performing proposed system. We tried our best to
re-implement the baseline methods using the same
basic NMT setting as the proposed method.
</p>
<p>4.4 Results and Analyses
In Tables 2 and 3, the in, out and in + out
indicate that the in-domain, out-of-domain and
their mixture were used as the NMT training
corpora. δf , δe and δfe indicate that corresponding
criterion was used to select sentences, and these
selected sentences were added to in-domain
corpus to construct the new training corpora.
+fur indicates that the selected sentences were
used to train an initial NMT system, and then this
initial system was further trained by in-domain
data (Luong and Manning, 2015). The threshold
for the sentence selection method was selected
on development data. That is, we selected the
top ranked 10%, 20%,...,90% out-of-domain data
to be added into the in-domain data, and the
best performing models on development data were
used in the evaluation on test data.
</p>
<p>The vocabulary was built by using the selected
corpus and in-domain corpus.8 Translation
performance was measured by case-insensitive
BLEU (Papineni et al., 2002). Since the proposed
method is a sentence selection approach, we can
also show the effect on standard PBSMT (Koehn
et al., 2007).
</p>
<p>In the IWSLT task, the observations were as
follows:
</p>
<p>• Adding out-of-domain to in-domain data, or
directly using out-of-domain data, degraded
</p>
<p>7Freitag and Al-Onaizan (2016)’s method is quite similar
to Luong and Manning (2015)’s, so we did not compare to
them.
</p>
<p>8According to our empirical comparison, the performance
did not significantly change if we used in + out to build the
vocabulary for all of the systems.
</p>
<p>Methods Sent. SMT SMT NMT NMT
No. tst10 tst11 tst10 tst11
</p>
<p>in 178.1K 31.06 32.50 29.23 30.00
out 17.7M 30.04 29.29 27.30 28.48
in+out 17.9 M 30.00 30.26 28.89 28.55
Random 5.5M 31.22 33.85 30.53 32.37
Luong 17.9 M N/A N/A 32.21 35.03
Axelrod 9.0M 32.06 34.81 32.26 35.54
Chen 7.3M 31.42 33.78 30.32 33.81
δf 7.3M 31.46 33.13 32.13 34.81
δe 3.7M 32.08 35.94 32.84 36.56
δfe 5.5M 31.79 35.66 32.67 36.64
δf+fur 7.3M N/A N/A 34.04 37.18
δe+fur 3.7M N/A N/A 33.88 38.04
δfe+fur 5.5M N/A N/A 34.52 39.02
</p>
<p>Table 2: IWSLT EN-FR results. Luong and
Manning (2015)’s further (shorted as fur in
Tables 2 and 3) training method can only be
applied to NMT.
</p>
<p>Methods Sent. SMT SMT NMT NMT
No. MT05 MT06 MT05 MT06
</p>
<p>in 430.8K 29.66 30.73 27.28 26.82
out 8.8M 29.91 30.13 28.67 27.79
in+out 9.3M 30.23 30.11 28.91 28.22
Random 5.7M 29.90 30.18 28.02 27.49
Luong 9.3M N/A N/A 29.91 29.61
Axelrod 2.2M 30.52 30.96 28.41 28.75
Chen 4.8M 30.64 31.05 28.39 28.06
δf 4.8M 30.90 31.96 29.21 30.14
δe 2.2M 30.94 31.33 30.00 30.63
δfe 5.7M 30.72 31.43 30.13 31.07
δf+fur 4.8M N/A N/A 30.80 31.54
δe+fur 2.2M N/A N/A 30.49 31.13
δfe+fur 5.7M N/A N/A 31.35 31.80
</p>
<p>Table 3: NIST ZH-EN results.
</p>
<p>PBSMT and NMT performance.
</p>
<p>• Adding data selected by δf , δe and δfe
substantially improved NMT performance
(3.9 to 6.6 BLEU points), and gave
rise to a modest improvement in PBSMT
performance (0.4 to 3.1 BLEU points). This
method also outperformed the best existing
baselines by up to 1.1 BLEU points for NMT
and 0.8 BLEU for PBSMT.
</p>
<p>• The proposed method worked synergistically
with Luong’s further training method, and
the combination was able to add up to an
additional 2-3 BLEU points, indicating that
the proposed method and Luong’s method are
essentially orthogonal.
</p>
<p>• The performance by using both sides of
sentence embeddings δfe was slightly better
</p>
<p>563</p>
<p />
</div>
<div class="page"><p />
<p>than using monolingual sentence embedding
δf and δe.
</p>
<p>In the NIST task, the observations were similar
to the IWSLT task, except:
</p>
<p>• Adding out-of-domain slightly improved
PBSMT and NMT performance.
</p>
<p>• The proposed method improved both
PBSMT and NMT performance, but not as
substantially as in IWSLT.
</p>
<p>These observations suggest that the out-of-
domain data was closer to the in-domain than in
IWSLT.
</p>
<p>5 Discussions
</p>
<p>5.1 Selected Size Effect
We show experimental results on varying the size
of additional data selected from the out-of-domain
dataset, in Figure 1. It shows that the proposed
method δfe reached the highest performance on
dev set, when top 30% out-of-domain sentences
are selected as pseudo in-domain data. δfe
outperforms the other methods in most of the cases
on development data.
</p>
<p>24
</p>
<p>25
</p>
<p>26
</p>
<p>27
</p>
<p>28
</p>
<p>0 10 20 30 40 50 60 70 80 90 100
</p>
<p>B
L
</p>
<p>E
U
</p>
<p> (
d
</p>
<p>ev
)
</p>
<p>Top precentage of out-of-domain data
</p>
<p>Selected Size Effect
</p>
<p>Axelrod Chen δfe
</p>
<p>Figure 1: Selected size tuning on IWSLT.
</p>
<p>5.2 Training Time Effect
We also show the relationship between BLEU and
batches of training in Figure 2.
</p>
<p>Most of the methods (without further training)
converged after similar batches training.
Specifically, in researched the highest BLEU
performance on dev faster than other methods
(without further training), then decreased and
finally converged.
</p>
<p>The further training methods, which firstly
trained the models using out-of-domain data and
then in-domain data, converged very soon after
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>30
</p>
<p>0 1 10 30 50 100 200 300 400 500
</p>
<p>B
L
</p>
<p>E
U
</p>
<p> (
d
</p>
<p>ev
)
</p>
<p>Batches (K)
</p>
<p>Learning Curves
</p>
<p>in out in+out fur δfe δfe+fur
</p>
<p>Figure 2: Training time on IWSLT.
</p>
<p>in-domain data were introduced. In further
training, the out-of-domain trained system could
be considered as a pre-trained NMT system. Then
the in-domain data training help NMT system
overfit at in-domain data and gained around two
BLEU improvement.
</p>
<p>6 Conclusion and Future Work
</p>
<p>In this paper, we proposed a straightforward
sentence selection method for NMT domain
adaptation. Instead of the existing external
selection criteria, we applied the internal NMT
sentence embedding similarity as the criterion.
Empirical results on IWSLT and NIST tasks
showed that the proposed method can substantially
improve NMT performances and outperform
state-of-the-art existing NMT adaptation methods
on NMT (even PBSMT) performances.
</p>
<p>In addition, we found that the combination
of sentence selection and further training has an
additional effect, with a fast convergence. In
our further work, we will investigate the effect
of training data order and batch data selection on
NMT training.
</p>
<p>Acknowledgments
</p>
<p>Thanks a lot for the helpful discussions with Dr.
Lemao Liu, Kehai Chen and Dr. Atsushi Fujita.
We also appreciate the insightful comments from
three anonymous reviewers.
</p>
<p>References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
</p>
<p>2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language
Processing. Edinburgh, Scotland, U.K., pages 355–
362. http://www.aclweb.org/anthology/D11-1033.
</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015. Neural machine translation by jointly
</p>
<p>564</p>
<p />
</div>
<div class="page"><p />
<p>learning to align and translate. In International
Conference on Learning Representations. San
Diego. http://arxiv.org/abs/1409.0473.
</p>
<p>Mauro Cettolo, Jan Niehues, Sebastian Stüker,
Luisa Bentivogli, and Marcello Federico.
2014. Report on the 11th IWSLT evaluation
campaign. In Proceedings of the International
Workshop on Spoken Language Translation.
Lake Tahoe, CA, USA, pages 2–17.
http://workshop2014.iwslt.org/64.php.
</p>
<p>Boxing Chen, Roland Kuhn, George Foster, Colin
Cherry, and Fei Huang. 2016. Bilingual methods
for adaptive training data selection for machine
translation. In The Twelfth Conference of
The Association for Machine Translation in
the Americas. Austin, Texas, pages 93–106.
https://amtaweb.org/amta-2016-in-austin-tx/.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar
Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014.
Learning phrase representations using RNN
encoder–decoder for statistical machine translation.
In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language
Processing. Doha, Qatar, pages 1724–1734.
http://www.aclweb.org/anthology/D14-1179.
</p>
<p>Kevin Duh, Graham Neubig, Katsuhito Sudoh, and
Hajime Tsukada. 2013. Adaptation data selection
using neural language models: Experiments
in machine translation. In Proceedings of
the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2:
Short Papers). Sofia, Bulgaria, pages 678–683.
http://www.aclweb.org/anthology/P13-2119.
</p>
<p>Nadir Durrani, Hassan Sajjad, Shafiq Joty, Ahmed
Abdelali, and Stephan Vogel. 2015. Using
joint models for domain adaptation in statistical
machine translation. In Proceedings of MT
Summit XV . Miami, FL, USA, pages 117–130.
https://amtaweb.org/mt-summit-xv-proceedings/.
</p>
<p>Markus Freitag and Yaser Al-Onaizan. 2016.
Fast domain adaptation for neural machine
translation. arXiv preprint arXiv:1612.06897
http://arxiv.org/abs/1612.06897.
</p>
<p>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita,
and Benjamin K. Tsou. 2011. Overview of the
patent machine translation task at the NTCIR-
9 workshop. In Proceedings of NTCIR-9
Workshop Meeting. Tokyo, Japan, pages 559–578.
http://research.nii.ac.jp/ntcir/.
</p>
<p>Cuong Hoang and Khalil Sima’an. 2014a. Latent
domain phrase-based models for adaptation.
In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language
Processing. Doha, Qatar, pages 566–576.
http://www.aclweb.org/anthology/D14-1062.
</p>
<p>Cuong Hoang and Khalil Sima’an. 2014b. Latent
domain translation models in mix-of-domains
haystack. In Proceedings of the 25th International
Conference on Computational Linguistics:
Technical Papers. Dublin, Ireland, pages 1928–
1939. http://www.aclweb.org/anthology/C14-1182.
</p>
<p>Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large
target vocabulary for neural machine translation.
In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics
and the 7th International Joint Conference
on Natural Language Processing (Volume 1:
Long Papers). Beijing, China, pages 1–10.
http://www.aclweb.org/anthology/P15-1001.
</p>
<p>Shafiq Joty, Hassan Sajjad, Nadir Durrani, Kamla
Al-Mannai, Ahmed Abdelali, and Stephan Vogel.
2015. How to avoid unwanted pregnancies:
Domain adaptation using neural network
models. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language
Processing. Lisbon, Portugal, pages 1259–1270.
http://aclweb.org/anthology/D15-1147.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions. Prague, Czech Republic, pages
177–180. http://www.aclweb.org/anthology/P07-
2045.
</p>
<p>Xiaoqing Li, Jiajun Zhang, and Chengqing
Zong. 2016. One sentence one model for
neural machine translation. arXiv preprint
http://arxiv.org/abs/1609.06490.
</p>
<p>Minh-Thang Luong and Christopher D Manning.
2015. Stanford neural machine translation
systems for spoken language domains. In
Proceedings of the International Workshop on
Spoken Language Translation. Da Nang, Vietnam,
pages 76–79. https://nlp.stanford.edu/pubs/luong-
manning-iwslt15.pdf.
</p>
<p>Haitao Mi, Zhiguo Wang, and Abe Ittycheriah.
2016. Vocabulary manipulation for neural
machine translation. In Proceedings of the
54th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers). Berlin, Germany, pages 124–129.
http://anthology.aclweb.org/P16-2021.
</p>
<p>Robert C Moore and William Lewis. 2010. Intelligent
selection of language model training data. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers). Uppsala, Sweden, pages 220–224.
http://www.aclweb.org/anthology/P10-2041.
</p>
<p>565</p>
<p />
</div>
<div class="page"><p />
<p>Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for
automatic evaluation of machine translation.
In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics.
Philadelphia, Pennsylvania, pages 311–318.
http://www.aclweb.org/anthology/P02-1040.
</p>
<p>Rico Sennrich. 2012. Perplexity minimization
for translation model domain adaptation in
statistical machine translation. In Proceedings
of the 13th Conference of the European
Chapter of the Association for Computational
Linguistics. Avignon, France, pages 539–549.
http://www.aclweb.org/anthology/E12-1055.
</p>
<p>Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model framework
for statistical machine translation. In Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1:
Long Papers). Sofia, Bulgaria, pages 832–840.
http://www.aclweb.org/anthology/P13-1082.
</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua
Liu, and Hang Li. 2016. Modeling coverage
for neural machine translation. In Proceedings
of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1:
Long Papers). Berlin, Germany, pages 76–85.
http://www.aclweb.org/anthology/P16-1008.
</p>
<p>Rui Wang, Hai Zhao, Bao-Liang Lu, Masao
Utiyama, and Eiichiro. Sumita. 2015.
Bilingual continuous-space language model
growing for statistical machine translation.
IEEE/ACM Transactions on Audio, Speech,
and Language Processing 23(7):1209–1220.
https://doi.org/10.1109/TASLP.2015.2425220.
</p>
<p>Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama,
and Eiichiro Sumita. 2016. Connecting phrase
based statistical machine translation adaptation.
In Proceedings of the 26th International
Conference on Computational Linguistics:
Technical Papers. Osaka, Japan, pages 3135–
3145. http://aclweb.org/anthology/C16-1295.
</p>
<p>Xiaolin Wang, Masao Utiyama, Andrew Finch, and
Eiichiro Sumita. 2014. Empirical study of
unsupervised Chinese word segmentation methods
for SMT on large-scale corpora. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short
Papers). Baltimore, Maryland, pages 752–758.
http://www.aclweb.org/anthology/P14-2122.
</p>
<p>Matthew D Zeiler. 2012. ADADELTA: an
adaptive learning rate method. arXiv preprint
arXiv:1212.5701 http://arxiv.org/abs/1212.5701.
</p>
<p>Biao Zhang, Deyi Xiong, jinsong su, Hong Duan,
and Min Zhang. 2016. Variational neural
machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
</p>
<p>Language Processing. Austin, Texas, pages 521–
530. https://aclweb.org/anthology/D16-1050.
</p>
<p>566</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 567–573
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2090
</p>
<p>Data Augmentation for Low-Resource Neural Machine Translation
</p>
<p>Marzieh Fadaee Arianna Bisazza Christof Monz
Informatics Institute, University of Amsterdam
</p>
<p>Science Park 904, 1098 XH Amsterdam, The Netherlands
{m.fadaee,a.bisazza,c.monz}@uva.nl
</p>
<p>Abstract
</p>
<p>The quality of a Neural Machine Transla-
tion system depends substantially on the
availability of sizable parallel corpora. For
low-resource language pairs this is not the
case, resulting in poor translation qual-
ity. Inspired by work in computer vi-
sion, we propose a novel data augmenta-
tion approach that targets low-frequency
words by generating new sentence pairs
containing rare words in new, synthetically
created contexts. Experimental results on
simulated low-resource settings show that
our method improves translation quality
by up to 2.9 BLEU points over the baseline
and up to 3.2 BLEU over back-translation.
</p>
<p>1 Introduction
</p>
<p>In computer vision, data augmentation techniques
are widely used to increase robustness and im-
prove learning of objects with a limited number of
training examples. In image processing the train-
ing data is augmented by, for instance, horizon-
tally flipping, random cropping, tilting, and al-
tering the RGB channels of the original images
(Krizhevsky et al., 2012; Chatfield et al., 2014).
Since the content of the new image is still the
same, the label of the original image is preserved
(see top of Figure 1). While data augmentation
has become a standard technique to train deep net-
works for image processing, it is not a common
practice in training networks for NLP tasks such
as Machine Translation.
</p>
<p>Neural Machine Translation (NMT) (Bahdanau
et al., 2015; Sutskever et al., 2014; Cho et al.,
2014) is a sequence-to-sequence architecture
where an encoder builds up a representation of the
source sentence and a decoder, using the previous
</p>
<p>A boy is holding a bat.
</p>
<p>A boy is holding a backpack.
</p>
<p>A boy is holding a bat.
</p>
<p>A boy is holding a bat.
Ein Junge hält einen Schläger. Ein Junge hält einen Rucksack.
</p>
<p>computer vision
</p>
<p>augmentation
</p>
<p>translation
</p>
<p>augmentation
</p>
<p>Figure 1: Top: flip and crop, two label-preserving
data augmentation techniques in computer vision.
Bottom: Altering one sentence in a parallel corpus
requires changing its translation.
</p>
<p>LSTM hidden states and an attention mechanism,
generates the target translation.
</p>
<p>To train a model with reliable parameter estima-
tions, these networks require numerous instances
of sentence translation pairs with words occurring
in diverse contexts, which is typically not avail-
able in low-resource language pairs. As a result
NMT falls short of reaching state-of-the-art per-
formances for these language pairs (Zoph et al.,
2016). The solution is to either manually annotate
more data or perform unsupervised data augmen-
tation. Since manual annotation of data is time-
consuming, data augmentation for low-resource
language pairs is a more viable approach. Re-
cently Sennrich et al. (2016a) proposed a method
to back-translate sentences from monolingual data
and augment the bitext with the resulting pseudo
parallel corpora.
</p>
<p>In this paper, we propose a simple yet effective
approach, translation data augmentation (TDA),
that augments the training data by altering existing
sentences in the parallel corpus, similar in spirit to
the data augmentation approaches in computer vi-
sion (see Figure 1). In order for the augmentation
process in this scenario to be label-preserving, any
change to a sentence in one language must pre-
</p>
<p>567</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2090">https://doi.org/10.18653/v1/P17-2090</a></div>
</div>
<div class="page"><p />
<p>serve the meaning of the sentence, requiring sen-
tential paraphrasing systems which are not avail-
able for many language pairs. Instead, we propose
a weaker notion of label preservation that allows to
alter both source and target sentences at the same
time as long as they remain translations of each
other.
</p>
<p>While our approach allows us to augment
data in numerous ways, we focus on augment-
ing instances involving low-frequency words, be-
cause the parameter estimation of rare words is
challenging, and further exacerbated in a low-
resource setting. We simulate a low-resource set-
ting as done in the literature (Marton et al., 2009;
Duong et al., 2015) and obtain substantial im-
provements for translating EnglishÑGerman and
GermanÑEnglish.
</p>
<p>2 Translation Data Augmentation
</p>
<p>Given a source and target sentence pair (S,T), we
want to alter it in a way that preserves the semantic
equivalence between S and T while diversifying as
much as possible the training examples. A number
of ways to do this can be envisaged, as for example
paraphrasing (parts of) S or T. Paraphrasing, how-
ever, is by itself a difficult task and is not guaran-
teed to bring useful new information into the train-
ing data. We choose instead to focus on a subset of
the vocabulary that we know to be poorly modeled
by our baseline NMT system, namely words that
occur rarely in the parallel corpus. Thus, the goal
of our data augmentation technique is to provide
novel contexts for rare words. To achieve this we
search for contexts where a common word can be
replaced by a rare word and consequently replace
its corresponding word in the other language by
that rare word’s translation:
</p>
<p>original pair augmented pair
S : s1, ..., si, ..., sn S
</p>
<p>1 : s1, ..., s1i, ..., sn
T : t1, ..., tj , ..., tm T
</p>
<p>1 : t1, ..., t1j , ..., tm
</p>
<p>where tj is a translation of si and word-aligned to
si. Plausible substitutions are those that result in a
fluent and grammatical sentence but do not neces-
sarily maintain its semantic content. As an exam-
ple, the rare word motorbike can be substituted in
different contexts:
</p>
<p>Sentence [original / substituted] Plausible
My sister drives a [car / motorbike] yes
My uncle sold his [house / motorbike] yes
Alice waters the [plant / motorbike] no (semantics)
John bought two [shirts / motorbike] no (syntax)
</p>
<p>Implausible substitutions need to be ruled out dur-
ing data augmentation. To this end, rather than re-
lying on linguistic resources which are not avail-
able for many languages, we rely on LSTM lan-
guage models (LM) (Hochreiter and Schmidhu-
ber, 1997; Jozefowicz et al., 2015) trained on large
amounts of monolingual data in both forward and
backward directions.
</p>
<p>Our data augmentation method involves the fol-
lowing steps:
</p>
<p>Targeted words selection: Following common
practice, our NMT system limits its vocabulary V
to the v most common words observed in the train-
ing corpus. We select the words in V that have
fewer than R occurrences and use this as our tar-
geted rare word list VR.
</p>
<p>Rare word substitution: If the LM suggests a
rare substitution in a particular context, we replace
that word and add the new sentence to the training
data. Formally, given a sentence pair pS, T q and a
position i in S we compute the probability distri-
bution over V by the forward and backward LMs
and select rare word substitutions C as follows:
ÝÑC “ ts1i P VR : topKPForwardLMS ps1i | si´11 quÐÝC “ ts1i P VR : topKPBackwardLMS ps1i | si`1n qu
C “ ts1i | s1i P ÝÑC ^ s1i P ÐÝC u
</p>
<p>where topK returns theK words with highest con-
ditional probability according to the context. The
selected substitutions s1i, are used to replace the
original word and generate a new sentence.
</p>
<p>Translation selection: Using automatic word
alignments1 trained over the bitext, we replace the
translation of word si in T by the translation of its
substitution s1i. Following a common practice in
statistical MT, the optimal translation t1j is chosen
by multiplying direct and inverse lexical transla-
tion probabilities with the LM probability of the
translation in context:
</p>
<p>t1j “ argmax
tPtransps1iq
</p>
<p>P ps1i | tqP pt | s1iqPLMT pt | tj´11 q
</p>
<p>If no translation candidate is found because the
word is unaligned or because the LM probability
</p>
<p>1We use fast-align (Dyer et al., 2013) to extract word
alignments and a bilingual lexicon with lexical translation
probabilities from the low-resource bitext.
</p>
<p>568</p>
<p />
</div>
<div class="page"><p />
<p>is less than a certain threshold, the augmented sen-
tence is discarded. This reduces the risk of gener-
ating sentence pairs that are semantically or syn-
tactically incorrect.
</p>
<p>Sampling: We loop over the original parallel
corpus multiple times, sampling substitution po-
sitions, i, in each sentence and making sure that
each rare word gets augmented at mostN times so
that a large number of rare words can be affected.
We stop when no new sentences are generated in
one pass of the training data.
</p>
<p>Table 1 provides some examples resulting from
our augmentation procedure. While using a large
LM to substitute words with rare words mostly re-
sults in grammatical sentences, this does not mean
that the meaning of the original sentence is pre-
served. Note that meaning preservation is not an
objective of our approach.
</p>
<p>Two translation data augmentation (TDA) se-
tups are considered: only one word per sentence
can be replaced (TDAr“1), or multiple words per
sentence can be replaced, with the condition that
any two replaced words are at least five positions
apart (TDArě1). The latter incurs a higher risk
of introducing noisy sentences but has the poten-
tial to positively affect more rare words within the
same amount of augmented data. We evaluate both
setups in the following section.
</p>
<p>En: I had been told that you would [not / voluntarily] be
speaking today.
</p>
<p>De: mir wurde signalisiert, sie würden heute [nicht / frei-
willig] sprechen.
</p>
<p>En: the present situation is [indefensible / confusing] and
completely unacceptable to the commission.
</p>
<p>De: die situation sei [unhaltbar / verwirrend] und für die
kommission gänzlich unannehmbar.
</p>
<p>En: ... agree wholeheartedly with the institution of an ad
hoc delegation of parliament on the turkish [prison /
missile] system.
</p>
<p>De: ... ad-hoc delegation des parlaments für das regime
in den türkischen [gefängnissen / flugwaffen] voll und
ganz zustimmen.
</p>
<p>Table 1: Examples of augmented data with high-
lighted [original / substituted] and [original /
translated] words.
</p>
<p>3 Evaluation
</p>
<p>In this section we evaluate the utility of our ap-
proach in a simulated low-resource NMT scenario.
</p>
<p>3.1 Data and experimental setup
</p>
<p>To simulate a low-resource setting we randomly
sample 10% of the EnglishØGerman WMT15
training data and report results on newstest 2014,
2015, and 2016 (Bojar et al., 2016). For reference
we also provide the result of our baseline system
on the full data.
</p>
<p>As NMT system we use a 4-layer attention-
based encoder-decoder model as described in (Lu-
ong et al., 2015) trained with hidden dimension
1000, batch size 80 for 20 epochs. In all experi-
ments the NMT vocabulary is limited to the most
common 30K words in both languages. Note that
data augmentation does not introduce new words
to the vocabulary. In all experiments we prepro-
cess source and target language data with Byte-
pair encoding (BPE) (Sennrich et al., 2016b) using
30K merge operations. In the augmentation exper-
iments BPE is performed after data augmentation.
</p>
<p>For the LMs needed for data augmentation, we
train 2-layer LSTM networks in forward and back-
ward directions on the monolingual data provided
for the same task (3.5B and 0.9B tokens in En-
glish and German respectively) with embedding
size 64 and hidden size 128. We set the rare word
thresholdR to 100, topK words to 1000 and max-
imum number N of augmentations per rare word
to 500. In all experiments we use the English LM
for the rare word substitutions, and the German
LM to choose the optimal word translation in con-
text. Since our approach is not label preserving we
only perform augmentation during training and do
not alter source sentences during testing.
</p>
<p>We also compare our approach to Sennrich et al.
(2016a) by back-translating monolingual data and
adding it to the parallel training data. Specifically,
we back-translate sentences from the target side of
WMT’15 that are not included in our low-resource
baseline with two settings: keeping a one-to-one
ratio of back-translated versus original data (1 : 1)
following the authors’ suggestion, or using three
times more back-translated data (3 : 1).
</p>
<p>We measure translation quality by single-
reference case-insensitive BLEU (Papineni et al.,
2002) computed with the multi-bleu.perl
script from Moses.
</p>
<p>3.2 Results
</p>
<p>All translation results are displayed in Table 2.
As expected, the low-resource baseline performs
much worse than the full data system, re-iterating
</p>
<p>569</p>
<p />
</div>
<div class="page"><p />
<p>De-En En-De
Model Data test2014 test2015 test2016 test2014 test2015 test2016
Full data (ceiling) 3.9M 21.1 22.0 26.9 17.0 18.5 21.7
Baseline 371K 10.6 11.3 13.1 8.2 9.2 11.0
Back-translation1:1 731K 11.4 (+0.8)Ĳ 12.2 (+0.9)Ĳ 14.6 (+1.5)Ĳ 9.0 (+0.8)Ĳ 10.4 (+1.2)Ĳ 12.0 (+1.0)Ĳ
</p>
<p>Back-translation3:1 1.5M 11.2 (+0.6) 11.2 (–0.1) 13.3 (+0.2) 7.8 (–0.4) 9.4 (+0.2) 10.7 (–0.3)
TDAr“1 4.5M 11.9 (+1.3)Ĳ,- 13.4 (+2.1)Ĳ,Ĳ 15.2 (+2.1)Ĳ,Ĳ 10.4 (+2.2)Ĳ,Ĳ 11.2 (+2.0)Ĳ,Ĳ 13.5 (+2.5)Ĳ,Ĳ
TDArě1 6M 12.6 (+2.0)Ĳ,Ĳ 13.7 (+2.4)Ĳ,Ĳ 15.4 (+2.3)Ĳ,Ĳ 10.7 (+2.5)Ĳ,Ĳ 11.5 (+2.3)Ĳ,Ĳ 13.9 (+2.9)Ĳ,Ĳ
Oversampling 6M 11.9 (+1.3)Ĳ,- 12.9 (+1.6)Ĳ,Ÿ 15.0 (+1.9)Ĳ,- 9.7 (+1.5)Ĳ,Ÿ 10.7 (+1.5)Ĳ,- 12.6 (+1.6)Ĳ,-
</p>
<p>Table 2: Translation performance (BLEU) on German-English and English-German WMT test sets (new-
stest2014, 2015, and 2016) in a simulated low-resource setting. Back-translation refers to the work
of Sennrich et al. (2016a). Statistically significant improvements are marked Ĳ at the p ă .01 and Ÿ at the
p ă .05 level, with the first superscript referring to baseline and the second to back-translation1:1.
</p>
<p>the importance of sizable training data for NMT.
Next we observe that both back-translation and
our proposed TDA method significantly improve
translation quality. However TDA obtains the best
results overall and significantly outperforms back-
translation in all test sets. This is an important
finding considering that our method involves only
minor modifications to the original training sen-
tences and does not involve any costly translation
process. Improvements are consistent across both
translation directions, regardless of whether rare
word substitutions are first applied to the source
or to the target side.
</p>
<p>We also observe that altering multiple words in
a sentence performs slightly better than altering
only one. This indicates that addressing more rare
words is preferable even though the augmented
sentences are likely to be noisier.
</p>
<p>To verify that the gains are actually due to the
rare word substitutions and not just to the repeti-
tion of part of the training data, we perform a fi-
nal experiment where each sentence pair selected
for augmentation is added to the training data un-
changed (Oversampling in Table 2). Surprisingly,
we find that this simple form of sampled data
replication outperforms both baseline and back-
translation systems,2 while TDArě1 remains the
best performing system overall.
</p>
<p>We also observe that the system trained on aug-
mented data tends to generate longer translations.
Averaging on all test sets, the length of translations
generated by the baseline is 0.88 of the average
reference length, while for TDAr“1 and TDArě1
it is 0.95 and 0.94, respectively. We attribute this
effect to the ability of the TDA-trained system to
generate translations for rare words that were left
</p>
<p>2Note that this effect cannot be achieved by simply con-
tinuing the baseline training for up to 50 epochs.
</p>
<p>untranslated by the baseline system.
</p>
<p>4 Analysis of the Results
</p>
<p>A desired effect of our method is to increase the
number of correct rare words generated by the
NMT system at test time.
</p>
<p>To examine the impact of augmenting the train-
ing data by creating contexts for rare words on
the target side, Table 3 provides an example for
GermanÑEnglish translation. We see that the
baseline model is not able to generate the rare
word centimetres as a correct translation of the
German word zentimeter . However, this word is
not rare in the training data of the TDArě1 model
after augmentation and is generated during trans-
lation. Table 3 also provides several instances of
augmented training sentences targeting the word
centimetres. Note that even though some aug-
mented sentences are nonsensical (e.g. the speed
limit is five centimetres per hour), the NMT sys-
tem still benefits from the new context for the rare
word and is able to generate it during testing.
</p>
<p>Figure 2 demonstrates that this is indeed the
case for many words: the number of rare words
occurring in the reference translation (VR X Vref )
is three times larger in the TDA system output
than in the baseline output. One can also see that
this increase is a direct effect of TDA as most
of the rare words are not ‘rare’ anymore in the
augmented data, i.e., they were augmented suffi-
ciently many times to occur more than 100 times
(see hatched pattern in Figure 2). Note that during
the experiments we did not use any information
from the evaluation sets.
</p>
<p>To gauge the impact of augmenting the con-
texts for rare words on the source side, we ex-
amine normalized attention scores of these words
before and after augmentation. When translating
</p>
<p>570</p>
<p />
</div>
<div class="page"><p />
<p>Source der tunnel hat einen querschnitt von 1,20 meter höhe und 90 zentimeter breite .
Baseline translation the wine consists of about 1,20 m and 90 of the canal .
TDArě1 translation the tunnel has a UNK measuring meters 1.20 metres high and 90 centimetres wide .
Reference the tunnel has a cross - section measuring 1.20 metres high and 90 centimetres across .
</p>
<p>Examples of ‚ the average speed of cars and buses is therefore around 20 [kilometres / centimetres] per hour .
augmented data ‚ grab crane in special terminals for handling capacities of up to 1,800 [tonnes / centimetres] per hour .
for the word ‚ all suites and rooms are very spacious and measure between 50 and 70 [m / centimetres]
centimetres ‚ all we have to do is lower the speed limit everywhere to five [kilometers / centimetres] per hour .
</p>
<p>Table 3: An example from newstest2014 illustrating the effect of augmenting rare words on generation
during test time. The translation of the baseline does not include the rare word centimetres, however, the
translation of our TDA model generates the rare word and produces a more fluent sentence. Instances of
the augmentation of the word centimetres in training data are also provided.
</p>
<p>1,000 2,000 3,000
</p>
<p>baseline
</p>
<p>TDA
</p>
<p>baseline
</p>
<p>TDA
</p>
<p>baseline
</p>
<p>TDA
</p>
<p>Words in VR X Vref generated during translation
Words in VR X Vref not generated during translation
</p>
<p>14
te
</p>
<p>st
15
</p>
<p>te
st
</p>
<p>16
te
</p>
<p>st
</p>
<p>Words in VR X Vref affected by augmentation
</p>
<p>Figure 2: Effect of TDA on the number of unique
rare words generated during DeÑEn translation.
VR is the set of rare words targeted by TDArě1
and Vref the reference translation vocabulary.
</p>
<p>EnglishÑGerman with our TDA model, the at-
tention scores for rare words on the source side
are on average 8.8% higher than when translating
with the baseline model. This suggests that hav-
ing more accurate representations of rare words
increases the model’s confidence to attend to these
words when encountered during test time.
</p>
<p>En: registered users will receive the UNK newsletter free
[of / yearly] charge.
</p>
<p>De: registrierte user erhalten zudem regelmäßig [den /
jährlich] markenticker newsletter.
</p>
<p>En: the personal contact is [essential / entrusted] to us
De: persönliche kontakt ist uns sehr [wichtig / betraut]
</p>
<p>Table 4: Examples of incorrectly augmented data
with highlighted [original / substituted] and [orig-
inal / translated] words.
</p>
<p>Finally Table 4 provides examples of cases
where augmentation results in incorrect sentences.
In the first example, the sentence is ungrammati-
</p>
<p>cal after substitution (of / yearly), which can be the
result of choosing substitutions with low probabil-
ities from the English LM topK suggestions.
</p>
<p>Errors can also occur during translation selec-
tion, as in the second example where betraut is an
acceptable translation of entrusted but would re-
quire a rephrasing of the German sentence to be
grammatically correct. Problems of this kind can
be attributed to the German LM, but also to the
lack of a more suitable translation in the lexicon
extracted from the bitext. Interestingly, this noise
seems to affect NMT only to a limited extent.
</p>
<p>5 Conclusion
</p>
<p>We have proposed a simple but effective ap-
proach to augment the training data of Neural
Machine Translation for low-resource language
pairs. By leveraging language models trained
on large amounts of monolingual data, we gen-
erate new sentence pairs containing rare words
in new, synthetically created contexts. We show
that this approach leads to generating more rare
words during translation and, consequently, to
higher translation quality. In particular we re-
port substantial improvements in simulated low-
resource EnglishÑGerman and GermanÑEnglish
settings, outperforming another recently proposed
data augmentation technique.
</p>
<p>Acknowledgments
</p>
<p>This research was funded in part by the
Netherlands Organization for Scientific Research
(NWO) under project numbers 639.022.213 and
639.021.646, and a Google Faculty Research
Award. We also thank NVIDIA for their hardware
support, Ke Tran for providing the neural machine
translation baseline system, and the anonymous
reviewers for their helpful comments.
</p>
<p>571</p>
<p />
</div>
<div class="page"><p />
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR).
</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Feder-
mann, Yvette Graham, Barry Haddow, Matthias
Huck, Antonio Jimeno Yepes, Philipp Koehn,
Varvara Logacheva, Christof Monz, Matteo Negri,
Aurelie Neveol, Mariana Neves, Martin Popel,
Matt Post, Raphael Rubino, Carolina Scarton,
Lucia Specia, Marco Turchi, Karin Verspoor,
and Marcos Zampieri. 2016. Findings of the
2016 conference on machine translation. In
Proceedings of the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 131–198.
http://www.aclweb.org/anthology/W/W16/W16-
2301.
</p>
<p>Ken Chatfield, Karen Simonyan, Andrea Vedaldi,
and Andrew Zisserman. 2014. Return of
the devil in the details: Delving deep into
convolutional nets. In Proceedings of the
British Machine Vision Conference. BMVA Press.
https://doi.org/http://dx.doi.org/10.5244/C.28.6.
</p>
<p>Kyunghyun Cho, B van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the proper-
ties of neural machine translation: Encoder-decoder
approaches. In Eighth Workshop on Syntax, Seman-
tics and Structure in Statistical Translation (SSST-
8), 2014.
</p>
<p>Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. A neural network model for low-
resource universal dependency parsing. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics, Lisbon, Portugal, pages
339–348. http://aclweb.org/anthology/D15-1040.
</p>
<p>Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 644–648.
http://www.aclweb.org/anthology/N13-1073.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.
</p>
<p>Rafal Jozefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An empirical exploration of re-
current network architectures. In Francis Bach
and David Blei, editors, Proceedings of the 32nd
International Conference on Machine Learning.
PMLR, Lille, France, volume 37 of Proceedings
of Machine Learning Research, pages 2342–2350.
http://proceedings.mlr.press/v37/jozefowicz15.pdf.
</p>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E
Hinton. 2012. Imagenet classification with deep
convolutional neural networks. In F. Pereira,
C. J. C. Burges, L. Bottou, and K. Q. Weinberger,
editors, Advances in Neural Information Process-
ing Systems 25, Curran Associates, Inc., pages
1097–1105. http://papers.nips.cc/paper/4824-
imagenet-classification-with-deep-convolutional-
neural-networks.pdf.
</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1412–
1421. http://aclweb.org/anthology/D15-1166.
</p>
<p>Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine
translation using monolingually-derived para-
phrases. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing. Association for Computa-
tional Linguistics, Singapore, pages 381–390.
http://www.aclweb.org/anthology/D/D09/D09-
1040.
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-Jing Zhu. 2002. Bleu: a method
for automatic evaluation of machine transla-
tion. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA, pages 311–318.
https://doi.org/10.3115/1073083.1073135.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation
models with monolingual data. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 86–96.
http://www.aclweb.org/anthology/P16-1009.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016b. Neural machine translation of
rare words with subword units. In Proceed-
ings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1715–1725.
http://www.aclweb.org/anthology/P16-1162.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Sys-
tems 27, Curran Associates, Inc., pages 3104–
3112. http://papers.nips.cc/paper/5346-sequence-
to-sequence-learning-with-neural-networks.pdf.
</p>
<p>572</p>
<p />
</div>
<div class="page"><p />
<p>Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1568–1575.
https://aclweb.org/anthology/D16-1163.
</p>
<p>573</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 574–579
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2091
</p>
<p>Speeding Up Neural Machine Translation Decoding by Shrinking
Run-time Vocabulary
</p>
<p>Xing Shi and Kevin Knight
Information Sciences Institute &amp; Computer Science Department
</p>
<p>University of Southern California
{xingshi,knight}@isi.edu
</p>
<p>Abstract
</p>
<p>We speed up Neural Machine Translation
(NMT) decoding by shrinking run-time
target vocabulary. We experiment with
two shrinking approaches: Locality Sensi-
tive Hashing (LSH) and word alignments.
Using the latter method, we get a 2x over-
all speed-up over a highly-optimized GPU
implementation, without hurting BLEU.
On certain low-resource language pairs,
the same methods improve BLEU by 0.5
points. We also report a negative re-
sult for LSH on GPUs, due to relatively
large overhead, though it was successful
on CPUs. Compared with Locality Sensi-
tive Hashing (LSH), decoding with word
alignments is GPU-friendly, orthogonal to
existing speedup methods and more robust
across language pairs.
</p>
<p>1 Introduction
</p>
<p>Neural Machine Translation (NMT) has been
demonstrated as an effective model and been put
into large-scale production (Wu et al., 2016; He,
2015). For online translation services, decoding
speed is a crucial factor to achieve a better user
experience. Several recently proposed training
methods (Shen et al., 2015; Wiseman and Rush,
2016) aim to solve the exposure bias problem, but
require decoding the whole training set multiple
times, which is extremely time-consuming for mil-
lions of sentences.
</p>
<p>Slow decoding speed is partly due to the large
target vocabulary size V, which is usually in the
tens of thousands. The first two columns of Ta-
ble 1 show the breakdown of the runtimes re-
quired by sub-modules to decode 1812 Japanese
sentences to English using a sequence-to-sequence
model with local attention (Luong et al., 2015).
</p>
<p>Sub-module Full vocab WA50 Speedup
Total 1002.78 s 481.52 s 2.08
– Beam
</p>
<p>expansion 174.28 s 76.52 s 2.28
</p>
<p>– Source-side 83.67 s 83.44 s 1
– Target-side 743.25 s 354.52 s 2.1
– – Softmax 402.77 s 20.68 s 19.48
– – Attention 123.05 s 123.12 s 1
– – 2nd layer 64.72 s 64.76 s 1
– – 1st layer 88.02 s 87.96 s 1
Shrink vocab - 0.39 s -
BLEU 25.16 25.13 -
</p>
<p>Table 1: Time breakdown and BLEU score of
full vocabulary decoding and our “WA50” decod-
ing, both with beam size 12. WA50 means de-
coding informed by word alignments, where each
source word can select at most 50 relevant target
words. The model is a 2-layer, 1000-hidden di-
mension, 50,000-target vocabulary LSTM seq2seq
model with local attention trained on the AS-
PEC Japanese-to-English corpus (Nakazawa et al.,
2016). The time is measured on a single Nvidia
Tesla K20 GPU.
</p>
<p>Softmax is the most computationally intensive
part, where each hidden vector ht ∈ Rd needs to
dot-product with V target embeddings ei ∈ Rd.
It occupies 40% of the total decoding time. An-
other sub-module whose computation time is pro-
portional to V is Beam Expansion, where we need
to find the top B words among all V vocabulary ac-
cording to their probability. It takes around 17% of
the decoding time.
</p>
<p>Several approaches have proposed to improve
decoding speed:
</p>
<p>1. Using special hardware, such as GPU and
Tensor Processing Unit (TPU), and low-
precision calculation (Wu et al., 2016).
</p>
<p>2. Compressing deep neural models through
knowledge distillation and weight pruning
(See et al., 2016; Kim and Rush, 2016).
</p>
<p>574</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2091">https://doi.org/10.18653/v1/P17-2091</a></div>
</div>
<div class="page"><p />
<p>3. Several variants of Softmax have been pro-
posed to solve its poor scaling properties
on large vocabularies. Morin and Bengio
(2005) propose hierarchical softmax, where
at each step log2 V binary classifications are
performed instead of a single classification
on a large number of classes. Gutmann and
Hyvärinen (2010) propose noise-contrastive
estimation which discriminate between pos-
itive labels and k (k &lt;&lt; V ) negative la-
bels sampled from a distribution, and is ap-
plied successfully on natural language pro-
cessing tasks (Mnih and Teh, 2012; Vaswani
et al., 2013; Williams et al., 2015; Zoph et al.,
2016). Although these two approaches pro-
vide good speedups for training, they still
suffer at test time. Chen et al. (2016) in-
troduces differentiated softmax, where fre-
quent words have more parameters in the em-
bedding and rare words have less, offering
speedups on both training and testing.
</p>
<p>In this work, we aim to speed up decoding by
shrinking the run-time target vocabulary size, and
this approach is orthogonal to the methods above.
It is important to note that approaches 1 and 2
will maintain or even increase the ratio of target
word embedding parameters to the total parame-
ters, thus the Beam Expansion and Softmax will
occupy the same or greater portion of the decod-
ing time. A small run-time vocabulary will dra-
matically reduce the time spent on these two por-
tions and gain a further speedup even after apply-
ing other speedup methods.
</p>
<p>To shrink the run-time target vocabulary, our
first method uses Locality Sensitive Hashing. Vi-
jayanarasimhan et al. (2015) successfully applies
it on CPUs and gains speedup on single step
prediction tasks such as image classification and
video identification. Our second method is to use
word alignments to select a very small number of
candidate target words given the source sentence.
Recent works (Jean et al., 2015; Mi et al., 2016;
L’Hostis et al., 2016) apply a similar strategy and
report speedups for decoding on CPUs on rich-
source language pairs.
</p>
<p>Our major contributions are:
</p>
<p>1. To our best of our knowledge, this work
is the first attempt to apply LSH technique
on sequence generation tasks on GPU other
than single-step classification on CPU. We
</p>
<p>find current LSH algorithms have a poor per-
formance/speed trade-off on GPU, due to
the large overhead introduced by many hash
table lookups and list-merging involved in
LSH.
</p>
<p>2. For our word alignment method, we find that
only the candidate list derived from lexical
translation table of IBM model 4 is adequate
to achieve good BLEU/speedup trade-off for
decoding on GPU. There is no need to com-
bine the top frequent words or words from
phrase table, as proposed in Mi et al. (2016).
</p>
<p>3. We conduct our experiments on GPU and
provide a detailed analysis of BLEU/speedup
trade-off on both resource-rich/poor language
pairs and both attention/non-attention NMT
models. We achieve more than 2x speedup
on 4 language pairs with only a tiny BLEU
drop, demonstrating the robustness and effi-
ciency of our methods.
</p>
<p>2 Methods
</p>
<p>At each step during decoding, the softmax function
is calculated as:
</p>
<p>P (y = j|hi) =
eh
</p>
<p>T
i wj+bj
</p>
<p>∑V
k=1 e
</p>
<p>hTi wk+bk
(1)
</p>
<p>where P (y = j|hi) is the probability of word
j = 1...V given the hidden vector hi ∈ Rd, i =
1...B. B represents the beam size. wj ∈ Rd is
output word embedding and bj ∈ R is the corre-
sponding bias. The complexity is O(dBV ). To
speed up softmax, we use word frequency, local-
ity sensitive hashing, and word alignments respec-
tively to select C (C &lt;&lt; V ) potential words and
evaluate their probability only, reducing the com-
plexity to O(dBC + overhead).
</p>
<p>2.1 Word Frequency
A simple baseline to reduce target vocabulary is to
select the top C words based on their frequency in
the training corpus. There is no run-time overhead
and the overall complexity is O(dBC).
</p>
<p>2.2 Locality Sensitive Hashing
The word j = argmaxk P (y = k|hi) will have
the largest value of hTi wj + bj . Thus the argmax
problem can be converted to finding the near-
est neighbor of vector [hi; 1] among the vectors
[wj ; bj ] under the distance measure of dot-product.
</p>
<p>575</p>
<p />
</div>
<div class="page"><p />
<p>Locality Sensitive Hashing (LSH) is a powerful
technique for the nearest neighbor problem. We
employ the winner-take-all (WTA) hashing (Yag-
nik et al., 2011) defined as:
</p>
<p>WTA(x ∈ Rd) = [I1; ...; Ip; ...; IP ] (2)
Ip = argmax
</p>
<p>K
k=1 Permutep(x)[k] (3)
</p>
<p>WTAband(x) = [B1; ...;Bw; ...;BW ] (4)
</p>
<p>Bw = [I(w−1)∗u+1; ...; I(w−1)∗u+i; ...; Iw∗u] (5)
</p>
<p>u = P/W (6)
</p>
<p>where P distinct permutations are applied and the
index of the maximum value of the first K ele-
ments of each permutations is recorded. To per-
form approximate nearest neighbor searching, we
follow the scheme used in (Dean et al., 2013; Vi-
jayanarasimhan et al., 2015):
</p>
<p>1. Split the hash code WTA(x) into W bands
(as shown in equation 4), with each band
P
W log2(K) bits long.
</p>
<p>2. Create W hash tables [T1, ..., Tw, ..., TW ],
and hash every word index j into every table
Tw using WTAband(wj)[w] as the key.
</p>
<p>3. Given the hidden vector hi, extract a list of
word indexes from each table Tw using the
key WTAband(hi)[w]. Then we merge the
W lists and count the number of the occur-
rences of each word index. Select the top C
word indexes with the largest counts, and cal-
culate their probability using equation 1.
</p>
<p>The 4 hyper-parameters that define a WTA-LSH
are {K,P,W,C}. The run-time overhead com-
prises hashing the hidden vector, W times hash
table lookups and W lists merging. The overall
complexity isO(B(dC+K∗P+W+W∗Navg))),
where Navg is the average number of the word in-
dexes stored in a hash bin of Tw. Although the
complexity is much smaller than O(dBV ), the
runtime in practice is not guaranteed to be shorter,
especially on GPUs, as hash table lookups in-
troduce too many small kernel launches and list
merging is hard to parallelize.
</p>
<p>2.3 Word Alignment
</p>
<p>Intuitively, LSH shrinks the search space utilizing
the spatial relationship between the query vector
and database vectors in high dimension space. It
</p>
<p>is a task-independent technique. However, when
focusing on our specific task (MT), we can employ
translation-related heuristics to prune the run-time
vocabulary precisely and efficiently.
</p>
<p>One simple heuristic relies on the fact that each
source word can only be translated to a small set of
target words. The word alignment model, a foun-
dation of phrase-base machine translation, also
follows the same spirit in its generative story: each
source word is translated to zero, one, or more tar-
get words and then reordered to form target sen-
tences. Thus, we apply the following algorithm to
reduce the run-time vocabulary size:
</p>
<p>1. Apply IBM Model 4 and the grow-diag-final
heuristic on the training data to get word
alignments. Calculate the lexical translation
table P(e|f) based on word alignments.
</p>
<p>2. For each word f in the source vocabulary of
the neural machine translation model, store
the top M target words according to P(e|f)
in a hash table Tf2e = {f : [e1, ...eM ]}
</p>
<p>3. Given a source sentence s = [f1, ..., fN ], ex-
tract the candidate target word list from Tf2e
for each source word fi. Merge the N lists to
form the reduced target vocabulary Vnew;
</p>
<p>4. Construct the new embedding matrix and bias
vector according to Vnew, then perform the
normal beam search on target side.
</p>
<p>The only hyper-parameter is {M}, the number
of candidate target words for each source word.
Given a source sentence of length Ls, the run-time
overhead includesLs times hash table lookups and
Ls lists merging. The complexity for each decod-
ing step isO(dB|Vnew|+(Ls+LsM)/Lt), where
Lt is the maximum number of decoding steps. Un-
like LSH, these table lookups and list mergings are
performed once per sentence, and do not depend
on the any hidden vectors. Thus, we can overlap
the computation with source side forward propa-
gation.
</p>
<p>3 Experiments
</p>
<p>To examine the robustness of these decoding
methods, we vary experiment settings in dif-
ferent ways: 1) We train both attention (Lu-
ong et al., 2015) and non-attention (Sutskever
et al., 2014) models; 2) We train models on
</p>
<p>576</p>
<p />
</div>
<div class="page"><p />
<p>J2E E2J F2E U2E
TC BLEU X TC BLEU X TC BLEU X TC BLEU X
</p>
<p>Full 0.87 25.16 1 0.95 33.87 1 0.84 28.12 1 0.9 11.67 1
TF1K 0.14 13.42 2.11 0.15 18.91 2.42 0.1 12.1 2.32 0.29 8.78 1.65
TF5K 0.49 21.31 1.93 0.56 29.77 2.23 0.38 21.98 2.04 0.67 11.54 1.51
TF10K 0.67 23.62 1.76 0.75 32.28 2.04 0.56 24.88 1.78 0.81 11.67 1.33
TF20K 0.78 24.61 1.48 0.87 33.41 1.74 0.72 26.95 1.42 0.89 11.66 1.09
LSH1K - 19.45 0.026 - 22.23 0.027 - 3.43 0.036 - 9.41 0.025
LSH5K - 23.43 0.023 - 30.63 0.025 - 12.81 0.031 - 11.41 0.022
LSH10K - 24.82 0.022 - 32.63 0.024 - 18.45 0.028 - 11.63 0.020
LSH20K - 25.20 0.020 - 33.78 0.022 - 24.31 0.025 - 11.73 0.018
WA10 0.75 24.74 2.12 0.77 33.24 2.46 0.72 27.9 2.37 0.66 12.17 1.7
WA50 0.82 25.13 2.08 0.85 33.79 2.43 0.77 27.94 2.34 0.71 12.01 1.67
WA250 0.84 25.13 1.89 0.88 34.05 2.27 0.8 27.95 2.1 0.73 11.94 1.62
WA1000 0.85 25.17 1.57 0.9 33.97 1.93 0.82 28.08 1.67 0.75 11.89 1.58
</p>
<p>Table 2: Word type coverage (TC), BLEU score, and speedups (X) for full-vocabulary decoding (Full),
top frequency vocabulary decoding (TF*), LSH decoding (LSH*), and decoding with word align-
ments(WA*). TF10K represents decoding with top 10,000 frequent target vocabulary (C = 10, 000).
WA10 means decoding with word alignments, where each source word can select at most 10 candidate
target words (M = 10). For LSH decoding, we choose (32, 5000, 1000) for (K,P ,W ), and vary C.
</p>
<p>both resource-rich language pairs, French to En-
glish (F2E) and Japanese to English (J2E), and
a resource-poor language pair, Uzbek to English
(U2E); 3) We translate both to English (F2E, J2E,
and U2E) and from English (E2J). We use 2-
layer LSTM seq2seq models with different at-
tention settings, hidden dimension sizes, dropout
rates, and initial learning rates, as shown in Ta-
ble 3. We use the ASPEC Japanese-English Cor-
pus (Nakazawa et al., 2016), French-English Cor-
pus from WMT2014 (Bojar et al., 2014), and
Uzbek-English Corpus (Linguistic Data Consor-
tium, 2016).
</p>
<p>Table 2 shows the decoding results of the
three methods. Decoding with word alignments
achieves the best performance/speedup trade-off
across all four translation directions. It can halve
the overall decoding time with less than 0.17
BLEU drop. Table 1 compares the detailed time
breakdown of full-vocabulary decoding and WA50
decoding. WA50 can gain a speedup of 19.48x
and 2.28x on softmax and beam expansion respec-
tively, leading to an overall 2.08x speedup with
only 0.03 BLEU drop. In contrast, decoding with
top frequent words will hurt the BLEU rapidly as
the speedup goes higher. We calculate the word
type coverage (TC) for the test reference data as
</p>
<p>J2E E2J F2E U2E
Source Vocab 80K 88K 200K 50K
Target Vocab 50K 66K 40K 25K
</p>
<p>#Tokens 70.4M 70.4M 652M 3.3M
#Sent pairs 1.4M 1.4M 12M 88.7K
Attention Yes Yes No Yes
</p>
<p>Dimension 1000 1000 1000 500
Dropout 0.2 0.2 0.2 0.5
</p>
<p>Learning rate 0.5 1 0.35 0.5
</p>
<p>Table 3: Training configurations on different lan-
guage pairs.
</p>
<p>follows:
</p>
<p>TC =
|{run-time vocab} ∩ {word types in test}|
</p>
<p>|{word types in test}|
The top 1000 words only cover 14% word types of
J2E test data, whereas WA10 covers 75%, whose
run-time vocabulary is no more than 200 for a 20
words source sentence.
</p>
<p>The speedup of English-to-Uzbek translation is
relatively low (around 1.7x). This is because the
original full vocabulary size is small (25k), leaving
less room for shrinkage.
</p>
<p>LSH achieves better BLEU than decoding with
top frequent words of the same run-time vocabu-
lary size C on attention models. However, it in-
</p>
<p>577</p>
<p />
</div>
<div class="page"><p />
<p>troduces too large an overhead (50 times slower),
especially when softmax is highly optimized on
GPU. When doing sequential beam search, search
error accumulates rapidly. To reach reasonable
performance, we have to apply an adequately large
number of permutations (P = 5000).
</p>
<p>We also find that decoding with word align-
ments can even improve BLEU on resource-poor
languages (12.17 vs. 11.67). Our conjecture is that
rare words are not trained enough, so neural mod-
els confuse them, and word alignments can pro-
vide a hard constraint to rule out the unreasonable
word choices.
</p>
<p>4 Conclusion
</p>
<p>We apply word alignments to shrink run-time
vocabulary to speed up neural machine transla-
tion decoding on GPUs, and achieve more than
2x speedup on 4 translation directions without
hurting BLEU. We also compare with two other
speedup methods: decoding with top frequent
words and decoding with LSH. Experiments and
analyses demonstrate that word alignments pro-
vides accurate candidate target words and in-
troduces only a tiny overhead over a highly-
optimized GPU implementation.
</p>
<p>References
Ondrej Bojar, Christian Buck, Christian Federmann,
</p>
<p>Barry Haddow, Philipp Koehn, Matous Machacek,
Christof Monz, Pavel Pecina, Matt Post, Herv Saint-
Amand, Radu Soricut, and Lucia Specia, editors.
2014. Proc. Ninth Workshop on Statistical Machine
Translation.
</p>
<p>Welin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proc. ACL.
</p>
<p>Thomas Dean, Mark A Ruzon, Mark Segal, Jonathon
Shlens, Sudheendra Vijayanarasimhan, and Jay Yag-
nik. 2013. Fast, accurate detection of 100,000 object
classes on a single machine. In Proc. CVPR.
</p>
<p>Michael Gutmann and Aapo Hyvärinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proc. AIS-
TATS.
</p>
<p>Zhongjun He. 2015. Baidu translate: research and
products. In Proc. ACL-IJCNLP.
</p>
<p>Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
Proc. ACL.
</p>
<p>Yoon Kim and Alexander M Rush. 2016. Sequence-
level knowledge distillation. In Proc. EMNLP.
</p>
<p>Gurvan L’Hostis, David Grangier, and Michael
Auli. 2016. Vocabulary Selection Strategies
for Neural Machine Translation. Arxiv preprint
arXiv:1610.00072.
</p>
<p>Linguistic Data Consortium. 2016. (bolt lrl uzbek rep-
resentative language pack v1.0. ldc2016e29.
</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proc. EMNLP.
</p>
<p>Haitao Mi, Zhiguo Wang, and Abraham Ittycheriah.
2016. Vocabulary Manipulation for Neural Machine
Translation. In Proc. ACL.
</p>
<p>Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic lan-
guage models. Proc. ICML .
</p>
<p>Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proc. AISTATS.
</p>
<p>Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. ASPEC:
Asian Scientific Paper Excerpt Corpus. In Proc.
LREC.
</p>
<p>Abigail See, Minh-Thang Luong, and Christopher D
Manning. 2016. Compression of neural machine
translation models via pruning.
</p>
<p>Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. In Proc.
ACL.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proc. NIPS.
</p>
<p>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proc. EMNLP.
</p>
<p>Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat
Monga, and Jay Yagnik. 2015. Deep networks with
large output spaces. In Proc. ICLR.
</p>
<p>Will Williams, Niranjani Prasad, David Mrva, Tom
Ash, and Tony Robinson. 2015. Scaling recurrent
neural network language models. In Proc. ICASSP.
</p>
<p>Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In Proc. EMNLP.
</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
</p>
<p>578</p>
<p />
</div>
<div class="page"><p />
<p>Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144 .
</p>
<p>Jay Yagnik, Dennis Strelow, David A Ross, and Ruei-
sung Lin. 2011. The power of comparative reason-
ing. In Proc. ICCV .
</p>
<p>Barret Zoph, Ashish Vaswani, Jonathan May, and
Kevin Knight. 2016. Simple, fast noise-contrastive
estimation for large rnn vocabularies. In Proc.
NAACL-HLT .
</p>
<p>579</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2092
</p>
<p>Chunk-Based Bi-Scale Decoder for Neural Machine Translation
</p>
<p>Hao Zhou∗
Nanjing University
</p>
<p>zhouh@nlp.nju.edu.cn
</p>
<p>Zhaopeng Tu∗
Tencent AI Lab
</p>
<p>tuzhaopeng@gmail.com
</p>
<p>Shujian Huang
Nanjing University
</p>
<p>huangsh@nlp.nju.edu.cn
</p>
<p>Xiaohua Liu
Huawei Noah’s Ark Lab
</p>
<p>liuxiaohua3@huawei.com
</p>
<p>Hang Li
Huawei Noah’s Ark Lab
</p>
<p>hangli.hl@huawei.com
</p>
<p>Jiajun Chen
Nanjing University
</p>
<p>chenjj@nlp.nju.edu.cn
</p>
<p>Abstract
</p>
<p>In typical neural machine transla-
tion (NMT), the decoder generates a
sentence word by word, packing all
linguistic granularities in the same time-
scale of RNN. In this paper, we propose
a new type of decoder for NMT, which
splits the decode state into two parts and
updates them in two different time-scales.
Specifically, we first predict a chunk
time-scale state for phrasal modeling, on
top of which multiple word time-scale
states are generated. In this way, the
target sentence is translated hierarchically
from chunks to words, with information
in different granularities being leveraged.
Experiments show that our proposed
model significantly improves the transla-
tion performance over the state-of-the-art
NMT model.
</p>
<p>1 Introduction
</p>
<p>Recent work of neural machine translation (NMT)
models propose to adopt the encoder-decoder
framework for machine translation (Kalchbrenner
and Blunsom, 2013; Cho et al., 2014; Sutskever
et al., 2014), which employs a recurrent neural net-
work (RNN) encoder to model the source context
information and a RNN decoder to generate trans-
lations, which is significantly different from previ-
ous statistical machine translation systems (Koehn
et al., 2003; Chiang, 2005). This framework is
then extended by an attention mechanism, which
acquires source sentence context dynamically at
different decoding steps (Bahdanau et al., 2014;
Luong et al., 2015).
</p>
<p>∗Work was done when Hao Zhou was interning and
Zhaopeng Tu was working at Huawei Noah’s Ark Lab.
</p>
<p>The decoder state stores translation information
at different granularities, determining which seg-
ment should be expressed (phrasal), and which
word should be generated (lexical), respectively.
However, due to the extensive existence of multi-
word phrases and expressions, the varying speed
of the lexical component is much faster than the
phrasal one. As in the generation of “the French
Republic”, the lexical component in the decoder
will change thrice, each of which for a sepa-
rate word. But the phrasal component may only
change once. The inconsistent varying speed of
the two components may cause translation errors.
</p>
<p>Typical NMT model generates target sentences
in the word level, packing the phrasal and lexi-
cal information in one hidden state, which is not
necessarily the best for translation. Much previ-
ous work propose to improve the NMT model by
adopting fine-grained translation levels such as the
character or sub-word levels, which can learn the
intermediate information inside words (Ling et al.,
2015; Costa-jussà and Fonollosa, 2016; Chung
et al., 2016; Luong et al., 2016; Lee et al., 2016;
Sennrich and Haddow, 2016; Sennrich et al., 2016;
Garcı́a-Martı́nez et al., 2016). However, high level
structures such as phrases has not been explicitly
explored in NMT, which is very useful for ma-
chine translation (Koehn et al., 2007).
</p>
<p>We propose a chunk-based bi-scale decoder
for NMT, which explicitly splits the lexical and
phrasal components into different time-scales.1
</p>
<p>The proposed model generates target words in a
hierarchical way, which deploys a standard word
time-scale RNN (lexical modeling) on top of an
additional chunk time-scale RNN (phrasal model-
ing). At each step of decoding, our model first
predict a chunk state with a chunk attention, based
on which multiple word states are generated with-
</p>
<p>1In this work, we focus on chunk-based well-formed
phrases, which generally contain two to five words.
</p>
<p>580</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2092">https://doi.org/10.18653/v1/P17-2092</a></div>
</div>
<div class="page"><p />
<p>out attention. The word state is updated at every
step, while the chunk state is only updated when
the chunk boundary is detected by a boundary gate
automatically. In this way, we incorporate soft
phrases into NMT, which makes the model flex-
ible at capturing both global reordering of phrases
and local translation inside phrases. Our model
has following benefits:
</p>
<p>1. The chunk-based NMT model explicitly
splits the lexical and phrasal components of
the decode state for different time-scales,
which addresses the issue of inconsistent up-
dating speeds of different components, mak-
ing the model more flexible.
</p>
<p>2. Our model recognizes phrase structures ex-
plicitly. Phrase information are then used
for word predictions, the representations of
which are then used to help predict corre-
sponding words.
</p>
<p>3. Instead of incorporating source side linguistic
information (Eriguchi et al., 2016; Sennrich
and Haddow, 2016), our model incorporates
linguistic knowledges in the target side (for
deciding chunks), which will guide the trans-
lation more in line with linguistic grammars.
</p>
<p>4. Given the predicted phrase representation,
our NMT model could extract attentive
source context by chunk attention, which is
more specific and thus more useful compared
to the word-level counterpart.
</p>
<p>Experiments show that our proposed model
obtains considerable BLEU score improvements
upon an attention-based NMT baseline on the
Chinese to English and the German to English
datasets simultaneously.
</p>
<p>2 Standard Neural Machine Translation
Model
</p>
<p>Generally, neural machine translation system di-
rectly models the conditional probability of the
translation y word by word (Bahdanau et al.,
2014). Formally, given an input sequence x =
[x1, x2, . . . , xJ ], and the previously generated
sequence y&lt;t = [y1, y2, . . . , yt−1], the probabil-
ity of next target word yt is
</p>
<p>P (yt|x) = softmax(f(eyt−1 , st, ct)) (1)
where f(·) is a non-linear function, eyt−1 is the
embedding of yt−1; st is the decode state at the
time step t, which is computed by
</p>
<p>st = g(st−1, eyt−1 , ct) (2)
</p>
<p>bush visits the french republic &lt;eos&gt;
</p>
<p>UPDATE UPDATE UPDATECOPYCOPY
</p>
<p>Word
Time-scale
</p>
<p>Chunk
Time-scale
</p>
<p>Attention
</p>
<p>布什 访问 法兰西 共和国
</p>
<p>Hidden
State
</p>
<p>Boundary
Gate
</p>
<p>Figure 1: The architecture of the chunk-based bi-
scale NMT.
</p>
<p>Here g(·) is a transition function of decoder RNN.
ct is the context vector computed by
</p>
<p>ct =
J∑
</p>
<p>j=1
</p>
<p>ATT(st−1, hj) · hj =
J∑
</p>
<p>j=1
</p>
<p>αt,j · hj (3)
</p>
<p>where ATT is an attention operation, which out-
puts alignment distribution α:
</p>
<p>αt,j =
exp(et,j)∑Tx
</p>
<p>k=1 exp(et,k)
(4)
</p>
<p>et,j = v
T
a tanh(Wast−1 + Uahj) (5)
</p>
<p>and h is the annotation of x from a bi-directional
RNNs. The training objective is to maximize the
likelihood of the training data. Beam search is
adopted for decoding.
</p>
<p>3 Chunk-Based Bi-Scale Neural Machine
Translation Model
</p>
<p>Instead of the word-based decoder, we propose to
use a chunk-based bi-scale decoder, which gen-
erates translation hierarchically with chunk and
word time-scales, as shown in Figure 1. Intu-
itively, we firstly generate a chunk state with the
attention model, which extracts the source con-
text for the current phrasal scope. Then we gen-
erate multiple lexical words based on the same
chunk state, which does not require attention oper-
ations. The boundary of a chunk is determined by
a boundary gate, which decides whether to update
the chunk state or not at each step.
</p>
<p>Formally, the probability of next word yt is
</p>
<p>P (yt|x) = softmax(f(eyt−1 , st, pt)) (6)
st = g(st−1, eyt−1 , pt) (7)
</p>
<p>581</p>
<p />
</div>
<div class="page"><p />
<p>here pt is the chunk state at step t. Compared with
Equations 1 and 2, the generation of target word
is based on the chunk state instead of the context
vector ct produced by the attention model.
</p>
<p>Since a chunk may correspond to multiple
words, we employ a boundary gate bt to decide
the boundary of each chunk:
</p>
<p>p(bt) = softmax(st−1, eyt−1) (8)
</p>
<p>bt will be 0 or 1, where 1 denotes this is the bound-
ary of a new chunk while 0 denotes not. Two dif-
ferent operations would be executed:
</p>
<p>pt =
</p>
<p>{
pt−1, bt = 0 (COPY)
g(pt−1, ept−1 , pct), bt = 1 (UPDATE)
</p>
<p>In the COPY operation, the chunk state is kept the
same as the previous step. In the UPDATE op-
eration, ept−1 is the representation of last chunk,
which is computed by the LSTM-minus approach
(Wang and Chang, 2016):
</p>
<p>ept−1 = m(st−1, eyt−1) − m(st′ , eyt′ ) (9)
</p>
<p>here t′ is the boundary of last chunk and m(·) is a
linear function. pct is the context vector for chunk
pt, which is calculated by a chunk attention model:
</p>
<p>pct =
</p>
<p>Ts∑
</p>
<p>j=1
</p>
<p>ATT(pt−1, hj) · hj (10)
</p>
<p>The chunk attention model differs from the stan-
dard word attention model (i.e., Equation 3) at:
1) it reads chunk state pt−1 rather than word state
st−1, and 2) it is only executed at boundary of each
chunk rather than at each decoding step.
</p>
<p>In this way, our model only extracts source con-
text once for a chunk, and the words in one chunk
will share the same context for word generation.
The chunk attention mechanism adds a constrain
that target words in the same chunk shares the
same source context.
</p>
<p>Training To encourage the proposed model to
learn reasonable chunk state, we add two addi-
tional objectives in training:
Chunk Tag Prediction: For each chunk, we
predict the probability of its tag P (lk|x) =
softmax
</p>
<p>(
f(pt, ept , ct)
</p>
<p>)
, where lk is the syntactic
</p>
<p>tag of the k-th chunk such as NP (noun phrase) and
VP (verb phrase), and t is time step of its bound-
ary.
</p>
<p>Chunk Boundary Prediction: At each decoding
step, we predict the probability of chunk boundary
P (bt|x) = softmax(st−1, eyt−1).
</p>
<p>Accordingly, given a set of training examples
{[xn,yn]}Nn=1, the new training objective is
</p>
<p>J(θ, γ) = arg max
</p>
<p>N∑
</p>
<p>n=1
</p>
<p>{
log P (yn|xn)
</p>
<p>+ log P (ln|xn) + log P (bn|xn)
}
</p>
<p>(11)
</p>
<p>where ln and bn are chunk tag sequence and
boundary sequence on yn, respectively.
</p>
<p>4 Experiments
</p>
<p>We carry out experiments on a Chinese-English
translation task. Our training data consists of
1.16M2 sentence pairs extracted from LDC cor-
pora, with 25.1M Chinese words and 27.1M
English words, respectively. We choose the
NIST 2002 (MT02) dataset as our development
set, and the NIST 2003 (MT03), 2004 (MT04)
2005 (MT05) datasets as our test sets. We
also evaluate our model on the WMT translation
task of German-English, newstest2014 (DE14) is
adopted as development set and newstest2012,
newstest2013 (DE1213) are adopted as testing set.
The English sentences are labeled by a neural
chunker, which is implemented according to Zhou
et al. (2015). We use the case-insensitive 4-gram
NIST BLEU score as our evaluation metric (Pap-
ineni et al., 2002).
</p>
<p>In training, we limit the source and target vo-
cabularies to the most frequent 30K words. We
train each model with the sentences of length up
to 50 words. Sizes of the chunk representation and
chunk hidden state are set to 1000. All the other
settings are the same as in Bahdanau et al. (2014).
</p>
<p>4.1 Results on Chinese-English
We list the BLEU score of our proposed model
in Table 1, comparing with Moses (Koehn et al.,
2007) and dl4mt3 (Bahdanau et al., 2014), which
are state-of-the-art models of SMT and NMT, re-
spective. For Moses, we use the default config-
uration with a 4-gram language model trained on
the target portion of the training data. For dl4mt,
we also report the results (dl4mt-2) by using two
</p>
<p>23LDC2002E18, LDC2003E14, the Hansards portion of
LDC2004T08, and LDC2005T06.
</p>
<p>3https://github.com/nyu-dl/
dl4mt-tutorial
</p>
<p>582</p>
<p />
</div>
<div class="page"><p />
<p>System MT02 MT03 MT04 MT05 Ave.
Moses 30.10 28.82 31.22 27.78 29.48
dl4mt 31.66 29.92 32.76 28.88 30.81
dl4mt-2 31.01 28.74 31.71 27.95 29.85
This Work 33.43 32.06 34.21 30.01 32.42
</p>
<p>Table 1: BLEU scores for different systems.
</p>
<p>Attention MT02 MT03 MT04 MT05 Ave.
Word 32.69 31.36 33.55 29.77 31.56
Chunk 33.43 32.06 34.21 30.01 32.42
</p>
<p>Table 2: Results with different attention models.
</p>
<p>decoder layers (Wu et al., 2016) for better compar-
ison.
</p>
<p>As shown in Table 1, our proposed model out-
performs different baselines on all sets, which ver-
ifies that the chunk-based bi-scale decoder is ef-
fective for NMT. Our model gives a 1.6 BLEU
score improvement upon the standard NMT base-
line (dl4mt). We conduct experiment with dl4mt-2
to see whether the neural NMT system can model
the bi-scale components with different varying
speeds automatically. Surprisingly, we find that
dl4mt-2 obtains lower BLEU scores than dl4mt.
We speculate that the more complex model dl4mt-
2 may need more training data for obtaining rea-
sonable results.
</p>
<p>Effectiveness of Chunk Attention As de-
scribed in Section 3, we propose to use the chunk
attention to replace the word level attention in our
model, in which the source context extracted by
the chunk attention will be used for the corre-
sponding word generations in the chunk. We also
report the result of our model using conventional
word attention for comparison. As shown in Table
2, our model with the chunk attention gives higher
BLEU score than the word attention.
</p>
<p>Intuitively, we think chunks are more specific in
semantics, thus could extract more specific source
context for translation. The chunk attention could
be considered as a compromise approach between
encoding the whole source sentence into decoder
without attention (Sutskever et al., 2014) and uti-
lizing word level attention at each step (Bahdanau
et al., 2014). We also draw the figure of align-
ments by chunk attention (Figure 2), from which
we can see that our chunk attention model can well
explore the alignments from phrases to words.
</p>
<p>Figure 2: Alignments with chunk attention.
</p>
<p>MT02 MT03 MT04 MT05
Boundary 89.97 88.81 89.64 89.25
</p>
<p>Label 47.00 44.75 45.54 44.41
</p>
<p>Table 3: Accuracies of predicted chunk boundary
and chunk label.
</p>
<p>Predictions of the Chunk Boundary and Chunk
Label We also compute predicted accuracies of
chunk boundaries and chunk labels on the auto-
chunked development and testing data (Table 3).
We find that the chunk boundary could be pre-
dicted well, with an average accuracy of 89%,
which shows that our model could capture the
phrasal boundary information in the translation
process. However, our model could not pre-
dict chunk labels as well as chunk boundaries.
We speculate that more syntactic context features
should be added to improve the performance of
predicting chunk labels.
</p>
<p>Subjective Evaluation Following Tu et al.
(2016, 2017a,b), we also compare our model with
the dl4mt baseline by subjective evaluation. Two
human evaluators are asked to evaluate the trans-
lations of 100 source sentences randomly sampled
from the test sets without knowing which system
</p>
<p>583</p>
<p />
</div>
<div class="page"><p />
<p>Model dl4mt Our Work
Adequacy 3.26 3.35
</p>
<p>Fluency 3.69 3.71
Under-Translation 50% 47%
</p>
<p>Over-Translation 32% 26%
</p>
<p>Table 4: Subjective evaluation results.
</p>
<p>System DE-14 DE-1213
dl4mt 16.53 16.78
</p>
<p>This Work 17.40 17.45
</p>
<p>Table 5: Results on German-English
</p>
<p>the translation is translated by. The human eval-
uator is asked to give 4 scores: adequacy score
and fluency score, which are between 0 and 5, the
larger, the better; under-translation score and over-
translation score, which are set to 1 when under or
over translation errors occurs, otherwise set to 0.
</p>
<p>We list the averaged scores in Table 5. We find
that our proposed model improves the dl4mt base-
line on both the translation adequacy and fluency
aspects. Specifically, the over translation error rate
drops by 6%, which confirms the assumption in
the introduction that splitting the fast and slow
varying components in different time-scales could
help alleviate the over translation errors.
</p>
<p>4.2 Results on German-English
</p>
<p>We evaluate our model on the WMT15 translation
task from German to English. We find that our
proposed chunk-based NMT model also obtains
considerable accuracy improvements on German-
English. However, the BLEU score gains are not
as significant as on Chinese-English. We speculate
that the difference between Chinese and English is
larger than German and English. The chunk-based
NMT model may be more useful for bilingual data
with bigger difference.
</p>
<p>5 Related Work
</p>
<p>NMT with Various Granularities. A line of
previous work propose to utilize other granulari-
ties besides words for NMT. By further exploit-
ing the character level (Ling et al., 2015; Costa-
jussà and Fonollosa, 2016; Chung et al., 2016; Lu-
ong et al., 2016; Lee et al., 2016), or the sub-word
level (Sennrich and Haddow, 2016; Sennrich et al.,
2016; Garcı́a-Martı́nez et al., 2016) information,
the corresponding NMT models capture the infor-
</p>
<p>mation inside the word and alleviate the problem
of unknown words. While most of them focus on
decomposing words into characters or sub-words,
our work aims at composing words into phrases.
</p>
<p>Incorporating Syntactic Information in NMT
Syntactic information has been widely used in
SMT (Liu et al., 2006; Marton and Resnik, 2008;
Shen et al., 2008), and a lot of previous work ex-
plore to incorporate the syntactic information in
NMT, which shows the effectiveness of the syntac-
tic information (Stahlberg et al., 2016). Shi et al.
(2016) give some empirical results that the deep
networks of NMT are able to capture some use-
ful syntactic information implicitly. Luong et al.
(2016) propose to use a multi-task framework for
NMT and neural parsing, achieving promising re-
sults. Eriguchi et al. (2016) propose a string-to-
tree NMT system by end-to-end training. Differ-
ent to previous work, we try to incorporate the syn-
tactic information in the target side of NMT. Ishi-
watari et al. (2017) concurrently propose to use
chunk-based decoder to cope with the problem of
free word-order languages. Differently, they adopt
word-level attention, and predict the end of chunk
by generating end-of-chunk tokens instead of us-
ing boundary gate.
</p>
<p>6 Conclusion
</p>
<p>We propose a chunk-based bi-scale decoder for
neural machine translation, in which way, the
target sentence is translated hierarchically from
chunks to words, with information in different
granularities being leveraged. Experiments show
that our proposed model outperforms the standard
attention-based neural machine translation base-
line. Future work includes abandoning labeled
chunk data, adopting reinforcement learning to ex-
plore the boundaries of phrase automatically (Mou
et al., 2016). Our code is released on https:
//github.com/zhouh/chunk-nmt.
</p>
<p>Acknowledge
</p>
<p>We would like to thank the anonymous reviewers
for their insightful comments. We also thank Lili
Mou for helpful discussion and Hongjie Ji, Zhent-
ing Yu, Xiaoxue Hou and Wei Zou for their help
in data preparation and subjective evaluation. This
work was partially founded by the Natural Science
Foundation of China (61672277, 71503124) and
the China National 973 project 2014CB340301.
</p>
<p>584</p>
<p />
</div>
<div class="page"><p />
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2014. Neural machine translation by jointly
learning to align and translate. In ICLR 2015.
</p>
<p>David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, pages 263–270.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, pages 1724–1734.
https://doi.org/10.3115/v1/D14-1179.
</p>
<p>Junyoung Chung, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. A character-level decoder without ex-
plicit segmentation for neural machine translation.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1693–1703.
http://www.aclweb.org/anthology/P16-1160.
</p>
<p>R. Marta Costa-jussà and R. José A. Fonollosa. 2016.
Character-based neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers). Association for Computational Linguistics,
pages 357–361. https://doi.org/10.18653/v1/P16-
2058.
</p>
<p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Associ-
ation for Computational Linguistics, pages 823–833.
https://doi.org/10.18653/v1/P16-1078.
</p>
<p>Mercedes Garcı́a-Martı́nez, Loı̈c Barrault, and Fethi
Bougares. 2016. Factored neural machine transla-
tion. arXiv preprint arXiv:1609.04621 .
</p>
<p>Shonosuke Ishiwatari, Jingtao Yao, Shujie Liu, Mu Li,
Ming Zhou, Naoki Yoshinaga, Masaru Kitsuregawa,
and Weijia Jia. 2017. Chunk-based decoder for
neural machine translation. In Proceedings of the
55th annual meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
</p>
<p>Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1700–1709.
http://aclweb.org/anthology/D13-1176.
</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions. As-
sociation for Computational Linguistics, pages 177–
180.
</p>
<p>Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics, pages 48–54.
</p>
<p>Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2016. Fully character-level neural machine trans-
lation without explicit segmentation. arXiv preprint
arXiv:1610.03017 .
</p>
<p>Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W
Black. 2015. Character-based neural machine trans-
lation. arXiv preprint arXiv:1511.04586 .
</p>
<p>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, pages 609–616.
</p>
<p>Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In International Con-
ference on Learning Representations (ICLR). San
Juan, Puerto Rico.
</p>
<p>Thang Luong, Hieu Pham, and D. Christopher Man-
ning. 2015. Effective approaches to attention-
based neural machine translation. In Proceed-
ings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1412–1421.
https://doi.org/10.18653/v1/D15-1166.
</p>
<p>Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In ACL. pages 1003–1011.
</p>
<p>Lili Mou, Zhengdong Lu, Hang Li, and Zhi Jin.
2016. Coupling distributed and symbolic execu-
tion for natural language queries. arXiv preprint
arXiv:1612.02741 .
</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics.
</p>
<p>585</p>
<p />
</div>
<div class="page"><p />
<p>Rico Sennrich and Barry Haddow. 2016. Linguis-
tic input features improve neural machine trans-
lation. In Proceedings of the First Conference
on Machine Translation. Association for Computa-
tional Linguistics, Berlin, Germany, pages 83–91.
http://www.aclweb.org/anthology/W16-2209.
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 1715–1725.
https://doi.org/10.18653/v1/P16-1162.
</p>
<p>Libin Shen, Jinxi Xu, and Ralph M Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In ACL. pages 577–585.
</p>
<p>Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, pages 1526–
1534. http://aclweb.org/anthology/D16-1159.
</p>
<p>Felix Stahlberg, Eva Hasler, Aurelien Waite, and Bill
Byrne. 2016. Syntactically guided neural ma-
chine translation. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 299–305.
https://doi.org/10.18653/v1/P16-2049.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.
</p>
<p>Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,
and Hang Li. 2017a. Context gates for neural ma-
chine translation. Transactions of the Association
for Computational Linguistics 5:87–99.
</p>
<p>Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017b. Neural machine translation
with reconstruction. In Proceedings of AAAI 2017.
pages 3097–3103.
</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neu-
ral machine translation. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Asso-
ciation for Computational Linguistics, pages 76–85.
https://doi.org/10.18653/v1/P16-1008.
</p>
<p>Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional lstm. In Pro-
ceedings of ACL. volume 1, pages 2306–2315.
</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144 .
</p>
<p>Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun
Chen. 2015. A neural probabilistic structured-
prediction model for transition-based dependency
parsing. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Lin-
guistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume
1: Long Papers). Association for Computational
Linguistics, Beijing, China, pages 1213–1222.
http://www.aclweb.org/anthology/P15-1117.
</p>
<p>586</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 587–593
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2093
</p>
<p>Model Transfer for Tagging Low-resource Languages using a Bilingual
Dictionary
</p>
<p>Meng Fang and Trevor Cohn
School of Computing and Information Systems
</p>
<p>The University of Melbourne
meng.fang@unimelb.edu.au, t.cohn@unimelb.edu.au
</p>
<p>Abstract
</p>
<p>Cross-lingual model transfer is a com-
pelling and popular method for predicting
annotations in a low-resource language,
whereby parallel corpora provide a bridge
to a high-resource language and its associ-
ated annotated corpora. However, parallel
data is not readily available for many lan-
guages, limiting the applicability of these
approaches. We address these drawbacks
in our framework which takes advantage
of cross-lingual word embeddings trained
solely on a high coverage bilingual dictio-
nary. We propose a novel neural network
model for joint training from both sources
of data based on cross-lingual word em-
beddings, and show substantial empirical
improvements over baseline techniques.
We also propose several active learning
heuristics, which result in improvements
over competitive benchmark methods.
</p>
<p>1 Introduction
</p>
<p>Part-of-speech (POS) tagging is an important first
step in most natural language processing (NLP)
applications. Typically this is modelled using
sequence labelling methods to predict the con-
ditional probability of taggings given word se-
quences, using linear graphical models (Lafferty
et al., 2001), or neural network models, such as
recurrent neural networks (RNN) (Mikolov et al.,
2010; Huang et al., 2015). These supervised learn-
ing algorithms rely on large labelled corpora; this
is particularly true for state-of-the-art neural net-
work models. Due to the expense of annotating
sufficient data, such techniques are not well suited
to applications in low-resource languages.
</p>
<p>Prior work on low-resource NLP has primarily
focused on exploiting parallel corpora to project
</p>
<p>information between a high- and low-resource
language (Yarowsky and Ngai, 2001; Täckström
et al., 2013; Guo et al., 2015; Agić et al., 2016;
Buys and Botha, 2016). For example, POS tags
can be projected via word alignments, and the pro-
jected POS is then used to train a model in the low-
resource language (Das and Petrov, 2011; Zhang
et al., 2016; Fang and Cohn, 2016). These meth-
ods overall have limited effectiveness due to errors
in the alignment and fundamental differences be-
tween the languages. They also assume a large
parallel corpus, which may not be available for
many low-resource languages.
</p>
<p>To address these limitations, we propose a new
technique for low resource tagging, with more
modest resource requirements: 1) a bilingual dic-
tionary; 2) monolingual corpora in the high and
low resource languages; and 3) a small annotated
corpus of around 1, 000 tokens in the low-resource
language. The first two resources are used as a
form of distant supervision through learning cross-
lingual word embeddings over the monolingual
corpora and bilingual dictionary (Ammar et al.,
2016). Additionally, our model jointly incor-
porates the language-dependent information from
the small set of gold annotations. Our approach
combines these two sources of supervision us-
ing multi-task learning, such that the kinds of er-
rors that occur in cross-lingual transfer can be ac-
counted for, and corrected automatically.
</p>
<p>We empirically demonstrate the validity of our
observation by using distant supervision to im-
prove POS tagging performance with little super-
vision. Experimental results show the effective-
ness of our approach across several low-resource
languages, including both simulated and true low-
resource settings. Furthermore, given the clear su-
periority of training with manual annotations, we
compare several active learning heuristics. Active
learning using uncertainty sampling with a word-
</p>
<p>587</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2093">https://doi.org/10.18653/v1/P17-2093</a></div>
</div>
<div class="page"><p />
<p>cross-lingual
word 
</p>
<p>embedding, e
</p>
<p>text, x
</p>
<p>shared
layers
</p>
<p>distant label, y Noun Verb Noun
</p>
<p>...
</p>
<p>h
</p>
<p>h
</p>
<p>Det NounConj
</p>
<p>Augmented layer
</p>
<p>label, y
</p>
<p>Distantly)supervised)data) Manually)Labeled)data
</p>
<p>raha ny marina tsara fa misaotra 
</p>
<p>Figure 1: Illustration of the architecture of the joint model, which performs joint inference over both
distant supervision (left) and manually labelled data (right).
</p>
<p>type bias leads to substantial gains over bench-
mark methods such as token or sentence level un-
certainty sampling.
</p>
<p>2 Related work
</p>
<p>POS tagging has been studied for many years.
Traditionally, probabilistic models are a popular
choice, such as Hidden Markov Models (HMM)
and Conditional Random Fields (CRF) (Lafferty
et al., 2001). Recently, neural network mod-
els have been developed for POS tagging and
achieved good performance, such as RNN and
bidirectional long short-term memory (BiLSTM)
and CRF-BiLSTM models (Mikolov et al., 2010;
Huang et al., 2015). For example, the CRF-
BiLSTM POS tagger obtained the state-of-the-
art performance on Penn Treebank WSJ cor-
pus (Huang et al., 2015).
</p>
<p>However, in low-resource languages, these
models are seldom used because of limited la-
belled data. Parallel data therefore appears to be
the most realistic additional source of informa-
tion for developing NLP systems in low-resource
languages (Yarowsky and Ngai, 2001; Das and
Petrov, 2011; Täckström et al., 2013; Fang and
Cohn, 2016; Zhang et al., 2016). Yarowsky and
Ngai (2001) pioneered the use of parallel data for
projecting POS tag information from one language
to another language. Das and Petrov (2011) used
parallel data and exploited graph-based label prop-
agation to expand the coverage of labelled tokens.
</p>
<p>Täckström et al. (2013) constructed tag dictionar-
ies by projecting tag information from a high-
resource language to a low-resource language via
alignments in the parallel text. Fang and Cohn
(2016) used parallel data to obtain projected tags
as distant labels and proposed a joint BiLSTM
model trained on both the distant data and 1, 000
tagged tokens. Zhang et al. (2016) used a few
word translations pairs to find a linear transfor-
mation between two language embeddings. Then
they used unsupervised learning to refine embed-
ding transformations and model parameters. In-
stead we use minimal supervision to refine ‘dis-
tant’ labels through modelling the tag transforma-
tion, based on a small set of annotations.
</p>
<p>3 Model
</p>
<p>We now describe the modelling framework for
POS tagging in a low-resource language, based on
very limited linguistic resources. Our approach
extends the work of Fang and Cohn (2016), who
present a model based on distant supervision in
the form of cross-lingual projection and use pro-
jected tags generated from parallel corpora as dis-
tant annotations. There are three main differences
between their work and ours: 1) We do not use par-
allel corpora, but instead use a bilingual dictionary
for knowledge transfer. 2) Our model uses a more
expressive multi-layer perceptron when generat-
ing the gold standard tags. The multi-layer per-
ceptron can capture both language-specific infor-
</p>
<p>588</p>
<p />
</div>
<div class="page"><p />
<p>Cross-lingual word 
embeddings
</p>
<p>Bidirectional LSTM
</p>
<p>Output y
</p>
<p>Input x
</p>
<p>Softmax
</p>
<p>Figure 2: Architecture of the universal POS tag-
ger. Cross-lingual word embeddings are pre-
trained using monolingual corpora and bilingual
dictionaries.
</p>
<p>mation and consistent tagging errors arising from
this method of supervision. 3) We propose a num-
ber of active learning methods to further reduces
the annotation requirements. Our method is illus-
trated in Figure 1, and we now elaborate on the
model components.
</p>
<p>Distant cross-lingual supervision In order to
transfer tag information between the high- and
low-resource languages, we start by learning
cross-lingual word embeddings, which operate
by learning vector valued embeddings such that
words and their translations tend to be close to-
gether in the vector space. We use the embeddings
from Ammar et al. (2016) which trains mono-
lingual word2vec distributional representations,
which are then projected into a common space,
learned from bilingual dictionaries.
</p>
<p>We then train a POS tagger on the high-resource
language, using the cross-lingual word embed-
dings as the first, fixed, layer of a bidirectional
LSTM tagger. The tagger is a language-universal
model based on cross-lingual word embeddings,
for processing an arbitrary language, given a
monolingual corpus and a bilingual dictionary,
as shown in Figure 2. Next we apply this tag-
ger to unannotated text in the low-resource lan-
guage; this application is made possible through
the use of cross-lingual word embeddings. We
refer to text tagged this way as distantly super-
vised data, and emphasize that although much bet-
ter than chance, the outputs are often incorrect and
are of limited utility on their own.
</p>
<p>As illustrated in Figure 1, the distant compo-
nents are generated directly as softmax outputs,
</p>
<p>yt ∼ Categorial(ot), with parameters ot =
Softmax(Wht + b) as a linear classifier over a
sentence encoding, ht, which is the output of a
bidirectional LSTM encoder over the words.
</p>
<p>Ground truth supervision The second compo-
nent of the model is manually labelled text in the
low-resource language. To model this data we em-
ploy the same model structure as above but aug-
mented with a second perceptron output layer, as
illustrated in Figure 1 (right). Formally, ỹt ∼
Categorial(õt) where õt = MLP(ot) is a single
hidden layer perceptron with tanh activation and
softmax output transformation. This component
allows for a more expressive label mapping than
Fang and Cohn (2016)’s linear matrix translation.
</p>
<p>Joint multi-task learning To combine the two
sources of information, we use a joint objective,
</p>
<p>J = −γ
∑
</p>
<p>t∈N
〈ỹt, log õt〉 −
</p>
<p>∑
</p>
<p>t∈M
〈yt, log ot〉 , (1)
</p>
<p>where N and M index the token positions in
the distant and ground truth corpora, respectively,
and γ is a constant balancing the two components
which we set for uniform weighting, γ = |M||N | .
</p>
<p>Consider the training effect of the true POS
tags: when performing error backpropagation, the
cross-entropy error signal must pass through the
transformation linking õwith o, which can be seen
as a language-specific step, after which the gener-
alised error signal can be further backpropagated
to the rest of the model.
</p>
<p>Active learning Given the scarcity of ground
truth labels and the high cost of annotation, a natu-
ral question is whether we can optimise which text
to be annotated in order achieve the high accuracy
for the lowest cost. We now outline a range of
active learning approaches based on the following
heuristics, which are used to select the instances
for annotation from a pool of candidates:
</p>
<p>TOKEN Select the token xt
with the highest uncertainty,
H(x, t) = −∑y P (y|x, t) logP (y|x, t);
</p>
<p>SENT Select the sentence x with the highest ag-
gregate uncertainty, H(x) =
</p>
<p>∑
tH(x, t);
</p>
<p>FREQTYPE Select the most frequent unanno-
tated word type (Garrette and Baldridge,
2013), in which case all token instances are
</p>
<p>589</p>
<p />
</div>
<div class="page"><p />
<p>annotated with the most frequent label for the
type in the training corpus;1
</p>
<p>SUMTYPE Select a word type, z, for an-
notation with the highest aggregate
uncertainty over token occurrences,
H(z) =
</p>
<p>∑
i∈D
</p>
<p>∑
xi,t=z
</p>
<p>H(xi, t), which
effectively combines uncertainty sampling
with a bias towards high frequency types;
and
</p>
<p>RANDOM Select word types randomly.
</p>
<p>4 Experiments
</p>
<p>We evaluate the effectiveness of the proposed
model for several different languages, including
both simulated low-resource and true low-resource
settings. The first evaluation set uses the CoNLL-
X datasets of European languages (Buchholz and
Marsi, 2006), comprising Danish (da), Dutch (nl),
German (de), Greek (el), Italian (it), Portuguese
(pt), Spanish (es) and Swedish (sv). We use the
standard corpus splits. The first 20 sentences of
training set are used for training as the tiny la-
belled (gold) data and the last 20 sentences are
used for development (early stopping). We report
accuracy on the held-out test set.
</p>
<p>The second evaluation set includes two highly
challenging languages, Turkish (tk) and Malagasy
(mg), both having high morphological complexity
and the latter has truly scant resources. Turkish
data was drawn from CoNLL 20032 and Malagasy
data was collected from Das and Petrov (2011), in
both cases using the same training configuration
as above.
</p>
<p>In all cases English is used as the source ‘high
resource’ language, on which we train a tagger
using the Penn Treebank, and we evaluate on
each of the remaining languages as an indepen-
dent target. For cross-lingual word embeddings,
we evaluate two techniques from Ammar et al.
(2016): CCA-based word embeddings and cluster-
based word embeddings. Both types of word em-
bedding techniques are based on bilingual dictio-
naries. The dictionaries were formed by trans-
lating the 20k most common words in the En-
</p>
<p>1We could support more than one class label, by marginal-
ising over the set of valid labels for all tokens in the training
objective.
</p>
<p>2http://www.cnts.ua.ac.be/conll2003/ner/
</p>
<p>glish monolingual corpus with Google Translate.3
</p>
<p>The monolingual corpora were constructed from
a combination of text from the Leipzig Corpora
Collection and Europarl. We trained the language-
universal POS tagger based on the cross-lingual
word embeddings with the universal POS tagset
(Petrov et al., 2011), and then applied to the tar-
get language using the embedding lookup table
for the corresponding language embeddings. We
implement our learning procedure with the DyNet
toolkit (Neubig et al., 2017).4 The BiLSTM layer
uses 128 hidden units, and 32 hidden units for the
transformation step. We used SGD with momen-
tum to train models, with early stopping based on
development performance.
</p>
<p>For benchmarks, we compare the proposed
model against various state-of-the-art supervised
learning methods, namely: a BILSTM tagger,
BILSTM-CRF tagger (Huang et al., 2015), and
a state-of-the-art semi-supervised POS tagging
algorithm, MINITAGGER (Stratos and Collins,
2015), which is also focusing on minimising
the amount of labelled data. Note these meth-
ods do not use cross-lingual supervision. For a
more direct comparison, we include BILSTM-
DEBIAS (Fang and Cohn, 2016), applied using our
proposed cross-lingual supervision based on dic-
tionaries, instead of parallel corpora; accordingly
the key difference is their linear transformation for
the distant data, versus our non-linear transforma-
tion to the gold data.
</p>
<p>Results Table 1 reports the tagging accuracy,
showing that our models consistently outperform
the baseline techniques. The poor performance of
the supervised methods suggests they are overfit-
ting the small training set, however this is much
less of a problem for our approach (labelled Joint).
Note that distant supervision alone gives reason-
able performance (labelled DISTANT) however the
joint modelling of the ground truth and distant
data yields significant improvements in almost all
cases. BILSTM-DEBIAS (Fang and Cohn, 2016)
performs worse than our proposed method, indi-
cating that a linear transformation is insufficient
for modelling distant supervision. The accuracies
are higher overall for the European cf. Turkic lan-
guages, presumably because these languages are
</p>
<p>3Although the use of a translation system conveys a de-
pendence on parallel text, high quality word embeddings can
be learned directly from bilingual dictionaries such as Panlex
(Kamholz et al., 2014).
</p>
<p>4Code available at https://github.com/mengf1/trpos
</p>
<p>590</p>
<p />
</div>
<div class="page"><p />
<p>da nl de el it pt es sv tk mg
Random 23.2 30.5 27.1 23.2 25.9 24.3 26.9 21.6 36.9 34.5
BILSTM 61.8 62.1 60.5 70.1 73.6 67.6 63.6 57.2 44.0 63.4
BILSTM-CRF 46.3 47.7 53.2 35.1 41.2 44.1 25.5 54.9 43.1 41.4
MINITAGGER 77.0 72.5 75.9 75.7 67.3 75.1 73.5 77.7 49.8 67.2
DISTANT +CCA 73.5 64.5 57.7 53.1 59.5 67.8 63.5 66.0 57.2 49.7
DISTANT +Cluster 70.4 61.7 65.9 65.5 64.8 66.9 68.4 64.1 51.7 50.2
BILSTM-DEBIAS +CCA 73.2 72.8 72.5 71.2 70.7 72.1 71.1 73.1 49.2 65.9
BILSTM-DEBIAS +Cluster 72.5 70.1 71.2 68.7 69.1 72.5 70.6 73.3 48.7 64.5
JOINT +CCA 81.1 82.3 76.1 77.5 75.9 82.1 79.7 78.1 72.6 75.3
JOINT +Cluster 81.9 81.5 78.9 80.1 81.9 76.7 81.2 78.0 70.4 75.7
</p>
<p>Table 1: POS tagging accuracy on over the ten target languages, showing first approaches using only the
gold data; next methods using only distant cross-lingual supervision, and lastly joint multi-task learning.
English is used as the source language and columns correspond to a specific target language.
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
● ●
</p>
<p>● ● ● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>● ●
</p>
<p>●
● ●
</p>
<p>● ●
●
</p>
<p>de el
</p>
<p>10 100 200 400 1000 10 100 200 400 1000
</p>
<p>0.25
</p>
<p>0.50
</p>
<p>0.75
</p>
<p>num. words (tokens or types)
</p>
<p>ac
cu
</p>
<p>ra
cy
</p>
<p>● FreqType
Random
Sent(Joint)
Sent(Trad)
SumType(Joint)
SumType(Trad)
Token(Joint)
Token(Trad)
</p>
<p>Figure 3: Active learning evaluation on German and Greek, using CCA trained cross-lingual word em-
beddings. Trad means traditional active learning; Joint means joint multi-task learning.
</p>
<p>closer to English, have higher quality dictionaries
and in most cases are morphologically simpler. Fi-
nally, note the difference between CCA and Clus-
ter methods for learning word embeddings which
arise from the differing quality of distant supervi-
sion between the languages.
</p>
<p>Figure 3 compares various active learning
heuristics (see §3) based on different taggers, ei-
ther a supervised BILSTM (labelled Trad) or
our multi-task model which also includes cross-
lingual supervision (JOINT).
</p>
<p>Traditional uncertainty-based sampling strate-
gies (TOKEN(Trad) and SENT(Trad)) do not work
well because models based on limited supervision
do not provide accurate uncertainty information,5
</p>
<p>and moreover, annotating at the type rather than
token level provides a significantly stronger su-
pervision signal. The difference is apparent from
the decent performance of Random sampling over
word types. Overall, SUMTYPE(Joint) outper-
forms the other heuristics consistently, underlin-
ing the importance of cross-lingual distant super-
</p>
<p>5Sentence level annotation is likely to be much faster than
token or type level annotation, however even if it were an
order of magnitude faster it is still not a competitive active
learning strategy.
</p>
<p>vision, as well as combining the benefits of un-
certainty sampling, type selection and a frequency
bias. Comparing the amount of annotation re-
quired between the best traditional active learn-
ing method SUMTYPE(Trad) and our best method
SUMTYPE(Joint), we achieve the same perfor-
mance with an order of magnitude less annotated
data (100 vs. 1, 000 labelled words).
</p>
<p>5 Conclusion
</p>
<p>In this paper, we proposed a means of tagging
a low-resource language without the need for
bilingual parallel corpora. We introduced a new
cross-lingual distant supervision method based on
a bilingual dictionary. Furthermore, deep neu-
ral network models can be effective with limited
supervision by incorporating distant supervision,
in the form of model transfer with cross-lingual
word embeddings. We show that traditional un-
certainty sampling strategies do not work well on
low-resource settings, and introduce new methods
based around labelling word types. Overall our
approach leads to consistent and substantial im-
provements over benchmark methods.
</p>
<p>591</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgments
</p>
<p>This work was sponsored by the Defense Ad-
vanced Research Projects Agency Information In-
novation Office (I2O) under the Low Resource
Languages for Emergent Incidents (LORELEI)
program issued by DARPA/I2O under Contract
No. HR0011-15-C-0114. The views expressed are
those of the author and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. Trevor Cohn was sup-
ported by the Australian Research Council Future
Fellowship (project number FT130101105).
</p>
<p>References
Željko Agić, Anders Johannsen, Barbara Plank,
</p>
<p>Héctor Alonso Martı́nez, Natalie Schluter, and An-
ders Søgaard. 2016. Multilingual projection for
parsing truly low-resource languages. Transactions
of the Association for Computational Linguistics
4:301–312.
</p>
<p>Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
Transactions of the Association for Computational
Linguistics 4:431–444.
</p>
<p>Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, pages 149–164.
</p>
<p>Jan Buys and Jan A. Botha. 2016. Cross-lingual mor-
phological tagging for low-resource languages. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL). Asso-
ciation for Computational Linguistics, Berlin, Ger-
many, pages 1954–1964.
</p>
<p>Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-
HLT). pages 600–609.
</p>
<p>Meng Fang and Trevor Cohn. 2016. Learning when
to trust distant supervision: An application to low-
resource pos tagging using cross-lingual projec-
tion. In Proceedings of the 20th SIGNLL Confer-
ence on Computational Natural Language Learning
(CoNLL). Berlin, Germany.
</p>
<p>Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT). Citeseer, pages 138–147.
</p>
<p>Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics (ACL).
Association for Computational Linguistics, pages
1234–1244.
</p>
<p>Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991 .
</p>
<p>David Kamholz, Jonathan Pool, and Susan M Colow-
ick. 2014. Panlex: Building a resource for panlin-
gual lexical translation. In Proceedings of the Ninth
International Conference on Language Resources
and Evaluation (LREC). pages 3145–3150.
</p>
<p>John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 8th Interna-
tional Conference on Machine Learning (ICML).
volume 1, pages 282–289.
</p>
<p>Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech. volume 2, page 3.
</p>
<p>Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .
</p>
<p>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086 .
</p>
<p>Karl Stratos and Michael Collins. 2015. Simple semi-
supervised pos tagging. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT). pages 79–
87.
</p>
<p>Oscar Täckström, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging.
Transactions of the Association for Computational
Linguistics 1:1–12.
</p>
<p>David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP brackets via robust
projection across aligned corpora. In Proceedings of
the 2001 Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
</p>
<p>592</p>
<p />
</div>
<div class="page"><p />
<p>Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi Jaakkola. 2016. Ten pairs to tag–
multilingual pos tagging via coarse mapping be-
tween embeddings. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT). pages 1307–
1317.
</p>
<p>593</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594–600
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2094
</p>
<p>EUROSENSE: Automatic Harvesting of Multilingual
Sense Annotations from Parallel Text
</p>
<p>Claudio Delli Bovi, Jose Camacho-Collados, Alessandro Raganato and Roberto Navigli
Department of Computer Science
</p>
<p>Sapienza University of Rome
{dellibovi,collados,raganato,navigli}@di.uniroma1.it
</p>
<p>Abstract
</p>
<p>Parallel corpora are widely used in a vari-
ety of Natural Language Processing tasks,
from Machine Translation to cross-lingual
Word Sense Disambiguation, where par-
allel sentences can be exploited to auto-
matically generate high-quality sense an-
notations on a large scale. In this paper
we present EUROSENSE, a multilingual
sense-annotated resource based on the
joint disambiguation of the Europarl par-
allel corpus, with almost 123 million sense
annotations for over 155 thousand distinct
concepts and entities from a language-
independent unified sense inventory. We
evaluate the quality of our sense annota-
tions intrinsically and extrinsically, show-
ing their effectiveness as training data for
Word Sense Disambiguation.
</p>
<p>1 Introduction
</p>
<p>One of the long-standing challenges in Natu-
ral Language Processing (NLP) lies in automat-
ically identifying the meaning of words in con-
text. Various lines of research have been geared
towards achieving this goal, most notably Word
Sense Disambiguation (Navigli, 2009, WSD) and
Entity Linking (Rao et al., 2013, EL). In both
tasks, supervised approaches (Zhong and Ng,
2010; Melamud et al., 2016; Iacobacci et al., 2016;
Kågebäck and Salomonsson, 2016) tend to ob-
tain the best performances over standard bench-
marks but, from a practical standpoint, they lose
ground to knowledge-based approaches (Agirre
et al., 2014; Moro et al., 2014b; Weissenborn
et al., 2015), which scale better in terms of
scope and number of languages. In fact, the
development of supervised disambiguation sys-
tems depends crucially on the availability of re-
</p>
<p>liable sense-annotated corpora, which are indis-
pensable in order to provide solid training and test-
ing grounds (Pilehvar and Navigli, 2014). How-
ever, hand-labeled sense annotations are notori-
ously difficult to obtain on a large scale, and man-
ually curated corpora (Miller et al., 1993; Passon-
neau et al., 2012) have a limited size. Given that
scaling the manual annotation process becomes
practically unfeasible when both lexicographic
and encyclopedic knowledge is addressed (Schu-
bert, 2006), recent years have witnessed efforts
to produce larger sense-annotated corpora auto-
matically (Moro et al., 2014a; Taghipour and
Ng, 2015a; Scozzafava et al., 2015; Raganato
et al., 2016). Even though these automatic ap-
proaches produce noisier corpora, it has been
shown that training on them leads to better su-
pervised and semi-supervised models (Taghipour
and Ng, 2015b; Raganato et al., 2016; Yuan et al.,
2016; Raganato et al., 2017), as well as to ef-
fective embedded representations for senses (Ia-
cobacci et al., 2015; Flekova and Gurevych, 2016).
</p>
<p>A convenient way of generating sense annota-
tions is to exploit parallel corpora and word align-
ments (Taghipour and Ng, 2015a): indeed, parallel
corpora exist in many flavours (Tiedemann, 2012)
and are widely used across the NLP community
for a variety of different tasks. In this paper we fo-
cus on Europarl (Koehn, 2005)1, one of the most
popular multilingual corpora, originally designed
to provide aligned parallel text for Machine Trans-
lation (MT) systems. Extracted from the proceed-
ings of the European Parliament, the latest release
of the Europarl corpus comprises parallel text for
21 European languages, with more than 743 mil-
lion tokens overall.
</p>
<p>Apart from its prominent role in MT as a
training set, the Europarl corpus has been used
</p>
<p>1http://opus.lingfil.uu.se/Europarl.
php
</p>
<p>594</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2094">https://doi.org/10.18653/v1/P17-2094</a></div>
</div>
<div class="page"><p />
<p>for cross-lingual WSD (Lefever and Hoste, 2010,
2013), including, more recently, preposition sense
disambiguation (Gonen and Goldberg, 2016),
and widely exploited to develop cross-lingual
word embeddings (Hermann and Blunsom, 2014;
Gouws et al., 2015; Coulmance et al., 2015; Vyas
and Carpuat, 2016; Vulić and Korhonen, 2016;
Artetxe et al., 2016) as well as multi-sense embed-
dings (Ettinger et al., 2016; Šuster et al., 2016).
</p>
<p>In this paper, our aim is to augment Europarl
with sense-level information for multiple lan-
guages, thereby constructing a large-scale sense-
annotated multilingual corpus which has the po-
tential to boost both WSD and MT research.
</p>
<p>We follow an approach that has already proved
effective in a definitional setting (Camacho-
Collados et al., 2016a): unlike previous cross-
lingual approaches, we do not rely on word align-
ments against a pivot language, but instead lever-
age all languages at the same time in a joint dis-
ambiguation procedure that is subsequently re-
fined using distributional similarity. We draw
on the wide-coverage multilingual encyclopedic
dictionary of BabelNet (Navigli and Ponzetto,
2012)2, which enables us to seamlessly cover lex-
icographic and encyclopedic knowledge in multi-
ple languages within a unified sense inventory.
</p>
<p>As a result of our disambiguation pipeline we
obtain and make available to the community EU-
ROSENSE, a multilingual sense-annotated corpus
with almost 123 million sense annotations of more
than 155 thousand distinct concepts and named en-
tities drawn from the multilingual sense inventory
of BabelNet, and covering all the 21 languages of
the Europarl corpus. As such EUROSENSE consti-
tutes, to our knowledge, the largest corpus of its
kind.
</p>
<p>2 Related Work
</p>
<p>Extending sense annotations to multiple languages
is a demanding endeavor, especially when man-
ual intervention is required. Despite the fact
that sense-annotated corpora for a number of
languages have been around for more than a
decade (Petrolito and Bond, 2014), they either in-
clude few samples per word sense, or only cover
a restricted set of ambiguous words (Passonneau
et al., 2012); as a result, multilingual WSD was
until recently almost exclusively tackled using
knowledge-based approaches (Agirre et al., 2014;
</p>
<p>2http://babelnet.org
</p>
<p>Moro et al., 2014b). Nowadays, however, the rapid
development of NLP pipelines for languages other
than English has been opening up the possibili-
ties for the automatic generation of multilingual
sense-annotated data. Nevertheless, the few ap-
proaches that have been proposed so far are ei-
ther focused on treating each individual language
in isolation (Otegi et al., 2016), or limited to short
and concise definitional text (Camacho-Collados
et al., 2016a).
</p>
<p>On the other hand, the use of parallel text to
perform WSD (Ng et al., 2003; Lefever et al.,
2011; Yao et al., 2012; Bonansinga and Bond,
2016) or even Word Sense Induction (Apidianaki,
2013) has been widely explored in the literature,
and has demonstrated its effectiveness in produc-
ing high-quality sense-annotated data (Chan and
Ng, 2005). This strategy, however, requires word
alignments for each language pair to be taken into
account, with alignment errors that might prop-
agate and hamper subsequent stages unless hu-
man supervision is employed to correct erroneous
annotations (Taghipour and Ng, 2015a). More-
over, cross-language disambiguation using par-
allel text requires a language-independent anno-
tation framework that goes beyond monolingual
WordNet-like sense inventories (Lefever et al.,
2011) in order for the annotations obtained to be
used effectively within end-to-end applications.
</p>
<p>With EUROSENSE, instead, the key idea is
to exploit at best parallel sentences to provide
enriched context for a joint multilingual disam-
biguation. Using BabelNet, a unified multilingual
sense inventory, we obtain language-independent
sense annotations for a wide variety of con-
cepts and named entities, which can be seam-
lessly mapped to individual semantic resources
(e.g WordNet, Wikipedia, DBpedia) via Babel-
Net’s inter-resource mappings.
</p>
<p>3 Building EUROSENSE
</p>
<p>Following Camacho-Collados et al. (2016a), our
fully automatic disambiguation pipeline for con-
structing EUROSENSE couples a graph-based mul-
tilingual joint WSD/EL system, Babelfy (Moro
et al., 2014b)3, and a language-independent
vector representation of concepts and entities,
NASARI (Camacho-Collados et al., 2016b).4 It
comprises two stages: multilingual disambigua-
</p>
<p>3http://babelfy.org
4http://lcl.uniroma1.it/nasari
</p>
<p>595</p>
<p />
</div>
<div class="page"><p />
<p>tion (Section 3.1) and refinement based on distri-
butional similarity (Section 3.2).
</p>
<p>3.1 Stage 1: Multilingual Disambiguation
</p>
<p>As a preprocessing step, we part-of-speech tag
and lemmatize the whole corpus using TreeTag-
ger (Schmid, 1995)5. We perform disambiguation
at the sentence level. However, instead of dis-
ambiguating each sentence in isolation, language
by language, we first identify all available trans-
lations of a given sentence and then gather these
together into a single multilingual text.
</p>
<p>Then, we disambiguate this multilingual text
using Babelfy. Given that Babelfy is capable of
handling text with multiple languages at the same
time, this multilingual extension effectively in-
creases the amount of context for each sentence,
and directly helps in dealing with highly ambigu-
ous words in any particular language (as the trans-
lations of these words may be less ambiguous in
some different language). Moreover, given the
multilingual nature of our sense inventory, Ba-
belfy’s high-coherence approach favors naturally
sense assignments that are consistent across lan-
guages at the sentence level (i.e. those having
fewer distinct senses shared by more translations
of the same sentence).
</p>
<p>As a result, we obtain a full, high-coverage ver-
sion of EUROSENSE where each disambiguated
word or multi-word expression (disambiguated in-
stance) is associated with a coherence score.6
</p>
<p>3.2 Stage 2: Similarity-based Refinement
</p>
<p>In this stage we aim at improving the sense anno-
tations obtained in the previous step (Section 3.1),
with a procedure specifically targeted at correct-
ing and extending these sense annotations. In gen-
eral, graph-based WSD systems, such as Babelfy,
have been shown to be heavily biased towards the
Most Common Sense (MCS) (Calvo and Gelbukh,
2015). In order to get a handle on this bias and im-
prove our pipeline’s disambiguation accuracy we
adopt a refinement based on distributional similar-
ity, which is not affected by the MCS.
</p>
<p>To this end, we exploit the 300-dimensional
embedded representations of concepts and en-
tities of NASARI to discard or refine disam-
</p>
<p>5We rely on the internal preprocessing pipeline of Babelfy
for those languages not supported by TreeTagger.
</p>
<p>6As in Camacho-Collados et al. (2016a), coherence score
is given by the normalized number of connections of a given
concept within the sentence.
</p>
<p>biguated instances that are less semantically co-
herent. These NASARI vector representations
were constructed by combining structural and dis-
tributional knowledge from Wikipedia and Word-
Net with Word2Vec word embeddings (Mikolov
et al., 2013) trained on textual corpora.
</p>
<p>For each sentence, we first identify a subset D
of high-confidence disambiguations7 from among
those given by Babelfy in the previous step. Then,
we calculate the centroid of all the NASARI vec-
tors corresponding to the elements of D, and
we re-disambiguate the mentions associated with
the remaining low-confidence disambiguated in-
stances (i.e. those not in D), by picking, for each
mention w, the concept or entity ŝ whose NASARI
vector8 is closest to the centroid of the sentence:
</p>
<p>ŝ = argmax
s∈Sw
</p>
<p>cos
</p>
<p>(∑
d∈D ~d
|D| , ~s
</p>
<p>)
(1)
</p>
<p>where Sw is the set of all candidate senses for
mention w according to BabelNet. Cosine simi-
larity (cos) is used as similarity measure. Finally,
in order to discard less confident annotations, we
consider the cosine value associated with each re-
fined disambiguation as confidence score, and use
it to compare each disambiguated instance against
an empirically validated threshold of 0.75.
</p>
<p>As a result, we obtain the refined high-precision
version of EUROSENSE, where each disam-
biguated instance is associated with both a coher-
ence score and a distributional similarity score.
</p>
<p>4 Corpus and Statistics
</p>
<p>Table 1 reports general statistics on EUROSENSE
regarding both its high-coverage (cf. Section
3.1) and high-precision (cf. Section 3.2) ver-
sions. Joint multilingual disambiguation with Ba-
belfy generated more than 215M sense annota-
tions of 247k distinct concepts and entities, while
similarity-based refinement retained almost 123M
high-confidence instances (56.96% of the total),
covering almost 156k distinct concepts and enti-
ties. 42.40% of these retained annotations were
corrected or validated using distributional similar-
ity. As expected, the distribution over parts of
speech is skewed towards nominal senses (64.79%
before refinement and 81.79% after refinement)
</p>
<p>7We follow Camacho-Collados et al. (2016a) and consider
disambiguated instances with a coherence score above 0.125.
</p>
<p>8Given a concept or entity s we indicate with ~s its corre-
sponding NASARI vector.
</p>
<p>596</p>
<p />
</div>
<div class="page"><p />
<p>Total EN FR DE ES
</p>
<p>Full
</p>
<p># Annotations 215 877 109 26 455 574 22 214 996 16 888 108 21 486 532
Distinct lemmas covered 567 378 60 853 30 474 66 762 43 892
</p>
<p>Distinct senses covered 247 706 138 115 65 301 75 008 74 214
Average coherence score 0.19 0.19 0.18 0.18 0.18
</p>
<p>Refined
</p>
<p># Annotations 122 963 111 15 441 667 12 955 469 9 165 112 12 193 260
Distinct lemmas covered 453 063 42 947 23 603 50 681 31 980
</p>
<p>Distinct senses covered 155 904 86 881 49 189 52 425 52 859
Average coherence score 0.29 0.28 0.25 0.28 0.27
</p>
<p>Table 1: General statistics on EUROSENSE before (full) and after refinement (refined) for all the 21
languages. Language-specific figures are also reported for the 4 languages of the intrinsic evaluation.
</p>
<p>followed by verbs (19.26% and 12.22%), adjec-
tives (11.46% and 5.24%) and adverbs (4.48%
and 0.73%). We note that the average coherence
score increases from 0.19 to 0.29 after refinement,
suggesting that distributional similarity tends to
favor sense annotations that are also consistent
across different languages. Table 1 also includes
language-specific statistics on the 4 languages of
the intrinsic evaluation, where the average lexi-
cal ambiguity ranges from 1.12 senses per lemma
(German) to 2.26 (English) and, as expected, de-
creases consistently after refinement.
</p>
<p>Interestingly enough, if we consider all the 21
languages, the total number of distinct lemmas
covered is more than twice the total number of dis-
tinct senses: this is a direct consequence of hav-
ing a unified, language-independent sense inven-
tory (BabelNet), a feature that sets EUROSENSE
apart from previous multilingual sense-annotated
corpora (Otegi et al., 2016). Finally we note from
the global figures on the number of covered senses
that 109 591 senses (44.2% of the total) are not
covered by the English sense annotations: this
suggests that EUROSENSE relies heavily on multi-
linguality in integrating concepts or named entities
that are tied to specific social or cultural aspects of
a given language (and hence would be underrepre-
sented in an English-specific sense inventory).
</p>
<p>5 Experimental Evaluation
</p>
<p>We assessed the quality of EUROSENSE’s sense
annotations both intrinsically, by means of a man-
ual evaluation on four samples of randomly ex-
tracted sentences in different languages (Section
5.1), as well as extrinsically, by augmenting the
training set of a state-of-the-art supervised WSD
system (Zhong and Ng, 2010) and showing that
</p>
<p>it leads to consistent performance improvements
over two standard WSD benchmarks (Section 5.2).
</p>
<p>5.1 Intrinsic Evaluation: Annotation Quality
</p>
<p>In order to assess annotation quality directly, we
carried out a manual evaluation on 4 different lan-
guages (English, French, German and Spanish)
with 2 human judges per language. We sampled
50 random sentences across the subset of sen-
tences in EUROSENSE featuring a translation in all
4 languages, totaling 200 sentences overall.
</p>
<p>For each sentence, we evaluated all sense anno-
tations both before and after the refinement stage,
along with the sense annotations obtained by a
baseline that disambiguates each sentence in iso-
lation with Babelfy. Overall, we manually verified
a total of 5818 sense annotations across the three
configurations (1518 in English, 1564 in French,
1093 in German and 1643 in Spanish). In every
language the two judges agreed in more than 85%
of the cases, with an inter-annotator agreement in
terms of Cohen’s kappa (Cohen, 1960) above 60%
in all evaluations (67.7% on average).
</p>
<p>Results, reported in Table 2, show that joint
multilingual disambiguation improves consis-
tently over the baseline. The similarity-based re-
finement boosts precision even further, at the ex-
pense of a reduced coverage (whereas both Ba-
belfy and the baseline attempt an answer for ev-
ery disambiguation target). Over the 4 languages,
sense annotations appear to be most reliable for
German, which is consistent with its lower lexical
ambiguity on the corpus (cf. Section 4).
</p>
<p>5.2 Extrinsic Evaluation: Word Sense
Disambiguation
</p>
<p>We additionally carried out an extrinsic evalua-
tion of EUROSENSE by using its refined sense an-
</p>
<p>597</p>
<p />
</div>
<div class="page"><p />
<p>EN FR DE ES
Prec. Cov. Prec. Cov. Prec. Cov. Prec. Cov.
</p>
<p>Babelfy 76.1 100 59.1 100 80.4 100 67.5 100
EUROSENSE (full) 80.3 100 67.9 100 84.6 100 76.7 100
EUROSENSE (refined) 81.5 75.0 71.8 63.5 89.3 53.8 82.5 62.9
</p>
<p>Table 2: Precision (Prec.) and coverage (Cov.) of EUROSENSE, manually evaluated on a random sample
in 4 languages. Precision is averaged between the two judges, and coverage is computed assuming each
content word in the sense inventory to be a valid disambiguation target.
</p>
<p>notations for English as a training set for a su-
pervised all-words WSD system, It Makes Sense
(Zhong and Ng, 2010, IMS). Following Taghipour
and Ng (2015a), we started with SemCor (Miller
et al., 1993) as initial training dataset, and then
performed a subsampling of EUROSENSE up to
500 additional training examples per word sense.
We then trained IMS on this augmented training
set and tested on the two most recent standard
benchmarks for all-words WSD: the SemEval-
2013 task 12 (Navigli et al., 2013) and the
SemEval-2015 task 13 (Moro and Navigli, 2015)
test sets. As baselines we considered IMS trained
on SemCor only and OMSTI, the sense-annotated
dataset constructed by Taghipour and Ng (2015a)
which also includes SemCor. Finally, we report
the results of UKB, a knowledge-based system
(Agirre et al., 2014).9 As shown in Table 3,
IMS trained on our augmented training set con-
sistently outperforms all baseline models, show-
ing the reliability of EUROSENSE as training cor-
pus, even against sense annotations obtained semi-
automatically (Taghipour and Ng, 2015a).
</p>
<p>6 Release
</p>
<p>EUROSENSE is available at http://lcl.
uniroma1.it/eurosense. We release two
different versions of the corpus:
</p>
<p>• A high-coverage version, obtained after the
first stage of the pipeline, i.e. multilingual
joint disambiguation with Babelfy. Here,
each sense annotation is associated with a co-
herence score (cf. Section 3.1);
</p>
<p>• A high-precision version, obtained after the
similarity-based refinement with NASARI. In
this version, sense annotations are associated
</p>
<p>9We include its two implementations using the full Word-
Net graph and the disambiguated glosses of WordNet as con-
nections: default and word by word (w2w).
</p>
<p>SemEval-2013 SemEval-2015
IMSSemCor 65.3 69.3
IMSOMSTI 65.0 69.1
IMSEUROSENSE 66.4 69.5
UKB 59.0 61.2
UKBw2w 62.9 63.3
MCS 63.0 67.8
</p>
<p>Table 3: F-Score on all-words WSD.
</p>
<p>with both a coherence score and a distribu-
tional similarity score (cf. Section 3.2).
</p>
<p>7 Conclusion
</p>
<p>In this paper we presented EUROSENSE, a large
multilingual sense-annotated corpus based on Eu-
roparl, and constructed automatically via a dis-
ambiguation pipeline that exploits the interplay
between a joint multilingual disambiguation al-
gorithm and a language-independent vector-based
representation of concepts and entities. Cru-
cially, EUROSENSE relies on the wide-coverage
unified sense inventory of BabelNet, which en-
abled the disambiguation process to exploit at
best parallel text and enforces cross-language co-
herence among sense annotations. We evaluated
EUROSENSE both intrinsically and extrinsically,
showing that it provides reliable sense annotations
that improve supervised models for WSD.
</p>
<p>Acknowledgments
</p>
<p>The authors gratefully acknowl-
edge the support of the ERC Con-
solidator Grant MOUSSE Contract
No. 726487.
</p>
<p>Claudio Delli Bovi is supported by a Sapienza
Research Grant ‘Avvio alla Ricerca 2016’. Jose
Camacho-Collados is supported by a Google PhD
Fellowship in Natural Language Processing.
</p>
<p>598</p>
<p />
</div>
<div class="page"><p />
<p>References
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
</p>
<p>2014. Random Walks for Knowledge-Based Word
Sense Disambiguation. Computational Linguistics
40(1):57–84.
</p>
<p>Marianna Apidianaki. 2013. LIMSI: Cross-lingual
Word Sense Disambiguation using Translation
Sense Clustering. In Proc. of SemEval. pages 178–
182.
</p>
<p>Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning Principled Bilingual Mappings of Word
Embeddings while Preserving Monolingual Invari-
ance. In Proc. of EMNLP. pages 2289–2294.
</p>
<p>Giulia Bonansinga and Francis Bond. 2016. Multilin-
gual Sense Intersection in a Parallel Corpus with Di-
verse Language Families. In Proc. of the 8th Global
WordNet Conference. pages 44–49.
</p>
<p>Hiram Calvo and Alexander Gelbukh. 2015. Is the
most frequent sense of a word better connected in
a semantic network? In International Conference
on Intelligent Computing. Springer, pages 491–499.
</p>
<p>José Camacho-Collados, Claudio Delli Bovi, Alessan-
dro Raganato, and Roberto Navigli. 2016a. A
Large-Scale Multilingual Disambiguation of
Glosses. In Proc. of LREC. pages 1701–1708.
</p>
<p>José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016b. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artifi-
cial Intelligence 240:36–64.
</p>
<p>Yee Seng Chan and Hwee Tou Ng. 2005. Scaling Up
Word Sense Disambiguation via Parallel Texts. In
Proc. of AAAI. volume 5, pages 1037–1042.
</p>
<p>J. A. Cohen. 1960. A coefficient of agreement of nom-
inal scales. Educational and Psychological Mea-
surement 20(1):37–46.
</p>
<p>Jocelyn Coulmance, Jean-Marc Marty, Guillaume
Wenzek, and Amine Benhalloum. 2015. Trans-
gram, Fast Cross-lingual Word-embeddings. In
Proc. of EMNLP. pages 1109–1113.
</p>
<p>Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retrofitting Sense-Specific Word Vectors Us-
ing Parallel Text. In Proc. of NAACL-HLT . pages
1378–1383.
</p>
<p>Lucie Flekova and Iryna Gurevych. 2016. Supersense
Embeddings: A Unified Model for Supersense In-
terpretation, Prediction and Utilization. In Proc. of
ACL. pages 2029–2041.
</p>
<p>Hila Gonen and Yoav Goldberg. 2016. Semi Super-
vised Preposition-Sense Disambiguation using Mul-
tilingual Data. In Proc. of COLING. pages 2718–
2729.
</p>
<p>Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast Bilingual Distributed Rep-
resentations without Word Alignments. In Proc. of
ICML. pages 748–756.
</p>
<p>Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual Distributed Representations without Word
Alignment. In Proc. of ICLR. pages 1–9.
</p>
<p>Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. SensEmbed: Learning
Sense Embeddings for Word and Relational Simi-
larity. In Proc. of ACL. pages 95–105.
</p>
<p>Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2016. Embeddings for Word Sense
Disambiguation: An Evaluation Study. In Proc. of
ACL. pages 897–907.
</p>
<p>Mikael Kågebäck and Hans Salomonsson. 2016. Word
Sense Disambiguation using a Bidirectional LSTM.
In Proc. of CogALex. pages 51–56.
</p>
<p>Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Proc. of MT
summit. volume 5, pages 79–86.
</p>
<p>Els Lefever and Veronique Hoste. 2010. SemEval-
2010 Task 3: Cross-lingual Word Sense Disam-
biguation. In Proc. of SemEval. pages 15–20.
</p>
<p>Els Lefever and Véronique Hoste. 2013. SemEval-
2013 task 10: Cross-lingual Word Sense Disam-
biguation. In Proc. of SemEval. pages 158–166.
</p>
<p>Els Lefever, Véronique Hoste, and Martine De Cock.
2011. ParaSense or How to Use Parallel Corpora for
Word Sense Disambiguation. In Proc. of ACL-HLT .
pages 317–322.
</p>
<p>Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning Generic Context Em-
bedding with Bidirectional LSTM. In Proc. of
CONLL. pages 51–61.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. CoRR abs/1301.3781.
</p>
<p>George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of HLT . pages 303–308.
</p>
<p>Andrea Moro and Roberto Navigli. 2015. SemEval-
2015 Task 13: Multilingual All-Words Sense Dis-
ambiguation and Entity Linking. Proc. of SemEval
pages 288–297.
</p>
<p>Andrea Moro, Roberto Navigli, Francesco Maria
Tucci, and Rebecca J. Passonneau. 2014a. Anno-
tating the MASC Corpus with BabelNet. In Proc. of
LREC. pages 4214–4219.
</p>
<p>Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014b. Entity Linking meets Word Sense Dis-
ambiguation: a Unified Approach. Transactions
of the Association for Computational Linguistics
2:231–244.
</p>
<p>599</p>
<p />
</div>
<div class="page"><p />
<p>Roberto Navigli. 2009. Word Sense Disambiguation:
A survey. ACM Computing Surveys 41(2):1–69.
</p>
<p>Roberto Navigli, David Jurgens, and Daniele Vannella.
2013. SemEval-2013 Task 12: Multilingual Word
Sense Disambiguation. In Proc. of SemEval. pages
222–231.
</p>
<p>Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The Automatic Construction, Evaluation
and Application of a Wide-Coverage Multilingual
Semantic Network. Artificial Intelligence 193:217–
250.
</p>
<p>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In Proc. of ACL.
pages 455–462.
</p>
<p>Arantxa Otegi, Nora Aranberri, Antonio Branco, Jan
Hajic, Steven Neale, Petya Osenova, Rita Pereira,
Martin Popel, Joao Silva, Kiril Simov, and Eneko
Agirre. 2016. QTLeap WSD/NED Corpora: Se-
mantic Annotation of Parallel Corpora in Six Lan-
guages. In Proc. of LREC. pages 3023–3030.
</p>
<p>Rebecca J. Passonneau, Collin Baker, Christiane Fell-
baum, and Nancy Ide. 2012. The MASC Word
Sense Sentence Corpus. In Proc. of LREC. pages
3025–3030.
</p>
<p>Tommaso Petrolito and Francis Bond. 2014. A Survey
of WordNet Annotated Corpora. In Proc. of the 7th
Global WordNet Conference. pages 236–245.
</p>
<p>Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A large-scale pseudoword-based evaluation frame-
work for state-of-the-art Word Sense Disambigua-
tion. Computational Linguistics 40(4):837–881.
</p>
<p>Alessandro Raganato, Claudio Delli Bovi, and Roberto
Navigli. 2016. Automatic Construction and Evalu-
ation of a Large Semantically Enriched Wikipedia.
In Proc. of IJCAI. pages 2894–2900.
</p>
<p>Alessandro Raganato, Jose Camacho-Collados, and
Roberto Navigli. 2017. Word Sense Disambigua-
tion: A Unified Evaluation Framework and Empiri-
cal Comparison. In Proc. of EACL. pages 99–110.
</p>
<p>Delip Rao, Paul McNamee, and Mark Dredze. 2013.
Entity Linking: Finding extracted entities in a
knowledge base. Multi-Source, Multilingual Infor-
mation Extraction and Summarization 11:93–115.
</p>
<p>Helmut Schmid. 1995. Improvements In Part-of-
Speech Tagging With an Application To German. In
Proc. of the ACL SIGDAT-Workshop. pages 47–50.
</p>
<p>Lenhart Schubert. 2006. Turing’s Dream and the
Knowledge Challenge. In Proc. of AAAI. pages
1534–1538.
</p>
<p>Federico Scozzafava, Alessandro Raganato, Andrea
Moro, and Roberto Navigli. 2015. Automatic iden-
tification and disambiguation of concepts and named
entities in the multilingual Wikipedia. In AI*IA,
Springer, pages 357–366.
</p>
<p>Simon Šuster, Ivan Titov, and Gertjan van Noord. 2016.
Bilingual Learning of Multi-sense Embeddings with
Discrete Autoencoders. In Proc. of NAACL-HLT .
pages 1346–1356.
</p>
<p>Kaveh Taghipour and Hwee Tou Ng. 2015a. One Mil-
lion Sense-Tagged Instances for Word Sense Disam-
biguation and Induction. In Proc. of CoNLL. pages
338–344.
</p>
<p>Kaveh Taghipour and Hwee Tou Ng. 2015b. Semi-
Supervised Word Sense Disambiguation Using
Word Embeddings in General and Specific Domains.
Proc. of NAACL-HLT pages 314–323.
</p>
<p>Jörg Tiedemann. 2012. Parallel Data, Tools and In-
terfaces in OPUS. In Proc. of LREC. pages 2214–
2218.
</p>
<p>Ivan Vulić and Anna Korhonen. 2016. On the Role of
Seed Lexicons in Learning Bilingual Word Embed-
dings. In Proc. of ACL. pages 247–257.
</p>
<p>Yogarshi Vyas and Marine Carpuat. 2016. Sparse
Bilingual Word Representations for Cross-lingual
Lexical Entailment. In Proc. of NAACL-HLT . pages
1187–1197.
</p>
<p>Dirk Weissenborn, Leonhard Hennig, Feiyu Xu, and
Hans Uszkoreit. 2015. Multi-Objective Optimiza-
tion for the Joint Disambiguation of Nouns and
Named Entities. In Proc. of ACL. pages 596–605.
</p>
<p>Xuchen Yao, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Expectations of Word Sense
in Parallel Corpora. In Proc. of NAACL-HLT . pages
621–625.
</p>
<p>Dayu Yuan, Julian Richardson, Ryan Doherty, Colin
Evans, and Eric Altendorf. 2016. Semi-supervised
Word Sense Disambiguation with Neural Models.
In Proc. of COLING. pages 1374–1385.
</p>
<p>Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A Wide-Coverage Word Sense Disambiguation Sys-
tem for Free Text. In Proc. of ACL System Demon-
strations. pages 78–83.
</p>
<p>600</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601–607
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2095
</p>
<p>Challenging Language-Dependent Segmentation for Arabic:
An Application to Machine Translation and Part-of-Speech Tagging
</p>
<p>Hassan Sajjad Fahim Dalvi Nadir Durrani Ahmed Abdelali
Yonatan Belinkov∗ Stephan Vogel
</p>
<p>Qatar Computing Research Institute – HBKU, Doha, Qatar
{hsajjad, faimaduddin, ndurrani, aabdelali, svogel}@qf.org.qa
</p>
<p>∗MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA
belinkov@mit.edu
</p>
<p>Abstract
</p>
<p>Word segmentation plays a pivotal role in
improving any Arabic NLP application.
Therefore, a lot of research has been spent
in improving its accuracy. Off-the-shelf
tools, however, are: i) complicated to use
and ii) domain/dialect dependent. We ex-
plore three language-independent alterna-
tives to morphological segmentation us-
ing: i) data-driven sub-word units, ii)
characters as a unit of learning, and iii)
word embeddings learned using a charac-
ter CNN (Convolution Neural Network).
On the tasks of Machine Translation and
POS tagging, we found these methods to
achieve close to, and occasionally surpass
state-of-the-art performance. In our anal-
ysis, we show that a neural machine trans-
lation system is sensitive to the ratio of
source and target tokens, and a ratio close
to 1 or greater, gives optimal performance.
</p>
<p>1 Introduction
</p>
<p>Arabic word segmentation has shown to signifi-
cantly improve output quality in NLP tasks such
as machine translation (Habash and Sadat, 2006;
Almahairi et al., 2016), part-of-speech tagging
(Diab et al., 2004; Habash and Rambow, 2005),
and information retrieval (M. Aljlayl and Gross-
man, 2002). A considerable amount of research
has therefore been spent on Arabic morphologi-
cal segmentation in the past two decades, rang-
ing from rule-based analyzers (Beesley, 1996) to
state-of-the-art statistical segmenters (Pasha et al.,
2014; Abdelali et al., 2016; Khalifa et al., 2016).
Morphological segmentation splits words into
morphemes. For example, ‘‘wktAbnA” “ A 	JK. A�J»ð”
(gloss: and our book) is decomposed into its stem
and affixes as: “w+ ktAb +nA” “ A 	K+ H. A�J» + ð”.
</p>
<p>Despite the gains obtained from using morpho-
logical segmentation, there are several caveats to
using these tools. Firstly, they make the train-
ing pipeline cumbersome, as they come with
complicated pre-processing (and additional post-
processing in the case of English-to-Arabic trans-
lation (El Kholy and Habash, 2012)). More impor-
tantly, these tools are dialect- and domain-specific.
A segmenter trained for modern standard Arabic
(MSA) performs significantly worse on dialectal
Arabic (Habash et al., 2013), or when it is applied
to a new domain.
</p>
<p>In this work, we explore whether we can avoid
the language-dependent pre/post-processing com-
ponents and learn segmentation directly from the
training data being used for a given task. We in-
vestigate data-driven alternatives to morphologi-
cal segmentation using i) unsupervised sub-word
units obtained using byte-pair encoding (Sennrich
et al., 2016), ii) purely character-based segmen-
tation (Ling et al., 2015), and iii) a convolutional
neural network over characters (Kim et al., 2016).
</p>
<p>We evaluate these techniques on the tasks
of machine translation (MT) and part-of-speech
(POS) tagging and compare them against mor-
phological segmenters MADAMIRA (Pasha et al.,
2014) and Farasa (Abdelali et al., 2016). On
the MT task, byte-pair encoding (BPE) performs
the best among the three methods, achieving very
similar performance to morphological segmenta-
tion in the Arabic-to-English direction and slightly
worse in the other direction. Character-based
methods, in comparison, perform better on the
task of POS tagging, reaching an accuracy of
95.9%, only 1.3% worse than morphological seg-
mentation. We also analyze the effect of segmen-
tation granularity of Arabic on the quality of MT.
We observed that a neural MT (NMT) system is
sensitive to source/target token ratio and performs
best when this ratio is close to or greater than 1.
</p>
<p>601</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2095">https://doi.org/10.18653/v1/P17-2095</a></div>
</div>
<div class="page"><p />
<p>2 Segmentation Approaches
</p>
<p>We experimented with three data-driven segmen-
tation schemes: i) morphological segmentation, ii)
sub-word segmentation based on BPE, and iii) two
variants of character-based segmentation. We first
map each source word to its corresponding seg-
ments (depending on the segmentation scheme),
embed all segments of a word in vector space
and feed them one-by-one to an encoder-decoder
model. See Figure 1 for illustration.
</p>
<p>2.1 Morphological Segmentation
</p>
<p>There is a vast amount of work on statistical seg-
mentation for Arabic. Here we use the state-
of-the-art Arabic segmenter MADAMIRA and
Farasa as our baselines. MADAMIRA involves
a morphological analyzer that generates a list of
possible word-level analyses (independent of con-
text). The analyses are provided with the original
text to a Feature Modeling component that
applies an SVM and a language model to make
predictions, which are scored by an Analysis
Ranking component. Farasa on the other hand
is a light weight segmenter, which ignores context
and instead uses a variety of features and lexicons
for segmentation.
</p>
<p>2.2 Data Driven Sub-word Units
</p>
<p>A number of data-driven approaches have been
proposed that learn to segment words into smaller
units from data (Demberg, 2007; Sami Virpioja
and Kurimo, 2013) and shown to improve phrase-
based MT (Fishel and Kirik, 2010; Stallard et al.,
2012). Recently, with the advent of neural MT,
a few sub-word-based techniques have been pro-
posed that segment words into smaller units to
tackle the limited vocabulary and unknown word
problems (Sennrich et al., 2016; Wu et al., 2016).
</p>
<p>In this work, we explore Byte-Pair Encoding
(BPE), a data compression algorithm (Gage, 1994)
as an alternative to morphological segmentation
of Arabic. BPE splits words into symbols (a se-
quence of characters) and then iteratively replaces
the most frequent symbols with their merged vari-
ants. In essence, frequent character n-gram se-
quences will be merged to form one symbol.
The number of merge operations is controlled by
a hyper-parameter OP which directly affects the
granularity of segmentation: a high value of OP
means coarse segmentation and a low value means
fine-grained segmentation.
</p>
<p>Figure 1: Segmentation approaches for the word
“b$rhm” “ÑëQå��.”; the blue vectors indicate the
embedding(s) used before the encoding layer.
</p>
<p>2.3 Character-level Encoding
Character-based models have been found to be
effective in translating closely related language
pairs (Durrani et al., 2010; Nakov and Tiedemann,
2012) and OOV words (Durrani et al., 2014). Ling
et al. (2016) used character embeddings to address
the OOV word problem. We explored them as an
alternative to morphological segmentation. Their
advantage is that character embeddings do not re-
quire any complicated pre- and post-processing
step other than segmenting words into characters.
The fully character-level encoder treats the source
sentence as a sequence of letters, encoding each
letter (including white-space) in the LSTM en-
coder (see Figure 1). The decoding may follow
identical settings. We restricted the character-level
representation to the Arabic side of the parallel
corpus and use words for the English side.
</p>
<p>Character-CNN Kim et al. (2016) presented a
neural language model that takes character-level
input and learns word embeddings using a CNN
over characters. The embedding are then pro-
vided to the encoder as input. The intuition is
that the character-based word embedding should
be able to learn the morphological phenomena
a word inherits. Compared to fully character-
level encoding, the encoder gets word-level em-
beddings as in the case of unsegmented words
(see Figure 1). However, the word embedding
is intuitively richer than the embedding learned
over unsegmented words because of the convolu-
tion over characters. The method was previously
shown to help neural MT (Belinkov and Glass,
2016; Costa-jussà and Fonollosa, 2016). Belinkov
et al. (2017) also showed character-based repre-
sentations learned using a CNN to be superior, at
learning word morphology, than their word-based
counter-parts. However, they did not compare
these against BPE-based segmentation. We use
character-CNN to aid Arabic word segmentation.
</p>
<p>602</p>
<p />
</div>
<div class="page"><p />
<p>Arabic-to-English English-to-Arabic
# SEG tst11 tst12 tst13 tst14 AVG. tst11 tst12 tst13 tst14 AVG.
</p>
<p>UNSEG 25.7 28.2 27.3 23.9 26.3 15.8 17.1 18.1 15.5 16.6
</p>
<p>MORPH 29.2 33.0 32.9 28.3 30.9 16.5 18.8 20.4 17.2 18.2
cCNN 29.0 32.0 32.5 28.0 30.3 14.3 12.8 13.6 12.6 13.3
CHAR 28.8 31.8 32.5 27.8 30.2 15.3 17.1 18.0 15.3 16.4
BPE 29.7 32.5 33.6 28.4 31.1 17.5 18.0 20.0 16.6 18.0
</p>
<p>Table 1: Results of comparing several segmentation strategies.
</p>
<p>3 Experiments
</p>
<p>In the following, we describe the data and system
settings and later present the results of machine
translation and POS tagging.
</p>
<p>3.1 Settings
</p>
<p>Data The MT systems were trained on 1.2 Mil-
lion sentences, a concatenation of TED corpus
(Cettolo et al., 2012), LDC NEWS data, QED
(Guzmán et al., 2013) and an MML-filtered (Axel-
rod et al., 2011) UN corpus.1 We used dev+test10
for tuning and tst11-14 for testing. For English-
Arabic, outputs were detokenized using MADA
detokenizer. Before scoring the output, we nor-
malized them and reference translations using the
QCRI normalizer (Sajjad et al., 2013).
</p>
<p>POS tagging We used parts 2-3 (v3.1-2) of the
Arabic Treebank (Mohamed Maamouri, 2010).
The data consists of 18268 sentences (483,909
words). We used 80% for training, 5% for devel-
opment and the remaining for test.
</p>
<p>Segmentation MADAMIRA and Farasa nor-
malize the data before segmentation. In order to
have consistent data, we normalize it for all seg-
mentation approaches. For BPE, we tuned the
value of merge operations OP and found 30k and
90k to be optimal for Ar-to-En and En-to-Ar re-
spectively. In case of no segmentation (UNSEG)
and character-CNN (cCNN), we tokenized the
Arabic with the standard Moses tokenizer, which
separates punctuation marks. For character-level
encoding (CHAR), we preserved word boundaries
by replacing space with a special symbol and then
separated every character with a space. English-
side is tokenized/truecased using Moses scripts.
</p>
<p>Neural MT Settings We used the seq2seq-
attn (Kim, 2016) implementation, with 2 layers of
</p>
<p>1We used 3.75% as reported to be optimal filtering thresh-
old in (Durrani et al., 2016).
</p>
<p>LSTM in the (bidirectional) encoder and the de-
coder, with a size of 500. We limit the sentence
length to 100 for MORPH, UNSEG, BPE, cCNN,
and 500 for CHAR experiments. The source and
target vocabularies are limited to 50k each.
</p>
<p>3.2 Machine Translation Results
Table 1 presents MT results using various segmen-
tation strategies. Compared to the UNSEG system,
the MORPH system2 improved translation quality
by 4.6 and 1.6 BLEU points in Ar-to-En and En-
to-Ar systems, respectively. The results also im-
proved by up to 3 BLEU points for cCNN and
CHAR systems in the Ar-to-En direction. How-
ever, the performance is lower by at least 0.6
BLEU points compared to the MORPH system.
</p>
<p>In the En-to-Ar direction, where cCNN and
CHAR are applied on the target side, the perfor-
mance dropped significantly. In the case of CHAR,
mapping one source word to many target char-
acters makes it harder for NMT to learn a good
model. This is in line with our finding on using
a lower value of OP for BPE segmentation (see
paragraph Analyzing the effect of OP). Surpris-
ingly, the cCNN system results were inferior to the
UNSEG system for En-to-Ar. A possible explana-
tion is that the decoder’s predictions are still done
at word level even when using the cCNN model
(which encodes the target input during training
but not the output). In practice, this can lead to
generating unknown words. Indeed, in the Ar-to-
En case cCNN significantly reduces the unknown
words in the test sets, while in the En-to-Ar case
the number of unknown words remains roughly
the same between UNSEG and cCNN.
</p>
<p>The BPE system outperformed all other systems
in the Ar-to-En direction and is lower than MORPH
by only 0.2 BLEU points in the opposite direction.
This shows that machine translation involving the
</p>
<p>2Farasa performed better in the Ar-to-En experiments and
MADAMIRA performed better in the En-to-Ar direction. We
used best results as our baselines for comparison and call
them MORPH.
</p>
<p>603</p>
<p />
</div>
<div class="page"><p />
<p>Arabic language can achieve competitive results
with data-driven segmentation. This comes with
an additional benefit of language-independent pre-
processing and post-processing pipeline. In an
attempt to find, whether the gains obtained from
data-driven segmentation techniques and morpho-
logical segmentation are additive, we applied BPE
to morphological segmented data. We saw further
improvement of up to 1 BLEU point by using the
two segmentations in tandem.
</p>
<p>Analyzing the effect of OP: The unsegmented
training data consists of 23M Arabic tokens and
28M English tokens. The parameter OP decides
the granularity of segmentation: a higher value
of OP means fewer segments. For example, at
OP=50k, the number of Arabic tokens is greater by
7% compared to OP=90k. We tested four differ-
ent values of OP (15k, 30k, 50k, and 90k). Figure
2 summarizes our findings on test-2011 dataset,
where x-axis presents the ratio of source to tar-
get language tokens and y-axis shows the BLEU
score. The boundary values for segmentation are
character-level segmentation (OP=0) and unseg-
mented text (OP=N ).3 For both language direc-
tions, we observed that a source to target token ra-
tio close to 1 and greater works best provided that
the boundary conditions (unsegmented Arabic and
character-level segmentation) are avoided. In the
En-to-Ar direction, the system improves for coarse
segmentation whereas in the Ar-to-En direction,
a much finer-grained segmentation of Arabic per-
formed better. This is in line with the ratio of to-
kens generated using the MORPH systems (Ar-to-
En ratio = 1.02). Generalizing from the perspec-
tive of neural MT, the system learns better when
total numbers of source and target tokens are close
to each other. The system shows better tolerance
towards modeling many source words to a few tar-
get words compared to the other way around.
</p>
<p>Discussion: Though BPE performed well for
machine translation, there are a few reservations
that we would like to discuss here. Since the
main goal of the algorithm is to compress data
and segmentation comes as a by-product, it often
produces different segmentations of a root word
when occurred in different morphological forms.
For example, the words driven and driving are seg-
mented as driv en and drivi ng respectively. This
adds ambiguity to the data and may result in un-
</p>
<p>3N is the number of types in the unsegmented corpus.
</p>
<p>Figure 2: Source/Target token ratio with varying
OP versus BLEU. Character and unsegmented sys-
tems can be seen as BPE with OP=0 and OP=N .
</p>
<p>expected translation errors. Another limitation of
BPE is that at test time, it may divide the unknown
words to semantically different known sub-word
units which can result in a semantically wrong
translation. For example, the word “Q¢�̄” is un-
known to our vocabulary. BPE segmented it into
known units which ended up being translated to
courage. One possible solution to this problem is;
at test time, BPE is applied to those words only
which were known to the full vocabulary of the
training corpus. In this way, the sub-word units
created by BPE for the word are already seen in
a similar context during training and the model
has learned to translate them correctly. The down-
side of this method is that it limits BPE’s power to
segment unknown words to their correct sub-word
units and outputs them as UNK in translation.
</p>
<p>3.3 Part of Speech Tagging
</p>
<p>We also experimented with the aforementioned
segmentation strategies for the task of Arabic
POS tagging. Probabilistic taggers like HMM-
based (Brants, 2000) and sequence learning mod-
els like CRF (Lafferty et al., 2001) consider pre-
vious words and/or tags to predict the tag of
the current word. We mimic a similar setting
but in a sequence-to-sequence learning frame-
work. Figure 3 describes a step by step procedure
to train a neural encoder-decoder tagger. Con-
sider an Arabic phrase “klm &gt;SdqA}k b$rhm”
“ÑëQå��. ½KA�̄Y
</p>
<p>
@ ÕÎ¿” (gloss: call your friends
</p>
<p>give them the good news), we want to learn the tag
</p>
<p>604</p>
<p />
</div>
<div class="page"><p />
<p>Figure 3: Seq-to-Seq POS Tagger: The number of
segments and the embeddings depend on the seg-
mentation scheme used (See Figure 1).
</p>
<p>of the word “ÑëQå��.” using the context of the pre-
vious two words and their tags. First, we segment
the phrase using a segmentation approach (step 1)
and then add POS tags to context words (step 2).
The entire sequence with the words and tags is
fed to the sequence-to-sequence framework. The
embeddings (for both words and tags) are learned
jointly with other parameters in an end-to-end
fashion, and optimized on the target tag sequence;
for example, “NOUN PRON” in this case.
</p>
<p>For a given word wi in a sentence s =
{w1, w2, ..., wM} and its POS tag ti, We formu-
late the neural TAGGER as follows:
</p>
<p>SEGMENTER(τ) : ∀wi 7→ Si
TAGGER : Si−2 Si−1 Si 7→ ti
</p>
<p>where Si is the segmentation of word wi. In case
of UNSEG and cCNN, Si would be same as wi.
SEGMENTER here is identical to the one described
in Figure 1. TAGGER is a NMT architecture that
learns to predict a POS tag of a segmented/unseg-
mented word given previous two words.4
</p>
<p>Table 2 summarizes the results. The MORPH
system performed best with an improvement of
5.3% over UNSEG. Among the data-driven meth-
ods, CHAR model performed best and was behind
MORPH by only 0.3%. Even though BPE was infe-
rior compared to other methods, it was still better
than UNSEG by 4%.5
</p>
<p>Analysis of POS outputs We performed a
comparative error analysis of predictions made
</p>
<p>4We also tried using previous words with their POS tags
as context but did not see any significant difference in the end
result.
</p>
<p>5Optimizing the parameter OP did not yield any difference
in accuracy. We used 10k operations.
</p>
<p>SEG UNSEG MORPH CHAR cCNN BPE
</p>
<p>ACC 90.9 96.2 95.9 95.8 94.9
</p>
<p>Table 2: POS tagging with various segmentations
</p>
<p>through MORPH, CHAR and BPE based segmen-
tations. MORPH and CHAR observed very similar
error patterns, with most confusion between For-
eign and Particle tags. In addition to this confu-
sion, BPE had relatively scattered errors. It had
lower precision in predicting nouns and had con-
fused them with adverbs, foreign words and adjec-
tives. This is expected, since most nouns are out-
of-vocabulary terms, and therefore get segmented
by BPE into smaller, possibly known fragments,
which then get confused with other tags. However,
since the accuracies are quite close, the overall er-
rors are very few and similar between the various
systems. We also analyzed the number of tags that
are output by the sequence-to-sequence model us-
ing various segmentation schemes. In 99.95% of
the cases, the system learned to output the correct
number of tags, regardless of the number of source
segments.
</p>
<p>4 Conclusion
</p>
<p>We explored several alternatives to language-
dependent segmentation of Arabic and evaluated
them on the tasks of machine translation and POS
tagging. On the machine translation task, BPE
segmentation produced the best results and even
outperformed the state-of-the-art morphological
segmentation in the Arabic-to-English direction.
On the POS tagging task, character-based models
got closest to using the state-of-the-art segmen-
tation. Our results showed that data-driven seg-
mentation schemes can serve as an alternative to
heavily engineered language-dependent tools and
achieve very competitive results. In our analy-
sis we showed that NMT performs better when
the source to target token ratio is close to one or
greater.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank the three anonymous re-
viewers for their useful suggestions. This re-
search was carried out in collaboration between
the HBKU Qatar Computing Research Institute
(QCRI) and the MIT Computer Science and Ar-
tificial Intelligence Laboratory (CSAIL).
</p>
<p>605</p>
<p />
</div>
<div class="page"><p />
<p>References
Ahmed Abdelali, Kareem Darwish, Nadir Durrani, and
</p>
<p>Hamdy Mubarak. 2016. Farasa: A fast and furious
segmenter for arabic. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Demon-
strations. San Diego, California.
</p>
<p>Amjad Almahairi, Cho Kyunghyun, Nizar Habash,
and Aaron Courville. 2016. First result
on Arabic neural machine translation. In
https://arxiv.org/abs/1606.02680.
</p>
<p>Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Edinburgh, United Kingdom, EMNLP ’11.
</p>
<p>Kenneth R Beesley. 1996. Arabic finite-state morpho-
logical analysis and generation. In ACL. pages 89–
94.
</p>
<p>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-
san Sajjad, and James Glass. 2017. What do Neural
Machine Translation Models Learn about Morphol-
ogy? In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, Vancouver,
Canada.
</p>
<p>Yonatan Belinkov and James Glass. 2016. Arabic and
hebrew: Available corpora and initial results. In
Proceedings of the Workshop on Semitic Machine
Translation.
</p>
<p>Thorsten Brants. 2000. Tnt: A statistical part-
of-speech tagger. In Proceedings of the
Sixth Conference on Applied Natural Lan-
guage Processing. ANLC ’00, pages 224–231.
https://doi.org/10.3115/974147.974178.
</p>
<p>Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16th
Conference of the European Association for Ma-
chine Translation (EAMT). Trento, Italy, pages 261–
268.
</p>
<p>Marta R. Costa-jussà and José A. R. Fonollosa. 2016.
Character-based neural machine translation. In
Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 2: Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 357–361.
http://anthology.aclweb.org/P16-2058.
</p>
<p>Vera Demberg. 2007. A language independent unsu-
pervised model for morphological segmentation. In
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics.
</p>
<p>Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic tagging of arabic text: From raw
</p>
<p>text to base phrase chunks. In Proceedings of HLT-
NAACL 2004: Short Papers. Stroudsburg, PA, USA,
HLT-NAACL-Short ’04.
</p>
<p>Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and
Stephan Vogel. 2016. QCRI machine translation
systems for IWSLT 16. In Proceedings of the 15th
International Workshop on Spoken Language Trans-
lation. Seattle, WA, USA, IWSLT ’16.
</p>
<p>Nadir Durrani, Hassan Sajjad, Alexander Fraser, and
Helmut Schmid. 2010. Hindi-to-Urdu Machine
Translation through Transliteration. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics. Association for Compu-
tational Linguistics, Uppsala, Sweden, pages 465–
474. http://www.aclweb.org/anthology/P10-1048.
</p>
<p>Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an unsupervised translit-
eration model into statistical machine translation. In
Proceedings of the 15th Conference of the European
Chapter of the ACL (EACL 2014). Gothenburg, Swe-
den.
</p>
<p>Ahmed El Kholy and Nizar Habash. 2012. Ortho-
graphic and morphological processing for English–
Arabic statistical machine translation. Machine
Translation 26(1-2).
</p>
<p>Mark Fishel and Harri Kirik. 2010. Linguistically
motivated unsupervised segmentation for machine
translation. In In Proceedings of the seventh In-
ternational Conference on Language Resources and
Evaluation (LREC.
</p>
<p>Philip Gage. 1994. A new algorithm for
data compression. C Users J. 12(2):23–38.
http://dl.acm.org/citation.cfm?id=177910.177914.
</p>
<p>Francisco Guzmán, Hassan Sajjad, Stephan Vogel,
and Ahmed Abdelali. 2013. The AMARA corpus:
Building resources for translating the web’s educa-
tional content. In Proceedings of the 10th Interna-
tional Workshop on Spoken Language Technology
(IWSLT-13).
</p>
<p>Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics. Stroudsburg, PA, USA, ACL
’05.
</p>
<p>Nizar Habash, Ryan Roth, Owen Rambow, Ramy Esk,
and Nadi Tomeh. 2013. Morphological analysis and
disambiguation for dialectal arabic. In In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT).
</p>
<p>Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter of
</p>
<p>606</p>
<p />
</div>
<div class="page"><p />
<p>the Association for Computational Linguistics (HLT-
NAACL’06). New York, NY, USA.
</p>
<p>Salam Khalifa, Nasser Zalmout, and Nizar Habash.
2016. Yamama: Yet another multi-dialect arabic
morphological analyzer. Proceedings of COLING
2016, the 26th International Conference on Compu-
tational Linguistics: System Demonstrations pages
223–227.
</p>
<p>Yoon Kim. 2016. Seq2seq-attn. https://
github.com/harvardnlp/seq2seq-attn.
</p>
<p>Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der Rush. 2016. Character-Aware Neural Language
Models. In AAAI Conference on Artificial Intelli-
gence.
</p>
<p>John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning. ICML
’01, pages 282–289.
</p>
<p>Wang Ling, Isabel Trancoso, Chris Dyer, and
Alan W. Black. 2015. Character-based neu-
ral machine translation. CoRR abs/1511.04586.
http://arxiv.org/abs/1511.04586.
</p>
<p>Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W.
Black. 2016. Character-based neural machine trans-
lation. arXiv preprint arXiv:1511.04586 .
</p>
<p>O. Frieder M. Aljlayl and D. Grossman. 2002.
On Arabic-English cross-language information re-
trieval: A machine translation approach. In
IEEE Third International Conference on Informa-
tion Technology: Coding and Computing (ITCC).
</p>
<p>Ann Bies Seth Kulick Sondos Krouna Fatma Gaddeche
Wajdi Zaghouani Mohamed Maamouri. 2010. Ara-
bic treebank: Part 3 v 3.2 ldc2010t08. web down-
load. Philadelphia: Linguistic Data Consortium .
</p>
<p>Preslav Nakov and Jörg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers). Jeju, Korea, ACL ’12, pages 301–
305.
</p>
<p>Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan M
Roth. 2014. MADAMIRA: A fast, comprehensive
tool for morphological analysis and disambiguation
of Arabic. In Proceedings of the Language Re-
sources and Evaluation Conference. Reykjavik, Ice-
land, LREC ’14, pages 1094–1101.
</p>
<p>Hassan Sajjad, Francisco Guzmn, Preslav Nakov,
Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli,
and Stephan Vogel. 2013. QCRI at IWSLT 2013:
Experiments in Arabic-English and English-Arabic
</p>
<p>spoken language translation. In Proceedings of the
10th International Workshop on Spoken Language
Technology (IWSLT-13).
</p>
<p>Peter Smit Stig-Arne Grnroos Sami Virpioja and
Mikko Kurimo. 2013. Morfessor 2.0: Python imple-
mentation and extensions for morfessor baseline. In
Technical Report, Aalto University publication se-
ries SCIENCE + TECHNOLOGY .
</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Berlin, Germany.
</p>
<p>David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012. Unsu-
pervised morphology rivals supervised morphology
for arabic mt. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers - Volume 2. ACL ’12.
</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, ukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR
abs/1609.08144. http://arxiv.org/abs/1609.08144.
</p>
<p>607</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 608–615
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2096
</p>
<p>Fast and Accurate Neural Word Segmentation for Chinese
</p>
<p>Deng Cai1,2, Hai Zhao1,2,∗, Zhisong Zhang1,2, Yuan Xin3, Yongjian Wu3, Feiyue Huang3
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
</p>
<p>2Key Lab of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong Univeristy, Shanghai, China
</p>
<p>thisisjcykcd@gmail.com, {zhaohai@cs, zzs2011@}sjtu.edu.cn
3Tencent Youtu Lab, Shanghai, China
</p>
<p>{macxin,littlekenwu,garyhuang}@tencent.com
</p>
<p>Abstract
</p>
<p>Neural models with minimal feature en-
gineering have achieved competitive per-
formance against traditional methods for
the task of Chinese word segmentation.
However, both training and working pro-
cedures of the current neural models are
computationally inefficient. This paper
presents a greedy neural word segmenter
with balanced word and character embed-
ding inputs to alleviate the existing draw-
backs. Our segmenter is truly end-to-
end, capable of performing segmentation
much faster and even more accurate than
state-of-the-art neural models on Chinese
benchmark datasets.
</p>
<p>1 Introduction
</p>
<p>Word segmentation is a fundamental task for pro-
cessing most east Asian languages, typically Chi-
nese. Almost all practical Chinese processing ap-
plications essentially rely on Chinese word seg-
mentation (CWS), e.g., (Zhao et al., 2017).
</p>
<p>Since (Xue, 2003), most methods formalize this
task as a sequence labeling problem. In a su-
pervised learning fashion, sequence labeling may
adopt various models such as Maximum Entropy
(ME) (Low et al., 2005) and Conditional Random
Fields (CRF) (Lafferty et al., 2001; Peng et al.,
2004). However, these models rely heavily on
hand-crafted features.
</p>
<p>∗Corresponding author. This paper was partially sup-
ported by Cai Yuanpei Program (CSC No. 201304490199
and No. 201304490171), National Natural Science Foun-
dation of China (No. 61170114, No. 61672343 and No.
61272248), National Basic Research Program of China (No.
2013CB329401), Major Basic Research Program of Shang-
hai Science and Technology Committee (No. 15JC1400103),
Art and Science Interdisciplinary Funds of Shanghai Jiao
Tong University (No. 14JCRZ04), Key Project of National
Society Science Foundation of China (No. 15-ZDA041), and
the joint research project with Youtu Lab of Tencent.
</p>
<p>To minimize the efforts in feature engineering,
neural word segmentation has been actively stud-
ied recently. Zheng et al. (2013) first adapted
the sliding-window based sequence labeling (Col-
lobert et al., 2011) with character embeddings
as input. A number of other researchers have
attempted to improve the segmenter of (Zheng
et al., 2013) by augmenting it with additional com-
plexity. Pei et al. (2014) introduced tag embed-
dings. Chen et al. (2015a) proposed to model n-
gram features via a gated recursive neural network
(GRNN). Chen et al. (2015b) used a Long short-
term memory network (LSTM) (Hochreiter and
Schmidhuber, 1997) to capture long-distance con-
text. Xu and Sun (2016) integrated both GRNN
and LSTM for deeper feature extraction.
</p>
<p>Besides sequence labeling schemes, Zhang
et al. (2016) proposed a transition-based frame-
work. Liu et al. (2016) used a zero-order semi-
CRF based model. However, these two models
rely on either traditional discrete features or non-
neural-network components for performance en-
hancement, their performance drops rapidly when
solely depending on neural models. Most closely
related to this work, Cai and Zhao (2016) proposed
to score candidate segmented outputs directly, em-
ploying a gated combination neural network over
characters for word representation generation and
an LSTM scoring model for segmentation result
evaluation.
</p>
<p>Despite the active progress of most existing
works in terms of accuracy, their computational
needs have been significantly increased to the ex-
tent that training a neural segmenter usually takes
days even using cutting-edge hardwares. Mean-
while, different applications often require diverse
segmenters and offer large-scale incoming data.
The efficiency of a word segmenter either for
training and decoding is crucial in practice. In
this paper, we propose a simple yet accurate neu-
</p>
<p>608</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2096">https://doi.org/10.18653/v1/P17-2096</a></div>
</div>
<div class="page"><p />
<p>ral word segmenter who searches greedily during
both training and working to overcome the exist-
ing efficiency obstacle. Our evaluation will be per-
formed on Chinese benchmark datasets.
</p>
<p>2 Related Work
</p>
<p>Statistical Chinese word segmentation has been
studied for decades (Huang and Zhao, 2007).
(Xue, 2003) was the first to cast it as a character-
based tagging problem. Peng et al. (2004) showed
CRF based model is particularly effective to solve
CWS in the sequence labeling fashion. This
method has been followed by most later seg-
menters (Tseng et al., 2005; Zhao et al., 2006;
Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al.,
2012; Zhang et al., 2013). The same spirit has
also be followed by most neural models (Zheng
et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen
et al., 2015a,b; Ma and Hinrichs, 2015; Xu and
Sun, 2016).
</p>
<p>Word based CWS to conveniently incorporate
complete word features has also be explored. An-
drew (2006) proposed a semi-CRF model. Zhang
and Clark (2007, 2011) used a perceptron algo-
rithm with inexact search. Both of them have
been followed by neural model versions (Liu et al.,
2016) and (Zhang et al., 2016) respectively. There
are also works integrating both character-based
and word-based segmenters (Huang and Zhao,
2006; Sun, 2010; Wang et al., 2014) and semi-
supervised learning (Zhao and Kit, 2008b, 2011;
Zeng et al., 2013; Zhang et al., 2013).
</p>
<p>Unlike most previous works, which extract fea-
tures within a fixed sized sliding window, Cai
and Zhao (2016) proposed a direct segmentation
framework that extends the feature window to
cover complete input and segmentation history
and uses beam search for decoding. In this work,
we will make a series of significant improvement
over the basic framework and especially adopt
greedy search instead.
</p>
<p>Another notable exception of embedding based
methods is (Ma and Hinrichs, 2015), which used
character-specified tags matching for fast decod-
ing and resulted in a character-based greedy seg-
menter.
</p>
<p>3 Models
</p>
<p>To segment a character sequence, we employ neu-
ral networks to score the likelihood of a candidate
segmented sequence being a true sentence, and the
</p>
<p>Gated flow
</p>
<p>Word embedding
</p>
<p>LSTM layer
</p>
<p>Character embeddings
</p>
<p>wi!1 wi
</p>
<p>hi!1 hi
</p>
<p>Figure 1: Neural network scoring for word candi-
date wi in a possible word sequence (..., wi, ...).
</p>
<p>one with the highest score will be picked as output.
</p>
<p>3.1 Neural Scorer
Our neural architecture to score a segmented se-
quence (word sequence) can be described in the
following three steps (illustrated in Figure 1).
</p>
<p>Encoding To make use of neural networks, sym-
bolic data needs to be transformed into distributed
representations. The most straightforward solu-
tion is to use a lookup table for word vectors (Ben-
gio et al., 2003). However, in the context of neural
word segmentation, it will generalize poorly due
to the severe word sparsity in Chinese. An alter-
native is employing neural networks to compose
word representations from character embedding
inputs. However, it is empirically hard to learn
a satisfactory composition function. In fact, quite
a lot of Chinese words, like “沙(sand)发(issue)”
(sofa) , are not semantically character-level com-
positional at all.
</p>
<p>For the dilemma that composing word represen-
tations from character may be insufficient while
the direct use of word embedding may lose gen-
eralization ability, we propose a hybrid mecha-
nism to alleviate the problem. Concretely, we
keep a short list H of the most frequent words
w = c1..cl to balance character composition. If w
inH, the immediate word embedding w ∈ Rdw is
attached via average pooling1, otherwise, the char-
acter composition is used alone.
</p>
<p>WORD(c1..cl) =
</p>
<p>{
COMP(c1..cl)+w[w]
</p>
<p>2 if c1..cl ∈ H
COMP(c1..cl) otherwise
</p>
<p>Our character composition function COMP(·) for
1We tried other two integration functions, concatenation
</p>
<p>and adaptive gating mechanism, but it finally shows that the
simplest averaging works best.
</p>
<p>609</p>
<p />
</div>
<div class="page"><p />
<p>c1
</p>
<p>cl
</p>
<p>Comp w
</p>
<p>c1
</p>
<p>cl
</p>
<p>Comp
</p>
<p>r1
</p>
<p>rl
</p>
<p>r1
</p>
<p>rl
</p>
<p>zc
</p>
<p>z1
</p>
<p>zl
</p>
<p>Figure 2: The difference between (Cai and Zhao,
2016) (left) and our model (right).
</p>
<p>l-length word is
</p>
<p>COMP(c1..cl) = tanh(Wcl [r1�c1; . . . ; rl�cl]+bcl )
</p>
<p>where � denotes the element-wise multiplication.
ri ∈ Rdc is the gate that controls the information
flow from character embedding ci ∈ Rdc to word.
Intuitively, the gating mechanism is used to deter-
mine which part of the character vectors should be
retrieved when composing a certain word. This is
indeed important due to the ambiguity of individ-
ual Chinese characters.
</p>
<p>[r1; . . . ; rl] = sigmoid(Wrl [c1; . . . ; cl] + b
r
l )
</p>
<p>In contrast, the model in (Cai and Zhao, 2016)
further combined COMP(·) and character embed-
dings ci via an update gate z (As in Figure 2),
which has been shown helpless to the performance
but requires huge computational cost according to
our empirical study.
</p>
<p>Linking To capture word interactions within a
word sequence, the resulted word vectors are then
linked sequentially via an LSTM (Sundermeyer
et al., 2012). At each time step i, a prediction
about next word is made according to the current
hidden state hi ∈ RH of LSTM. The procedure
can be described as the following equation.
</p>
<p>pi+1 = tanh(W
phi + b
</p>
<p>p)
</p>
<p>The predictions p ∈ Rdw will then be used to eval-
uate how reasonable the transition is between next
word and the preceding word sequence.
</p>
<p>Scoring The segmented sequence is evaluated
from two perspectives, (i) the legality of individ-
ual words, (ii) the smoothness or coherence of the
word sequence. The former is judged by a train-
able parameter vector u ∈ Rdw , which is supposed
to work like a hyperplane separating legal and ille-
gal words. For the latter, the prediction p made for
each position can be used to score the fitness of the
</p>
<p>actual word. Both scoring operations are imple-
mented via dot product in our settings. Summing
up all scores, the segmented sequence (sentence)
is scored as follow.
</p>
<p>s([w1, w2, . . . , wn]) =
n∑
</p>
<p>i=1
</p>
<p>(u+ pi)
TWORD(wi)
</p>
<p>3.2 Search
</p>
<p>The number of possible segmented sentences
grows exponentially with the length of the input
character sequence. Most existing methods made
Markov assumptions to keep the exact search
tractable.2 However, such assumptions cannot be
made in our model as the LSTM component takes
advantage of the full segmentation history. We
then adopt a beam search scheme, which works
iteratively on every prefix of the input charac-
ter sequence, approximating the k highest-scored
word sequences of each prefix (i.e., k is the beam
size). The time complexity of our beam search is
O(wkn), where w is the maximum word length
and n is the input sequence length.
</p>
<p>3.3 Training Criteria
</p>
<p>Our segmenter is trained using max-margin meth-
ods (Taskar et al., 2005) where the structured mar-
gin loss is defined as µ times the number of incor-
rectly segmented characters (Cai and Zhao, 2016).
However, according to (Huang et al., 2012), stan-
dard parameter update cannot guarantee conver-
gence in the case of inexact search. We thus addi-
tionally examine two strategies as follows.
</p>
<p>Early update This strategy proposed in
(Collins and Roark, 2004) can be simplified into
“update once the golden answer is unreachable”.
In our case, when the considering character pre-
fix can be correctly segmented but the correct one
falls off the beam, an update operation will be con-
ducted and the rest part will be ignored.
</p>
<p>LaSO update One drawback of early update
is that the search may never reach the end of a
training instance, which means the rest part of the
instance is “wasted”. Differently, LaSO method
of (Daumé III and Marcu, 2005) continues on the
same instance with correct hypothesis after each
update. In our case, the beam will be emptied and
the corresponding prefix of the correct word se-
quence will be inserted into the beam.
</p>
<p>2By assuming that tag interactions or word interactions
only exist in adjacent positions.
</p>
<p>610</p>
<p />
</div>
<div class="page"><p />
<p>PKU MSR
Train Test Train Test
</p>
<p>#sentences 19K 2K 87K 4K
#words 1,110K 104K 2,368K 107K
#characters 1,788K 169K 3,981K 181K
</p>
<p>Table 1: Data statistics.
</p>
<p>Character embedding size dc = 50
Word embedding size dw = 50
Hidden unit number H = 50
Margin loss discount µ = 0.2
Maximum word length w = 4
</p>
<p>Table 2: Model setting.
</p>
<p>4 Experiments
</p>
<p>4.1 Datasets and Settings
</p>
<p>We conduct experiments on two popular bench-
mark datasets, namely PKU and MSR, from the
second international Chinese word segmentation
bakeoff (Emerson, 2005) (Bakeoff-2005). Data
statistics are in Table 1.
</p>
<p>Throughout this paper, we use the same model
setting as shown in Table 2. These numbers are
tuned on development sets.3 We follow (Dyer
et al., 2015) to train model parameters. The learn-
ing rate at epoch t is set as ηt = 0.2/(1 +
γt), where γ = 0.1 for PKU dataset and γ =
0.2 for MSR dataset. The character embeddings
are either randomly initialized or pre-trained by
word2vec (Mikolov et al., 2013) toolkit on Chi-
nese Wikipedia corpus (which will be indicated by
+pre-train in tables.), while the word embeddings
are always randomly initialized. The beam size is
kept the same during training and working. By de-
fault, early update strategy is adopted and the word
table H is top half of in-vocabulary (IV) words by
frequency.
</p>
<p>4.2 Model Analysis
</p>
<p>Beam search collapses into greedy search Fig-
ure 3 demonstrates the effect of beam size. To
our surprise, beam size change has little influ-
ence on the performance. Namely, simple step-
wise greedy search nearly achieves the best per-
formance, which suggests that word segmentation
can be greedily solvable at word-level. It may
be due to that right now the model is optimal
</p>
<p>3Following conventions, the last 10% sentences of train-
ing corpus are used as development set.
</p>
<p>95
</p>
<p>96
</p>
<p>97
</p>
<p>1 2 3 4 5 6 7 8
</p>
<p>PKU
MSR
</p>
<p>beam size
</p>
<p>F
1
-s
co
re
</p>
<p>(%
)
</p>
<p>Figure 3: The effect of different beam sizes.
</p>
<p>Methods
PKU MSR
</p>
<p>F1 #epochs F1 #epochs
Standard 95.6 50 96.7 50
</p>
<p>Early update 95.8 30 97.1 30
LaSO update 95.7 45 97.0 30
</p>
<p>Table 3: The effect of different update methods.
#epochs denotes the number of training epochs to
convergence.
</p>
<p>enough to make correct decisions at the first po-
sition. In fact, similar phenomenon was observed
at character-level (Ma and Hinrichs, 2015). The
rest experiments will thus only report the results
of our greedy segmenter.
</p>
<p>Comparing different update methods Table 3
compares the concerned three training strategies.
We find that early update leads to faster conver-
gence and a bit better performance compared to
both standard and LaSO update.
</p>
<p>Character composition versus word embedding
Following Section 3.1, direct use of word em-
bedding may bring efficiency and effectiveness
for identifying IV words, but weaken the abil-
ity to recognize out-of-vocabulary (OOV) words.
We accordingly conduct experiments on different
sizes of word table H. Concretely, sorting all
IV words by frequency, the first {0, 25%, 50%,
75%, 100%} fraction of them respectively forms
the word table. The corresponding results on PKU
in Figure 4 demonstrate that by the use of direct
word embedding, F1 score increases first but then
drops rapidly. In contrast, OOV recall, which par-
tially reflects the model generalization ability, de-
creases consistently. In addition, we also found
the number of training epochs to convergence de-
creases continually. Overall, the results indicate
that word-aware segmenter learns faster and fits
better on training set, but generalizes relatively
poorly.
</p>
<p>611</p>
<p />
</div>
<div class="page"><p />
<p>Models
PKU MSR
</p>
<p>F1
+ pre-train F1
</p>
<p>Training
(hours)
</p>
<p>Test
(sec.)
</p>
<p>F1
+ pre-train F1
</p>
<p>Training
(hours)
</p>
<p>Test
(sec.)
</p>
<p>(Zhao and Kit, 2008c) - 95.4 - - - 97.6 - -
(Chen et al., 2015a) 94.5* 94.4* 50 105 95.4* 95.1* 100 120
(Chen et al., 2015b) 94.8* 94.3* 58 105 95.6* 95.0* 117 120
</p>
<p>(Ma and Hinrichs, 2015) - 95.1 1.5 24 - 96.6 3 28
(Zhang et al., 2016) 95.1 - 6 110 97.0 - 13 125
</p>
<p>(Liu et al., 2016) 93.91 - - - 95.21 - - -
(Cai and Zhao, 2016) 95.5 95.2 48 95 96.5 96.4 96 105
</p>
<p>Our results 95.8 95.4 3 25 97.1 97.0 6 30
</p>
<p>Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016).4
</p>
<p>94
</p>
<p>94.5
</p>
<p>95
</p>
<p>95.5
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>0 25 50 75 100
</p>
<p>F1-score
OOV recall
</p>
<p>word fraction (%)
</p>
<p>O
O
V
</p>
<p>reca
ll
(%
</p>
<p>)F
1
-s
co
re
</p>
<p>(%
)
</p>
<p>Figure 4: Performance with different sizes of word
table on PKU test set.
</p>
<p>4.3 Main Results
</p>
<p>Table 4 compares our final results (greedy search
is adopted by setting k=1) to prior neural mod-
els. Pre-training character embeddings on large-
scale unlabeled corpus (not limited to the training
corpus) has been shown helpful for extra perfor-
mance improvement. The results with or without
pre-trained character embeddings are listed sepa-
rately for following the strict closed test setting
of SIGHAN Bakeoff in which no linguistic re-
source other than training corpus is allowed. We
also show the state-of-the-art results in (Zhao and
Kit, 2008b) of traditional methods. The compari-
son shows our neural word segmenter outperforms
all state-of-the-art neural systems with much less
computational cost.
</p>
<p>Finally, we present the results on all four
Bakeoff-2005 datasets compared to (Zhao and Kit,
2008c) in Table 5. Note (Zhao and Kit, 2008c)
used AV features, which are derived from global
</p>
<p>4To distinguish the performance improvement from
model optimization, we especially list the results of stand-
alone neural models in (Zhang et al., 2016) and (Liu et al.,
2016). All the running time results are from our runs of
released implementations on a single CPU (Intel i7-5960X)
with two threads only, except for those of (Zhang et al., 2016)
which are from personal communication. The results of (Xu
and Sun, 2016) are not listed due to their use of external Chi-
nese idiom dictionary.
</p>
<p>Models PKU MSR CityU AS
(Zhao and Kit, 2008c) 95.4 97.6 96.1 95.7
</p>
<p>–AV 95.2 97.4 94.8 95.3
ours 95.4 97.0 95.4 95.2
</p>
<p>+pre-train 95.8 97.1 95.6 95.3
</p>
<p>Table 5: Results on all four Bakeoff-2005 datasets.
</p>
<p>statistics over entire training corpus in a similar
way of unsupervised segmentation (Zhao and Kit,
2008a), for performance enhancement.5 The com-
parison to their results without AV features show
that our neural models for the first time present
comparable performance against state-of-the-art
traditional ones under strict closed test setting.6
</p>
<p>5 Conclusion
</p>
<p>In this paper, we presented a fast and accurate
word segmenter using neural networks. Our ex-
periments show a significant improvement over
existing state-of-the-art neural models by adopting
the following key model refinements.
</p>
<p>(1) A novel character-word balanced mecha-
nism for word representation generation. (2) A
more efficient model for character composition by
dropping unnecessary designs. (3) Early update
strategy during max-margin training. (4) With the
above modifications, we discover that beam size
has little influence on the performance. Actually,
greedy search achieves very high accuracy.
</p>
<p>Through these improvement from both neural
models and linguistic motivation, our model be-
comes simpler, faster and more accurate.7
</p>
<p>5In fact, this kind of features may also be incorporated to
our model. We leave it as future work.
</p>
<p>6To our knowledge, none of previous neural models has
ever performed a complete evaluation over all four segmen-
tation corpora of Bakeoff-2005, in which only two, PKU and
MSR, are used since (Pei et al., 2014).
</p>
<p>7Our code based on Dynet (Neubig et al., 2017) is re-
leased at https://github.com/jcyk/greedyCWS.
</p>
<p>612</p>
<p />
</div>
<div class="page"><p />
<p>References
Galen Andrew. 2006. A hybrid markov/semi-markov
</p>
<p>conditional random field for sequence segmentation.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing. Sydney,
Australia, pages 465–472.
</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search 3:1137–1155.
</p>
<p>Deng Cai and Hai Zhao. 2016. Neural word segmenta-
tion learning for Chinese. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Berlin,
Germany, pages 409–420.
</p>
<p>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
Chinese word segmentation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Beijing, China, pages 1744–
1753.
</p>
<p>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015b. Long short-term mem-
ory neural networks for Chinese word segmentation.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Lisbon,
Portugal, pages 1197–1206.
</p>
<p>Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Meeting of the Association for Com-
putational Linguistics, Main Volume. Barcelona,
Spain, pages 111–118.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research
12:2493–2537.
</p>
<p>Hal Daumé III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proceedings
of the 22nd international conference on Machine
learning. Bonn, Germany, pages 169–176.
</p>
<p>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers). Beijing, China, pages 334–343.
</p>
<p>Thomas Emerson. 2005. The second international Chi-
nese word segmentation bakeoff. In Proceedings of
the fourth SIGHAN workshop on Chinese language
Processing. Jeju Island, Korea, pages 123–133.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Chang-Ning Huang and Hai Zhao. 2006. Which is
essential for Chinese word segmentation: Charac-
ter versus word. In The 20th Pacific Asia Confer-
ence on Language, Information and Computation.
Wuhan, China, pages 1–12.
</p>
<p>Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing 21(3):8–20.
</p>
<p>Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Montréal, Canada, pages 142–151.
</p>
<p>John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
terntional Conference on Machine Learning. San
Francisco, USA, volume 1, pages 282–289.
</p>
<p>Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, and
Ting Liu. 2016. Exploring segment representations
for neural segmentation models. In Proceedings of
the Twenty-Fifth International Joint Conference on
Artificial Intelligence. New York, USA, pages 2880–
2886.
</p>
<p>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing. Jeju Is-
land, Korea, pages 448–455.
</p>
<p>Jianqiang Ma and Erhard Hinrichs. 2015. Accurate
linear-time Chinese word segmentation via embed-
ding matching. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers). Beijing, China, pages 1733–1743.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .
</p>
<p>613</p>
<p />
</div>
<div class="page"><p />
<p>Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for Chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Baltimore, Maryland,
pages 293–303.
</p>
<p>Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceed-
ings of the 20th international conference on Com-
putational Linguistics. Geneva, Switzerland, pages
562–568.
</p>
<p>Yanjun Qi, Sujatha G Das, Ronan Collobert, and Jason
Weston. 2014. Deep learning for character-based in-
formation extraction. In Advances in Information
Retrieval, pages 668–674.
</p>
<p>Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and com-
bination. In Proceedings of the 23rd International
Conference on Computational Linguistics. Beijing,
China, pages 1211–1219.
</p>
<p>Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for Chinese word segmentation and new word detec-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics. Jeju
Island, Korea, pages 253–262.
</p>
<p>Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
2012. LSTM neural networks for language model-
ing. In 13th Annual Conference of the International
Speech Communication Association. Portland, Ore-
gon, USA, pages 194–197.
</p>
<p>Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd international conference on Ma-
chine learning. Bonn, Germany, pages 896–903.
</p>
<p>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
bakeoff 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing. Jeju Is-
land, Korea, pages 168–171.
</p>
<p>Mengqiu Wang, Rob Voigt, and Christopher D. Man-
ning. 2014. Two knives cut better than one: Chi-
nese word segmentation with dual decomposition.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers). Baltimore, Maryland, pages 193–
198.
</p>
<p>Jingjing Xu and Xu Sun. 2016. Dependency-based
gated recursive neural network for Chinese word
segmentation. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Berlin, Germany,
pages 567–572.
</p>
<p>Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing 8(1):29–48.
</p>
<p>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint Chinese word segmentation and part-
of-speech tagging. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Sofia, Bul-
garia, pages 770–779.
</p>
<p>Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing. Seattle, USA, pages 311–321.
</p>
<p>Meishan Zhang, Yue Zhang, and Guohong Fu. 2016.
Transition-based neural word segmentation. In Pro-
ceedings of the 54nd Annual Meeting of the Asso-
ciation for Computational Linguistics. Berlin, Ger-
many, pages 421–431.
</p>
<p>Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics. Prague, Czech
Republic, pages 840–847.
</p>
<p>Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational linguistics 37(1):105–151.
</p>
<p>Hai Zhao, Deng Cai, Yang Xin, Wang Yuzhu, and
Zhongye Jia. 2017. A hybrid model for Chi-
nese spelling check. ACM Transactions on Asian
and Low-Resource Language Information Process-
ing 16(3).
</p>
<p>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In The 20th Pacific Asia Conference on
Language, Information and Computation. Wuhan,
China, pages 87–94.
</p>
<p>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for Chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing
16(21).
</p>
<p>Hai Zhao and Chunyu Kit. 2008a. An empirical com-
parison of goodness measures for unsupervised Chi-
nese word segmentation with a unified framework.
In Proceedings of the Third International Joint Con-
ference on Natural Language Processing. Hyder-
abad, India, pages 9–16.
</p>
<p>Hai Zhao and Chunyu Kit. 2008b. Exploiting unla-
beled text with different unsupervised segmentation
criteria for Chinese word segmentation. Research in
Computing Science 33:93–104.
</p>
<p>614</p>
<p />
</div>
<div class="page"><p />
<p>Hai Zhao and Chunyu Kit. 2008c. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named en-
tity recognition. In The Sixth SIGHAN Workshop
on Chinese Language Processing. Hyderabad, India,
pages 106–111.
</p>
<p>Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
</p>
<p>role of goodness measures. Information Sciences
181(1):163–183.
</p>
<p>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for Chinese word segmentation and
POS tagging. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language
Processing. Seattle, Washington, USA, pages 647–
657.
</p>
<p>615</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 616–622
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2097
</p>
<p>Pay Attention to the Ending:
Strong Neural Baselines for the ROC Story Cloze Task
</p>
<p>Zheng Cai1 Lifu Tu2 Kevin Gimpel2
</p>
<p>1University of Chicago, Chicago, IL, 60637, USA
2Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA
</p>
<p>jontsai@uchicago.edu, {lifu,kgimpel}@ttic.edu
</p>
<p>Abstract
</p>
<p>We consider the ROC story cloze
task (Mostafazadeh et al., 2016) and
present several findings. We develop a
model that uses hierarchical recurrent
networks with attention to encode the
sentences in the story and score candidate
endings. By discarding the large training
set and only training on the validation
set, we achieve an accuracy of 74.7%.
Even when we discard the story plots
(sentences before the ending) and only
train to choose the better of two endings,
we can still reach 72.5%. We then analyze
this “ending-only” task setting. We
estimate human accuracy to be 78% and
find several types of clues that lead to this
high accuracy, including those related to
sentiment, negation, and general ending
likelihood regardless of the story context.
</p>
<p>1 Introduction
</p>
<p>The ROC story cloze task (Mostafazadeh et al.,
2016) tests a system’s ability to choose the more
plausible of two endings to a story. The incorrect
ending is written to still fit the world of the story,
e.g., the protagonist typically appears in both end-
ings. The task is designed to test for “common-
sense” knowledge, where the difference between
the two endings lies in the plausibility of the char-
acters’ actions. The best system of Mostafazadeh
et al. (2016) achieves 58.5% accuracy.
</p>
<p>The ROC training and evaluation data differ in
a key way. The training set contains 5-sentence
stories. But the evaluation datasets (the validation
and test sets) contain both a correct ending and
an incorrect ending. This means that the task is
one of outlier detection: systems must estimate the
density of correct endings in the training data and
</p>
<p>then detect which of the two endings is an outlier.
This becomes difficult when the evaluation con-
tains distractors that are still somewhat plausible.
For example, a model may place mass on stories
that consistently mention the same characters, but
this will not be useful for the task because even the
incorrect ending uses the correct character names.
</p>
<p>In this paper, we discard the 50k training sto-
ries and train only on the 1871-story validation
set. We develop several neural models based on
recurrent networks, comparing flat and hierarchi-
cal models for encoding the sentences in the story.
We also use an attention mechanism based on the
ending to identify useful parts of the plot. Our final
model achieves 74.7% on the test set, outperform-
ing all systems of Mostafazadeh et al. (2016) and
approaching the state of the art results of concur-
rent work (Schwartz et al., 2017b).
</p>
<p>We then discard the first four sentences of each
story and use our model to score endings alone.
We achieve 72.5% on the test set, outperforming
most prior work without even looking at the story
plots. We do a small-scale manual study of this
ending-only task, finding that humans can identify
the better ending in approximately 78% of cases.
We report several reasons for the high accuracy
of this ending-only setting, including some that
are readily captured by automatic methods, such
as sentiment analysis and the presence of negation
words, as well as others that are more difficult, like
those derived from world knowledge. Our results
and analysis, combined with the similar concur-
rent observations of Schwartz et al. (2017a), sug-
gest that any meaningful system for the ROC task
must outperform the best ending-only baselines.
</p>
<p>2 Task and Datasets
</p>
<p>We refer to a 5-sentence sequence as a story, the
incomplete 4-sentence sequence as a plot, and the
</p>
<p>616</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2097">https://doi.org/10.18653/v1/P17-2097</a></div>
</div>
<div class="page"><p />
<p>fifth sentence as an ending. The ROC story cor-
pus (Mostafazadeh et al., 2016) contains training,
validation, and test sets. The training set contains
5-sentence stories. The validation and test sets
contain 4-sentence plots followed by two candi-
date endings, with only one correct.
</p>
<p>Mostafazadeh et al. (2016) evaluated several
methods for solving the task. Since the training
set does not contain incorrect endings, their meth-
ods are based on computing similarity between
the plot and ending. Their best results were ob-
tained with the Deep Structured Semantic Model
(DSSM) (Huang et al., 2013) which represents
texts using character trigram counts followed by
neural network layers and a similarity function.
</p>
<p>Concurrently with our work, the LSDSem 2017
shared task was held (Mostafazadeh et al., 2017),
focusing on the ROC story cloze task. Several of
the participants made similar observations to what
we describe here, namely that supervised learning
on the validation set is more effective than learn-
ing directly from the training set, as well as not-
ing certain biases in the endings (Schwartz et al.,
2017a,b; Bugert et al., 2017; Flor and Somasun-
daran, 2017; Schenk and Chiarcos, 2017; Roem-
mele et al., 2017; Goel and Singh, 2017; Mihaylov
and Frank, 2017).
</p>
<p>3 Models and Training
</p>
<p>We now describe our model variations. The first
(ENCPLOTEND) encodes the plot and ending sep-
arately, then scores them with a scoring func-
tion. The second (ENCSTORY) concatenates the
plot and ending to form a story, then encodes that
story and scores its representation with a scoring
function. When encoding a sequence of multiple
sentences, whether with ENCPLOTEND or ENC-
STORY, we consider two choices: a hierarchical
encoder (HIER) that first encodes each sentence
and then encodes the sentence representations, and
a non-hierarchical encoder (FLAT) that simply en-
codes the concatenation of all sentences. We also
consider the possibility of including an ending-
oriented attention mechanism (ATT). For training,
we use a simple supervised hinge loss objective.
</p>
<p>3.1 Encoders
</p>
<p>Our encoders encode text sequences into represen-
tations. When using our HIER model, we use a
hierarchical recurrent neural network (RNN) (Li
et al., 2015) with two levels. The first RNN en-
</p>
<p>codes the sequence of words in a sentence; the
same RNN is used for sentences in the plot and
for each candidate ending. The second RNN en-
codes the sequence of sentence representations in
a plot or story. When using our FLAT model, we
only use the first RNN described above; the only
change is that the input becomes the concatena-
tion of multiple sentences (separated by sentence
boundary tokens).
</p>
<p>Below we use i as a subscript to index sentences
in the story or plot, and j as a superscript to index
individual words in sentences. E.g., we use wi to
indicate the ith sentence of the story/plot and we
use wi(j) to denote the word embedding vector of
the jth word in the ith sentence.
</p>
<p>3.1.1 Encoding Word Sequences
We use a bidirectional long short-term memory
(BiLSTM) RNN (Hochreiter and Schmidhuber,
1997) to encode a sentence. For sentence wi:
</p>
<p>fi = forward-LSTM1(wi)
</p>
<p>bi = backward-LSTM1(wi)
</p>
<p>where fi and bi are hidden vector sequences. We
add the forward and backward vectors at each step
to obtain vectors hi, then average to obtain sen-
tence representation Si:
</p>
<p>hi = fi + bi Si =
1
</p>
<p>|wi|
</p>
<p>|wi|∑
</p>
<p>j=1
</p>
<p>h
(j)
i (1)
</p>
<p>We define this function from word sequence wi to
sentence representation Si by ENCWORDS(wi).
</p>
<p>3.1.2 Adding Attention
Attention mechanisms (Bahdanau et al., 2015;
Mnih et al., 2014) have yielded considerable per-
formance gains for machine comprehension (Her-
mann et al., 2015; Sukhbaatar et al., 2015; Chen
et al., 2016), parsing (Vinyals et al., 2015), and
machine translation (Luong et al., 2015).
</p>
<p>After generating the representation e = S5 =
ENCWORDS(w5) for candidate ending w5, we
use it to compute the attention over the individual
hidden vectors of each sentence to compute modi-
fied sentence representations S†i . That is:
</p>
<p>α
(j)
i = e
</p>
<p>&gt;Mh(j)i β
(j)
i ∝ exp{α
</p>
<p>(j)
i }
</p>
<p>S†i =
|wi|∑
</p>
<p>j=1
</p>
<p>β
(j)
i h
</p>
<p>(j)
i (2)
</p>
<p>617</p>
<p />
</div>
<div class="page"><p />
<p>where h(j)i is the jth entry of hi and M is a bilin-
ear attention matrix.1 Figure 1 shows this architec-
ture. We define this attention-augmented encoder
as ATTENCWORDS(wi, e).
</p>
<p>3.1.3 Encoding Sentence Sequences
We use another BiLSTM to encode the sequence
S of sentence representations Si:
</p>
<p>F = forward-LSTM2(S)
</p>
<p>B = backward-LSTM2(S)
</p>
<p>ENCSENTS(S) = F-1 +B-1
</p>
<p>where F-1 is the final hidden vector in F . We also
use this encoder to encode the ending e by treating
it as a sequence containing only one element.
</p>
<p>3.2 Model Variations
Given our encoders, we now define the final rep-
resentations D for our modeling variations, com-
bining each of HIER and FLAT with each of ENC-
STORY and ENCPLOTEND:
</p>
<p>wk1 = 〈w1, ...,wk−1,wk〉 Sk1 = 〈S1, ...,Sk〉
DFLATS = ENCWORDS(w51)
</p>
<p>Si = ENCWORDS(wi)
</p>
<p>DFLATPE = 〈ENCWORDS(w41),S5〉
DHIERS = ENCSENTS(S51)
</p>
<p>DHIERPE = 〈ENCSENTS(S41), ENCSENTS(S5)〉
</p>
<p>When using attention, we replace ENCWORDS
above with ATTENCWORDS.
</p>
<p>After encoding the story as D, we use a feed-
forward network to act as a score function that
takes D as input and generates a one-dimensional
(scalar) output. We use tanh as the activation
function on each layer of the feed-forward net-
work and tune the numbers of hidden layers and
the layer widths.
</p>
<p>3.3 Training
Since we are training on the validation set which
contains both correct and incorrect endings, we
minimize the following hinge loss:
</p>
<p>L = max(0,−score(D+) + score(D−) + δ)
</p>
<p>where D+ is the representation of the correct
story, D− is the representation of the incorrect
story, and δ = 1 is the margin.
</p>
<p>1In preliminary experiments we found bilinear attention
to work better than attention based on cosine similarity.
</p>
<p>Figure 1: Attention-augmented BiLSTM for en-
coding a 4-word sentence wi into a 3-dimensional
representation S†i . The attention function uses the
ending representation e.
</p>
<p>4 Experimental Setup
</p>
<p>We shuffle and split the validation set into 5 folds
and do 5-fold cross validation. For modeling
decisions, we tune based on the average accu-
racy of the held-out folds. For final experiments,
we choose the fold with the best held-out accu-
racy and report its test set accuracy. We use
Adam (Kingma and Ba, 2015) for optimization
with learning rate 0.001 and mini-batch size 50.
We use pretrained 300-dimensional GloVe embed-
dings trained on Wikipedia and Gigaword (Pen-
nington et al., 2014) and keep them fixed during
training. We use L2 regularization for the score
feed-forward network, which has a single hidden
layer of size 512. We use 300 for the LSTM hid-
den vector dimensionality for both encoders.
</p>
<p>618</p>
<p />
</div>
<div class="page"><p />
<p>FLAT HIER
ENCSTORY 79.08 80.22
</p>
<p>ENCPLOTEND 71.75 79.84
</p>
<p>Table 1: Accuracies (%) averaged over held-out
folds of 5-fold cross validation. Comparing hier-
archical (HIER) and non-hierarchical (FLAT) en-
coders, and encoding story (ENCSTORY) vs. sepa-
rately encoding plot and ending (ENCPLOTEND).
No attention is used.
</p>
<p>-ATT +ATT
ENCSTORY 80.22 79.95
</p>
<p>ENCPLOTEND 79.84 81.24
</p>
<p>Table 2: For the ENCSTORY and ENCPLOTEND
models, showing the contribution of adding atten-
tion (+ATT). All models use the HIER encoder.
</p>
<p>5 Results
</p>
<p>Modeling Decisions. We first evaluate our mod-
eling decisions, using the averaged held-out fold
accuracy as our model selection criterion. Table 1
shows results when comparing FLAT/HIER and
ENCSTORY/ENCPLOTEND. Hierarchical model-
ing helps especially with ENCPLOTEND.
</p>
<p>Table 2 shows the contribution of attention
when using HIER. Attention helps when sep-
arately encoding the plot and ending, but not
when encoding the entire story. We sus-
pect this is because when we use ENCSTORY,
the higher BiLSTM processes the sequence
〈S†1,S†2,S†3,S†4,S5〉. That is, the first four sen-
tence representations are in a different space from
the ending due to the use of attention.
</p>
<p>Final Results. Table 3 shows final results. We
report the best result from Mostafazadeh et al.
(2016), the best result from the concurrently-held
LSDSem shared task (Schwartz et al., 2017b),
and our final system configuration (with decisions
tuned via cross validation as shown in Tables 1-2,
then using the model with the best held-out fold
accuracy). Our model achieves 74.7%, which is
close to the state of the art result of 75.2%.2
</p>
<p>We also report the results of stripping away the
plots and running our system on just the endings
(“ending only”). We use the FLAT BiLSTM model
on the ending followed by the feed-forward scor-
ing function, using the same loss as above for
training. We again use 5-fold cross validation
</p>
<p>2We also tried to train the DSSM on the validation set, but
were unable to approach the performance of our model. The
DSSM appears to benefit greatly from the training set.
</p>
<p>val test
DSSM‡ 60.4 58.5
UW (Schwartz et al., 2017b) - 75.2
UW (ending only) - 72.4
trigram LM (estimated from stories) 52.4 53.6
trigram LM (estimated from endings) 53.8 54.6
Our model (HIER, ENCPLOTEND, ATT) - 74.7
Our model (ending only) - 72.5
Human‡ (story + ending) 100 100
Human (ending only) 78∗ -
</p>
<p>Table 3: Final results. ∗ = estimate from 100; see
Section 6.1. ‡ = from Mostafazadeh et al. (2016).
</p>
<p>on the validation set and choose the model with
the highest held-out fold accuracy. We achieve
72.5%, matching the similar ending-only result of
Schwartz et al. (2017b). We estimate human per-
formance in the ending-only setting to be 78%.
We provide more details in Section 6.1. These re-
sults suggest that the dataset contains systematic
biases in the composition of its endings and that
any meaningful system for the task must outper-
form the best ending-only baseline.
</p>
<p>We also report the results of two n-gram lan-
guage model baselines. We estimated trigram
models using KenLM (Heafield, 2011) from two
different datasets: (1) the entire training stories,
and (2) only the endings from the training stories.
Using only the endings works better, even though
it uses one fifth of the data; this further shows the
importance of focusing on endings for this task.
</p>
<p>6 Analysis
</p>
<p>We analyze the attention weights in our final
model. Figure 2 shows the distribution of attention
weights over position bins, aggregated over the
plot sentences in the test set. We find that the atten-
tions generated by the correct ending show higher
weight for words early in the sentences, while the
attentions for incorrect endings are higher at the
ends of the sentences.
</p>
<p>We also study the ending-only task to uncover
the different kinds of bias that lead to high accura-
cies in this setting. We consider automatic features
that can be computed on the endings and evaluate
the accuracy of relying solely upon each feature as
a classification rule. We then compute correlations
between our ending-only model and each feature.
In addition to the trigram model described above,
we consider the following rules:
</p>
<p>• sentiment: choose ending with higher predicted
sentiment score from the Stanford sentiment an-
</p>
<p>619</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: Attention weight distribution in test
set plot sentences. Sentences were divided into
eighths, then attention weights were averaged for
each and normalized. For correct endings, there is
more attention on early words in plot sentences.
</p>
<p>rule ruleapplicability
test
</p>
<p>accuracy
Spearman
correlation
</p>
<p>sentiment 65.9% 58.7% 0.214
negation words 20.7% 55.4% -
length 100% 53.2% 0.047
language model 100% 54.6% 0.135
</p>
<p>Table 4: Ending selection rules exhibiting biases
in endings. Final column shows correlation be-
tween each feature and the score of our model.
</p>
<p>alyzer (Socher et al., 2013).
</p>
<p>• negation: choose ending with fewer words
from {not, neither, nor, never, n’t, no, rarely}.
• length: choose the longer ending.
</p>
<p>Table 4 shows the results. Each rule yields accu-
racy at least 53%, with the sentiment rule nearing
59%. Even though the negation rule is only appli-
cable in 20% of cases, its bias is strong enough to
yield 5% improvement over the random baseline.
These results show several reasons why an ending-
only model can perform well, and suggests that
our model may be identifying positive sentiment,
due to its correlation of 0.214 with that feature.
</p>
<p>We counted words in the correct and incorrect
endings and in Table 5 we show some that differ
between the top-50 lists for each category. E.g.,
“never” appears among the top 50 words in incor-
rect endings but not correct endings. The word
count differences are accordant with the results
from the sentiment and negation word rules, with
non-overlapping words showing significant senti-
ment difference and that correct endings are more
neutral or positive than incorrect ones.
</p>
<p>correct endings: out, !, great, new, found
incorrect endings: n’t, did, not, never, hated
</p>
<p>Table 5: Non-overlapping words in the top 50
most frequent word list of each category.
</p>
<p>6.1 Human Ending-Only Performance
In order to assess human performance, we ran-
domly chose 100 ending pairs from the validation
set and gave them to a human annotator, a native
speaker of English who is familiar with the ROC
task. The annotator was asked to select the more
likely ending based only on the two endings pro-
vided. He was correct on 78, observing several
kinds of cues in the endings alone in addition to
those mentioned above.
</p>
<p>In some cases, one ending sentence is simply
much more likely than the other based on world
knowledge. For example, the ending “the glasses
fixed his headaches immediately” is much more
likely than “the optometrist gave him comfortable
sneakers”. It is possible that the plot could change
the preferred ending to the second, but this appears
to be rare in the ROC dataset. In another example,
“I practice all the time now” is more likely than “I
hope I drop the batons” because it seems unlikely
that anyone would ever hope to drop batons in the
surmised world of the story. While these instances
still test for a kind of “commonsense” or “world”
knowledge, they do not require the plot to answer.
</p>
<p>7 Conclusions and Future Work
</p>
<p>Our models use none of the ROC training data but
achieve strong performance, even when discard-
ing the story plots. We uncovered several sources
of bias in the endings that make the ending-only
task solvable with greater than 70% accuracy. Our
results suggest that any meaningful system for the
ROC story cloze task should perform better than
the best ending-only system. In future work, we
will experiment with additional modeling choices,
including adding attention to the higher BiLSTM
and adding a decoder and a multi-task objective
during training to improve stability.
</p>
<p>Acknowledgments
</p>
<p>We acknowledge Zhongtian Dai for his assistance
and expertise and we thank Yejin Choi, Nasrin
Mostafazadeh, Michael Roth, Roy Schwartz, and
the anonymous reviewers for valuable discussions
and insights.
</p>
<p>620</p>
<p />
</div>
<div class="page"><p />
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
International Conference on Learning Representa-
tions.
</p>
<p>Michael Bugert, Yevgeniy Puzikov, Andreas Rücklé,
Judith Eckle-Kohler, Teresa Martin, Eugenio
Martı́nez-Cámara, Daniil Sorokin, Maxime Peyrard,
and Iryna Gurevych. 2017. LSDSem 2017: Ex-
ploring data generation methods for the story cloze
test. In Proceedings of the 2nd Workshop on Linking
Models of Lexical, Sentential and Discourse-level
Semantics.
</p>
<p>Danqi Chen, Jason Bolton, and Christopher D. Man-
ning. 2016. A thorough examination of the
CNN/Daily Mail reading comprehension task. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
</p>
<p>Michael Flor and Swapna Somasundaran. 2017. Sen-
timent analysis and lexical cohesion for the story
cloze task. In Proceedings of the 2nd Work-
shop on Linking Models of Lexical, Sentential and
Discourse-level Semantics.
</p>
<p>Pranav Goel and Anil Kumar Singh. 2017. IIT (BHU):
System description for LSDSem’17 shared task. In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Seman-
tics.
</p>
<p>Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation.
</p>
<p>Karl Moritz Hermann, Tomáš Kočiský, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems (NIPS).
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation 9(8).
</p>
<p>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
International Conference on Information &amp; Knowl-
edge Management (CIKM).
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of International Conference on Learning Represen-
tations.
</p>
<p>Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A hierarchical neural autoencoder for paragraphs
and documents. In Proceedings of the 53rd Annual
</p>
<p>Meeting of the Association for Computational Lin-
guistics (ACL) and the 7th International Joint Con-
ference on Natural Language Processing (Volume 1:
Long Papers).
</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
</p>
<p>Todor Mihaylov and Anette Frank. 2017. Story cloze
ending selection baselines and data examination. In
Proceedings of the 2nd Workshop on Linking Models
of Lexical, Sentential and Discourse-level Seman-
tics.
</p>
<p>Volodymyr Mnih, Nicolas Heess, Alex Graves, and Ko-
ray Kavukcuoglu. 2014. Recurrent models of visual
attention. In Advances in Neural Information Pro-
cessing Systems (NIPS).
</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding of
commonsense stories. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
</p>
<p>Nasrin Mostafazadeh, Michael Roth, Annie Louis,
Nathanael Chambers, and James Allen. 2017. LSD-
Sem 2017 shared task: The story cloze test. In Pro-
ceedings of the 2nd Workshop on Linking Models of
Lexical, Sentential and Discourse-level Semantics.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors
for word representation. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP).
</p>
<p>Melissa Roemmele, Sosuke Kobayashi, Naoya Inoue,
and Andrew Gordon. 2017. An RNN-based binary
classifier for the story cloze test. In Proceedings
of the 2nd Workshop on Linking Models of Lexical,
Sentential and Discourse-level Semantics.
</p>
<p>Niko Schenk and Christian Chiarcos. 2017. Resource-
lean modeling of coherence in commonsense sto-
ries. In Proceedings of the 2nd Workshop on Linking
Models of Lexical, Sentential and Discourse-level
Semantics.
</p>
<p>Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017a. The
effect of different writing tasks on linguistic style:
A case study of the ROC story cloze task. CoRR
abs/1702.01841.
</p>
<p>Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017b. Story
cloze task: UW NLP system. In Proceedings of the
2nd Workshop on Linking Models of Lexical, Sen-
tential and Discourse-level Semantics.
</p>
<p>621</p>
<p />
</div>
<div class="page"><p />
<p>Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
Christopher Potts, et al. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
</p>
<p>Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and
Rob Fergus. 2015. End-to-end memory networks.
In Advances in Neural Information Processing Sys-
tems (NIPS).
</p>
<p>Oriol Vinyals, Ł ukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems (NIPS).
</p>
<p>622</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623–628
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2098
</p>
<p>Neural Semantic Parsing over Multiple Knowledge-bases
</p>
<p>Jonathan Herzig1,2 and Jonathan Berant1
</p>
<p>1Tel-Aviv University, Tel Aviv-Yafo, Israel
2IBM Research, Haifa 31905, Israel
</p>
<p>jherzig@gmail.com, joberant@cs.tau.ac.i
</p>
<p>Abstract
</p>
<p>A fundamental challenge in developing se-
mantic parsers is the paucity of strong su-
pervision in the form of language utter-
ances annotated with logical form. In this
paper, we propose to exploit structural reg-
ularities in language in different domains,
and train semantic parsers over multiple
knowledge-bases (KBs), while sharing in-
formation across datasets. We find that
we can substantially improve parsing ac-
curacy by training a single sequence-to-
sequence model over multiple KBs, when
providing an encoding of the domain at de-
coding time. Our model achieves state-of-
the-art performance on the OVERNIGHT
dataset (containing eight domains), im-
proves performance over a single KB
baseline from 75.6% to 79.6%, while ob-
taining a 7x reduction in the number of
model parameters.
</p>
<p>1 Introduction
</p>
<p>Semantic parsing is concerned with translat-
ing language utterances into executable logi-
cal forms and constitutes a key technology for
developing conversational interfaces (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Kwiatkowski et al., 2011; Liang et al., 2011; Artzi
and Zettlemoyer, 2013; Berant and Liang, 2015).
</p>
<p>A fundamental obstacle to widespread use of se-
mantic parsers is the high cost of annotating log-
ical forms in new domains. To tackle this prob-
lem, prior work suggested strategies such as train-
ing from denotations (Clarke et al., 2010; Liang
et al., 2011; Artzi and Zettlemoyer, 2013), from
paraphrases (Berant and Liang, 2014; Wang et al.,
2015) and from declarative sentences (Krishna-
murthy and Mitchell, 2012; Reddy et al., 2014).
</p>
<p>Example 1
Domain: HOUSING
“Find a housing that is no more than 800 square feet.”,
Type.HousingUnit u Size.≤ .800
Domain: PUBLICATIONS
“Find an article with no more than two authors”
Type.Article uR[λx.count(AuthorOf.x)].≤ .2
Example 2
Domain: RESTAURANTS
“which restaurant has the most ratings?”
argmax(Type.Restaurant,R[λx.count(R[Rating].x)])
Domain: CALENDAR
“which meeting is attended by the most people?”
argmax(Type.Meeting,R[λx.count(R[Attendee].x)])
</p>
<p>Figure 1: Examples for natural language utterances with log-
ical forms in lambda-DCS (Liang, 2013) in different domains
that share structural regularity (a comparative structure in the
first example and a superlative in the second).
</p>
<p>In this paper, we suggest an orthogonal solu-
tion: to pool examples from multiple datasets in
different domains, each corresponding to a sepa-
rate knowledge-base (KB), and train a model over
all examples. This is motivated by an observation
that while KBs differ in their entities and proper-
ties, the structure of language composition repeats
across domains (Figure 1). E.g., a superlative in
language will correspond to an ‘argmax’, and a
verb followed by a noun often denotes a join op-
eration. A model that shares information across
domains can improve generalization compared to
a model that is trained on a single domain only.
</p>
<p>Recently, Jia and Liang (2016) and Dong and
Lapata (2016) proposed sequence-to-sequence
models for semantic parsing. Such neural mod-
els substantially facilitate information sharing, as
both language and logical form are represented
with similar abstract vector representations in all
domains. We build on their work and examine
models that share representations across domains
during encoding of language and decoding of log-
ical form, inspired by work on domain adaptation
</p>
<p>623</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2098">https://doi.org/10.18653/v1/P17-2098</a></div>
</div>
<div class="page"><p />
<p>(Daume III, 2007) and multi-task learning (Caru-
ana, 1997; Collobert et al., 2011; Luong et al.,
2016; Firat et al., 2016; Johnson et al., 2016). We
find that by providing the decoder with a represen-
tation of the domain, we can train a single model
over multiple domains and substantially improve
accuracy compared to models trained on each do-
main separately. On the OVERNIGHT dataset, this
improves accuracy from 75.6% to 79.6%, setting
a new state-of-the-art, while reducing the number
of parameters by a factor of 7. To our knowledge,
this work is the first to train a semantic parser over
multiple KBs.
</p>
<p>2 Problem Setup
</p>
<p>We briefly review the model presented by Jia and
Liang (2016), which we base our model on.
</p>
<p>Semantic parsing can be viewed as a sequence-
to-sequence problem (Sutskever et al., 2014),
where a sequence of input language tokens x =
x1, . . . , xm is mapped to a sequence of output log-
ical tokens y1, . . . , yn .
</p>
<p>The encoder converts x1, . . . , xm into a
sequence of context sensitive embeddings
b1, . . . , bm using a bidirectional RNN (Bahdanau
et al., 2015): a forward RNN generates hidden
states hF1 , . . . , h
</p>
<p>F
m by applying the LSTM re-
</p>
<p>currence (Hochreiter and Schmidhuber, 1997):
hFi = LSTM(φ
</p>
<p>(in)(xi), h
F
i−1), where φ
</p>
<p>(in) is
an embedding function mapping a word xi to
a fixed-dimensional vector. A backward RNN
similarly generates hidden states hBm, . . . , h
</p>
<p>B
1 by
</p>
<p>processing the input sequence in reverse. Finally,
for each input position i, the representation bi is
the concatenation [hFi , h
</p>
<p>B
i ] . An attention-based
</p>
<p>decoder (Bahdanau et al., 2015; Luong et al.,
2015) generates output tokens one at a time. At
each time step j, it generates yj based on the
current hidden state sj , then updates the hidden
state sj+1 based on sj and yj . Formally, the
decoder is defined by the following equations:
</p>
<p>s1 = tanh(W
(s)[hFm, h
</p>
<p>B
1 ]),
</p>
<p>eji = s
&gt;
j W
</p>
<p>(a)bi,
</p>
<p>αji =
exp(eji)∑m
i′=1 eji′
</p>
<p>,
</p>
<p>cj =
</p>
<p>m∑
</p>
<p>i=1
</p>
<p>αjibi,
</p>
<p>p(yj = w | x, y1:j−1) ∝ exp(U [sj , cj ]),
sj+1 = LSTM([φ
</p>
<p>(out)(yj), cj ], sj),
</p>
<p>(1)
</p>
<p>where i ∈ {1, . . . ,m} and j ∈ {1, . . . , n}. The
matrices W (s), W (a), U , and the embedding func-
tion φ(out) are decoder parameters. We also em-
ploy attention-based copying as described by Jia
and Liang (2016), but omit details for brevity.
</p>
<p>The entire model is trained end-to-end by max-
imizing p(y | x) =∏nj=1 p(yj | x, y1:j−1).
</p>
<p>3 Models over Multiple KBs
</p>
<p>In this paper, we focus on a setting where we have
access to K training sets from different domains,
and each domain corresponds to a different KB. In
all domains the input is a language utterance and
the label is a logical form (we assume annotated
logical forms can be converted to a single formal
language such as lambda-DCS in Figure 1). While
the mapping from words to KB constants is spe-
cific to each domain, we expect that the manner in
which language expresses composition of mean-
ing to be shared across domains. We now describe
architectures that share information between the
encoders and decoders of different domains.
</p>
<p>3.1 One-to-one model
This model is similar to the baseline model de-
scribed in Section 2. As illustrated in Figure 2, it
consists of a single encoder and a single decoder,
which are used to generate outputs for all domains.
Thus, all model parameters are shared across do-
mains, and the model is trained from all examples.
Note that the number of parameters does not de-
pend on the number of domains K.
</p>
<p>Since there is no explicit representation of the
domain that is being decoded, the model must
learn to identify the domain given only the input.
To alleviate that, we encode the k’th domain by a
one-hot vector dk ∈ RK . At each step, the de-
coder updates the hidden state conditioned on the
domain’s one-hot vector, as well as on the previ-
ous hidden state, the output token and the context.
Formally, for domain k, Equation 1 is changed:1
</p>
<p>sj+1 = LSTM([φ
(out)(yj), cj , dk], sj). (2)
</p>
<p>Recently Johnson et al. (2016) used a similar in-
tuition for neural machine translation, where they
added an artificial token at the beginning of each
source sentence to specify the target language.
We implemented their approach and compare to
it in Section 4.
</p>
<p>1For simplicity, we omit the domain index k from our no-
tation whenever it can be inferred from context.
</p>
<p>624</p>
<p />
</div>
<div class="page"><p />
<p>“Thai restaurants at NYC”
</p>
<p>“3bd apt. near NYU”
</p>
<p>encoder decoder
</p>
<p>[0,1] [1, 0]
</p>
<p>“Thai restaurants at NYC”
</p>
<p>“3bd apt. near NYU”
</p>
<p>domain 1 
</p>
<p>encoder
</p>
<p>domain 2 
</p>
<p>encoder
</p>
<p>general 
</p>
<p>encoder
</p>
<p>domain 1 
</p>
<p>decoder
</p>
<p>domain 2 
</p>
<p>decoder
</p>
<p>Figure 2: Illustration of models with one example from the RESTAURANTS domain and another from the HOUSING domain.
Left: One-to-one model with optional domain encoding. Right: many-to-many model.
</p>
<p>Since we have one decoder for multiple do-
mains, tokens which are not in the domain vocab-
ulary could possibly be generated. We prevent that
at test time by excluding out-of-domain tokens be-
fore the softmax (p(yj | x, y1:j−1)) takes place.
</p>
<p>3.2 Many-to-many model
In this model, we keep a separate encoder and de-
coder for every domain, but augment the model
with an additional encoder that consumes exam-
ples from all domains (see Figure 2). This is moti-
vated by prior work on domain adaptation (Daume
III, 2007; Blitzer et al., 2011), where each example
has a representation that captures domain-specific
aspects of the example and a representation that
captures domain-general aspects. In our case, this
is achieved by encoding examples with a domain-
specific encoder as well as a domain-general en-
coder, and passing both representations to the de-
coder.
</p>
<p>Formally, we now have K + 1 encoders and
K decoders, and denote by hF,ki , h
</p>
<p>B,k
i , b
</p>
<p>k
i the for-
</p>
<p>ward state, backward state and their concatenation
at position i (the domain-general encoder has in-
dex K + 1). The hidden state of the decoder in
domain k is initialized from the domain-specific
and domain-general encoder:
</p>
<p>s1 = tanh(W
(s)[hF,km , h
</p>
<p>B,k
1 , h
</p>
<p>F,K+1
m , h
</p>
<p>B,K+1
1 ]).
</p>
<p>Then, we compute unnormalized attention
scores based on both encoders, and represent the
language context with both domain-general and
domain-specific representations. Equation 1 for
domain k is changed as follows:
</p>
<p>eji = s
&gt;
j W
</p>
<p>(a)[bki , b
K+1
i ],
</p>
<p>cj =
</p>
<p>m∑
</p>
<p>i=1
</p>
<p>αji[b
k
i , b
</p>
<p>K+1
i ].
</p>
<p>In this model, the number of encoding parameters
grows by a factor of 1k , and the number of decod-
ing parameters grows by less than a factor of 2.
</p>
<p>3.3 One-to-many model
</p>
<p>Here, a single encoder is shared, while we keep
a separate decoder for each domain. The shared
encoder captures the fact that the input in each do-
main is a sequence of English words. The domain-
specific decoders learn to output tokens from the
right domain vocabulary.
</p>
<p>4 Experiments
</p>
<p>4.1 Data
</p>
<p>We evaluated our system on the OVERNIGHT se-
mantic parsing dataset, which contains 13, 682 ex-
amples of language utterances paired with log-
ical forms across eight domains. OVERNIGHT
was constructed by generating logical forms from
a grammar and annotating them with language
through crowdsourcing. We evaluated on the same
train/test split as Wang et al. (2015), using the
same accuracy metric, that is, the proportion of
questions for which the denotations of the pre-
dicted and gold logical forms are equal.
</p>
<p>4.2 Implementation Details
</p>
<p>We replicate the experimental setup of Jia and
Liang (2016): We used the same hyper-parameters
without tuning; we used 200 hidden units and 100-
dimensional word vectors; we initialized parame-
ters uniformly within the interval [−0.1, 0.1], and
maximized the log likelihood of the correct logical
form with stochastic gradient descent. We trained
the model for 30 epochs with an initial learning
rate of 0.1, and halved the learning rate every 5
epochs, starting from epoch 15. We replaced word
vectors for words that occur only once in the train-
ing set with a universal &lt;unk&gt; word vector. At
test time, we used beam search with beam size 5.
We then picked the highest-scoring logical form
that does not yield an executor error when its de-
notation is computed. Our models were imple-
mented in Theano (Bergstra et al., 2010).
</p>
<p>625</p>
<p />
</div>
<div class="page"><p />
<p>Model Basketball Blocks Calendar Housing Publications Recipes Restaurants Social Avg. # Model Params
INDEP 85.2 61.2 77.4 67.7 74.5 79.2 79.5 80.2 75.6 14.1 M
MANY2MANY 83.9 63.2 79.8 75.1 75.2 81.5 79.8 82.4 77.6 22.8 M
ONE2MANY 84.4 59.1 79.8 74.6 80.1 81.5 80.7 81.1 77.7 8.6 M
INPUTTOKEN 85.9 63.2 79.2 77.8 75.8 80.6 82.5 81.0 78.2 2 M
ONE2ONE 84.9 63.4 75.6 76.7 78.9 83.8 81.3 81.4 78.3 2 M
DOMAINENCODING 86.2 62.7 82.1 78.3 80.7 82.9 82.2 81.7 79.6 2 M
</p>
<p>Table 1: Test accuracy for all models on all domains, along with the number of parameters for each model.
</p>
<p>4.3 Results
</p>
<p>For our main result, we trained on all eight
domains all models described in Section 3:
ONE2ONE, DOMAINENCODING and INPUTTO-
KEN representing respectively the basic one-to-
one model, with extensions of one-hot domain en-
coding or an extra input token, as described in
Section 3.1. MANY2MANY and ONE2MANY are
the models described in Sections 3.2 and 3.3, re-
spectively. INDEP is the baseline sequence-to-
sequence model described in Section 2, which
trained independently on each domain.
</p>
<p>Results show (Table 1) that training on multi-
ple KBs improves average accuracy over all do-
mains for all our proposed models, and that per-
formance improves as more parameters are shared.
Our strongest results come when parameter shar-
ing is maximal (i.e., single encoder and single de-
coder), coupled with a one-hot domain represen-
tation at decoding time (DOMAINENCODING). In
this case accuracy improves not only on average,
but also for each domain separately. Moreover, the
number of model parameters necessary for train-
ing the model is reduced by a factor of 7.
</p>
<p>Our baseline, INDEP, is a reimplementation of
the NORECOMBINATION model described in Jia
and Liang (2016), which achieved average accu-
racy of 75.8% (corresponds to our 75.6% result).
Jia and Liang (2016) also introduced a framework
for generating new training examples in a single
domain through recombination. Their model that
uses the most training data achieved state-of-the-
art average accuracy of 77.5% on OVERNIGHT.
We show that by training over multiple KBs we
can achieve higher average accuracy, and our best
model, DOMAINENCODING, sets a new state-of-
the-art average accuracy of 79.6%.
</p>
<p>Figure 3 shows a learning curve for all mod-
els on the test set, when training on a frac-
tion of the training data. We observe that the
difference between models that share parame-
ters (INPUTTOKEN, ONE2ONE and DOMAINEN-
CODING) and models that keep most of the pa-
</p>
<p>0 0.1 0.2 0.5 1
30
</p>
<p>40
</p>
<p>50
</p>
<p>60
</p>
<p>70
</p>
<p>80
</p>
<p>Training data fraction
</p>
<p>A
ve
</p>
<p>ra
ge
</p>
<p>ac
cu
</p>
<p>ra
cy
</p>
<p>INDEP
MANY2MANY
ONE2MANY
INPUTTOKEN
</p>
<p>ONE2ONE
DOMAINENCODING
</p>
<p>Figure 3: Learning curves for all models on the test set.
</p>
<p>rameters separate (INDEP, MANY2MANY and
ONE2MANY) is especially pronounced when the
amount of data is small, reaching a difference of
more than 15 accuracy point with 10% of the train-
ing data. This highlights the importance of using
additional data from a similar distribution without
increasing the number of parameters when there is
little data. The learning curve also suggests that
the MANY2MANY model improves considerably
as the amount of data increases, and it would be
interesting to examine its performance on larger
datasets.
</p>
<p>4.4 Analysis
Learning a semantic parser involves mapping lan-
guage phrases to KB constants, as well as learning
how language composition corresponds to logical
form composition. We hypothesized that the main
benefit of training on multiple KBs lies in learn-
ing about compositionality. To verify that, we ap-
pend the domain index to the name of every con-
stant in every KB, and therefore constant names
are disjoint across datasets. We train DOMAINEN-
CODING on this dataset and obtain an accuracy
of 79.1% (comparing to 79.6%), which hints that
most of the gain is attributed to compositionality
rather than mapping of language to KB constants.
</p>
<p>We also inspected cases where DOMAINEN-
</p>
<p>626</p>
<p />
</div>
<div class="page"><p />
<p>CODING performed better than INDEP, by ana-
lyzing errors on a development set (20% of the
training data). We found 45 cases where INDEP
makes an error (and DOMAINENCODING does
not) by predicting a wrong comparative or superla-
tive structure (e.g., &gt; instead of ≥). However,
the opposite case occurs only 29 times. This re-
iterates how we learn structural linguistic regular-
ities when sharing parameters.
</p>
<p>Lastly, we observed that the domain’s training
set size negatively correlates with its relative im-
provement in performance (DOMAINENCODING
accuracy compared to INDEP), where Spearman’s
ρ = −0.86. This could be explained by the ten-
dency of smaller domains to cover a smaller frac-
tion of structural regularities in language, thus,
they gain more by sharing information.
</p>
<p>5 Conclusion
</p>
<p>In this paper we address the challenge of obtaining
training data for semantic parsing from a new per-
spective. We propose that one can improve pars-
ing accuracy by training models over multiple KBs
and demonstrate this on the eight domains of the
OVERNIGHT dataset.
</p>
<p>In future work, we would like to further reduce
the burden of data gathering by training character-
level models that learn to map language phrases to
KB constants across datasets, and by pre-training
language side models that improve the encoder
from data that is independent of the KB. We also
plan to apply this method on datasets where only
denotations are provided rather than logical forms.
</p>
<p>Reproducibility
</p>
<p>All code, data, and experiments for
this paper are available on the CodaLab
platform at https://worksheets.
codalab.org/worksheets/
0xdec998f58deb4829aba80fbf49f69236/.
</p>
<p>Acknowledgments
</p>
<p>We thank Shimi Salant and the anonymous re-
viewers for their constructive feedback. This
work was partially supported by the Israel Science
Foundation, grant 942/16.
</p>
<p>References
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
</p>
<p>learning of semantic parsers for mapping instruc-
</p>
<p>tions to actions. Transactions of the Association for
Computational Linguistics (TACL) 1:49–62.
</p>
<p>D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural
machine translation by jointly learning to align and
translate. In International Conference on Learning
Representations (ICLR).
</p>
<p>J. Berant and P. Liang. 2014. Semantic parsing via
paraphrasing. In Association for Computational
Linguistics (ACL).
</p>
<p>J. Berant and P. Liang. 2015. Imitation learning of
agenda-based semantic parsers. Transactions of the
Association for Computational Linguistics (TACL)
3:545–558.
</p>
<p>J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pas-
canu, G. Desjardins, J. Turian, D. Warde-Farley, and
Y. Bengio. 2010. Theano: a CPU and GPU math
expression compiler. In Python for Scientific Com-
puting Conference.
</p>
<p>J. Blitzer, S. Kakade, and D. P. Foster. 2011. Domain
adaptation with coupled subspaces. In Artificial In-
telligence and Statistics (AISTATS). pages 173–181.
</p>
<p>R. Caruana. 1997. Multitask learning. Machine Learn-
ing 28:41–75.
</p>
<p>J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL). pages 18–27.
</p>
<p>R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research (JMLR) 12:2493–2537.
</p>
<p>H. Daume III. 2007. Frustratingly easy domain adap-
tation. In Association for Computational Linguistics
(ACL).
</p>
<p>L. Dong and M. Lapata. 2016. Language to logical
form with neural attention. In Association for Com-
putational Linguistics (ACL).
</p>
<p>O. Firat, K. Cho, and Y. Bengio. 2016. Multi-way, mul-
tilingual neural machine translation with a shared at-
tention mechanism. In North American Association
for Computational Linguistics (NAACL).
</p>
<p>S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation 9(8):1735–
1780.
</p>
<p>R. Jia and P. Liang. 2016. Data recombination for neu-
ral semantic parsing. In Association for Computa-
tional Linguistics (ACL).
</p>
<p>M. Johnson, M. Schuster, Q. V. Le, M. Krikun,
Y. Wu, Z. Chen, N. Thorat, F. Vigas, M. Wat-
tenberg, G. Corrado, M. Hughes, and J. Dean.
2016. Google’s multilingual neural machine trans-
lation system: Enabling zero-shot translation. arXiv
preprint arXiv:1611.04558 .
</p>
<p>627</p>
<p />
</div>
<div class="page"><p />
<p>J. Krishnamurthy and T. Mitchell. 2012. Weakly
supervised training of semantic parsers. In Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP/CoNLL). pages 754–765.
</p>
<p>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in CCG
grammar induction for semantic parsing. In Em-
pirical Methods in Natural Language Processing
(EMNLP). pages 1512–1523.
</p>
<p>P. Liang. 2013. Lambda dependency-based composi-
tional semantics. arXiv .
</p>
<p>P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
pages 590–599.
</p>
<p>M. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and
L. Kaiser. 2016. Multi-task sequence to sequence
learning. In International Conference on Learning
Representations (ICLR).
</p>
<p>M. Luong, H. Pham, and C. D. Manning. 2015. Effec-
tive approaches to attention-based neural machine
translation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP). pages 1412–1421.
</p>
<p>S. Reddy, M. Lapata, and M. Steedman. 2014. Large-
scale semantic parsing without question-answer
pairs. Transactions of the Association for Compu-
tational Linguistics (TACL) 2(10):377–392.
</p>
<p>I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence
to sequence learning with neural networks. In Ad-
vances in Neural Information Processing Systems
(NIPS). pages 3104–3112.
</p>
<p>Y. Wang, J. Berant, and P. Liang. 2015. Building a
semantic parser overnight. In Association for Com-
putational Linguistics (ACL).
</p>
<p>M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic program-
ming. In Association for the Advancement of Arti-
ficial Intelligence (AAAI). pages 1050–1055.
</p>
<p>L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI). pages 658–
666.
</p>
<p>628</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 629–634
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2099
</p>
<p>Representing Sentences as Low-Rank Subspaces
</p>
<p>Jiaqi Mu, Suma Bhat, and Pramod Viswanath
University of Illinois at Urbana Champaign
</p>
<p>{jiaqimu2, spbhat2, pramodv}@illinois.edu
</p>
<p>Abstract
</p>
<p>Sentences are important semantic units of
natural language. A generic, distributional
representation of sentences that can cap-
ture the latent semantics is beneficial to
multiple downstream applications. We ob-
serve a simple geometry of sentences –
the word representations of a given sen-
tence (on average 10.23 words in all Se-
mEval datasets with a standard deviation
4.84) roughly lie in a low-rank subspace
(roughly, rank 4). Motivated by this ob-
servation, we represent a sentence by the
low-rank subspace spanned by its word
vectors. Such an unsupervised represen-
tation is empirically validated via seman-
tic textual similarity tasks on 19 different
datasets, where it outperforms the sophis-
ticated neural network models, including
skip-thought vectors, by 15% on average.
</p>
<p>1 Introduction
</p>
<p>Real-valued word representations have brought a
fresh approach to classical problems in NLP, rec-
ognized for their ability to capture linguistic reg-
ularities: similar words tend to have similar rep-
resentations; similar word pairs tend to have sim-
ilar difference vectors (Bengio et al., 2003; Mnih
and Hinton, 2007; Mikolov et al., 2010; Collobert
et al., 2011; Huang et al., 2012; Dhillon et al.,
2012; Mikolov et al., 2013; Pennington et al.,
2014; Levy and Goldberg, 2014; Arora et al.,
2015; Stratos et al., 2015). Going beyond words,
sentences capture much of the semantic informa-
tion. Given the success of lexical representations,
a natural question of great topical interest is how to
extend the power of distributional representations
to sentences.
</p>
<p>There are currently two approaches to represent
</p>
<p>sentences. A sentence contains rich syntactic in-
formation and can be modeled through sophisti-
cated neural networks (e.g., convolutional neural
networks (Kim, 2014; Kalchbrenner et al., 2014),
recurrent neural networks (Sutskever et al., 2014;
Le and Mikolov, 2014; Kiros et al., 2015; Hill
et al., 2016) and recursive neural networks (Socher
et al., 2013)). Another simple and common ap-
proach ignores the latent structure of sentences:
a prototypical approach is to represent a sentence
by summing or averaging over the vectors of the
words in this sentence (Wieting et al., 2015; Adi
et al., 2016; Kenter et al., 2016).
</p>
<p>Recently, Wieting et al. (2015); Adi et al. (2016)
reveal that even though the latter approach ignores
all syntactic information, it is simple, straightfor-
ward, and remarkably robust at capturing the sen-
tential semantics. Such an approach successfully
outperforms the neural network based approaches
on textual similarity tasks in both supervised and
unsupervised settings.
</p>
<p>We follow the latter approach but depart from
representing sentences in a vector space as in these
prior works; we present a novel Grassmannian
property of sentences. The geometry is motivated
by (Gong et al., 2017; Mu et al., 2016) where an
interesting phenomenon is observed – the local
context of a given word/phrase can be well rep-
resented by a low rank subspace. We propose to
generalize this observation to sentences: not only
do the word vectors in a snippet of a sentence (i.e.,
a context for a given word defined as several words
surrounding it) lie in a low-rank subspace, but the
entire sentence (on average 10.23 words in all Se-
mEval datasets with standard deviation 4.84) fol-
lows this geometric property as well:
</p>
<p>Geometry of Sentences: The word rep-
resentations lie in a low-rank subspaces
(rank 3-5) for all words in a target sen-
</p>
<p>629</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2099">https://doi.org/10.18653/v1/P17-2099</a></div>
</div>
<div class="page"><p />
<p>tence.
</p>
<p>The observation indicates that the subspace con-
tains most of the information about this sentence,
and therefore motivates a sentence representation
method: the sentences should be represented in the
space of subspaces (i.e., the Grassmannian mani-
fold) instead of a vector space; formally:
</p>
<p>Sentence Representation: A sentence
can be represented by a low-rank sub-
space spanned by its word representa-
tions.
</p>
<p>Analogous to word representations of similar
words being similar vectors, the principle of sen-
tence representations is: similar sentences should
have similar subspaces. Two questions arise: (a)
how to define the similarity between sentences
and (b) how to define the similarity between sub-
spaces.
</p>
<p>The first question has been already addressed
by the popular semantic textual similarity (STS)
tasks. Unlike textual entailment (which aims at in-
ferring directional relation between two sentences)
and paraphrasing (which is a binary classifica-
tion problem), STS provides a unified framework
of measuring the degree of semantic equivalence
(Agirre et al., 2012, 2013, 2014, 2015) in a contin-
uous fashion. Motivated by the cosine similarity
between vectors being a good index for word sim-
ilarity, we generalize this metric to subspaces: the
similarity between subspaces defined in this paper
is the `2-norm of the singular values between two
subspaces; note that the singular values are in fact
the cosine of the principal angles.
</p>
<p>The key justification for our approach comes
from empirical results that outperform state-of-
the-art in some cases, and being comparable in
others. In summary, representing sentences by
subspaces outperforms representing sentences by
averaged word vectors (by 14% on average) and
sophisticated neural networks (by 15%) on 19 dif-
ferent STS datasets, ranging over different do-
mains (News, WordNet definition, and Twitter).
</p>
<p>2 Geometry of Sentences
</p>
<p>Assembling successful distributional word repre-
sentations (for example, GloVe (Pennington et al.,
2014)) into sentence representations is an active
research topic. Different from previous studies
(for example, doc2vec (Mikolov et al., 2013),
skip-thought vectors (Kiros et al., 2015), Siamese
</p>
<p>CBOW (Kenter et al., 2016)), our main contri-
bution is to represent sentences using non-vector
space representations: a sentence can be well rep-
resented by the subspace spanned by the con-
text word vectors – such a method naturally
builds on any word representation method. Due
to the widespread use of word2vec and GloVe,
we use their publicly available word representa-
tions – word2vec(Mikolov et al., 2013) trained us-
ing Google News1 and GloVe (Pennington et al.,
2014) trained using Common Crawl2 – to test our
observations.
</p>
<p>Observation Let v(w) ∈ Rd be the d-
dimensional word representation for a given word
w ∈ V , and s = (w1, . . . , wn) be a given sen-
tence. Consider the following sentence where
n = 32:
</p>
<p>They would not tell me if there was any
pension left here, and would only tell me
if there was (and how much there was) if
they saw I was entitled to it.
</p>
<p>After stacking the (non-functional) word vectors
v(w) to form a d×n matrix, we observe that most
energy (80% for GloVe and 72% for word2vec) of
such a matrix is contained in a rank-N subspace,
where N is much smaller than n (for comparison,
we choose N to be 4 and therefore N/n ≈ 13%).
Figure 1 provides a visual representation of this
geometric phenomenon, where we have projected
the d-dimensional word representations into 3-
dimensional vectors and use these 3-dimensional
word vectors to get the subspace for this sentence
(we set N = 2 here for visualization), and plot the
subspaces as 2-dimensional planes.
</p>
<p>Geometry of Sentences The example above
generalizes to a vast majority of the sentences: the
word representations of a given sentence roughly
reside in a low-rank subspace, which can be ex-
tracted by principal component analysis (PCA).
</p>
<p>Verification We empirically validate this ge-
ometric phenomenon by collecting 53,396 sen-
tences from the SemEval STS share tasks (Agirre
et al., 2012, 2013, 2014, 2015) and plotting the
fraction of energy being captured by the top N
components of PCA in Figure 2 for N = 3, 4, 5,
</p>
<p>1https://code.google.com/archive/p/
word2vec/
</p>
<p>2http://nlp.stanford.edu/projects/
glove/
</p>
<p>630</p>
<p />
</div>
<div class="page"><p />
<p>54
32
10
</p>
<p>12
3
</p>
<p>3210123
3
</p>
<p>2
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>GloVe
word2vec
</p>
<p>Figure 1: Geometry of sentences.
</p>
<p>from where we can observe that on average 70%
of the energy is captured by a rank-3 subspace,
and 80% for a rank-4 subspace and 90% for rank-5
subspace. For comparison, the fraction of energy
of random sentences (generated i.i.d. from the un-
igram distribution) are also plotted in Figure 2.
</p>
<p>2 4 6 8 10
rank N
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>fra
ct
</p>
<p>io
n 
</p>
<p>of
 e
</p>
<p>ne
rg
</p>
<p>y
</p>
<p>GloVe-SemEval
GloVe-random
word2vec-SemEval
word2vec-random
</p>
<p>Figure 2: Fraction of energy captured by the top
principal components.
</p>
<p>Representation The observation above moti-
vates our sentence representation algorithm: since
the words in a sentence concentrate on a few di-
rections, the subspace spanned by these directions
could in principle be a proper representation for
this sentence. The direction and subspace in turn
can be extracted via PCA as in Algorithm 1.
</p>
<p>Similarity Metric The principle of sentence
representations is that similar sentences should
have similar representations. In this case, we
expect the similarity between subspaces to be a
good index for the semantic similarity of sen-
tences. In our paper, we define the similarity be-
tween subspaces as follows: let u1(s), ..., uN (s)
be the N orthonormal basis for a sentence s. Af-
ter stacking the N vectors in a d × N matrix
</p>
<p>Algorithm 1: The algorithm for sentence rep-
resentations.
Input : a sentence s, word embeddings v(·),
</p>
<p>and a PCA rank N .
1 Compute the first N principle components of
</p>
<p>samples v(w′), w′ ∈ c,
</p>
<p>u1, ..., uN ← PCA(v(w′), w′ ∈ s),
</p>
<p>S ←
{
</p>
<p>N∑
</p>
<p>n=1
</p>
<p>: αnun, αn ∈ R
}
</p>
<p>Output: N orthonormal basis u1, ..., uN and
a subspace S.
</p>
<p>U(s) = (u1(s), ..., uN (s)), we define the corre-
sponding cosine similarity as, CosSim(s1, s2) =√∑N
</p>
<p>t=1 σ
2
t , where σt is the t-th singular value of
</p>
<p>U(s1)
TU(s2).
</p>
<p>Note that σt = cos(θt) where θt is the t-th
“principle angle” between two subspaces. Such
a metric is naturally related to the cosine similar-
ity between vectors, which has been empirically
validated to be a good measure of word similarity.
</p>
<p>3 Experiments
</p>
<p>In this section we evaluate our sentence represen-
tations empirically on the STS tasks. The objec-
tive of this task is to test the degree to which the al-
gorithm can capture the semantic equivalence be-
tween two sentences. For example, the similarity
between “a kid jumping a ledge with a bike” and
“a black and white cat playing with a blanket” is 0
and the similarity between “a cat standing on tree
branches” and “a black and white cat is high up on
tree branches” is 3.6. The algorithm is then evalu-
ated in terms of Pearson’s correlation between the
predicted score and the human judgment.
</p>
<p>Datasets We test the performances of our algo-
rithm on 19 different datasets, which include Se-
mEval STS share tasks (Agirre et al., 2012, 2013,
2014, 2015), sourced from multiple domains (for
example, News, WordNet definitions and Twitter).
</p>
<p>Baselines and Preliminaries Our main compar-
isons are with algorithms that perform unsuper-
vised sentence representation: average of word
representations (i.e., avg. (of GloVe and skip-
gram) where we use the average of word vec-
tors), doc2vec (D2V) (Le and Mikolov, 2014) and
</p>
<p>631</p>
<p />
</div>
<div class="page"><p />
<p>sophisticated neural networks (i.e., skip-thought
vectors (ST) (Kiros et al., 2015), Siamese CBOW
(SC) (Kenter et al., 2016)). In order to enable a
fair comparison, we use the Toronto Book Cor-
pus (Zhu et al., 2015) to train word embeddings.
In our experiment, we adapt the same setting as
in (Kenter et al., 2016) where we use skip-gram
(Mikolov et al., 2013) of and GloVe (Pennington
et al., 2014) to train a 300-dimensional word vec-
tors for the words that occur 5 times or more in the
training corpus. The rank of subspaces is set to be
4 for both word2vec and GloVe.
</p>
<p>Results The detailed results are reported in Ta-
ble 1, from where we can observe two phenom-
ena: (a) representing sentences by its averaged
word vectors provides a strong baseline and the
performances are remarkably stable across differ-
ent datasets; (b) our subspace-based method out-
performs the average-based method by 14% on av-
erage and the neural network based approaches by
15%. This suggests that representing sentences by
subspaces maintains more information than sim-
ply taking the average, and is more robust than
highly-tuned sophisticated models.
</p>
<p>When we average over the words, the average
vector is biased because of many irrelevant words
(for example, function words) in a given sentence.
Therefore, given a longer sentence, the effect of
useful word vectors become smaller and thus the
average vector is less reliable at capturing the se-
mantics. On the other hand, the subspace repre-
sentation is immune to this phenomenon: the word
vectors capturing the semantics of the sentence
tend to concentrate on a few directions which
dominate the subspace representation.
</p>
<p>4 Conclusion
</p>
<p>This paper presents a novel unsupervised sentence
representation leveraging the Grassmannian ge-
ometry of word representations. While the cur-
rent approach relies on the pre-trained word rep-
resentations, the joint learning of both word and
sentence representations and in conjunction with
supervised datasets such the Paraphrase Database
(PPDB) (Ganitkevitch et al., 2013) is left to fu-
ture research. Also interesting is the exploration of
neural network architectures that operate on sub-
spaces (as opposed to vectors), allowing for down-
stream evaluations of our novel representation.
</p>
<p>References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
</p>
<p>Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207 .
</p>
<p>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, et al. 2015. Semeval-2015 task 2: Se-
mantic textual similarity, english, spanish and pilot
on interpretability. In Proceedings of the 9th inter-
national workshop on semantic evaluation (SemEval
2015). pages 252–263.
</p>
<p>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. Semeval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th international workshop on semantic evaluation
(SemEval 2014). pages 81–91.
</p>
<p>Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. sem 2013 shared
task: Semantic textual similarity, including a pilot
on typed-similarity. In In* SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics. Association for Computational Linguistics.
Citeseer.
</p>
<p>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics-Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation. Association for Computa-
tional Linguistics, pages 385–393.
</p>
<p>Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
and Andrej Risteski. 2015. Rand-walk: A latent
variable model approach to word embeddings. arXiv
preprint arXiv:1502.03520 .
</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. journal of machine learning research
3(Feb):1137–1155.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12(Aug):2493–2537.
</p>
<p>Paramveer Dhillon, Jordan Rodu, Dean Foster, and
Lyle Ungar. 2012. Two step cca: A new spec-
tral method for estimating vector models of words.
arXiv preprint arXiv:1206.6403 .
</p>
<p>Juri Ganitkevitch, Benjamin Van Durme, and
Chris Callison-Burch. 2013. PPDB: The para-
phrase database. In Proceedings of NAACL-
</p>
<p>632</p>
<p />
</div>
<div class="page"><p />
<p>HLT . Association for Computational Lin-
guistics, Atlanta, Georgia, pages 758–764.
http://cs.jhu.edu/ ccb/publications/ppdb.pdf.
</p>
<p>Hongyu Gong, Suma Bhat, and Pramod Viswanath.
2017. Geometry of compositionality. Association
for Advancement of Artificial Intelligence (AAAI) .
</p>
<p>Felix Hill, Kyunghyun Cho, and Anna Korhonen.
2016. Learning distributed representations of
sentences from unlabelled data. arXiv preprint
arXiv:1602.03483 .
</p>
<p>Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1. Association for Com-
putational Linguistics, pages 873–882.
</p>
<p>Nal Kalchbrenner, Edward Grefenstette, and Phil
Blunsom. 2014. A convolutional neural net-
work for modelling sentences. arXiv preprint
arXiv:1404.2188 .
</p>
<p>Tom Kenter, Alexey Borisov, and Maarten de Rijke.
2016. Siamese cbow: Optimizing word embed-
dings for sentence representations. arXiv preprint
arXiv:1606.04640 .
</p>
<p>Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .
</p>
<p>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems.
pages 3294–3302.
</p>
<p>Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.
</p>
<p>Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems.
pages 2177–2185.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech. volume 2, page 3.
</p>
<p>Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning. ACM, pages 641–648.
</p>
<p>Jiaqi Mu, Suma Bhat, and Pramod Viswanath.
2016. Geometry of polysemy. arXiv preprint
arXiv:1610.07569 .
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
43.
</p>
<p>Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP).
Citeseer, volume 1631, page 1642.
</p>
<p>Karl Stratos, Michael Collins, and Daniel Hsu. 2015.
Model-based word embeddings from decomposi-
tions of count matrices. In Proceedings of ACL.
pages 1282–1291.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.
</p>
<p>John Wieting, Mohit Bansal, Kevin Gimpel, and
Karen Livescu. 2015. Towards universal para-
phrastic sentence embeddings. arXiv preprint
arXiv:1511.08198 .
</p>
<p>Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watch-
ing movies and reading books. In arXiv preprint
arXiv:1506.06724.
</p>
<p>633</p>
<p />
</div>
<div class="page"><p />
<p>dataset l ST SC D2V
GloVe skip-gram
</p>
<p>avg. subspace avg. subspace
</p>
<p>2012
</p>
<p>MSRpar 17.70 5.60 43.79 14.85 46.18 40.74 16.82 35.71
MSRvid 6.63 58.07 45.22 19.82 63.75 73.90 58.28 63.19
OnWn 7.57 60.45 64.44 35.73 56.72 63.21 42.22 58.43
</p>
<p>SMTeuroparl 10.70 42.03 45.03 36.18 52.51 45.83 37.99 45.35
SMTnews 11.72 39.11 39.02 52.78 38.99 45.73 23.44 37.73
</p>
<p>2013
FNWN 19.90 31.24 23.22 51.07 39.29 41.03 19.35 26.43
OnWn 7.17 24.18 49.85 49.26 52.48 72.03 58.30 56.52
</p>
<p>headlines 7.21 38.61 65.34 28.90 49.07 66.13 41.53 62.84
</p>
<p>2014
</p>
<p>OnWn 7.74 48.82 60.73 60.84 60.15 76.28 55.38 67.13
deft-forum 8.38 37.36 40.82 22.63 22.75 42.60 32.87 45.30
deft-news 15.78 46.17 59.13 18.93 62.91 64.40 38.72 53.62
headlines 7.43 40.31 63.64 24.31 46.00 62.42 36.46 61.44
images 9.12 42.57 64.97 39.92 55.19 73.38 45.17 71.84
</p>
<p>tweet-news 10.03 51.38 73.15 33.56 60.45 74.29 44.16 73.87
</p>
<p>2015
</p>
<p>answer-forum 15.03 27.84 21.81 28.59 31.39 69.50 34.83 57.62
answer-studetns 10.44 26.61 36.71 11.14 48.46 63.43 43.85 59.01
</p>
<p>belief 13.00 45.84 47.69 30.58 44.73 69.65 49.24 64.48
headlines 7.50 12.48 21.51 22.64 44.80 65.67 44.23 68.02
images 9.12 21.00 25.60 34.14 66.40 80.12 56.47 70.53
</p>
<p>Table 1: Pearson’s correlation (x100) on SemEval textual similarity task using 19 different datasets,
where l is the average sentence length of each dataset. Results that are better than the baselines are
marked with underlines and the best results are in bold.
</p>
<p>634</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 635–640
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2100
</p>
<p>Improving Semantic Relevance for Sequence-to-Sequence Learning of
Chinese Social Media Text Summarization
</p>
<p>Shuming Ma1,2, Xu Sun1,2, Jingjing Xu1,2, Houfeng Wang1,2, Wenjie Li3, Qi Su4
1MOE Key Laboratory of Computational Linguistics, Peking University
</p>
<p>2School of Electronics Engineering and Computer Science, Peking University
3Department of Computing, The Hong Kong Polytechnic University
</p>
<p>4School of Foreign Languages, Peking University
{shumingma, xusun, jingjingxu, wanghf, sukia}@pku.edu.cn
</p>
<p>cswjli@comp.polyu.edu.hk
</p>
<p>Abstract
</p>
<p>Current Chinese social media text summa-
rization models are based on an encoder-
decoder framework. Although its gener-
ated summaries are similar to source texts
literally, they have low semantic relevance.
In this work, our goal is to improve se-
mantic relevance between source texts and
summaries for Chinese social media sum-
marization. We introduce a Semantic Rel-
evance Based neural model to encourage
high semantic similarity between texts and
summaries. In our model, the source tex-
t is represented by a gated attention en-
coder, while the summary representation
is produced by a decoder. Besides, the
similarity score between the representa-
tions is maximized during training. Our
experiments show that the proposed mod-
el outperforms baseline systems on a so-
cial media corpus.
</p>
<p>1 Introduction
</p>
<p>Text summarization is to produce a brief sum-
mary of the main ideas of the text. For long
and normal documents, extractive summarization
achieves satisfying performance by selecting a few
sentences from source texts (Radev et al., 2004;
Woodsend and Lapata, 2010; Cheng and Lapata,
2016). However, it does not apply to Chinese
social media text summarization, where texts are
comparatively short and often full of noise. There-
fore, abstractive text summarization, which is
based on encoder-decoder framework, is a better
choice (Rush et al., 2015; Hu et al., 2015).
</p>
<p>For extractive summarization, the selected sen-
tences often have high semantic relevance to the
text. However, for abstractive text summariza-
tion, current models tend to produce grammatical
</p>
<p>Text: 昨晚，中联航空成都飞北京一架航班
被发现有多人吸烟。后因天气原因，飞机
备降太原机场。有乘客要求重新安检，机
长决定继续飞行，引起机组人员与未吸烟
乘客冲突。
Last night, several people were caught to smo-
ke on a flight of China United Airlines from
Chendu to Beijing. Later the flight temporari-
ly landed on Taiyuan Airport. Some passeng-
ers asked for a security check but were denied
by the captain, which led to a collision betwe-
en crew and passengers.
</p>
<p>RNN:中联航空机场发生爆炸致多人死亡。
China United Airlines exploded in the airport,
leaving several people dead.
</p>
<p>Gold: 航班多人吸烟机组人员与乘客冲突。
Several people smoked on a flight which led
to a collision between crew and passengers.
</p>
<p>Figure 1: An example of RNN generated summa-
ry. It has high similarity to the text literally, but
low semantic relevance.
</p>
<p>and coherent summaries regardless of its semantic
relevance with source texts. Figure 1 shows that
the summary generated by a current model (RNN
encoder-decoder) is similar to the source text liter-
ally, but it has low semantic relevance.
</p>
<p>In this work, our goal is to improve the seman-
tic relevance between source texts and generat-
ed summaries for Chinese social media text sum-
marization. To achieve this goal, we propose a
Semantic Relevance Based neural model. In our
model, a similarity evaluation component is intro-
duced to measure the relevance of source texts and
generated summaries. During training, it maxi-
mizes the similarity score to encourage high se-
mantic relevance between source texts and sum-
</p>
<p>635</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2100">https://doi.org/10.18653/v1/P17-2100</a></div>
</div>
<div class="page"><p />
<p>maries. The representation of source texts is pro-
duced by an encoder, while that of summaries is
computed by a decoder. We introduce a gated at-
tention encoder to better represent the source tex-
t. Besides, our decoder generates summaries and
provide the summary representation. Experiments
show that our proposed model has better perfor-
mance than baseline systems on the social media
corpus.
</p>
<p>2 Background: Chinese Abstractive Text
Summarization
</p>
<p>Current Chinese social media text summarization
model is based on encoder-decoder framework.
Encoder-decoder model is able to compress source
texts x = {x1, x2, ..., xN} into continuous vec-
tor representation with an encoder, and then gen-
erate the summary y = {y1, y2, ..., yM} with a de-
coder. In the previous work (Hu et al., 2015), the
encoder is a bi-directional gated recurrent neural
network, which maps source texts into sentence
vector {h1, h2, ..., hN}. The decoder is a uni-
directional recurrent neural network, which pro-
duces the distribution of output words yt with pre-
vious hidden state st−1 and word yt−1:
</p>
<p>p(yt|x) = softmaxf(st−1, yt−1) (1)
</p>
<p>where f is recurrent neural network output func-
tion, and s0 is the last hidden state of encoder hN .
</p>
<p>Attention mechanism is introduced to bet-
ter capture context information of source
texts (Bahdanau et al., 2014). Attention vector ct
is represented by the weighted sum of encoder
hidden states:
</p>
<p>ct =
</p>
<p>N∑
</p>
<p>i=1
</p>
<p>αtihi (2)
</p>
<p>αti =
eg(st,hi)
</p>
<p>∑N
j=1 e
</p>
<p>g(st,hj)
(3)
</p>
<p>where g(st, hi) is a relevant score between de-
coder hidden state st and encoder hidden state
hi. When predicting an output word, the decoder
takes account of attention vector, which contain-
s the alignment information between source texts
and summaries.
</p>
<p>3 Proposed Model
</p>
<p>Our assumption is that source texts and sum-
maries have high semantic relevance, so our pro-
posed model encourages high similarity between
</p>
<p>A B C
</p>
<p>X Y
</p>
<p>Y Z
</p>
<p>Attention
</p>
<p>Cos
</p>
<p>Figure 2: Our Semantic Relevance Based neural
model. It consists of decoder (above), encoder
(below) and cosine similarity function.
</p>
<p>their representations. Figure 2 shows our pro-
posed model. The model consists of three compo-
nents: encoder, decoder and a similarity function.
The encoder compresses source texts into seman-
tic vectors, and the decoder generates summaries
and produces semantic vectors of the generated
summaries. Finally, the similarity function eval-
uates the relevance between the sematic vectors of
source texts and generated summaries. Our train-
ing objective is to maximize the similarity score so
that the generated summaries have high semantic
relevance with source texts.
</p>
<p>3.1 Text Representation
There are several methods to represent a text or
a sentence, such as mean pooling of RNN output
or reserving the last state of RNN. In our model,
source text is represented by a gated attention en-
coder (Hahn and Keller, 2016). Every upcoming
word is fed into a gated attention network, which
measures its importance. The gated attention net-
work outputs the important score with a feedfor-
ward network. At each time step, it inputs a word
vector et and its previous context vector ht, then
outputs the score βt. Then the word vector et is
multiplied by the score βt, and fed into RNN en-
coder. We select the last output hN of RNN en-
coder as the semantic vector of the source text Vt.
</p>
<p>A natural idea to get the semantic vector of a
summary is to feed it into the encoder as well.
However, this method wastes much time because
</p>
<p>636</p>
<p />
</div>
<div class="page"><p />
<p>we encode the same sentence twice. Actually, the
last output sM contains information of both source
text and generated summaries. We simply com-
pute the semantic vector of the summary by sub-
tracting hN from sM :
</p>
<p>Vs = sM − hN (4)
</p>
<p>Previous work has proved that it is effective to
represent a span of words without encoding them
once more (Wang and Chang, 2016).
</p>
<p>3.2 Semantic Relevance
Our goal is to compute the semantic relevance of
source text and generated summary given seman-
tic vector Vt and Vs. Here, we use cosine simi-
larity to measure the semantic relevance, which is
represented with a dot product and magnitude:
</p>
<p>cos(Vs, Vt) =
Vs · Vt
</p>
<p>∥Vs∥∥Vt∥
(5)
</p>
<p>Source text and summary share the same language,
so it is reasonable to assume that their semantic
vectors are distributed in the same space. Cosine
similarity is a good way to measure the distance
between two vectors in the same space.
</p>
<p>3.3 Training
Given the model parameter θ and input text x, the
model produces corresponding summary y and se-
mantic vector Vs and Vt. The objective is to mini-
mize the loss function:
</p>
<p>L = −p(y|x; θ) − λcos(Vs, Vt) (6)
</p>
<p>where p(y|x; θ) is the conditional probability of
summaries given source texts, and is computed by
the encoder-decoder model. cos(Vs, Vt) is cosine
similarity of semantic vectors Vs and Vt. This term
tries to maximize the semantic relevance between
source input and target output.
</p>
<p>4 Experiments
</p>
<p>In this section, we present the evaluation of our
model and show its performance on a popular so-
cial media corpus. Besides, we use a case to ex-
plain the semantic relevance between generated
summary and source text.
</p>
<p>4.1 Dataset
Our dataset is Large Scale Chinese Short Tex-
t Summarization Dataset (LCSTS), which is con-
structed by Hu et al. (2015). The dataset consists
</p>
<p>of more than 2.4 million text-summary pairs, con-
structed from a famous Chinese social media web-
site called Sina Weibo1. It is split into three parts,
with 2,400,591 pairs in PART I, 10,666 pairs in
PART II and 1,106 pairs in PART III. All the text-
summary pairs in PART II and PART III are man-
ually annotated with relevant scores ranged from
1 to 5, and we only reserve pairs with scores no
less than 3. Following the previous work, we use
PART I as training set, PART II as development
set, and PART III as test set.
</p>
<p>4.2 Experiment Setting
</p>
<p>To alleviate the risk of word segmentation mis-
takes (Xu and Sun, 2016), we use Chinese charac-
ter sequences as both source inputs and target out-
puts. We limit the model vocabulary size to 4000,
which covers most of the common characters.
Each character is represented by a random initial-
ized word embedding. We tune our parameter on
the development set. In our model, the embed-
ding size is 400, the hidden state size of encoder-
decoder is 500, and the size of gated attention net-
work is 1000. We use Adam optimizer to learn
the model parameters, and the batch size is set as
32. The parameter λ is 0.0001. Both the encoder
and decoder are based on LSTM unit. Follow-
ing the previous work (Hu et al., 2015), our eval-
uation metric is F-score of ROUGE: ROUGE-1,
ROUGE-2 and ROUGE-L (Lin and Hovy, 2003).
</p>
<p>4.3 Baseline Systems
</p>
<p>RNN. We denote RNN as the basic sequence-to-
sequence model with bi-directional GRU encoder
and uni-directional GRU decoder. It is a widely
used language generated framework, so it is an im-
portant baseline.
RNN context. RNN context is a sequence-to-
sequence framework with neural attention. Atten-
tion mechanism helps capture the context informa-
tion of source texts. This model is a stronger base-
line system.
</p>
<p>4.4 Results and Discussions
</p>
<p>We compare our model with above baseline sys-
tems, including RNN and RNN context. We refer
to our proposed Semantic Relevance Based neural
model as SRB. Besides, SRB with a gated atten-
tion encoder is denoted as +Attention. Table 1
</p>
<p>1weibo.sina.com
</p>
<p>637</p>
<p />
</div>
<div class="page"><p />
<p>Model ROUGE-1 ROUGE-2 ROUGE-L
RNN (W) (Hu et al., 2015) 17.7 8.5 15.8
RNN (C) (Hu et al., 2015) 21.5 8.9 18.6
</p>
<p>RNN context (W) (Hu et al., 2015) 26.8 16.1 24.1
RNN context (C) (Hu et al., 2015) 29.9 17.4 27.2
</p>
<p>RNN context + SRB (C) 32.1 18.9 29.2
+Attention (C) 33.3 20.0 30.1
</p>
<p>Table 1: Results of our model and baseline systems. Our models achieve substantial improvement of all
ROUGE scores over baseline systems. (W: Word level; C: Character level).
</p>
<p>Text:仔细一算，上海的互联网公司不乏成功
案例，但最终成为BAT一类巨头的几乎没有,
这也能解释为何纳税百强的榜单中鲜少互联
网公司的身影。有一类是被并购，比如：易
趣、土豆网、PPS、PPTV、一号店等；有一
类是数年偏安于细分市场。
With careful calculation, there are many succe-
ssful Internet companies in Shanghai, but few
of them becomes giant company like BAT. Th-
is is also the reason why few Internet compan-
ies are listed in top hundred companies of pay-
ing tax. Some of them are merged, such as Eb-
ay, Tudou, PPS, PPTV, Yihaodian and so on.
Others are satisfied with segment market for
years.
</p>
<p>Gold:为什么上海出不了互联网巨头？
Why Shanghai comes out no giant company?
</p>
<p>RNN context:上海的互联网巨头。
Shanghai’s giant company.
</p>
<p>SRB:上海鲜少互联网巨头的身影。
Shanghai has few giant companies.
</p>
<p>Figure 3: An Example of RNN generated summa-
ry on LCSTS corpus.
</p>
<p>shows the results of our models and baseline sys-
tems. We can see SRB outperforms both RN-
N and RNN context in the F-score of ROUGE-1,
ROUGE-2 and ROUGE-L. It concludes that SR-
B generates more key words and phrases. With
a gated attention encoder, SRB achieves a bet-
ter performance with 33.3 F-score of ROUGE-1,
20.0 ROUGE-2 and 30.1 ROUGE-L. It shows that
the gated attention reduces noisy and unimportan-
t information, so that the remaining information
represents a clear idea of source text. The bet-
ter representation of encoder leads to a better se-
</p>
<p>Model level R-1 R-2 R-L
RNN context Word 26.8 16.1 24.1
</p>
<p>(Hu et al., 2015) Char 29.9 17.4 27.2
COPYNET Word 35.0 22.3 32.0
</p>
<p>(Gu et al., 2016) Char 34.4 21.6 31.3
this work Char 33.3 20.0 30.1
</p>
<p>Table 2: Results of our model and state-of-the-art
systems. COPYNET incorporates copying mech-
anism to solve out-of-vocabulary problem, so its
has higher ROUGE scores. Our model does not
incorporate this mechanism currently. In the fu-
ture work, we will implement this technic to fur-
ther improve the performance. (Word: Word level;
Char: Character level; R-1: F-score of ROUGE-
1; R-2: F-score of ROUGE-2; R-L: F-score of
ROUGE-L)
</p>
<p>mantic relevance evaluation by the similarity func-
tion. Therefore, SRB with gated attention encoder
is able to generate summaries with high semantic
relevance to source text.
</p>
<p>Figure 3 is an example to show the semantic rel-
evance between the source text and the summary.
It shows that the main idea of the source text is
about the reason why Shanghai has few giant com-
pany. RNN context produces “Shanghai’s giant
companies” which is literally similar to the source
text, while SRB generates “Shanghai has few giant
companies”, which is closer to the main idea in
semantics. It concludes that SRB produces sum-
maries with higher semantic similarity to texts.
</p>
<p>Table 2 summarizes the results of our model and
state-of-the-art systems. COPYNET has the high-
est socres, because it incorporates copying mech-
anism to deals with out-of-vocabulary word prob-
lem. In this paper, we do not implement this mech-
anism in our model. In the future work, we will try
to incorporates copying mechanism to our model
to solve the out-of-vocabulary problem.
</p>
<p>638</p>
<p />
</div>
<div class="page"><p />
<p>5 Related Work
</p>
<p>Abstractive text summarization has achieved suc-
cessful performance thanks to the sequence-to-
sequence model (Sutskever et al., 2014) and at-
tention mechanism (Bahdanau et al., 2014). Rush
et al. (2015) first used an attention-based en-
coder to compress texts and a neural network lan-
guage decoder to generate summaries. Follow-
ing this work, recurrent encoder was introduced
to text summarization, and gained better perfor-
mance (Lopyrev, 2015; Chopra et al., 2016). To-
wards Chinese texts, Hu et al. (2015) built a large
corpus of Chinese short text summarization. To
deal with unknown word problem, Nallapati et
al. (2016) proposed a generator-pointer model so
that the decoder is able to generate words in source
texts. Gu et al. (2016) also solved this issue by in-
corporating copying mechanism.
</p>
<p>Our work is also related to neural attention
model. Neural attention model is first proposed
by Bahdanau et al. (2014). There are many
other methods to improve neural attention mod-
el (Jean et al., 2015; Luong et al., 2015) and accel-
erate the training process (Sun, 2016).
</p>
<p>6 Conclusion
</p>
<p>Our work aims at improving semantic relevance
of generated summaries and source texts for Chi-
nese social media text summarization. Our model
is able to transform the text and the summary in-
to a dense vector, and encourage high similarity
of their representation. Experiments show that our
model outperforms baseline systems, and the gen-
erated summary has higher semantic relevance.
</p>
<p>Acknowledgements
</p>
<p>This work was supported in part by National High
Technology Research and Development Program
of China (863 Program, No. 2015AA015404),
and National Natural Science Foundation of Chi-
na (No. 61673028). Xu Sun is the corresponding
author of this paper.
</p>
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2014. Neural machine translation by joint-
ly learning to align and translate. CoRR ab-
s/1409.0473.
</p>
<p>Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and word-
</p>
<p>s. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics, ACL
2016, August 7-12, 2016, Berlin, Germany, Volume
1: Long Papers.
</p>
<p>Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016. pages 93–98.
</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Papers.
</p>
<p>Michael Hahn and Frank Keller. 2016. Modeling hu-
man reading with neural attention. In Proceed-
ings of the 2016 Conference on Empirical Method-
s in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016. pages 85–
95.
</p>
<p>Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LC-
STS: A large scale chinese short text summarization
dataset. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015. pages 1967–1972.
</p>
<p>Sébastien Jean, KyungHyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large
target vocabulary for neural machine translation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 1: Long Papers. pages 1–10.
</p>
<p>Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics, HLT-
NAACL 2003, Edmonton, Canada, May 27 - June 1,
2003.
</p>
<p>Konstantin Lopyrev. 2015. Generating news head-
lines with recurrent neural networks. CoRR ab-
s/1512.01712.
</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015. pages 1412–1421.
</p>
<p>Ramesh Nallapati, Bowen Zhou, Cı́cero Nogueira dos
Santos, Çaglar Gülçehre, and Bing Xiang. 2016.
</p>
<p>639</p>
<p />
</div>
<div class="page"><p />
<p>Abstractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of the
20th SIGNLL Conference on Computational Natural
Language Learning, CoNLL 2016, Berlin, Germany,
August 11-12, 2016. pages 280–290.
</p>
<p>Dragomir R. Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Çelebi, Stanko
Dimitrov, Elliott Drábek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - A platform
for multidocument multilingual text summarization.
In Proceedings of the Fourth International Confer-
ence on Language Resources and Evaluation, LREC
2004, May 26-28, 2004, Lisbon, Portugal.
</p>
<p>Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015. pages 379–389.
</p>
<p>Xu Sun. 2016. Asynchronous parallel learning for neu-
ral networks and structured models with dense fea-
tures. In COLING 2016, 26th International Confer-
ence on Computational Linguistics, Proceedings of
the Conference: Technical Papers, December 11-16,
2016, Osaka, Japan. pages 192–202.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in Neural Information Processing
Systems 27: Annual Conference on Neural Informa-
tion Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada. pages 3104–3112.
</p>
<p>Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional LSTM. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.
</p>
<p>Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In ACL 2010,
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, July 11-16,
2010, Uppsala, Sweden. pages 565–574.
</p>
<p>Jingjing Xu and Xu Sun. 2016. Dependency-based gat-
ed recursive neural network for chinese word seg-
mentation. In Meeting of the Association for Com-
putational Linguistics. pages 567–572.
</p>
<p>640</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 641–646
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2101
</p>
<p>Determining Whether and When People Participate
in the Events They Tweet About
</p>
<p>Krishna C. Sanagavarapu, Alakananda Vempala and Eduardo Blanco
Human Intelligence and Language Technologies Lab
</p>
<p>University of North Texas
Denton, TX, 76203
</p>
<p>{KrishnaSanagavarapu,AlakanandaVempala}@my.unt.edu, eduardo.blanco@unt.edu
</p>
<p>Abstract
</p>
<p>This paper describes an approach to de-
termine whether people participate in the
events they tweet about. Specifically, we
determine whether people are participants
in events with respect to the tweet times-
tamp. We target all events expressed by
verbs in tweets, including past, present and
events that may occur in the future. We
present new annotations using 1,096 event
mentions, and experimental results show-
ing that the task is challenging.
</p>
<p>1 Introduction
</p>
<p>Twitter has quickly become one of the most popu-
lar social media sites: it has 313 million monthly
active users, and 500 million tweets are published
daily. People tweet about breaking news, world
and local events (e.g., eclipses, road closures), and
personal events ranging from important life events
(e.g., graduating from college) to mundane events
such as commuting and attending a party.
</p>
<p>People tweet not only about events in which
they participate, but also events in which they
do not participate but are somehow relevant (e.g.,
John Doe may tweet about his nephew graduating
from college). More specifically, people can par-
ticipate in the events they tweet about (underlining
indicates events of interest below) prior to tweet-
ing (e.g., When I come back to London, I realise
how much I miss living here), while tweeting (e.g.,
Nope. Not yet. Still in my car, enjoying traffic),
or after tweeting (e.g., Can’t wait to fly home this
summer). In the third example, it is not guaran-
teed that fly will occur, so one can only say that
the author will probably participate in fly.
</p>
<p>In this paper, we determine whether people par-
ticipate in the events they tweet about. More
specifically, we determine whether they are par-
</p>
<p>ticipants before tweeting, while tweeting and after
tweeting, and define event participants as people
directly involved in an event, regardless of whether
they are the agent, recipient or play another role.
The main contributions of this paper are: (a) an-
notations using 1,096 events from 826 tweets; (b)
analysis showing that authors of tweets are often
not participants in the events they tweet about be-
fore or after tweeting; and (c) experimental results
showing that the task can be automated.
</p>
<p>2 Previous Work
</p>
<p>Most previous efforts on detecting events from
Twitter focus on events of general importance
(e.g., death of a celebrity, natural disasters) or ma-
jor life events of individuals (e.g. John Doe getting
married, having a baby, being promoted).
</p>
<p>Extracting events of general importance often
includes extracting the entities involved, date and
location, and classifying events into classes such
as trial, product launch or death (Ritter et al.,
2012). Exploiting redundancy in tweets to ex-
tract events is common (Zhou et al., 2014), as well
as spatio-temporal information (Cheng and Wicks,
2014), i.e., when and where tweets originate from.
</p>
<p>Extracting major life events consists on pin-
pointing significant events from mundane events
(e.g., having lunch, exercising) (Di Eugenio et al.,
2013; Li et al., 2014; Dickinson et al., 2015), and
determining whether significant events are rele-
vant to Twitter users (e.g., Why doesn’t John marry
Mary already? [not relevant to the author]).
</p>
<p>Unlike these previous efforts, the work pro-
posed here determines whether people participate
in the events they tweet about, and specifies when
with respect to tweet timestamps. As a result,
we target past events, ongoing events, and events
likely to occur in the future. Additionally, we tar-
get all events regardless of importance.
</p>
<p>641</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2101">https://doi.org/10.18653/v1/P17-2101</a></div>
</div>
<div class="page"><p />
<p>Figure 1: Label distribution per temporal span. Percentages are shown if they are greater or equal than
2%, e.g., percentages for unk label are not shown because they range between 0.91% and 1.91%.
</p>
<p>3 Corpus Creation
</p>
<p>We created a corpus of tweets, events and annota-
tions indicating whether and when the authors of
tweets participate in the events as follows.
Selecting Tweets and Events. First, we collected
5,017 tweets from corpora released by previous
projects (Owoputi et al., 2013; Kong et al., 2014;
Ritter et al., 2011). Second, we run an event de-
tector to tag tokens that are events (Ritter et al.,
2012). Third, we selected as events all tokens
tagged as events that are verbs. Fourth, we filtered
out tweets that did not contain pronouns I, me or
we in order to target tweets that are likely to dis-
cuss events that involve the author. Finally, we run
a dependency parser for tweets (Kong et al., 2014).
We decided to work with an automated event de-
tector and parser to experiment with a system that
could be deployed in the real world, and discarded
events that are nouns because manual inspection
revealed that the event detector makes many more
mistakes with nouns than with verbs.
</p>
<p>The steps above resulted in 1,096 events from
826 tweets. The part-of-speech tags of events are
as follows: VBP: 553 (verb, non-3rd person sin-
gular present), VBG: 345 (verb, gerund or present
participle), VBN: 198 (verb, past participle).
Annotation Process and Quality. For each event
in each tweet, we asked annotators “Is the author
of the tweet a participant in the event?” Dur-
ing pilot annotations with two graduate students,
it became clear that a major source of disagree-
ments was due to annotators answering the ques-
tion for different times with respect to the tweet
timestamp. After discussing errors, we decided
to ask for five answers: over 24 hours and within
</p>
<p>24 hours before tweeting, when tweeting (tweet
timestamp), and within 24 hours and over 24 hours
after tweeting. Additionally, we allow for six an-
swers partially inspired by previous work on fac-
tuality (Sauri and Pustejovsky, 2009):
• cYes, cNo: I am certain that the author is
</p>
<p>(or is not) a participant in the event.
• pYes, pNo: It is probably the case that the
</p>
<p>author is (or is not) a participant in the event.
• unk: the question is intelligible, but none of
</p>
<p>the four labels above would be correct.
• inv: the event at hand is not an event.
The temporal spans and labels were tuned until
</p>
<p>the two annotators obtained 0.70 Kappa agreement
with 10% of selected events. Kappa agreement
between 0.60 and 0.80 is considered substantial
agreement (Landis and Koch, 1977). After tuning,
the remaining events were annotated once.1
</p>
<p>4 Corpus Analysis
</p>
<p>In this section, we present a corpus analysis con-
sisting of label distribution per temporal span, la-
bel distribution for the top 5 most frequent events,
and label distribution per part-of-speech tag.
</p>
<p>Figure 1 plots percentages of each label per
temporal span. Overall (bottom bar), 28.1% of
answers are cYes, and 46.7% are cNo, i.e., an-
notators are certain that the author of the tweet
is or is not a participant in the event. Percent-
ages for pYes and pNo are much lower: 11.4%
and 2.9%. Regarding time spans, people do not
usually tweet about events in which they partic-
ipate while tweeting (cYes: 33.4% vs. cNo:
54.5%). People are more likely to tweet about
</p>
<p>1Available at http://www.cse.unt.edu/
˜blanco/
</p>
<p>642</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: Label distribution for the top 5 most frequent events (frequency is shown between parentheses
beside the event). Percentages are shown if they are greater or equal than 5%.
</p>
<p>Figure 3: Label distribution per part-of-speech tags of the event. Percentages are shown if they are
greater or equal than 5%.
</p>
<p>events in which they participated within the last
24 hours (39.1%) than longer before (26.6%), and
after tweeting (24.9% and 16.8%). Finally, the
percentages of pYes and pNo are below 2% for
tweet timestamp. Intuitively, it is easier to deter-
mine whether somebody participates in the event
he tweets about when tweeting rather than before
or after. Percentages for labels that do not indicate
event participation (unk and inv) are low: unk
ranges from 0.91% to 1.91%, and inv is 9.5% for
all spans. inv is often used for automatically de-
tected events that are actually states, e.g., been.
</p>
<p>In Figure 2, we portray the label distribution
for the top 5 frequent events and all temporal
spans. The top 3 most frequent events (love, hate
and miss) have the highest percentages of cYes
and pYes labels (&gt; 80% combined), and the
fourth most frequent verb (see) a lower percent-
age (cYes: 42.4%, pYes: 22.4%). Need has a
high percentage of inv (47.1%). This is due to
the fact the need is often a state (e.g., Look, I need
less friends more bread, less talk, more head), and
asking whether the author is a participant in the
event before, during or after the tweet timestamp
is nonsensical.
</p>
<p>Figure 3 presents the label distribution per part-
of-speech tag of the event. VBP (non-3rd per-
son singular present) has the highest percentage of
</p>
<p>cYes + pYes (48.5% vs. 28.8% and 31.5%), in-
dicating that the author is likely to participate in
such events. VBG (gerund or present participle)
has highest percentage of cNo + pNo (61.1%), in-
dicating that the author is not likely to participate
in those events.
</p>
<p>Annotation Examples. Table 1 presents real an-
notation examples. In Tweet (1), annotators under-
stood that the author has certainly been addicted
to Twitter for 24 hours before and after tweet-
ing, and probably longer. The author of Tweet (2)
was clearly talking about YOU before but not af-
ter tweeting; annotators indicated that talking most
likely occurred within 24 hours before tweeting.
The annotations in Tweet (3) indicate that the au-
thor was certainly a participant of selling when he
tweeted and within 24 hours after tweeting, and
probably also within 24 hours before and over 24
hours after. Event seeing in Tweet (4) may occur
next week, and annotations capture this informa-
tion (all cNo except 24h after, which is pYes).
Finally, the author of Tweet (5) was never a partic-
ipant in scrapbooking despite she witnessed it.
</p>
<p>Note that the annotations also provide informa-
tion regarding event durations, e.g., addicted in
Tweet (1) is likely ongoing a day after, but talk-
ing in Tweet (2) has ended and was a short event.
</p>
<p>643</p>
<p />
</div>
<div class="page"><p />
<p>Tweet before tweet after
≥24h &lt;24h ts. &lt;24h ≥24h
</p>
<p>1 I’m so addicted to Twitter now that I can tweet all the time. Not good. pYes cYes cYes cYes pYes
2 I wonder if you realize we were talking about YOU. pNo cYes cNo cNo cNo
3 I’m selling my snorkle, text me for details. cNo pYes cYes cYes pYes
4 @Genuine Will I be seeing you at #typeamom next week? cNo cNo cNo cNo pYes
5 Today the mall was full of moms who love scrapbooking. kill me. cNo cNo cNo cNo cNo
</p>
<p>Table 1: Annotation examples. We show annotations for the underlined events only. Recall that events
are detected automatically (Section 3). Tweet ts. stands for twitter timestamp.
</p>
<p>Type No. Description
</p>
<p>Event 1–2 Event word form and POS tag3 Token number of event within the tweet
</p>
<p>Situation
Entities
</p>
<p>7–8 Event tense and flag indicating whether event tense is perfect tense
9 Modal type of event, if any
</p>
<p>10 Flag indicating whether event is a reporting verb
11 WordNet lexical file name of event
</p>
<p>Context
</p>
<p>12 Position of any pronouns with respect to event: left, right, both or none
13 Position of pronouns I, me, we with respect to event: left, right, both or none
14 Number of outgoing dependencies from event
15 Flag indicating whether there is a dependency between event and pronouns I, me, or we
</p>
<p>Table 2: Features to determine whether the author of a tweet is a participant in the events he tweets about.
</p>
<p>5 Experiments and Results
</p>
<p>We follow a standard supervised machine learning
approach. We divided the 826 tweets into train-
ing and test splits (80% / 20%), and created an in-
stance for each event and temporal span (1,096 ×
5 = 5,480 instances). We trained a Support Vector
Machine with RBF kernel per temporal span us-
ing scikit-learn (Pedregosa et al., 2011) and tuned
SVM parameters (C and γ) using 5-fold cross-
validation with the training set. We report results
when evaluating with the test set.
</p>
<p>5.1 Feature Selection
</p>
<p>The feature set is presented in Table 2. Most
features are fairly simple and well-known, but
we borrow some features from previous work on
identifying situation entity types (Friedrich et al.,
2016) and include features especially designed to
capture context around the event we work with.
</p>
<p>Event features include the actual event (word
form and part-of-speech tag) and number of to-
kens to left of the event. Situation Entities fea-
tures further characterize the event at hand. To
extract them, we first retrieve the clause contain-
ing the event by collecting all tokens to the left
of the event until we reach a token that is not a
verb (including auxiliaries), a modal, or adverb.
Friedrich et al. (2016) propose many more fea-
tures for identifying situation entities, but we se-
lected those that are useful for our task: fine event
tense, flag indicating whether the event is in per-
</p>
<p>fect tense (past perfect, present perfect, etc.), the
type of modals present in the verb clause (if any),
whether the event is in a list of 88 reporting verbs,
and the WordNet lexical file containing the event
(Miller, 1995). Finally, Context features mostly
indicate the presence of pronouns in the tweet.
Specifically, we include the position of any pro-
noun with respect to the event, and a specializa-
tion of this feature for pronouns I, me, and we. We
also extract the number of outgoing dependencies
from the event, and whether one of those depen-
dencies is between the event and a pronoun I, me
or we. Note that the dependency parser for tweets
extracts untyped dependencies (Kong et al., 2014),
thus we have available information about syntac-
tic dependents but not about specific syntactic re-
lationships (nsubj, dobj, auxpass, etc.).
</p>
<p>5.2 Experimental Results
</p>
<p>Results obtained in the test split with several com-
binations of features are depicted in Table 3. The
baseline always predicts the majority label (cNo
for all temporal spans), and is outperformed by all
feature combinations.
</p>
<p>Feature ablation experiments show (a) little im-
provement with respect to only training with Event
features, i.e., the simplest features, and (b) that
the optimal combination of features depends on
the temporal span for which author participation is
being predicted. More specifically, including Sit-
uation Entities features yields the same results for
</p>
<p>644</p>
<p />
</div>
<div class="page"><p />
<p>Label ≥24h Before &lt;24h Before tweet timestamp &lt;24h After ≥24h AfterP R F P R F P R F P R F P R F
</p>
<p>Baseline
cNo 0.45 1.00 0.62 0.38 1.00 0.55 0.50 1.00 0.67 0.33 1.00 0.49 0.39 1.00 0.56
others 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Avg. 0.20 0.45 0.28 0.15 0.38 0.21 0.25 0.50 0.34 0.11 0.33 0.16 0.15 0.39 0.22
</p>
<p>Event
</p>
<p>cYes 0.60 0.51 0.55 0.71 0.71 0.71 0.94 0.69 0.79 0.84 0.59 0.69 0.85 0.69 0.76
pYes 0.00 0.00 0.00 0.33 0.06 0.10 0.00 0.00 0.00 0.51 0.33 0.40 0.37 0.25 0.30
cNo 0.56 0.85 0.67 0.54 0.74 0.63 0.69 0.93 0.79 0.45 0.79 0.58 0.48 0.74 0.59
pNo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.14 0.11 0.12
unk 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Avg. 0.48 0.56 0.50 0.57 0.61 0.57 0.74 0.75 0.73 0.55 0.53 0.51 0.50 0.52 0.49
cYes 0.61 0.52 0.56 0.71 0.69 0.70 0.91 0.73 0.81 0.75 0.60 0.67 0.85 0.71 0.78
</p>
<p>Event + pYes 0.00 0.00 0.00 0.50 0.06 0.11 0.00 0.00 0.00 0.50 0.30 0.37 0.42 0.31 0.36
Situation cNo 0.57 0.81 0.67 0.56 0.73 0.64 0.72 0.87 0.79 0.46 0.74 0.57 0.49 0.69 0.58
Entities pNo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.25 0.11 0.15
</p>
<p>unk 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Avg. 0.47 0.55 0.50 0.58 0.60 0.57 0.74 0.74 0.73 0.52 0.51 0.49 0.51 0.53 0.51
cYes 0.71 0.55 0.62 0.69 0.69 0.69 0.91 0.73 0.81 0.73 0.56 0.63 0.85 0.69 0.76
</p>
<p>Event + pYes 0.00 0.00 0.00 0.50 0.06 0.11 0.00 0.00 0.00 0.55 0.40 0.46 0.35 0.31 0.33
Situation cNo 0.57 0.84 0.68 0.55 0.69 0.61 0.73 0.89 0.80 0.42 0.61 0.49 0.50 0.65 0.56
Entities + pNo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20 0.11 0.14
Context unk 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
</p>
<p>Avg. 0.50 0.58 0.52 0.57 0.59 0.56 0.75 0.76 0.74 0.50 0.48 0.48 0.49 0.50 0.49
</p>
<p>Table 3: Results obtained with several feature combinations in the test split. Average is the weighted
average; we do not show results for invalid but they are included in the averages.
</p>
<p>≥24h before, &lt;24h before and tweet timestamp,
slightly worse results for&lt; 24h after (F-measures:
0.51 vs. 0.49) and slightly better for ≥24h after
(F-measures: 0.49 vs. 0.51). Training with all fea-
tures (Event + Situation Entities + Context), ob-
tains the best results for ≥24h before (F-measure:
0.52) and tweet timestamp (F-measure: 0.74), but
results are slightly lower for the other time spans.
</p>
<p>Note that while the results with Event features
are only outperformed for some temporal spans
when training with all features, information be-
yond the event at hand is needed to solve this task.
For example, the correct labels for I love living in
NYC and I miss living in NYC are different (living
is an ongoing and past event respectively).
</p>
<p>6 Conclusions
</p>
<p>We have presented a corpus and machine learning
models to predict whether people participate in the
events they tweet about. More specifically, we de-
termine whether people participate in events when
they tweet about them, and also before and after.
</p>
<p>Unlike most previous work, we target any event
in a tweet regardless of its importance. While ma-
jor life events (e.g., graduating from college) are
arguably more important, we believe that mundane
events (e.g., studying for an exam) provide key in-
formation to retrieve major life events and predict
future events, e.g., studying for an exam is (most
probably) more likely to lead to graduating from
</p>
<p>college than going to parties on a regular basis.
Experimental results show that the task can be
</p>
<p>automated but is challenging. We believe that fea-
tures derived from subsequent tweets by the same
author and tweet replies would yield better results,
and plan to incorporate them in future work.
</p>
<p>References
Tao Cheng and Thomas Wicks. 2014. Event detection
</p>
<p>using twitter: A spatio-temporal approach. PLOS
ONE 9(6):1–10.
</p>
<p>Barbara Di Eugenio, Nick Green, and Rajen Subba.
2013. Detecting life events in feeds from twitter.
In Semantic Computing (ICSC), 2013 IEEE Seventh
International Conference on. Ieee, pages 274–277.
</p>
<p>Thomas Dickinson, Miriam Fernandez, Lisa A.
Thomas, Paul Mulholland, Pam Briggs, and Harith
Alani. 2015. Identifying prominent life events on
twitter. In Proceedings of the 8th International Con-
ference on Knowledge Capture. ACM, New York,
NY, USA, K-CAP 2015, pages 4:1–4:8.
</p>
<p>Annemarie Friedrich, Alexis Palmer, and Manfred
Pinkal. 2016. Situation entity types: automatic
classification of clause-level aspect. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Berlin, Germany, pages 1757–1768.
http://www.aclweb.org/anthology/P16-1166.
</p>
<p>Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
</p>
<p>645</p>
<p />
</div>
<div class="page"><p />
<p>Noah A. Smith. 2014. A dependency parser
for tweets. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 1001–1012.
http://www.aclweb.org/anthology/D14-1108.
</p>
<p>J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics 33(1).
</p>
<p>Jiwei Li, Alan Ritter, Claire Cardie, and Ed-
uard Hovy. 2014. Major life event extraction
from twitter based on congratulations/condolences
speech acts. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 1997–2007.
http://www.aclweb.org/anthology/D14-1214.
</p>
<p>George A. Miller. 1995. Wordnet: A lexical
database for english. Commun. ACM 38(11):39–41.
https://doi.org/10.1145/219717.219748.
</p>
<p>Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging
for online conversational text with word clus-
ters. In Proceedings of the 2013 Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 380–390.
http://www.aclweb.org/anthology/N13-1039.
</p>
<p>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research
12:2825–2830.
</p>
<p>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Edinburgh, Scotland, UK., pages 1524–1534.
http://www.aclweb.org/anthology/D11-1141.
</p>
<p>Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter.
In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining. ACM, New York, NY, USA, KDD ’12,
pages 1104–1112.
</p>
<p>Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation 43(3):227–268.
</p>
<p>Deyu Zhou, Liangyu Chen, and Yulan He. 2014.
A simple bayesian modelling approach to event
extraction from twitter. In Proceedings of the
</p>
<p>52nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers). Association for Computational Lin-
guistics, Baltimore, Maryland, pages 700–705.
http://www.aclweb.org/anthology/P14-2114.
</p>
<p>646</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 647–653
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2102
</p>
<p>Separating Facts from Fiction: Linguistic Models to Classify Suspicious
and Trusted News Posts on Twitter
</p>
<p>Svitlana Volkova, Kyle Shaffer, Jin Yea Jang and Nathan Hodas
Data Sciences and Analytics Group, National Security Directorate
</p>
<p>Pacific Northwest National Laboratory
902 Battelle Blvd, Richland, WA 99354
firstname.lastname@pnnl.gov
</p>
<p>Abstract
</p>
<p>Pew research polls report 62 percent of
U.S. adults get news on social media (Got-
tfried and Shearer, 2016). In a December
poll, 64 percent of U.S. adults said that
“made-up news” has caused a “great deal
of confusion” about the facts of current
events (Barthel et al., 2016). Fabricated
stories in social media, ranging from de-
liberate propaganda to hoaxes and satire,
contributes to this confusion in addition to
having serious effects on global stability.
</p>
<p>In this work we build predictive models to
classify 130 thousand news posts as sus-
picious or verified, and predict four sub-
types of suspicious news – satire, hoaxes,
clickbait and propaganda. We show that
neural network models trained on tweet
content and social network interactions
outperform lexical models. Unlike previ-
ous work on deception detection, we find
that adding syntax and grammar features
to our models does not improve perfor-
mance. Incorporating linguistic features
improves classification results, however,
social interaction features are most in-
formative for finer-grained separation be-
tween four types of suspicious news posts.
</p>
<p>1 Introduction
</p>
<p>Popular social media platforms such as Twitter
and Facebook have proven to be effective chan-
nels for disseminating falsified information, un-
verified claims, and fabricated attention-grabbing
stories due to their wide reach and the speed at
which this information can be shared. Recently,
there has been an increased number of disturbing
incidents of fabricated stories proliferated through
</p>
<p>social media having a serious impact on real-world
events (Perrott, 2016; Connolly et al., 2016)
</p>
<p>False news stories distributed in social me-
dia vary depending on the intent behind falsifica-
tion. Unlike verified news, suspicious news tends
to build narratives rather than report facts. On
one extreme is disinformation which communi-
cates false facts to deliberately deceive readers or
promote a biased agenda. These include posts
generated and retweeted from propaganda and
so-called clickbait (“eye-catching” headlines) ac-
counts. The intent behind propaganda and click-
bait varies from opinion manipulation and atten-
tion redirection to monetization and traffic attrac-
tion. Hoaxes are another type of disinformation
that aims to deliberately deceive the reader (Tam-
buscio et al., 2015; Kumar et al., 2016). On the
other extreme is satire, e.g., @TheOnion, where
the writer’s primary purpose is not to mislead the
reader, but rather entertain or criticize (Conroy
et al., 2015). However, satirical news and hoaxes
may also be harmful, especially when they are
shared out of context (Rubin et al., 2015).
</p>
<p>Our novel contributions in this paper are
twofold. We first investigate several features
and neural network architectures for automatically
classifying verified and suspicious news posts, and
four sub-types of suspicious news. We find that
incorporating linguistic and network features via
a “late fusion” technique boosts performance. We
then investigate differences between verified and
suspicious news tweets by conducting a statistical
analysis of linguistic features in both types of ac-
count. We show significant differences in use of
biased, subjective language and moral foundations
behind suspicious and trustworthy news posts.
</p>
<p>Our analysis and experiments rely on a large
Twitter corpus1 collected during a two-week pe-
</p>
<p>1Data available at: http://www.cs.jhu.edu/∼svitlana/
</p>
<p>647</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2102">https://doi.org/10.18653/v1/P17-2102</a></div>
</div>
<div class="page"><p />
<p>TYPE NEWS POSTS RTPA EXAMPLES
Propaganda 99 56,721 572 ActivistPost
Satire 9 3,156 351 ClickHole
Hoax 8 4,549 569 TheDcGazette
Clickbait 18 1,366 76 chroniclesu
Verified 166 65,792 396 USATODAY
</p>
<p>Table 1: Twitter dataset statistics: news accounts, posts and
retweets per account (RTPA).
</p>
<p>riod around terrorist attacks in Brussels in 2016.
Our method of collection ensures that our models
learn from verified and suspicious news within a
predefined timeframe, and further ensures homo-
geneity of deceptive texts in length and writing
manner (Rubin et al., 2015).
</p>
<p>Several tools have been recently developed to
verify and reestablish trusted sources of informa-
tion online e.g., Google fact checking (Gindras,
2016) and Facebook repost verification (Mosseri,
2016). These projects, among others, teach news
literacy2 and contribute to fact-checking online.3
</p>
<p>We believe our models and novel findings on lin-
guistic differences between suspicious and ver-
ified news will contribute to these fact-checking
systems, as well as help readers to judge the accu-
racy of information they consume in social media.
</p>
<p>2 Data
</p>
<p>Suspicious News We relied on several public re-
sources that annotate suspicious Twitter accounts
or their corresponding websites as propaganda,
hoax, clickbait and satire. They include propa-
ganda accounts identified by PropOrNot,4 satire,
clickbait and hoax accounts.5 In total we collected
174 suspicious news accounts.6 In addition, we
manually confirmed that accounts and their cor-
responding webpages labeled by PropOrNot have
one or more signs of propaganda listed below: (a)
tries to persuade; (b) influences the specific emo-
tions, attitudes, opinions, and actions; (c) target
audiences for political, ideological, and religious
purposes; and (d) contains selectively-omitting
and one-sided messages.
</p>
<p>Figure 1 presents a communication network be-
tween verified and suspicious news accounts. We
observe that verified accounts are connected to
</p>
<p>2News Literacy: http://www.thenewsliteracyproject.org/
3Fact checking: http://reporterslab.org/fact-checking/
</p>
<p>Hoaxy: http://hoaxy.iuni.iu.edu/
4Propaganda: http://www.propornot.com/p/the-list.html
5http://www.fakenewswatch.com/
6To ensure the quality of suspicious account labels we
</p>
<p>manually verified them.
</p>
<p>21WIRE
RT_COM
</p>
<p>ANTIWARCOM
</p>
<p>CONSORTIUMNEWS
</p>
<p>CORBETTREPORT
</p>
<p>ACTIVISTPOST
</p>
<p>LIBERTYBLITZ
</p>
<p>MINTPRESSNEWS
</p>
<p>ANTIMEDIA
</p>
<p>RUPTLY
</p>
<p>RUSSIAINSIDER
RUSSIABEYOND
</p>
<p>LEWROCKWELL
</p>
<p>INFOWARS
</p>
<p>SPUTNIKINT
</p>
<p>WIKILEAKS
</p>
<p>THERUSSOPHILE
</p>
<p>HEALTHRANGER
</p>
<p>YAHOONEWS
</p>
<p>JOINGAIA
REUTERS
</p>
<p>MOONOFA
</p>
<p>PRESSTV
</p>
<p>REALTRUTHKINGSCNN
</p>
<p>INDEPENDENT
</p>
<p>AP
</p>
<p>VDARE
</p>
<p>BUSINESS
</p>
<p>BUZZFEEDNEWS
NYTIMES
</p>
<p>Figure 1: Communication network (@mention) among ver-
ified (blue), propaganda (pink), and clickbait (orange) ac-
counts (no shared edges with Hoax and Satire accounts).
</p>
<p>(via RTs and mentions) some suspicious news ac-
counts – clickbaits and propaganda.
Verified News We manually constructed a list of
252 “trusted” news accounts that tweet in English
and checked whether they are verified on Twitter.
We release the final verified list of trusted and sus-
picious news accounts used in our analysis.7
</p>
<p>Tweet Corpus We query the Twitter firehose from
Mar 15 to Mar 29 2016 – one week before and af-
ter Brussels bombing on Mar 22 2016 for 174 sus-
picious and 252 verified news accounts. We col-
lected retweets generated by any user that men-
tions one of these accounts and assign the cor-
responding label propagated from suspicious or
trusted news.8 We de-duplicated, lowercased, and
tokenized these posts and applied standard NLP
preprocessing. We extracted part-of-speech tags
and dependency parses for 130 thousand tweets
using SyntaxNet (Petrov, 2016).
</p>
<p>3 Models
</p>
<p>We propose linguistically-infused neural network
models to classify social media posts retweeted
from news accounts into verified and suspicious
categories – propaganda, hoax, satire and click-
bait. Our models incorporate tweet text, social
graph, linguistic markers of bias and subjectiv-
ity, and moral foundation features. We experiment
with several baseline models, and develop neural
network architectures presented in Figure 2 in the
</p>
<p>7The lists of verified and suspicious news:
http://www.cs.jhu.edu/∼svitlana/TwitterList
</p>
<p>8Suspicious news annotations should be done on a tweet
rather than an account level. However, these annotations are
extremely costly and time consuming.
</p>
<p>648</p>
<p />
</div>
<div class="page"><p />
<p>Keras framework.9 We rely on state-of-the-art lay-
ers effectively used in text classification – Long
Short-Term Memory (LSTM) and Convolutional
Neural Networks (CNN) (Johnson and Zhang,
2014; Zhang and Wallace, 2015). The content sub-
network consists of an embedding layer and either
(a) one LSTM layer or (b) two 1-dimensional con-
volution layers followed by a max-pooling layer.
</p>
<p>We initialize our embedding layer with pre-
trained GloVe embeddings (Pennington et al.,
2014). The social graph sub-network is a sim-
ple feed-forward network that takes one-hot vec-
tors of user interactions, e.g. @mentions, as in-
put. We are careful to exclude source @mentions
from these vectors, as these were used to derive
labels for our networks and would likely lead to
overfitting. In addition to content and network
signals, we incorporate other linguistic cues into
our networks. For this we rely on the “late fu-
sion” approach that has been shown to be effec-
tive in vision tasks (Karpathy et al., 2014; Park
et al., 2016). “Fusion” allows for a network to
learn a combined representation of multiple input
streams. This fusion can be done early (in the
feature extraction layers) or later (in the later ex-
traction layers, or in classification layers). In our
case, we use fusion as a technique for training net-
works to learn how to combine data representa-
tions from different modalities (network and text
features) to boost performance. We train our mod-
els for 10 epochs using the ADAM optimization
algorithm, and evaluate them using 10 fold cross-
validation (Kingma and Ba, 2014).
</p>
<p>Baselines We compare our neural network ar-
chitectures to several baselines. Word- and
document-level embeddings have been shown to
be effective as input to simpler classifiers. We
experiment with several feature inputs for test-
ing baseline classifiers: (a) TFIDF features, (b)
Doc2Vec vectors and (c) Doc2Vec or TFIDF fea-
tures concatenated with linguistic or network fea-
tures. In the case of Doc2Vec features, we induce
200-dimensional vectors for each tweet using the
gensim library,10 training for 15 epochs.
</p>
<p>Bias cues Inspired by earlier work on identifying
biased language on Wikipedia (Recasens et al.,
2013) we extract hedges (expressions of tenta-
tiveness and possibility) (Hyland, 2005), assertive
verbs (the level of certainty in the complement
</p>
<p>9Keras: https://keras.io/
10https://radimrehurek.com/gensim/models/doc2vec.html
</p>
<p>LSTM/ 
Convolutional 
</p>
<p>Layer (100 units)
</p>
<p>Embedding 
Layer 
</p>
<p>(200 units)
</p>
<p>Input Word 
Sequences
</p>
<p>Dense Layer 
(100 units)
</p>
<p>Probability Activation  
Layer (sigmoid/softmax)
</p>
<p>Final Output Probabilities
</p>
<p>…
}
</p>
<p>…
</p>
<p>}
</p>
<p>}
</p>
<p>}
</p>
<p>}
</p>
<p>…
…}Dense Layer 
</p>
<p>(100 units)
</p>
<p>…}Dense Layer 
(100 units)
</p>
<p>}Network/ 
Linguistic 
</p>
<p>Cues
</p>
<p>…
} Tensor Concatenation
} Dense Layer (100 units)
</p>
<p>Figure 2: Neural network architecture for news classification
fused with social network and linguistic cues.
</p>
<p>clause) (Hooper, 1975), factive verbs (presuppose
the truth of their complement clause) (Kiparsky
and Kiparsky, 1968), implicative verbs (imply the
truth or untruth of their complement) (Karttunen,
1971) and report verbs (Recasens et al., 2013)
from preprocessed tweets.
</p>
<p>Subjectivity cues We rely on external publicly
available subjectivity, and positive and negative
opinion lexicons to extract strongly and weakly
subjective words (Riloff and Wiebe, 2003), posi-
tive and negative opinion words (Liu et al., 2005).
</p>
<p>Psycholinguistic cues In addition to biased and
subjective language cues, we extract Linguistic In-
quiry Word Count (LIWC) features (Pennebaker
et al., 2001) to capture additional signals of per-
suasive and biased language in tweets. LIWC fea-
tures have been successfully used for deception
detection before (Hancock et al., 2007; Vrij et al.,
2007; Mihalcea and Strapparava, 2009). For ex-
ample, persuasive language cues in LIWC include
statistics and factual data, rhetorical questions, im-
perative commands, personal pronouns, and emo-
tional language. Additional biased language cues
captured by LIWC are quotations, markers of cer-
tainty, inclusions and conjunctions. Extra subjec-
tive language cues in LIWC cover positive and
negative emotion and anxiety words.
</p>
<p>Moral foundation cues According to Haidt and
Graham (2007); Graham et al. (2009), there is a
small number of basic widely supported moral val-
ues, and people differ in the way they endorse
these values. Moral foundations include care and
harm, fairness and cheating, loyalty and betrayal,
authority and subversion, and purity and degrada-
tion. We hypothesize that suspicious news could
appeal to specific moral foundations of their read-
</p>
<p>649</p>
<p />
</div>
<div class="page"><p />
<p>ers in a way that is distinct from verified news ac-
counts. Thus, they could help in predicting veri-
fied vs. suspicious news, as well as different sus-
picious news types.
</p>
<p>4 Results
</p>
<p>4.1 Classification
</p>
<p>Table 2 presents classification results for Task 1
(binary) – suspicious vs. verified news posts and
Task 2 (multi-class) – four types of suspicious
tweets e.g., propaganda, hoaxes, satire and click-
bait. We report performance for different model
and feature combinations.
</p>
<p>We find that our neural network models (both
CNNs and RNNs) significantly outperform logis-
tic regression baselines learned from all feature
combinations.11 The accuracy improvement for
the binary task is 0.2 and F1-macro boost for the
multi-class task is 0.07. We also observe that
all models learned from network and tweet text
signals outperform models trained exclusively on
tweets. We report 0.05 accuracy improvement for
Task 1, and 0.02 F1 boost for Task 2. Adding lin-
guistic cues to basic tweet representations signifi-
cantly improves results across all models. Finally,
by combining basic content with network and lin-
guistic features via late fusion, our neural network
models achieve best results in binary experiments.
Interestingly, models perform best in the multi-
class case when trained on tweet embeddings and
fused network features alone. We report 0.95 ac-
curacy when inferring suspicious vs. verified news
posts, and 0.7 F1-macro when classifying types of
suspicious news.
</p>
<p>Syntax and grammar features have been pre-
dictive of deception in the product review do-
main (Feng et al., 2012; Pérez-Rosas and Mihal-
cea, 2015). However, unlike earlier work we find
that fusing these features into our models signifi-
cantly decreases performance – by 0.02 accuracy
for the binary task and 0.02 F1 for multi-class.
This may be explained by the domain differences
between reviews and tweets which are shorter,
more noisy and difficult to parse.
</p>
<p>4.2 Linguistic Analysis
</p>
<p>We measure statistically significant differences in
linguistic markers of bias, subjectivity and moral
</p>
<p>11We experimented with other baseline models, such as
Random Forest, but found negligible difference between
these results and those obtained via logistic regression.
</p>
<p>BINARY MULTI-CLASS
Features A ROC AP F1 F1 macro
</p>
<p>BASELINE 1: LOGISTIC REGRESSION (DOC2VEC)
Tweets 0.65 0.70 0.68 0.82 0.40
+ network 0.72 0.80 0.82 0.88 0.57
+ cues 0.69 0.74 0.73 0.83 0.46
ALL 0.75 0.84 0.84 0.88 0.59
</p>
<p>BASELINE 2: LOGISTIC REGRESSION (TFIDF)
Tweets 0.72 0.81 0.81 0.84 0.48
+ network 0.78 0.87 0.88 0.88 0.59
+ cues 0.75 0.85 0.85 0.86 0.49
ALL 0.79 0.88 0.89 0.89 0.59
</p>
<p>RECURRENT NEURAL NETWORK
Tweets 0.78 0.87 0.88 0.90 0.63
+ network 0.83 0.91 0.92 0.92 0.71
+ cues 0.93 0.98 0.99 0.90 0.63
+ syntax 0.93 0.96 0.96 0.90 0.64
ALL 0.95 0.99 0.99 0.91 0.66
</p>
<p>CONVOLUTIONAL NEURAL NETWORK
Tweets 0.76 0.85 0.87 0.91 0.63
+ network 0.81 0.9 0.91 0.92 0.70
+ cues 0.93 0.98 0.98 0.90 0.61
ALL 0.95 0.98 0.99 0.91 0.64
</p>
<p>Table 2: Classification results: predicting suspicion and ver-
ified posts reported as A – accuracy, AP – average precision,
ROC – the area under the receiver operator characteristics
curve, and inferring types of suspicious news reported using
F1 micro and F1 macro scores.
</p>
<p>foundations across different types of suspicious
news, and contrast them with verified news us-
ing resources described in Section 3. These novel
findings presented in Table 3 provide deeper un-
derstanding of model performance in Table 2.
</p>
<p>Verified news tweets contain significantly less
bias markers, hedges and subjective terms and less
harm/care, loyalty/betrayal and authority moral
cues compared to suspicious news tweets. Satir-
ical news are the most different from propaganda
and hoaxes; and propaganda, hoax and clickbait
news are the most similar based on moral, bias and
subjectivity cues.
</p>
<p>Propaganda news target morals more than satire
and hoaxes, but less than clickbait. Satirical news
contains more loyalty and less betrayal morals
compared to propaganda, hoaxes and clickbait
news. Propaganda news target authority more than
satire and hoaxes, and fairness more than satire.
</p>
<p>Hoaxes and propaganda news contain signifi-
cantly less bias markers (e.g, hedging, implica-
tive and factive verbs) compared to satire. How-
ever, propaganda and clickbait news contain sig-
nificantly more factive verbs and bias language
markers compared to hoaxes. Satirical news use
significantly more subjective terms compared to
other news, while clickbait news use more subjec-
tive cues than propaganda and hoaxes.
</p>
<p>650</p>
<p />
</div>
<div class="page"><p />
<p>CUES V↔ F P↔ S P↔ H P↔ C S↔ H S↔ C H↔ C
MORAL FOUNDATION CUES
</p>
<p>Harm 2.1↓↓↓ 2.8 2.8↑ 2.1 – – 2.0↓↓ 2.6 – –
Care 5.8↓↓↓ 9.0 9.4↑↑↑ 6.3 9.4↑↑↑ 5.1 9.4↓ 11.3 – 6.3↓↓↓ 11.3 5.1↓↓↓ 11.3
Fairness – 0.8↑ 0.4 – – – 0.4↓ 1.0 –
Cheating 0.3↑ 0.2 – – – – – –
Loyalty 2.1↓↓↓ 2.5 2.2↓↓↓ 7.6 – – 7.6↑↑↑ 2.0 7.6↑↑↑ 2.3 –
Betrayal 1.7↓↓↓ 3.1 3.4↑↑↑ 0.2 3.4↑↑↑ 2.2 – 0.2↓↓↓ 2.2 0.2↓↓↓ 3.0 –
Authority 2.4↓↓↓ 2.9 3.0↑ 2.1 3.0↑ 2.3 – – – –
</p>
<p>BIASED LANGUAGE CUES
Assertive 12.6↓↓↓ 13.8
Bias 142.6↓↓↓ 164.4 – 165.5↑↑↑ 148.8 – 165↑↑↑ 148.8 – 148.8↓↓167.1
Factive 4.9↓↓↓ 5.5 5.5↓ 6.3 5.5↑ 4.7 5.5↓ 6.8 6.3↑ 4.7 – 4.7↓↓ 6.8
Hedges 14.2↓↓↓ 15.7 15.6↓↓↓ 20.0 – – 20↑↑↑ 15.8 20↑↑↑ 13.4 –
Implicative 7.6↓↓↓ 8.9 8.6↓↓↓ 15.2 – – 15.2↑↑↑ 8.8 15.2↑↑↑ 8.3 –
Report 30↓↓↓ 34.5 34.3↓ 36.0 – – – – –
</p>
<p>SUBJECTIVE LANGUAGE CUES
Subjective 28.8↓↓↓ 32.8 32.6↓↓↓ 39.5 – – 39.5↑↑↑ 30.9 39.5↑↑↑ 32.5 –
Strong Subj 23.5↓↓↓ 25.3 24.8↓↓↓ 31.5 24.8↓↓↓ 26.3 24.8↓↓↓ 27.5 31.5↑↑↑ 26.3 – –
Weak Subj 24.8↓↓↓ 30.8 31.2↓↓↓ 32.8 31.2↑↑↑ 24.1 – 32.8↑↑↑ 24.1 32.8↑↑ 30.7 24.1↓↓↓ 30.7
</p>
<p>Table 3: Linguistic analysis of moral foundations, bias and subjective language shown as the percentage of tweets with one
or more cues across verified (V) and suspicious (F) news – propaganda (P), hoaxes (H), satire (S) and clickbait (C). We report
only statistically significant differences: p-value ≤ 0.05↑, ≤ 0.01↑↑, ≤ 0.001↑↑↑ estimated using the Mann-Whitney U test.
Subjective lexicon is from (Liu et al., 2005), weekly and strongly subjective terms are from (Riloff and Wiebe, 2003).
</p>
<p>0
</p>
<p>50
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>22 24 26 28
Date
</p>
<p>F
re
</p>
<p>qu
en
</p>
<p>cy
</p>
<p>3/22 False flag
Brussels crisis actor
Erdogan knew bombing
</p>
<p>Israel involved
Merkel terrorist selfie
US behind ISIS
</p>
<p>(a) Rumors
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>22 24 26 28
Date
</p>
<p>F
re
</p>
<p>qu
en
</p>
<p>cy
</p>
<p>Iraq invasion
Israeli security Brussels
Muslims victims
</p>
<p>Operation Condor
Palestinian Israeli land
Putin liberated Syria
</p>
<p>(b) Clickbaits
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>22 24 26 28
Date
</p>
<p>F
re
</p>
<p>qu
en
</p>
<p>cy
</p>
<p>Bomers nuke plant
Brussels bomber
CNN Trump muslim
</p>
<p>Morgan Trump
Obama ideology
Republicans poll
</p>
<p>(c) Hoaxes
</p>
<p>0
</p>
<p>100
</p>
<p>200
</p>
<p>300
</p>
<p>400
</p>
<p>22 24 26 28
Date
</p>
<p>F
re
</p>
<p>qu
en
</p>
<p>cy
</p>
<p>Belgium attackers
Brussels explosion
Hillary emails I
</p>
<p>Hillary emails II
Hillary rejected Qaddafi
Wikileaks Saudi Arabia
</p>
<p>(d) Propaganda
</p>
<p>Figure 3: The most popular retweets over time across suspicious news types in contrast to rumors.
</p>
<p>4.3 Suspicious News Retweet Patterns
</p>
<p>In addition to contrasting linguistic realizations
behind different types of suspicious news on Twit-
ter, we are interested in qualitatively evaluating
differences in retweet patterns across suspicious
news types (Lumezanu and Klein, 2012; Men-
doza et al., 2010; Kumar et al., 2016). Figure 3
presents top retweeted tweets over time across
three types of suspicious news – hoaxes, propa-
ganda, and clickbaits and contrasts them to well-
studied retweeting behaviors of rumors. We ob-
serve that users retweeting propaganda, clickbaits
and hoaxes send high volumes of tweets over short
periods of time. Rumors12 are less spiky but active
over significantly longer periods of time compared
to other suspicious news. We also notice that ru-
mors and propaganda contain the majority of top-
ics related to Brussels bombing, but clickbaits and
hoaxes promote very divergent set of topics.
</p>
<p>12We identified rumors relevant to Brussels bombing:
http://www.cs.jhu.edu/∼svitlana/BrusselsRumorList
</p>
<p>5 Summary
</p>
<p>We built linguistically-infused neural network
models that jointly learn from tweet content and
social network interactions to classify suspicious
and verified news tweets and infer specific types of
suspicious news. Future work may focus on utiliz-
ing more sophisticated discourse and pragmatics
features, and inferring degrees of credibility. We
hope our findings on bias and subjectivity in suspi-
cious news will help readers to better judge about
credibility of news in social media.
</p>
<p>6 Acknowledgments
</p>
<p>The research described in this paper was con-
ducted under the High-Performance Analytics
Program and the Laboratory Directed Research
and Development Program at Pacific Northwest
National Laboratory, a multiprogram national lab-
oratory operated by Battelle for the U.S. Depart-
ment of Energy.
</p>
<p>651</p>
<p />
</div>
<div class="page"><p />
<p>References
Michael Barthel, Amy Mitchell, and Jesse
</p>
<p>Holcomb. 2016. Many americans be-
lieve fake news is sowing confusion.
http://www.journalism.org/2016/12/15/many-
americans-believe-fake-news-is-sowing-confusion/.
Accessed: 2017-01-31.
</p>
<p>Kate Connolly, Angelique Chrisafis, Poppy
McPherson, Stephanie Kirchgaessner, Ben-
jamin Haas, Dominic Phillips, Elle Hunt, and
Michael Safi. 2016. Fake news: an insidi-
ous trend that’s fast becoming a global problem.
https://www.theguardian.com/media/2016/dec/02/fake-
news-facebook-us-election-around-the-world.
Accessed: 2017-01-30.
</p>
<p>Niall J Conroy, Victoria L Rubin, and Yimin Chen.
2015. Automatic deception detection: methods for
finding fake news. Proceedings of the Association
for Information Science and Technology 52(1):1–4.
</p>
<p>Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceedings of ACL. pages 171–175.
</p>
<p>Richard Gindras. 2016. Labeling fact-check articles in
google news. https://blog.google/topics/journalism-
news/labeling-fact-check-articles-google-news/.
Accessed: 2016-12-12.
</p>
<p>Jeffrey Gottfried and Elisa Shearer. 2016. News
use across social media platforms 2016.
http://www.journalism.org/2016/05/26/news-
use-across-social-media-platforms-2016. Accessed:
2017-01-30.
</p>
<p>Jesse Graham, Jonathan Haidt, and Brian A Nosek.
2009. Liberals and conservatives rely on different
sets of moral foundations. Journal of personality
and social psychology 96(5):1029.
</p>
<p>Jonathan Haidt and Jesse Graham. 2007. When moral-
ity opposes justice: Conservatives have moral intu-
itions that liberals may not recognize. Social Justice
Research 20(1):98–116.
</p>
<p>Jeffrey T Hancock, Lauren E Curry, Saurabh Goorha,
and Michael Woodworth. 2007. On lying and be-
ing lied to: A linguistic analysis of deception in
computer-mediated communication. Discourse Pro-
cesses 45(1):1–23.
</p>
<p>Joan B. Hooper. 1975. On assertive predicates. In
J. Kimball, editor, Syntax and Semantics. volume 4,
pages 91–124.
</p>
<p>Ken Hyland. 2005. Metadiscourse. Wiley Online Li-
brary.
</p>
<p>Rie Johnson and Tong Zhang. 2014. Effective
use of word order for text categorization with
convolutional neural networks. arXiv preprint
arXiv:1412.1058 .
</p>
<p>Andrej Karpathy, George Toderici, Sanketh Shetty,
Thomas Leung, Rahul Sukthankar, and Li Fei-Fei.
2014. Large-scale video classification with convo-
lutional neural networks. In Proceedings of CVPR.
pages 1725–1732.
</p>
<p>Lauri Karttunen. 1971. Implicative verbs. Language
pages 340–358.
</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.
</p>
<p>Paul Kiparsky and Carol Kiparsky. 1968. Fact. Indiana
University.
</p>
<p>Srijan Kumar, Robert West, and Jure Leskovec. 2016.
Disinformation on the web: Impact, characteristics,
and detection of wikipedia hoaxes. In Proceedings
of WWW. pages 591–602.
</p>
<p>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Proceedings of WWW. pages
342–351.
</p>
<p>Feamster Lumezanu and H Klein. 2012. Measuring the
tweeting behavior of propagandists. In Proceedings
of ICWSM.
</p>
<p>Marcelo Mendoza, Barbara Poblete, and Carlos
Castillo. 2010. Twitter under crisis: can we trust
what we rt? In Proceedings of the first workshop on
social media analytics. ACM, pages 71–79.
</p>
<p>Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP. pages 309–312.
</p>
<p>Adam Mosseri. 2016. News feed fyi:
Addressing hoaxes and fake news.
http://newsroom.fb.com/news/2016/12/news-feed-
fyi-addressing-hoaxes-and-fake-news/. Accessed:
2017-01-30.
</p>
<p>Eunbyung Park, Xufeng Han, Tamara L Berg, and
Alexander C Berg. 2016. Combining multiple
sources of knowledge in deep cnns for action recog-
nition. In Proceedings of AWACV . IEEE, pages 1–8.
</p>
<p>James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates 71:2001.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP. pages 1532–1543.
</p>
<p>Verónica Pérez-Rosas and Rada Mihalcea. 2015. Ex-
periments in open domain deception detection. Pro-
ceedings of EMNLP pages 1120–1125.
</p>
<p>652</p>
<p />
</div>
<div class="page"><p />
<p>Kathryn Perrott. 2016. ’fake news’ on social me-
dia influenced us election voters, experts say.
http://www.abc.net.au/news/2016-11-14/fake-
news-would-have-influenced-us-election-experts-
say/8024660. Accessed: 2017-01-30.
</p>
<p>Slav Petrov. 2016. Announcing syntaxnet: The world’s
most accurate parser goes open source. Google Re-
search Blog, May 12:2016.
</p>
<p>Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic models for an-
alyzing and detecting biased language. In Proceed-
ings of ACL. pages 1650–1659.
</p>
<p>Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP. pages 105–112.
</p>
<p>Victoria L Rubin, Yimin Chen, and Niall J Conroy.
2015. Deception detection for news: three types of
fakes. Proceedings of the Association for Informa-
tion Science and Technology 52(1):1–4.
</p>
<p>Marcella Tambuscio, Giancarlo Ruffo, Alessandro
Flammini, and Filippo Menczer. 2015. Fact-
checking effect on viral hoaxes: A model of misin-
formation spread in social networks. In Proceedings
of WWW. pages 977–982.
</p>
<p>Aldert Vrij, Samantha Mann, Susanne Kristen, and
Ronald P Fisher. 2007. Cues to deception and ability
to detect lies as a function of police interview styles.
Law and human behavior 31(5):499–518.
</p>
<p>Ye Zhang and Byron Wallace. 2015. A sensitivity anal-
ysis of (and practitioners’ guide to) convolutional
neural networks for sentence classification. arXiv
preprint arXiv:1510.03820 .
</p>
<p>653</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 654–658
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2103
</p>
<p>Recognizing Counterfactual Thinking in Social Media Texts
</p>
<p>Youngseo Son†, Anneke Buffone§, Anthony Janocko§, Allegra Larche§,
Joseph Raso§, Kevin Zembroski§, H Andrew Schwartz†, Lyle Ungar§
</p>
<p>†Stony Brook University, §University of Pennsylvania
yson@cs.stonybrook.edu, ungar@cis.upenn.edu
</p>
<p>Abstract
</p>
<p>Counterfactual statements, describing
events that did not occur and their con-
sequents, have been studied in areas
including problem-solving, affect man-
agement, and behavior regulation. People
with more counterfactual thinking tend to
perceive life events as more personally
meaningful. Nevertheless, counterfactuals
have not been studied in computational
linguistics. We create a counterfactual
tweet dataset and explore approaches for
detecting counterfactuals using rule-based
and supervised statistical approaches.
A combined rule-based and statistical
approach yielded the best results (F1 =
0.77) outperforming either approach used
alone.
</p>
<p>1 Introduction
</p>
<p>Counterfactuals describe events that did not occur,
and what would have happened (or not happened),
had the event occurred (e.g., “If I hadn’t broken
my arm, I never would have met her.”). More pre-
cisely, counterfactual conditionals have the form
“If it had been the case that A (or not A), it would
have been the case that B (or not B).”
</p>
<p>Counterfactuals have been studied in many dif-
ferent domains. Logicians and philosophers fo-
cus on literally logical relations between the an-
tecedent and consequent of counterfactual forms
and the outcomes (Goodman, 1947). In contrast,
political scientists usually conduct counterfactual
thought experiments for hypothetical tests on his-
torical events, policies, or other aspects of a soci-
ety and assess them (Tetlock, 1996).
</p>
<p>Counterfactual thoughts are defined, especially
in psychology, as mental representations of alter-
natives to past events, actions, or states. Their use
</p>
<p>has been explored for correlations with many dif-
ferent demographics (age, gender) and psycholog-
ical variables (depression, religiosity) (Kray et al.,
2010; Markman and Miller, 2006). Counterfac-
tual thinking has been linked to perceiving life
events as more meaningful, fated, and even as in-
fluenced by the divine (Kray et al., 2010; Buf-
fone et al., 2016), as well as with problem-solving,
because imagining alternate outcomes can eas-
ily bring to mind the steps needed for improve-
ment (Epstude and Roese, 2008; Roese, 1994). It
has also been shown to be associated with affect
management, particularly when imagining reali-
ties that are worse than what actually happened
(Epstude and Roese, 2008; Roese, 1994)
</p>
<p>Despite the extensive research on counterfac-
tual thinking, counterfactual language forms have
not been studied in computational linguistics.
Language-based models to recognize counterfac-
tual thinking in social media would potentially
allow for psychological analysis on users based
on their everyday language, avoiding the high ex-
pense of capturing counterfactual thinking at a
large scale using traditional psychological assess-
ments.
</p>
<p>Therefore, in this paper, we build a language-
based model to recognize counterfactual forms in
social media texts of Twitter and Facebook. There
are many challenges for this task. First, counter-
factual statements have a low base rate; we found
only 2% of status updates on Facebook and 1%
of tweets contain counterfactual statements. Sec-
ondly, counterfactual statements can take on many
forms in natural language.1 For example, they
may or may not use explicit if- or then- clauses
(e.g, consider “If I had not met him then I would
be better off” versus “I wish I had not met him”).
</p>
<p>1Simply looking for words like ‘if’ fails to produce useful
results; only 2 percent of sentences of tweets containing ’if’
are counterfactuals.
</p>
<p>654</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2103">https://doi.org/10.18653/v1/P17-2103</a></div>
</div>
<div class="page"><p />
<p>The low base rate and high variability of natu-
ral language counterfactuals in social media texts
make them difficult recognize using simple lin-
guistic or statistical features. We address theses
challenges by using a combined rule-based and
statistical approach. Key to our success is defin-
ing seven sub-types of counterfactuals, allowing
better coverage of rarer sub-types.
</p>
<p>2 Related Work
</p>
<p>Identifying counterfactuals is in many ways sim-
ilar to identifying discourse relations. In terms
of relation classification, the counterfactual con-
ditionals can be viewed as a subset of Condi-
tion type of Contingency class in the Penn Dis-
course Treebank (PDTB) (Prasad et al., 2008) or
the Condition relation of Rhetorical Structure The-
ory (RST) (Mann and Thompson, 1987). Also,
like all discourse relations in the PDTB, counter-
factuals have implicit and explicit forms, and so
cannot by uniquely identified by the presence of
specific words.
</p>
<p>There have been many researchers who have
tried end-to-end discourse relation parsing with
the PDTB and RST(Biran and McKeown, 2015;
Lin et al., 2009; Ji and Eisenstein, 2014). Many
of them used dependency parsing or constituency
parsing for argument detection or elementary dis-
course unit (EDU) segmentation to infer the rela-
tion between them. However, the short lengths and
poor quality of parses of social media texts make
dependency constituents unreliable.2 For exam-
ple, posters frequently drop the subject of a sen-
tence.
</p>
<p>Other work mostly focuses on relation clas-
sification with an assumption that arguments of
the given relations are already identified (Park
and Cardie, 2012; Pitler et al., 2009). They ex-
plore various learning algorithms and types of fea-
tures in the given arguments of discourse relations.
Then, they report which combinations give the
best performance of each discourse relation.
</p>
<p>Our work, while possible to view as a task in
discourse relation classification, focuses on criti-
cal features of counterfactuals rather than on ac-
curate demarcation of each argument of the re-
lation. Most downstream applications, such as
</p>
<p>2In our preliminary experiments on our causality tweet
dataset (κ = 0.61), Lin’s parser (Lin et al., 2009) obtained
0.45 F1 while a linear support vector machine (SVM) with
n-gram obtained 0.58 F1 for causality detection.
</p>
<p>psychological studies, require knowing the pres-
ence/absence of counterfactuals rather than their
exact extent.
</p>
<p>3 Method
</p>
<p>We use a combination of a rule-based approach
and a supervised classifier to capture counterfac-
tual statements from Twitter.
</p>
<p>3.1 Data Set
</p>
<p>No existing corpus of counterfactual statements
was available, so we collected our own data set,
starting from a random set of tweets from May
2014 and July 2014. As noted previously, couter-
factual statements are rare, so we first limited the
random tweet set to 1,637 containing keywords3
</p>
<p>that can signal counterfactuals (Train and Test row
from Table 1). Keywords were in part based on
prior literature on spontaneous counterfactual gen-
eration, such as should have, could have, at least,
if only, or next time (Sanna and Turley, 1996). We
identified further counterfactual forms (e.g. wish)
based on visual inspection of the data. Next we
used the overall list of keywords to draw samples
of 500 Tweets for further visual inspection. Words
or phrases which had an unreasonably high false
positive rate for containing counterfactuals were
eliminated. Well-trained annotators then manually
labeled each of the 1,637 tweets for counterfactu-
als with a 10% postive rate, results in 166 coun-
terfactuals and 1,471 negative samples. A random
set of 500 of these instances were used in training
and the rest were reserved for testing. To build out
our training set to capture examples of all forms
of counterfactuals, we added a train supplement
from random tweets from 2012 – at least thirty
tweets from each of seven counterfactual forms
we defined for our statistical model using the regu-
lar expressions4 with brown-clusters and the tweet
PTB tagging model (described next). With this
process, we enabled the model to be less biased
towards only the samples with the counterfactual
cue phrases used for data collection. Additionally,
the model learned syntactically different forms of
counterfactuals identified in prior work. To evalu-
ate counterfactual form annotation, inter-annotator
agreement was established on 1,637 tweets with a
</p>
<p>3e.g., ‘should’, ‘shulda’; full list available in our supple-
mentary data.
</p>
<p>4The regular expression table is included in our supple-
mentary data.
</p>
<p>655</p>
<p />
</div>
<div class="page"><p />
<p>Dataset CF Non-CF Total
Train 49 451 500
</p>
<p>+ Supplement 768 498 1,266
Test 84 1,053 1,137
</p>
<p>Table 1: Data Collection. ‘CF’ is counterfactual
and ‘Non-CF’ is non-counterfactual
</p>
<p>second rater with achieving κ = 0.774 and human
annotation F1 0.791.
</p>
<p>3.2 Classification
</p>
<p>We first use a rule-based model to capture counter-
factual patterns from social media texts. We then
use a statistical model (Linear SVM) to increase
precision by identifying tricky false positives with
forms similar to counterfactuals (e.g., ”wish you
the best”).
</p>
<p>Rule-based Classification. Our rule-based ap-
proach is based on seven forms of counterfactuals
(Table 2). Central to our method is our theorizing,
based on reading the literature, especially (Kray
et al., 2010) and examining many counterfactual
examples, that counterfactuals come in seven dif-
ferent forms, shown with examples in (Table 2).
First, we remove sentences ending in question
marks predicted as ‘end of sentence’ by the tweet
part-of-speech (POS) tagger (Gimpel et al., 2011).
We then use pattern matching with regular expres-
sion using a combination of cue phrases (bold),
POS tags, and word clusters. The word clusters,
based on a set of Twitter Brown clusters5 are used
to capture the numerous variations of words in so-
cial media texts (e.g., ‘shuldve’ for ‘should have’).
This approach requires matching both the token
and its part-of-speech, since the POS tag of each
token is important for counterfactual form.
</p>
<p>The rule-based approach is also useful in that
it allows us to detect the arguments for counter-
factual relations; conditional statement and conse-
quent statement from Conjunctive Normal/ Con-
verse form and Verb Inversion form, one coun-
terfactual statement from Wish Verb and Could /
Would / Should have. We customized Biran’s de-
marcation methods using the first verb phrase or
the connective as a boundary to capture the more
informative argument of the statement: For one
argument detection, we demarcate from the cue
phrase (e.g., would have) to the end of sentence.
For two arguments, we demarcate from condi-
</p>
<p>5http://www.cs.cmu.edu/˜ark/TweetNLP/
clusters/50mpaths2
</p>
<p>tional word (e.g., if, unless) to the end of statement
or before the start of the second verb phrase.
</p>
<p>Part of Speech Tagging We use the Penn Tree-
bank (PTB)-style Tweet POS tags6 instead of
Tweet POS tags (Gimpel et al., 2011) as it contains
more fine-grained categories and yields higher ac-
curacy of pattern matching. For instance, Tweet
POS tags do not differentiate modal verbs, past
tense verbs, and other types of verbs, but catego-
rize all of them as ‘V’. However, in many forms
of counterfactuals, the distinction between modal
verbs and past particles from other types of verbs
are critical (e.g., in Should / Could / Would Have
forms). Finally, we conduct a postprocessing on
the Tweet POS parsing results for the more ac-
curate prediction. First, we delete RT tags along
with the token since it is not informative for our
task. Then, we convert ‘USR’ to nouns because
the word token tagged as ‘user’ usually plays the
role of a common noun from the discourse rela-
tion perspective. Additionally, in order to enhance
the POS tagging, we use the brown clusters to tag
empirical variations of modal verbs as ‘MD’ and
we define ‘CCJ’, a new tag to distinguish condi-
tional conjunctions (i.e. Brown clusters for ‘if’)
from other types of conjunctions.
</p>
<p>Statistical Modeling. Each counterfactual form
has a different number of arguments for the rela-
tion, and different types of features that cause the
most errors. Therefore, we analyze the errors of
each form separately and use different approaches
expected to ensure the best performance.
</p>
<p>If a tweet matches rules for counterfactual
forms 1, 2, 3, 4, or 5, it is further classified using
a statistical model trained with features of sequen-
tial words (n-gram) and POS tags of demarcated
arguments and the whole sentence.
</p>
<p>A statistical model is expected to capture some
implicit relations between arguments as well as
lexical and part-of-speech patterns, but may also
hurt performance in situations where the rule-
based approach achieves high precision. There-
fore, we applied statistical approaches to counter-
factual forms which cannot be easily differentiated
by their superficial patterns. These forms were se-
lected by both theoretical and empirical analysis;
we discuss these forms further in our evaluation
section.
</p>
<p>6http://www.cs.cmu.edu/˜ark/TweetNLP/
model.ritter_ptb_alldata_fixed.20130723
</p>
<p>656</p>
<p />
</div>
<div class="page"><p />
<p>Counterfactual Form Example
1.Wish Verb I wish I had been richer
2. Conjunctive Normal If everyone put differences aside and get along, everything would be so much enjoyable
3. Conjunctive Converse I would be stronger, if I had lifted weights
4. Modal Normal They should of shown this guy gettin shot, that woulda been TV gold.
5. Verb Inversion Had I left the event early, I would not have met John
6. Should Have I should have joined the event early
7. (Would / Could) Have I would have been happier without John
</p>
<p>Table 2: Counterfactual Forms
</p>
<p>Performance CF Parser Rules only SVM
Precision 0.7131 0.5864 0.7143
Recall 0.8365 0.9134 0.2791
F1 0.7699 0.7143 0.4013
</p>
<p>Table 3: Performance of Classifiers
</p>
<p>4 Evaluation
</p>
<p>As discussed, counterfactuals are not easily iden-
tified by rules or specific words. Given their low
base rate and multiplicity of forms, traditional
machine learning approaches trained on a ran-
dom tweet sample tend to label all tweets as the
most frequent class (non-counterfactual). Use of a
counterfactual-enriched training set increases pre-
cision, but still gives a low F1 on the imbalanced
test set.
</p>
<p>Thus, in order to make the classifier robust to
the imbalanced dataset, we designed a rule-based
model with counterfactual forms, which resulted
in significantly lower false negative rate of the sta-
tistical model. Moreover, the rule-based model
captured more positive samples of all possible
forms, despite its lack of presence in the training
set. This results in a substantial increase in over-
all performance F1. However, the precision is ex-
tremely low because of its incapability to detect
negative sample with subtle differences inside the
pattern.
</p>
<p>A combined approach therefor gives the best re-
sult. As Table 3 shows, the statistical model ob-
tained the highest precision, while the rule-based
model obtained, by a large margin, the highest re-
call. However, our whole pipeline (‘CF Parser’
in Table 3) obtained the best overall performance
with the combination of both approaches.
</p>
<p>For Wish Verb form prediction gets a big per-
formance boost from the statistical model be-
cause of highly frequent false positives which have
counterfactual-like forms such as birthday wishes
or new year’s day wishes. Among samples classi-
fied as Wish Verb form the counterfactual predic-
tion F1 increased from 0.82 to 0.90 after the final
</p>
<p>Process F1 Precision Recall
Whole Pipeline 0.7699 0.7131 0.8365
- Args 0.7595 0.6767 0.8653
- PTB 0.7456 0.6854 0.8173
- Form 1 0.7447 0.6592 0.8557
- Form 2,3,4,5 0.7352 0.6241 0.8942
</p>
<p>Table 4: Ablation Test for Each Process
</p>
<p>prediction by the statistical model.
Finally, we conducted an ablation test to ana-
</p>
<p>lyze how each process of the pipeline affects the
overall performance of the classifier (Table 4).
The argument detection was less effective (F1 0.01
drop) than we expected due to the relatively simple
and concise structure of tweets in general (Args in
Table 4).
</p>
<p>Using only n-grams as features for the statis-
tical model without PTB-style Tweet POS tags
gives a relatively large drop (0.02) from F1. From
the grammatical perspective, n-grams are less in-
formative than POS tags for counterfactuals espe-
cially considering that there are so many variations
of each word token in social media (e.g., ‘clda’,
‘coulda’, and ‘couldve’ for ‘could have’).
</p>
<p>We examined how the statistical model af-
fected the final performance of each counterfac-
tual form. The model we used for filtering out
frequent false positives (e.g., birth day wishes)
of Wish Verb form caused 0.03 F1 drop when it
is removed. Also, the models trained with two-
argument-relation forms (Conjunctive Normal /
Converse, Modal Normal, and Verb Inversion)
caused 0.04 F1 drop when they are removed from
the pipeline, since the classifier cannot use subtle
relations between arguments for its counterfactual
prediction.
</p>
<p>5 Conclusion
</p>
<p>This is the first work to identify counterfactuals in
social media, a task we hope more people will ad-
dress. Our best results came from combining rule-
based methods that exploit a theory of the differ-
ent forms of counterfactual with focused statistical
</p>
<p>657</p>
<p />
</div>
<div class="page"><p />
<p>methods for reclassification of challenging forms.
Our counterfactual predictor can now be applied
to large collections of tweets and Facebook posts
from people of known education, religiosity, po-
litical orientation, well-being, and other attributes
of interest to psychologists and political scientists,
allowing further study of their theories of counter-
factual use.
</p>
<p>References
Or Biran and Kathleen McKeown. 2015. Pdtb dis-
</p>
<p>course parsing as a tagging task: The two taggers
approach. In Proceedings of the 16th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue. pages 96–104.
</p>
<p>Anneke Buffone, Shira Gabriel, and Michael Poulin.
2016. There but for the grace of god counterfactu-
als influence religious belief and images of the di-
vine. Social Psychological and Personality Science
7(3):256–263.
</p>
<p>Kai Epstude and Neal J Roese. 2008. The functional
theory of counterfactual thinking. Personality and
Social Psychology Review 12(2):168–192.
</p>
<p>Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers-Volume 2. As-
sociation for Computational Linguistics, pages 42–
47.
</p>
<p>Nelson Goodman. 1947. The problem of counter-
factual conditionals. The Journal of Philosophy
44(5):113–128.
</p>
<p>Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
ACL (1). pages 13–24.
</p>
<p>Laura J Kray, Linda G George, Katie A Liljenquist,
Adam D Galinsky, Philip E Tetlock, and Neal J
Roese. 2010. From what might have been to what
must have been: counterfactual thinking creates
meaning. Journal of personality and social psychol-
ogy 98(1):106.
</p>
<p>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1. Association
for Computational Linguistics, pages 343–351.
</p>
<p>William C Mann and Sandra A Thompson. 1987.
Rhetorical structure theory: A theory of text orga-
nization. University of Southern California, Infor-
mation Sciences Institute.
</p>
<p>Keith D Markman and Audrey K Miller. 2006. Depres-
sion, control, and counterfactual thinking: Func-
tional for whom? Journal of Social and Clinical
Psychology 25(2):210–227.
</p>
<p>Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue. Association for Computational Lin-
guistics, pages 108–112.
</p>
<p>Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2. Association for Computational Linguistics, pages
683–691.
</p>
<p>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.
</p>
<p>Neal J Roese. 1994. The functional basis of counter-
factual thinking. Journal of personality and Social
Psychology 66(5):805.
</p>
<p>Lawrence J Sanna and Kandi Jo Turley. 1996. An-
tecedents to spontaneous counterfactual thinking:
Effects of expectancy violation and outcome va-
lence. Personality and Social Psychology Bulletin
22(9):906–919.
</p>
<p>Philip E Tetlock. 1996. Counterfactual thought exper-
iments in world politics: Logical, methodological,
and psychological perspectives. Princeton Univer-
sity Press.
</p>
<p>658</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 659–665
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2104
</p>
<p>Temporal Orientation of Tweets for Predicting Income of Users
</p>
<p>Mohammed Hasanuzzaman1, Sabyasachi Kamila2, Mandeep Kaur2,
Sriparna Saha2 and Asif Ekbal2
</p>
<p>1ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland
2Department of Computer Science and Engineering,
</p>
<p>Indian Institute of Technology Patna, India
hasanuzzaman.im@gmail.com
</p>
<p>Abstract
</p>
<p>Automatically estimating a user’s socio-
economic profile from their language use
in social media can significantly help so-
cial science research and various down-
stream applications ranging from business
to politics. The current paper presents the
first study where user cognitive structure
is used to build a predictive model of in-
come. In particular, we first develop a
classifier using a weakly supervised learn-
ing framework to automatically time-tag
tweets as past, present, or future. We
quantify a user’s overall temporal orienta-
tion based on their distribution of tweets,
and use it to build a predictive model of
income. Our analysis uncovers a corre-
lation between future temporal orientation
and income. Finally, we measure the pre-
dictive power of future temporal orienta-
tion on income by performing regression.
</p>
<p>1 Introduction
</p>
<p>User-generated content in social media such as
Twitter has enabled the study of author profiling
on an unprecedented scale. Author profiling in
social media aims at inferring various attributes
of the user from the text that they have written.
Most of the prior studies in this field have fo-
cused on age, gender prediction (Marquardt et al.,
2014; Sap et al., 2014), psychological well-being
(Dodds et al., 2011; Choudhury et al., 2013), and a
host of other behavioural, psychological and med-
ical phenomena (Kosinski et al., 2013). However,
there has been a lack of work looking at the socio-
economic characteristics of Twitter users. In this
paper, we focus on automatic estimation of Twit-
ter users’ income from their Twitter language. An
income predictor of social media users can be use-
ful for both social science research and a range of
</p>
<p>downstream applications in banking, marketing,
and politics.
</p>
<p>Previous social science studies on income
demonstrate that income of people is correlated
with various factors such as demographic fea-
ture (the congressional district in which the re-
spondent lived), educational categories, sex, age,
age squared, gender, race categories, marital sta-
tus categories, and height (Kahneman and Deaton,
2010). Other studies reveal that psychological
traits related to extroversion (e.g. larger social
networks) and conscientiousness (e.g. orderli-
ness) have a positive correlation with income,
while neurotic traits (e.g. anger, anxiety) are anti-
correlated (Roberts et al., 2007). Human temporal
orientation refers to individual differences in the
relative emphasis one places on the past, present,
or future (Zimbardo and Boyd, 2015). Past studies
have established consistent links between tempo-
ral orientation and most of the above-mentioned
income predictor factors such as age, sex, gender,
education, and psychological traits (Webley and
Nyhus, 2006; Adams and Nettle, 2009; Schwartz
et al., 2013; Zimbardo and Boyd, 2015). Accord-
ingly, this begs the question as to whether there is
any link between an individual’s temporal orien-
tation and their income level. Traditionally, tem-
poral orientation has been assessed by self-report
questionnaires. In this paper, we assess tempo-
ral orientation based on language use in Twitter.
Our method uses a tweet-level classifier of past,
present, and future, grouped over users to create
user-level assessments.
</p>
<p>Our learning framework uses convolutional
neural networks (CNNs) (Goodfellow et al., 2016)
to infer tweet vector representations, and consid-
ers them as the feature to develop a classification
model that can automatically detect the time ori-
entation (oriented towards past, present, and fu-
ture) of tweets. The framework leverages weak
supervision signals provided by a list of manu-
</p>
<p>659</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2104">https://doi.org/10.18653/v1/P17-2104</a></div>
</div>
<div class="page"><p />
<p>ally selected eighty (80) high-precision seed terms
(and automatically extracted similar terms) repre-
senting past, present, and future to train the CNN.
For example, tweets exclusively containing past
(resp. present and future) seed terms were marked
with weak labels past (resp. present and future).
We used the tweet-level temporal classifier to au-
tomatically classify a large dataset consisting of
≈10 million tweets from 5,191 users mapped to
their income, using fine-grained user occupation
as a proxy. Finally, we tested whether individual
differences in past, present, and future orientation
are related to income. In particular, we frame the
income prediction task as regression using linear
as well as non-linear learning algorithms where
temporal orientation served as predictive features.
To the best of our knowledge, this represents the
first work to study a temporal orientation-based in-
come prediction using Twitter language.
</p>
<p>In summary the proposed approach is different
from the previous works (Schwartz et al., 2015;
Preoţiuc-Pietro et al., 2015; Park et al., 2017) in
several ways. Unlike Schwartz et al. (2015), we
used a weakly supervised approach. The genera-
tion of training data is semi-automatic in our case.
Rather than manually identifying features, tweet
vectors are fed to a CNN classifier. Furthermore
while Schwartz et al. (2015) studied temporal ori-
entation of facebook data in order to predict differ-
ent human correlates like conscientiousness, age,
and gender, our current work focuses on predict-
ing the income of a user using temporal orientation
of their tweets. In Preoţiuc-Pietro et al. (2015),
the authors predict user income based on different
demographic and psychological features of users.
However, the process of extracting these features
is computationally complex. The current study is
therefore, the first of its kind to explore the use of
temporal orientation of user-tweets to predict in-
come.
</p>
<p>2 Related Work
</p>
<p>Existing message-/sentence-level temporal clas-
sification methods generally fall into two cate-
gories: (1) rule-based methods, and (2) supervised
machine-learning methods. Rule-based methods
mainly rely on manually designed classification
rules for each temporal class (Nie et al., 2015).
Despite their effectiveness, this kind of method re-
quires substantial efforts in rule design. Most re-
search on machine learning-based sentence tem-
</p>
<p>poral classification has revolved around feature
engineering for better classification performance.
Different kinds of features have been explored
such as bag-of-words, time expressions, part-of-
speech tags, and temporal class-specific lexicons
(Schwartz et al., 2015). Temporal class specific
lexicon creation and feature engineering also cost
a lot of human efforts. In addition, creation
of a large-scale training data set for supervised
machine-learning approaches is also very labori-
ous.
</p>
<p>3 Methodology
</p>
<p>In this section, we describe our proposed method-
ology to identify the underlying temporal orienta-
tion of tweets and a set of contrastive systems that
we used as baselines for comparative study.
</p>
<p>3.1 Tweet Temporal Orientation Classifier
</p>
<p>The task can be defined as given a tweet t and its
posting date d, predict its temporal class c ∈ {
past, present, or future} with reference to its is-
suing date.
Proposed Architecture: The proposed frame-
work has two main steps as: (i) training the model
parameters, and (ii) using the model to tag unseen
tweets. During training, we use the weakly la-
beled tweets to learn the parameters of the CNN
and temporal orientation classifier. For classifica-
tion, a linear Support Vector Machine (lSVM)1 is
used. In particular, we trained three binary clas-
sifiers (one per class)2 using one-vs.-rest, and la-
bel a tweet with the class that assigned the highest
score. In the second step, we pass tweets through
these two optimized components to detect their
temporal orientation.
</p>
<p>Figure 1: Proposed learning architecture.
</p>
<p>1Trained using the Weka implementation of LIBSVM
with linear kernels (polynomial kernels yielded worse per-
formance).
</p>
<p>2Multi-class classification yielded worse performance.
</p>
<p>660</p>
<p />
</div>
<div class="page"><p />
<p>The choice of CNN for feature extraction is mo-
tivated by:
</p>
<p>• CNNs have been successfully used as feature
extractors in various computer vision tasks
and achieved better results compared to hand-
crafted features. Research has shown that
CNN feature maps can be used with SVM to
yield classification results that outperform the
original CNN (Athiwaratkun et al., 2015)
</p>
<p>• Superior accuracies have also been achieved
by following a similar line of research in the
context of NLP tasks (Kim, 2014; Poria et al.,
2015).
</p>
<p>Convolutional Neural Networks (CNNs): The
task is challenging as tweets are short and noisy.
Moreover, English, like many languages, uses a
wide variety of ways to refer to the past, present,
and future. Unlike previous approaches which
mainly rely on hand-crafted rules and feature en-
gineering, we automatically extract features for
tweets to build our tweet-level Temporal Orienta-
tion Classifier. In particular, we use CNNs to auto-
matically extract tweet vectors as the features for
classification. Recently, CNNs have been shown
to be useful in many natural language process-
ing and information retrieval tasks by effectively
modeling natural language semantics (Collobert
et al., 2011). For our experiments, we trained a
simple CNN with one convolution layer followed
by one max pooling layer (Collobert et al., 2011;
Kim, 2014). In the CNN model, we use 3 filters
with window sizes of 5, 6 and 7 with 100 fea-
ture maps each. These window sizes will capture
5-gram, 6-gram, and 7-gram information in the
tweets. We employ dropout for regularization with
a dropout rate of 0.5 which is a reasonable default.
We also use rectified linear units and mini-batches
with size of 50. The parameters of the CNN were
fixed based on the performance of 3-fold cross-
validation. The tweet representations are trained
on top of pre-trained word vectors which are up-
dated during CNN training. We use the publicly
available word2vec3 vectors that are trained on
Google News corpus as well our own Word2vec
vectors4 trained during the labeled-data creation
phase. During the training phase, the parameters
</p>
<p>3https://code.google.com/p/word2vec/
4trained using gensim library available at
</p>
<p>https://radimrehurek.com/gensim/intro.html
</p>
<p>of the CNN model are learned by passing multi-
ple filters over word vectors and then applying the
max-over-time pooling operation to generate fea-
tures which are used in a fully connected softmax
layer. Finally, we use the cross-entropy loss func-
tion for learning the parameters of the model. Sim-
ilar to Kim (2014), we use dropout (Hinton et al.,
2012) to regularize the change of parameters by
randomly setting some weights to zero that pre-
vents overfitting.
</p>
<p>3.2 Income Predictor Model
Similar to Preoţiuc-Pietro et al. (2015), we for-
mulate the income prediction task as regression
using user-level temporal orientation as features.
First, the tweet temporal orientation classifier is
used to label whether a tweet focuses on past,
present, or future. Afterwards, at user-level, we
produce three categories of temporal orientation
(three separate variables summing to one), defined
simply as the proportion of a user’s total tweets
(tweets(user)all) classified in the given temporal
category (c ∈ { past, present, or future}), as in (1):
</p>
<p>orientationc(user) =
|tweetsc(user)|
|tweetsall(user)|
</p>
<p>(1)
</p>
<p>We use linear and non-linear methods. The lin-
ear method is logistic regression (LR) (Freed-
man, 2009) with Elastic Net regularisation. In or-
der to capture the non-linear relationship between
a user’s temporal orientation and their income,
we use Gaussian Processes (GP) (Rasmussen and
Nickisch, 2010) for regression. Given that our
dataset is very large and the number of features
is high, for GP inference we use the fully indepen-
dent training conditional approximation (Snelson
and Ghahramani, 2005) with 500 random induc-
ing points.
</p>
<p>4 Data Sets
</p>
<p>4.1 Training Data
Tweets are collected using the Twitter stream-
ing API.5 We downloaded English tweets during
the period 01.01.2015–31.01.2015, which gener-
ated about 40 million tweets. After collecting
the tweets, we filter past-, present-, and future-
oriented tweets using a manually selected high
precision list of 50 seed terms. These are terms
</p>
<p>5https://dev.twitter.com/streaming/
overview.
</p>
<p>661</p>
<p />
</div>
<div class="page"><p />
<p>that capture temporal dimensions of tweets with
very few false positives, though the recall of these
terms is low. In order to increase the recall, and to
capture new terms that are good paradigms of past,
present, and future, we expand our initial seed
terms using a query expansion technique. We em-
ploy a continuous distributed vector representation
of words using the continuous Skip-gram model
(also known as Word2Vec) proposed by Mikolov
et al. (2013). The model is trained on the whole
collection of 40 million tweets with dimension and
window size set to 300 and 7, respectively.
</p>
<p>Given the vector representations for the terms,
we calculate the similarity scores between pairs
of terms in our vocabulary using cosine similar-
ity. The top 10 similar terms for each seed term
are selected for the expansion of the initial seed
list. We again filter the whole collection of tweets
using the newly added seed terms. We finally
select 120,000 tweets equally distributed in past
(=40,000 tweets), present (=40,000), and future
(=40,000) temporal categories.6 Examples of fil-
tered tweets are as follows:
</p>
<p>• Thank you so much for coming in for our
show yesterday. (seed=yesterday)
</p>
<p>• @**** is currently out of the office working
his other job. (seed=currently)
</p>
<p>• I promise you don’t have to be afraid.
(seed=promise)
</p>
<p>Table 1 shows some examples of expanded
terms for some of the initial seed terms. There are
some unrelated keywords in the expanded seed list
due to the automatic process of keyword selection.
</p>
<p>4.2 Test Set
In order to evaluate the tweet temporal orientation
classification model, 2035 tweets were manually
annotated by three human annotators in four dif-
ferent categories: past, present, future and doubt-
ful. Majority voting is applied to assign the fi-
nal output class to a given tweet. Tweets whose
temporal orientation was not resolved by majority
voting were deleted from the test set.7 The final
distribution of annotated tweets was: past=423,
present=1252, future=325, doubtful=35.
</p>
<p>6Similar to Schwartz et al. (2015), we only considered
past, present and future categories.
</p>
<p>7Note that we approached the authors of Schwartz et al.
(2015) to obtain their dataset but they did not share the data
because of copyright issues. This is the reason for generating
our own gold-standard test set.
</p>
<p>4.3 Income data of Users
</p>
<p>We used a dataset developed by Preoţiuc-Pietro
et al. (2015), which contains 5,191 Twitter users
along with their platform statistics and ≈10 mil-
lion historical tweets. The dataset is based on
mapping a Twitter user to a job title and using this
as a proxy for the mean income for that specific
occupation.
</p>
<p>5 Experimental Results
</p>
<p>Temporal Orientation Classification Results:
The performance of our tweet temporal orienta-
tion classifier is evaluated using the manually an-
notated test set. We compare our approach with
two baselines that are the most relevant for our re-
search: (i) Baseline1: a rule-based method (Nie
et al., 2015) and (ii) Baseline2: a supervised learn-
ing strategy with bag-of-words, time expressions,
part-of-speech tags, and temporal class-specific
lexicon features (Schwartz et al., 2015). Com-
parative evaluation results are presented in Table
2. The results show that our weakly supervised
framework outperforms rule-based and supervised
learning technique in terms of accuracy.
</p>
<p>We examine the impact of the size of la-
beled training data on each method’s performance.
Baseline1 (rule-based approach) is not involved
since this does not depend on labeled training data.
We randomly select d% of the training data to train
the classifiers and test them on the test set, with d
ranging from 10 to 90. For each d, we generate
the training set 20 times and the averaged perfor-
mance is recorded. Accuracies of both approaches
over the test data are presented in Table 3. Re-
sults show that our proposed framework performs
consistently better than its counterpart. In particu-
lar, results show that with 30K training examples,
better results can be obtained by our approach than
relying on 120K training items for the state-of-the-
art supervised machine learning approach (Base-
line2).
Income Prediction Results: Similar to Preoţiuc-
Pietro et al. (2015), we measure the predictive
power of temporal orientation by performing re-
gression on the user income. Performance is
measured using 10-fold cross-validation: in each
round, 80% of the data is used to train the model,
10% is used to tune model parameters using grid
search and a different 10% is held out for test-
ing. The final results are computed over the ag-
gregate set of results of all 10 folds. Results us-
</p>
<p>662</p>
<p />
</div>
<div class="page"><p />
<p>Initial Seed Terms (Temporal Orientation) Extended Seed Terms
Yesterday (Past) yesterday!, started, yday, finished, already, yest, earlier, held, arrived
Currently (Present) now, still, presently, available, whilst, actively, contemplating, considering
Promise (Future) guarantee, expect, doubt, commitment, think, hope, opportunity, tomorrow
</p>
<p>Table 1: Examples of initial seed terms and expanded seed terms.
</p>
<p>Method Baseline1 Baseline2 Proposed Method1 Proposed Method2
</p>
<p>Accuracy 48.8 67.4 74.4 72.7
Past (p, r, f1) (52.0, 56.3, 54.0) (67.4, 81.9, 73.9) (84.5, 79.8, 82.0) (71.1, 79.5, 75.0)
</p>
<p>Present (p, r, f1) (58.2, 54.2, 56.1) (69.3, 82.6, 75.3) (81.3, 86.6, 83.8 ) (73.0, 71.5, 72.2)
Future (p, r, f1) (51.0, 53.3, 52.1) (64.4, 77.9, 70.5) (78.5, 79.8, 79.1) (79.4, 69.5, 74.0)
</p>
<p>Table 2: Accuracy for past, present, future classifications using different methods measured over test
data. Results are broken down by precision (p), recall (r), and f1-measure (f1) scores. Proposed Method1
</p>
<p>and Proposed Method2 represent our classification framework with Word2vec vectors derived from our
collected tweet and pre-trained Google News corpus, respectively.
</p>
<p>Training data size Baseline2 Proposed Method1
</p>
<p>10k 57.5 61.3
20k 60.2 66.4
30k 63.5 71.7
50k 65.4 73.6
70k 66.1 74.2
90k 67.4 74.1
</p>
<p>120K (all) 67.4 74.4
</p>
<p>Table 3: Tweets temporal orientation classification
accuracies with different sizes of training data.
</p>
<p>ing linear and non-linear regression methods and
past, present, future temporal orientation features
are presented in Table4. Performance is measured
using two standard metrics: Pearson’s correlation
coefficient r and Mean Absolute Error (MAE) be-
tween inferred and target values. Results show that
</p>
<p>Method Temporal Orientation Correlation coefficient MAE
</p>
<p>LR
Past 0.1449 £12365
</p>
<p>Present 0.0998 £14365
Future 0.4505 £ 10850
</p>
<p>GP
Past 0.1849 £11200
</p>
<p>Present 0.1099 £12125
Future 0.5104 £ 10235
</p>
<p>Table 4: Prediction of income using temporal ori-
entation features
</p>
<p>correlation between a user’s future temporal ori-
entation and their income is the highest, i.e. peo-
ple with higher future temporal orientation tend to
have higher income levels. Results also demon-
strate that predictive models with future tempo-
ral orientation as a feature can predict income
with high accuracy compared to past and present
temporal orientation. Our findings are consistent
</p>
<p>with previous research that suggests that future-
oriented thinking is linked to academic achieve-
ment, increased social involvement, lower dis-
tress, extroversion, and conscientiousness. These
factors are also positively correlated with income
(Kahana et al., 2005; Roberts et al., 2007). Note
also that, the non-linear methods outperform the
linear methods by a wide margin, showing the im-
portance of modeling non-linear relationships in
our data.
</p>
<p>6 Conclusions
</p>
<p>We presented the first large-scale study aiming to
predict the income of Twitter users from their tem-
poral orientation. Temporal orientation of users
is assessed from their tweets. Our weakly super-
vised learning framework automatically time-tags
tweets according to its underlying temporal ori-
entation: past, present, or future. The associa-
tions we found between user-level future tempo-
ral orientation and income are novel in the con-
text of well-established temporal orientation cor-
relates. As future work, we are in the process of
improving the temporal orientation classification
accuracy by incorporating linguistic and sentiment
related features into the deep learning phase.
</p>
<p>Acknowledgments
</p>
<p>The ADAPT Centre for Digital Content Technol-
ogy is funded under the SFI Research Centres Pro-
gramme (Grant 13/RC/2106) and is co-funded un-
der the European Regional Development Fund.
</p>
<p>663</p>
<p />
</div>
<div class="page"><p />
<p>References
Jean Adams and Daniel Nettle. 2009. Time perspec-
</p>
<p>tive, personality and smoking, body mass, and phys-
ical activity: An empirical study. British journal of
health psychology 14(1):83–105.
</p>
<p>Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Predicting postpartum changes in
emotion and behavior via social media. In 2013
ACM SIGCHI Conference on Human Factors in
Computing Systems, CHI ’13, Paris, France, April
27 - May 2, 2013. pages 3267–3276.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12(Aug):2493–2537.
</p>
<p>Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M Kloumann, Catherine A Bliss, and Christo-
pher M Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and twitter. PloS one 6(12):e26752.
</p>
<p>David A Freedman. 2009. Statistical models: theory
and practice. Cambridge University Press, Cam-
bridge, UK.
</p>
<p>Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
2016. Deep Learning. MIT Press, Massachusetts,
US. http://www.deeplearningbook.org.
</p>
<p>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580 .
</p>
<p>Eva Kahana, Boaz Kahana, and Jianping Zhang. 2005.
Motivational antecedents of preventive proactivity
in late life: Linking future orientation and exercise.
Motivation and emotion 29(4):438–459.
</p>
<p>Daniel Kahneman and Angus Deaton. 2010. High in-
come improves evaluation of life but not emotional
well-being. Proceedings of the national academy of
sciences 107(38):16489–16493.
</p>
<p>Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .
</p>
<p>Michal Kosinski, David Stillwell, and Thore Grae-
pel. 2013. Private traits and attributes are pre-
dictable from digital records of human behavior.
Proceedings of the National Academy of Sciences
110(15):5802–5805.
</p>
<p>James Marquardt, Golnoosh Farnadi, Gayathri Vasude-
van, Marie-Francine Moens, Sergio Davalos, Ankur
Teredesai, and Martine De Cock. 2014. Age and
gender identification in social media. In Working
Notes for CLEF 2014 Conference, Sheffield, UK,
September 15-18, 2014.. pages 1129–1136.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Advances in neural information pro-
cessing systems, Lake Tahoe, Nevada, United States,
December 5-8, 2013. pages 3111–3119.
</p>
<p>Aiming Nie, Jason Shepard, Jinho Choi, Bridget
Copley, and Phillip Wolff. 2015. Computational
exploration of the linguistic structures of future-
oriented expression: Classification and categoriza-
tion. In NAACL-HLT 2015 Student Research Work-
shop (SRW), Denver, Colorado, USA. volume 867,
page 168.
</p>
<p>Gregory Park, H. Andrew Schwartz, Maarten Sap,
Margaret L. Kern, Evan Weingarten, Johannes C.
Eichstaedt, Jonah Berger, David J. Stillwell, Michal
Kosinski, Lyle H. Ungar, and Martin E. P. Seligman.
2017. Living in the past, present, and future: Mea-
suring temporal orientation with language. Journal
of Personality 85(2):270–280.
</p>
<p>Soujanya Poria, Erik Cambria, and Alexander F. Gel-
bukh. 2015. Deep convolutional neural network
textual features and multiple kernel learning for
utterance-level multimodal sentiment analysis. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2015, Lisbon, Portugal, September 17-21, 2015.
pages 2539–2544.
</p>
<p>Daniel Preoţiuc-Pietro, Svitlana Volkova, Vasileios
Lampos, Yoram Bachrach, and Nikolaos Aletras.
2015. Studying user income through language,
behaviour and affect in social media. PloS one
10(9):e0138717.
</p>
<p>Carl Edward Rasmussen and Hannes Nickisch. 2010.
Gaussian processes for machine learning (GPML)
toolbox. Journal of Machine Learning Research
11:3011–3015.
</p>
<p>Brent W Roberts, Nathan R Kuncel, Rebecca Shiner,
Avshalom Caspi, and Lewis R Goldberg. 2007. The
power of personality: The comparative validity of
personality traits, socioeconomic status, and cogni-
tive ability for predicting important life outcomes.
Perspectives on Psychological Science 2(4):313–
345.
</p>
<p>Maarten Sap, Gregory J. Park, Johannes C. Eichstaedt,
Margaret L. Kern, David Stillwell, Michal Kosin-
ski, Lyle H. Ungar, and Hansen Andrew Schwartz.
2014. Developing age and gender predictive lex-
ica over social media. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL. pages 1146–1151.
</p>
<p>H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin EP Seligman, et al.
</p>
<p>664</p>
<p />
</div>
<div class="page"><p />
<p>2013. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one 8(9):e73791.
</p>
<p>H. Andrew Schwartz, Greg Park, Maarten Sap, Evan
Weingarten, Johannes Eichstaedt, Margaret Kern,
Jonah Berger, Martin Seligman, and Lyle Un-
gar. 2015. Extracting human temporal orienta-
tion in facebook language. In Proceedings of The
2015 Conference of the North American Chapter
of the Association for Computational Linguistics-
Human Language Technologies (NAACL), Denver,
Colorado, USA. pages 409–419.
</p>
<p>Edward Snelson and Zoubin Ghahramani. 2005.
Sparse gaussian processes using pseudo-inputs. In
Advances in Neural Information Processing Sys-
tems 18 [Neural Information Processing Systems,
NIPS 2005, December 5-8, 2005, Vancouver, British
Columbia, Canada]. pages 1257–1264.
</p>
<p>Paul Webley and Ellen K Nyhus. 2006. Parents in-
fluence on childrens future orientation and saving.
Journal of Economic Psychology 27(1):140–164.
</p>
<p>Philip G Zimbardo and John N Boyd. 2015. Putting
time in perspective: A valid, reliable individual-
differences metric. In Time Perspective Theory; Re-
view, Research and Application, Springer, Berlin,
Germany, pages 17–55.
</p>
<p>665</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 666–671
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2105
</p>
<p>Character-Aware Neural Morphological Disambiguation
</p>
<p>Alymzhan Toleu Gulmira Tolegen
National Laboratory Astana, Nazarbayev University
</p>
<p>53 Kabanbay batyr ave., Astana, Kazakhstan
alymzhan.toleu@gmail.com, gulmira.tolegen.cs@gmail.com,
</p>
<p>aibek.makazhanov@nu.edu.kz
</p>
<p>Aibek Makazhanov
</p>
<p>Abstract
</p>
<p>We develop a language-independent, deep
learning-based approach to the task of
morphological disambiguation. Guided
by the intuition that the correct analysis
should be “most similar” to the context,
we propose dense representations for mor-
phological analyses and surface context
and a simple yet effective way of com-
bining the two to perform disambiguation.
Our approach improves on the language-
dependent state of the art for two agglu-
tinative languages (Turkish and Kazakh)
and can be potentially applied to other
morphologically complex languages.
</p>
<p>1 Introduction
</p>
<p>Morphological disambiguation (MD) is a long
standing problem in processing morphologically
complex languages (MCL). POS tagging is a
somewhat related problem, however in MD, in
addition to POS tags, one typically has to pre-
dict lemmata (roots hereinafter) that surface forms
stem from and morphemes1 they bear. For
example, depending on the context, a Turkish
word adam can be analyzed as: (i) a man –
[adam]1+[Noun]2+[A3sg+Pnon+Nom]3 or (ii) my
island – [ada]1+[Noun]2+[A3sg+P1sg +Nom]3
(Hakkani-Tür et al., 2002). Thus, if one counts
analyses as tags, MD can be cast as a tagging prob-
lem with an extremely large tagset. This fact dis-
courages direct application of the state of the art
approaches designed for small fixed tagsets.
</p>
<p>To develop a language independent dense repre-
sentation of the analyses, we segment2 an analysis
</p>
<p>1We use the term morpheme for its universal recognition
within the community. A more appropriate term might be
grammeme, i.e. a value of grammatical category.
</p>
<p>2Such a segmentation is denoted by the squared brackets
numbered in the respective order (cf. Turkish example).
</p>
<p>into (i) the root, (ii) its POS and (iii) the morpheme
chain (MC). We then proceed to jointly learn the
embeddigns for the root and the POS segments and
to combine them and the MC segment representa-
tion into a single dense representation. MC seg-
ments are represented as binary vectors that, for
a given analysis, encode presence or absence of
each morpheme found in the train set. This en-
sures language independence and contrasts previ-
ous work (at least on Turkish and Kazakh), where
only certain morphemes are chosen as features de-
pending on their position (Assylbekov et al., 2016;
Hakkani-Tür et al., 2002) or presence (Makham-
betov et al., 2015) in an analysis, or the authors’
intuition (Yildiz et al., 2016; Tolegen et al., 2016;
Sak et al., 2007).
</p>
<p>Apart from the sparseness of analyses distribu-
tion MCL notoriously raise free word order and
long dependency issues. Thus, decoding analysis
sequences using only the leftmost context may not
be enough. To address this we leverage the right-
most context as well. We model the left- and right-
most surface context in two ways: using (i) BiL-
STM (Greff et al., 2015) with a character-based
sub-layer (Ling et al., 2015) and (ii) with a feed
forward network on word embeddings. We then
entertain the idea that given a word with multiple
analyses and its surface context, the correct analy-
sis might be “closer” to the context. Following our
intuition, we have tried computing the distance be-
tween the analysis and the context representations,
and a simple dot product (as in unnormalized co-
sine similarity) has yielded the best performance.
</p>
<p>We evaluate our approach on Turkish and
Kazakh data sets, using several baselines (includ-
ing the state of the art methods for both languages)
and a variety of settings and metrics. In terms
of general accuracy our approach has achieved a
nearly 1% improvement over the state of the art for
Turkish and a marginal improvement for Kazakh.
</p>
<p>666</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2105">https://doi.org/10.18653/v1/P17-2105</a></div>
</div>
<div class="page"><p />
<p>Our contribution amounts to the following: (i) a
general MD framework for MCL that can be ana-
lyzed in &lt;root, POS, MC&gt; triplets; (ii) improve-
ment on language-dependent state of the art for
Turkish and Kazakh.
</p>
<p>2 Models
</p>
<p>In this section we describe our approach to encod-
ing morphological analyses and the context into
the embeddings and combining them to perform
morphological disambiguation.
</p>
<p>2.1 Morphological Representation
We treat a morphological analysis as a combina-
tion of three main constituents: the root, its POS
and the morpheme chain. These constituents are
represented as dr, dp, and dm-dimensional vectors
respectively. The former two vectors correspond
to dense word embeddings (Collobert et al., 2011),
and the latter is a binary vector which encodes the
presence of a certain morpheme in the chain. The
size of the binary vector, dm, is equal to the size of
the morpheme dictionary obtained from the data.
</p>
<p>Given a sentence and the j-th surface word form
withN analyses, we represent the k-th analysis as:
</p>
<p>Akj = tanh(W
rrk + Wppk + Wmmk) (1)
</p>
<p>where Aj ∈ Rdh×|N |, dh is the dimension of
each analysis embedding, rk ∈ Rdr×1, pk ∈
Rdp×1,mk ∈ {0, 1}dp×1 are constituent vectors
of the k-th analysis, and Wr ∈ Rdh×dr , Wp ∈
Rdh×dp ,Wm ∈ Rdh×dm are the model parame-
ters. The bias term was left out for clarity. This
representation is shown on Figure 1 (bottom).
</p>
<p>2.2 Recurrent Neural Disambiguation
The model architecture is shown on Figure 1. It
consists of two main blocks that learn the surface
context (top) and the morphological analyses rep-
resentations (bottom).
</p>
<p>When it comes to modeling context via word
embeddings for morphologically complex lan-
guages, it is impractical to actually store vectors
for all words, since majority of words in such
languages has a large number of surface realiza-
tions. Our solution to this problem is to con-
struct a surface word representation from charac-
ters that not only reduces data sparseness, but also
help in dealing with the out-of-vocabulary (OOV)
words (Ling et al., 2015). We represent each char-
acter of each word as a vector xi ∈ Rdc and the
</p>
<p>Hidden layer:
Aj
</p>
<p>k
 = tanh(W
</p>
<p>rrk+ W
ppk+W
</p>
<p>mmk)
</p>
<p>LSTM
</p>
<p>hidden layer
</p>
<p>C a t
</p>
<p>Cat sat on the mat
</p>
<p>...
</p>
<p>... ...
</p>
<p>root pos MC
</p>
<p>Characters
</p>
<p>Context
</p>
<p>Input layer: [rk , pk , mk]
</p>
<p>Morphological representation
</p>
<p>...
LSTMLSTM
</p>
<p>LSTMLSTMLSTM
</p>
<p>LSTMLSTMLSTM
</p>
<p>LSTMLSTMLSTM
</p>
<p>LSTMLSTM
</p>
<p>LSTMLSTM
</p>
<p>...
......
</p>
<p>Sj = tanh(Dchc(j)+ DaLj+bj) Similarity
</p>
<p>hw = [h
f,hb]
</p>
<p>hc (j)
</p>
<p>Figure 1: Model architecture
</p>
<p>entire embedding matrix as Ec ∈ Rdc×|C|, where
C is the character vocabulary extracted from the
training set including alphanumeric characters and
other possible symbols. Given an input surface
word wi with its character embeddings x1, ..., xn,
the hidden state ht at the time step t can be com-
puted via the following Vanilla LSTM calcula-
tions:
</p>
<p>it = σ(Wixt + Uiht−1 + bi) (2)
</p>
<p>ft = σ(Wfxt + Ufht−1 + bf ) (3)
ot = σ(Woxt + Uoht−1 + bo) (4)
zt = tanh(Wzxt + Uzht−1 + bz) (5)
ct = ft � ct−1 + it � zt (6)
ht = ot � tanh(ct) (7)
</p>
<p>where σ(·) and tanh(·) are the non-linear func-
tions. it, ft, ot are referred to three gates: input,
forget, output that control the information flow of
inputs. Parameters of the LSTM are W ∗, U∗, b∗,
where ∗ can be any of {i, f, o, g}. The peephole
connections were left out for clarity.
</p>
<p>We use both forward and backward LSTM to
learn word representations obtained by concatena-
tion of the last states in both direction hf and hb,
e.g. hw =
</p>
<p>[
hf , hb
</p>
<p>]
. Character-based word em-
</p>
<p>beddings obtained in this manner do not yet con-
</p>
<p>667</p>
<p />
</div>
<div class="page"><p />
<p>tain the context information on a sentence level.
Thus, we adopt another LSTM to learn context-
sensitive information for each word in both direc-
tions. We denote the concatenation of the em-
beddings learned from the forward and backward
LSTM states as hc(j) ∈ R2hs×1 (where hs is the
output size for the j-th word), and represent sur-
face context as:
</p>
<p>Sj = tanh(Dchc(j) + bj) (8)
</p>
<p>where Sj ∈ Rdh×1 is a hidden layer output,
hc(j) ∈ R2hs×1is a context vector of the j-th
word, and bj ∈ Rdh×1 is a bias term.
</p>
<p>For the final prediction, we score each analysis
by computing the inner product between its repre-
sentation and the context’s representations:
</p>
<p>P kj = Sj · Akj (9)
</p>
<p>where Sj and Akj are computed as equations (8)
and (1) respectively. We normalize the obtained
scores using softmax and choose the analysis with
the maximum score as the correct one. In what
follows we refer to this model as BiLSTM.
</p>
<p>Finally, in a separate setting, in addition to the
surface context in the hidden layer we also incor-
porate the immediate (left and right) morphologi-
cal context in the form of the average of the anal-
yses representations:
</p>
<p>S∗j = tanh(Dchc(j) + DaLj + bj) (10)
</p>
<p>where Lj ∈ R2dh×1 is concatenation of averaged
representations of the leftmost and rightmost anal-
yses, and Dc ∈ Rdh×2hs and Da ∈ Rdh×2dh are
the model parameters. This advanced variation is
referred to as BiLSTM*.
</p>
<p>2.3 Alternative Context Representation
We also experiment with an alternative context
model that uses a feed-forward NN architecture
(Collobert et al., 2011; Zheng et al., 2013). In this
model word embeddings of fixed window size are
fed to the hidden layer, and the output represents
the context. The remaining parts of the architec-
ture stay the same: we use the same morpholog-
ical representation and choose the correct analy-
sis exactly as we did for BiLSTM model. As in
the case with BiLSTM, we leverage morphologi-
cal context, by performing a Viterbi decoding con-
ditioned on the leftmost analysis. We refer to this
</p>
<p>Lang. Train Test OOV ∆AT
</p>
<p>Kazakh 16,624 2,324 43.9% 2.85
</p>
<p>Turkish 752,332 20,536 10.24% 1.76
</p>
<p>Table 1: Corpora statistics: ∆AT denotes the av-
erage number of analysis per token.
</p>
<p>model as DNN (Deep NN), an advanced variation
of which uses the averaged rightmost morpholog-
ical context as well, and is referred to as DNN*.
</p>
<p>2.4 Training
In all models, the top layer of the networks has a
softmax that computes the normalized scores over
morphological candidates given the input word.
The networks are trained to minimize the cross
entropy of the predicted and true morphological
analyses. Back-propagation is employed to com-
pute the gradient of the corresponding object func-
tion with respect to the model parameters.
</p>
<p>3 Experiments and Evaluation
</p>
<p>3.1 Data Sets
We conduct our experiments on Kazakh (Assyl-
bekov et al., 2016) and Turkish (Yuret and Türe,
2006) data sets3. Table 1 shows the corpora statis-
tics. Kazakh data set is almost 50 times smaller
than that of Turkish, with four times the OOV rate
and almost twice as many analyses per word on
average. Given such a drastic difference in the
resources it would be interesting to see how our
models perform on otherwise similar languages
(both Turkic). Lastly, while the corpora provide
train and test splits, there are no tuning sets, so we
withdraw small portions from the training sets for
tuning hyper-parameters4.
</p>
<p>3.2 Baselines
We compare our models to three other approaches.
For Kazakh we use an HMM based tagger and
its version extended with the rule-based constraint
grammar (Assylbekov et al., 2016), which is con-
sidered the state of the art for the language. We
</p>
<p>3For Turkish, we used a test set that was manually re-
annotated by Yildiz et al. (2016).
</p>
<p>4The following hyper-parameters are used in all the ex-
periments: character embedding size dc = 35, character and
context LSTM states are 50, root and POS embedding sizes
are all set to 50, hidden layer size dh = 200, learning rate
is set to 0.01. The window size of DNN is set to 5. For
regularization we use dropout (Srivastava et al., 2014) with
probability 0.5 on the hidden layers. We further constrain the
norm of gradient to be below 2 by using gradient clipping.
</p>
<p>668</p>
<p />
</div>
<div class="page"><p />
<p>Models Kazakh Turkish
tok.
acc.
</p>
<p>tok.
amb.acc.
</p>
<p>OOV
acc.
</p>
<p>OOV
amb.acc.
</p>
<p>sen.
acc.
</p>
<p>tok.
acc.
</p>
<p>tok.
amb.acc.
</p>
<p>OOV
acc.
</p>
<p>OOV
amb.acc.
</p>
<p>sen.
acc.
</p>
<p>HMM 83.82 74.98 80.90 73.89 32.41 - - - - -
MANN 85.56 77.66 81.68 74.96 32.80 91.35 82.32 86.72 74.09 40.38
Voted Perceptron 88.41 82.07 86.19 81.12 40.31 91.89 83.47 87.98 76.87 41.52
DNN 86.33 78.86 84.13 78.31 40.71 92.24 84.14 87.05 74.74 41.16
DNN* 87.25 80.28 85.99 80.85 40.31 92.22 84.12 87.24 75.11 40.38
DNN*‡ 88.26 81.85 86.77 81.92 39.52 92.32 84.32 87.95 76.50 41.55
BiLSTM 87.49 80.65 85.21 79.78 39.52 91.37 82.39 86.91 74.46 38.13
BiLSTM* 90.92 85.95 88.73 84.60 50.19 92.03 83.73 88.01 76.60 40.54
BiLSTM*‡ 91.06 86.40 88.93 85.27 50.98 92.16 84.01 88.24 77.06 41.01
HMMCG 90.39 85.88 88.83 86.07 53.75 - - - - -
BiLSTM*‡+CG 91.74 87.45 90.00 86.74 54.54 - - - - -
</p>
<p>Table 2: Results: here, tok. acc. and tok. amb. acc. denote the accuracy over all and ambiguous tokens
respectively. Same goes for OOV acc. and OOV amb. acc.. Sentence accuracy is denoted as sen. acc..
</p>
<p>refer to these baselines as HMM and HMMCG.
Another baseline is a voted perceptron (Collins,
2002) based tagger. We use our implementation
of this baseline for Kazakh and the model devel-
oped by Sak et al. (2007) for Turkish. Lastly, we
use a neural network model proposed by Yildiz
et al. (2016), which is considered state of the art
for Turkish. For this baseline too we use our own
implementation (for both languages) and refer to
it as MANN5.
</p>
<p>3.3 Experimental Setup
</p>
<p>As described in the previous section, each of our
models has two settings: the one that does not in-
corporate surrounding morphological context and
the one that does (the starred one). In addition to
that we use pre-trained embeddings, by training
word2vec (Mikolov et al., 2013) skip-gram model
on Wikipedia texts. This setting is denoted by a
double dagger (‡).
</p>
<p>We perform a single run evaluation in terms of
token- and sentence- based accuracy. We consider
four types of tokens: (i) all tokens; (ii) ambiguous
tokens (the ones with at least two analyses); (iii)
OOV tokens; (iv) ambiguous OOV tokens. Thus,
we use a total of five metrics. In terms of strictness
we deem correct only the predictions that match
the golden truth completely, i.e. in root, POS and
MC (up to a single morpheme tag).
</p>
<p>5Note that all of the baselines are language dependent
to a certain degree, with MANN being the least dependent
and HMMCG the most. The latter baseline employs hand-
engineered constraint grammar rules to perform initial disam-
biguation, followed by application of the HMM tagger, which
cherry-picks the most informative grammatical features.
</p>
<p>3.4 Results and Discussion
</p>
<p>The results are given in Table 2. Unless stated oth-
erwise we refer to the general (all tokens) accuracy
when comparing model performances.
</p>
<p>For Kazakh, DNN conditioned on the leftmost
analysis yields 86.33% accuracy. DNN* that in
addition uses the rightmost analysis embeddings,
improves almost 1% over that result (87.25%).
On the other hand BiLSTM, whose context repre-
sentation uses surface forms only, performs even
better (87.49%). When this model incorporates
immediate morphological context, it (BiLSTM*)
performs at 90.92% and beats the HMMCG base-
line. However, the latter being a very strong lan-
guage dependent baseline still outperforms our
model in ambiguous OOV and sentence accuracy.
When we evaluate our model under equal condi-
tions (BiLSTM*‡+CG) it beats HMMCG on all of
the metrics. We separate this comparison from the
rest because of a language-dependent set up.
</p>
<p>In contrast, for Turkish DNN models outper-
form BiLSTM on seen tokens and yield an al-
most equal 92.2% accuracy regardless of using
the rightmost morphological context. This perfor-
mance is also higher than that of all baselines, in-
cluding the state of the art MANN. However BiL-
STM* is still better than DNN* in OOV token ac-
curacy, both overall and ambiguous.
</p>
<p>As it can be seen, pre-training boosts the per-
formance of DNN* and BiLSTM* across all met-
rics. For Kazakh pre-training results in .14%
improvement in general token accuracy for BiL-
STM*, which amounts to .67% improvement over
the state of the art. For Turkish this results in an
</p>
<p>669</p>
<p />
</div>
<div class="page"><p />
<p>almost 1% net improvement in overall token accu-
racy over MANN, the state of the art6.
</p>
<p>A cross-linguistic comparison reveals that al-
though Kazakh data set is much smaller than that
of Turkish and has more analyses per word on
average and higher OOV rate, on certain met-
rics the models perform on par or even better for
Kazakh7. To investigate this further we have made
data sets comparable in size by randomly choos-
ing 20.6K+ and 3.4K from Turkish training and
test sets. On this data BiLSTM*‡ yields 91.18,
82.0% general and ambiguous token accuracy and
respective scores for OOV are 87.0, 74.6%. This
result follows the pattern, where for Turkish only
the general accuracy is higher than that of Kazakh.
It turns out that Turkish data contains many unam-
biguous tokens: 49% and 48% for full and small
data sets (train + test average), against 36% for
Kazakh. This suggests that the higher general ac-
curacy on Turkish data can be explained by the
higher rate of the unambiguous tokens. Also Turk-
ish has a more complex derivational morphology,
which “lengthens” the analyses, e.g. an average
number of morphemes per analysis is higher for
Turkish (5.25) than for Kazakh (4.6). This adds
sparseness to the morpheme chains and certainly
further complicates disambiguation, especially in
an OOV scenario.
</p>
<p>We also observe that BiLSTM*‡ works best on
all metrics for Kazakh, but for Turkish it beats
DNN*‡ only on the OOV part. Due to BiLSTM*‡
being computationally prohibitive we ran it with
significantly less number of epochs than DNN,
and it also being a character-based model, we
speculate that it was able to learn character aware
context embeddings hence better at OOV.
</p>
<p>4 Related Work
</p>
<p>A morphology-aware NN (MANN) for MD was
proposed by Yildiz et al. (2016), and has been
reported to achieve ambiguous token accuracies
of 84.12, 88.35 and 93.78% for Turkish, Finish
and Hungarian respectively. This approach dif-
fers from ours in a number of ways. (i) Our
</p>
<p>6For the un-pretrained model original work reports
84.12% accuracy on ambiguous tokens (Yildiz et al., 2016),
which is lower than 84.14% that un-pretrained DNN achieves
on this metric.
</p>
<p>7 For instance, BiLSTM*‡ applied to Kazakh performs
better than any other model for Turkish in terms of sentence,
ambiguous and OOV token accuracy. Moreover all of the
models (including the baselines) perform better on Kazakh in
terms of ambiguous OOV accuracy.
</p>
<p>analysis representation treats morpheme tags in a
language-independent manner considering every
tag found in the training set, whereas in MANN
certain tags are chosen with a specific language
in mind. (ii) MANN is a feed-forward NN that,
unlike our approach, does not account for the sur-
face context. (iii) As we understood, at the de-
coding step MANN makes use of the golden truth,
whereas our models have no need for that.
</p>
<p>Although several statistical models have
been proposed for Kazakh MD, such as
HMM- (Makazhanov et al., 2014; Makham-
betov et al., 2015; Assylbekov et al., 2016),
voted perceptron- (Tolegen et al., 2016) and
transformation-based (Kessikbayeva and Cicekli,
2016) taggers, to our knowledge ours is the first
deep learning-based approach to the problem that
is also purely language independent.
</p>
<p>It is becoming increasingly popular to use richer
architectures to learn better embeddings from
characters/words (Yessenbayev and Makazhanov,
2016; Ling et al., 2015; Wieting et al., 2016). Ling
et al. (2015) used a BiLSTM to learn word vectors,
showing strong performance on language model-
ing and POS tagging. Melamud et al. (2016)
proposed context2vec, a BiLSTM based model
to learn context embedding of target words and
achieved state-of-the-art results on sentence com-
pletion and word sense disambiguation.
</p>
<p>5 Conclusion
</p>
<p>We have proposed a general MD framework for
MCL that can be analyzed in &lt;root, POS, MC&gt;
triplets. We have showed that the surface context
can be useful to MD, especially if combined with
morphological context. Our next step would be to
assess our claims on a larger number of typologi-
cally distant languages.
</p>
<p>Acknowledgments
</p>
<p>This work has been conducted under the tar-
geted program O.0743 (0115PK02473) of the
Committee of Science of the Ministry of Educa-
tion and Science of the Republic of Kazakhstan,
and the research grant 129-2017/022-2017 of the
Nazarbayev University.
</p>
<p>The authors would like to thank Xiaoqing
Zheng for tremendously helpful discussions, as
well as Eray Yildiz and Zhenisbek Assylbekov for
the data sets used in this study and prompt replies
to all questions regarding those.
</p>
<p>670</p>
<p />
</div>
<div class="page"><p />
<p>References
</p>
<p>Zhenisbek Assylbekov, Jonathan Washington, Fran-
cis Tyers, Assulan Nurkas, Aida Sundetova, Aidana
Karibayeva, Balzhan Abduali, and Dina Amirova.
2016. A free/open-source hybrid morphological dis-
ambiguation tool for Kazakh. In TurCLing 2016.
pages 18–26.
</p>
<p>Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10. Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, EMNLP ’02, pages 1–8.
</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. J. Mach. Learn. Res. 12:2493–2537.
</p>
<p>Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnı́k,
Bas R. Steunebrink, and Jürgen Schmidhuber.
2015. LSTM: A Search Space Odyssey. CoRR
abs/1503.04069.
</p>
<p>Dilek Z. Hakkani-Tür, Kemal Oflazer, and Gökhan Tür.
2002. Statistical Morphological Disambiguation for
Agglutinative Languages. Computers and the Hu-
manities 36(4):381–410.
</p>
<p>Gulshat Kessikbayeva and Ilyas Cicekli. 2016. A Rule
Based Morphological Analyzer and a Morphologi-
cal Disambiguator for Kazakh Language. Linguis-
tics and Literature Studies 4(4):96–104.
</p>
<p>Wang Ling, Chris Dyer, Alan W. Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Lus Marujo,
and Tiago Lus. 2015. Finding Function in Form:
Compositional Character Models for Open Vocabu-
lary Word Representation. In EMNLP. The Asso-
ciation for Computational Linguistics, pages 1520–
1530.
</p>
<p>Aibek Makazhanov, Zhandos Yessenbayev, Islam
Sabyrgaliyev, Anuar Sharafudinov, and Olzhas
Makhambetov. 2014. On certain aspects of Kazakh
part-of-speech tagging. In Application of Informa-
tion and Communication Technologies (AICT), 2014
IEEE 8th International Conference on. pages 1–4.
</p>
<p>Olzhas Makhambetov, Aibek Makazhanov, Islam
Sabyrgaliyev, and Zhandos Yessenbayev. 2015.
Data-Driven Morphological Analysis and Disam-
biguation for Kazakh. In Computational Linguistics
and Intelligent Text Processing - 16th International
Conference, CICLing 2015, Cairo, Egypt, April 14-
20, 2015, Proceedings Part I. pages 151–163.
</p>
<p>Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning Generic Context Em-
bedding with Bidirectional LSTM. In CoNLL.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, Curran Associates, Inc., pages 3111–3119.
</p>
<p>Haşim Sak, Tunga Güngör, and Murat Saraçlar. 2007.
Morphological Disambiguation of Turkish Text with
Perceptron Algorithm. In Proceedings of CICLing
2007. volume LNCS 4394, pages 107–118.
</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A Simple Way to Prevent Neural Networks
from Overfitting. Journal of Machine Learning Re-
search 15:1929–1958.
</p>
<p>Gulmira Tolegen, Alymzhan Toleu, and Zheng Xiao-
qing. 2016. Named Entity Recognition for Kazakh
Using Conditional Random Fields. In Proceedings
of the 4-th International Conference on Computer
Processing of Turkic Languages TurkLang 2016.
Izvestija KGTU im.I.Razzakova. TurkLang 2016,
pages 122–129.
</p>
<p>John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding Words and
Sentences via Character n-grams. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1504–1515.
</p>
<p>Zhandos Yessenbayev and Aibek Makazhanov. 2016.
Character-based Feature Extraction with LSTM
Networks for POS-tagging Task. In Application
of Information and Communication Technologies
(AICT), 2016 IEEE 10th International Conference
on. pages 62–66.
</p>
<p>Eray Yildiz, Caglar Tirkaz, H. Bahadir Sahin,
Mustafa Tolga Eren, and Omer Ozan Sonmez. 2016.
A Morphology-Aware Network for Morphological
Disambiguation. In Proceedings of AAAI. AAAI
Press, pages 2863–2869.
</p>
<p>Deniz Yuret and Ferhan Türe. 2006. Learning Morpho-
logical Disambiguation Rules for Turkish. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, HLT-NAACL ’06,
pages 328–334.
</p>
<p>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for Chinese word segmentation and
POS tagging. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 647–657.
</p>
<p>671</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 672–678
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2106
</p>
<p>Character Composition Model with Convolutional Neural Networks for
Dependency Parsing on Morphologically Rich Languages
</p>
<p>Xiang Yu and Ngoc Thang Vu
Institut für Maschinelle Sprachverarbeitung
</p>
<p>Universität Stuttgart
{xiangyu,thangvu}@ims.uni-stuttgart.de
</p>
<p>Abstract
We present a transition-based dependency
parser that uses a convolutional neural
network to compose word representations
from characters. The character compo-
sition model shows great improvement
over the word-lookup model, especially
for parsing agglutinative languages. These
improvements are even better than using
pre-trained word embeddings from extra
data. On the SPMRL data sets, our sys-
tem outperforms the previous best greedy
parser (Ballesteros et al., 2015) by a mar-
gin of 3% on average.1
</p>
<p>1 Introduction
As with many other NLP tasks, dependency pars-
ing also suffers from the out-of-vocabulary (OOV)
problem, and probably more than others since
training data with syntactical annotation is usually
scarce. This problem is particularly severe when
the target is a morphologically rich language. For
example, in the SPMRL shared task data sets (Sed-
dah et al., 2013, 2014), 4 out of 9 treebanks contain
more than 40% word types in the development set
that are never seen in the training set.
One way to tackle the OOV problem is to pre-
</p>
<p>train the word embeddings, e.g., with word2vec
(Mikolov et al., 2013), from a large set of unlabeled
data. This comes with two main advantages: (1)
more word types, which means that the vocabulary
is extended by the unlabeled data, so that some of
the OOVwords now have a learned representation;
(2) more word tokens per type, which means that
the syntactic and semantic similarities of the words
are better modeled than only using the parser train-
ing data.
</p>
<p>1The parser is available at http://www.ims.
uni-stuttgart.de/institut/mitarbeiter/
xiangyu/index.en.html
</p>
<p>Pre-trained word embeddings can alleviate the
OOV problem by expanding the vocabulary, but
it does not model the morphological information.
Instead of looking up word embeddings, many
researchers propose to compose the word repre-
sentation from characters for various tasks, e.g.,
part-of-speech tagging (dos Santos and Zadrozny,
2014; Plank et al., 2016), named entity recogni-
tion (dos Santos and Guimarães, 2015), language
modeling (Ling et al., 2015), machine translation
(Costa-jussà and Fonollosa, 2016). In particular,
Ballesteros et al. (2015) use a bidirectional long
short-term memory (LSTM) character model for
dependency parsing. Kim et al. (2016) present
a convolutional neural network (CNN) character
model for language modeling, but make no com-
parison among the character models, and state that
“it remains open as to which character composition
model (i.e., LSTM or CNN) performs better”.
We propose to apply the CNN model by Kim
</p>
<p>et al. (2016) in a greedy transition-based depen-
dency parser with feed-forward neural networks
(Chen and Manning, 2014; Weiss et al., 2015).
This model requires no extra unlabeled data but
performs better than using pre-trained word em-
beddings. Furthermore, it can be combined with
word embeddings from the lookup table since they
capture different aspects of word similarities.
Experimental results show that the CNN model
</p>
<p>works especially well on agglutinative languages,
where the OOV rates are high. On other morpho-
logically rich languages, the CNN model also per-
forms at least as good as the word-lookup model.
Furthermore, our CNN model outperforms both
</p>
<p>the original and our re-implementation of the bidi-
rectional LSTM model by Ballesteros et al. (2015)
by a large margin. It provides empirical evidence
to the aforementioned open question, suggesting
that the CNN is the better character composition
model for dependency parsing.
</p>
<p>672</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2106">https://doi.org/10.18653/v1/P17-2106</a></div>
</div>
<div class="page"><p />
<p>2 Parsing Models
2.1 Baseline Parsing Model
As the baseline parsing model, we re-implement
the greedy parser in Weiss et al. (2015) with some
modifications, which brings about 0.5% improve-
ment, outlined below.2
Since most treebanks contain non-projective
</p>
<p>trees, we use an approximate non-projective tran-
sition system similar to Attardi (2006). It has two
additional transitions (LEFT-2 and RIGHT-2) to the
Arc-Standard system (Nivre, 2004) that attach the
top of the stack to the third token on the stack, or
vice versa. We also extend the feature templates in
Weiss et al. (2015) by extracting the children of the
third token in the stack. The complete transitions
and feature templates are listed in Appendix A.
The feature templates consist of 24 tokens in the
</p>
<p>configuration, we look up the embeddings of the
word forms, POS tags and dependency labels of
each token.3 We then concatenate the embeddings
Eword(ti), Etag(ti), Elabel(ti) for each token ti, and
use a dense layer with ReLU non-linearity to ob-
tain a compact representation f (ti) of the token:
</p>
<p>x(ti) = [Eword(ti);Etag(ti);Elabel(ti)] (1)
f (ti) = max{0,Wfx(ti) + bf}
</p>
<p>We concatenate the representations of the to-
kens and feed them through two hidden layers with
ReLU non-linearity, and finally into the softmax
layer to compute the probability of each transition:
</p>
<p>h0 = [f (t1); f (t2); ...; f (t24)]
h1 = max{0,W1h0 + b1}
h2 = max{0,W2h1 + b2}
</p>
<p>p(⋅|t1, ..., t24) = sof tmax(W3h2 + b3)
Eword , Etag, Elabel, Wf , W1, W2, W3, bf , b1,
</p>
<p>b2, b3 are all the parameters that will be learned
through back propagation with averaged stochastic
gradient descent in mini-batches.
Note that Weiss et al. (2015) directly concate-
</p>
<p>nate the embeddings of the words, tags, and labels
of all the tokens together as input to the hidden
layer. Instead, we first group the embeddings of
the word, tag, and label of each token and compute
</p>
<p>2We only experiment with the greedy parser, since this pa-
per focuses on the character-level input features and is inde-
pendent of the global training and inference as in Weiss et al.
(2015); Andor et al. (2016).
</p>
<p>3The tokens in the stack and buffer do not have labels yet,
we use a special label &lt;NOLABEL&gt; instead.
</p>
<p>an intermediate representation with shared param-
eters, then concatenate all the representations as in-
put to the hidden layer.
2.2 LSTM Character Composition Model
To tackle the OOV problem, wewant to replace the
word-lookup table with a function that composes
the word representation from characters.
As a baseline character model, we re-implement
</p>
<p>the bidirectional LSTM character composition
model following Ballesteros et al. (2015). We re-
place the lookup table Eword in the baseline parser
with the final outputs of the forward and backward
LSTMs ⃖⃖⃖⃖⃖⃖⃖⃖lstm and ⃖⃖⃖⃖⃖⃖⃖⃗lstm. Equation (1) is then re-
placed with
</p>
<p>x(ti) = [⃖⃖⃖⃖⃖⃖⃖⃖lstm(ti); ⃖⃖⃖⃖⃖⃖⃖⃗lstm(ti);Etag(ti);Elabel(ti)].
</p>
<p>We refer the readers to Ling et al. (2015) for the
details of the bidirectional LSTM.
2.3 CNN Character Composition Model
In contrast to the LSTM model, we propose to use
a “flat” CNN as the character composition model,
similar to Kim et al. (2016).4
Equation (1) is thus replaced with
</p>
<p>x(ti) =[cnnl1(ti); cnnl2(ti); ...; cnnlk(ti);
Etag(ti);Elabel(ti)]. (2)
</p>
<p>Concretely, the input of the model is a concate-
nated matrix of character embeddings C ∈ ℝdi×w,
where di is the dimensionality of character em-
beddings (number of input channels) and w is the
length of the padded word.5 We apply k convo-
lutional kernels  ∈ ℝdo×di×lk with ReLU non-
linearity on the input, where do is the number of
output channels and lk is the length of the ker-
nel. The output of the convolution operation is
Oconv ∈ ℝdo×(l−k+1), and we apply a max-over-
time pooling that takes the maximum activations
of the kernel along each channel, obtaining the fi-
nal output Ofinal ∈ ℝdo , which corresponds to
the most salient n-gram representation of the word,
denoted cnnlk in Equation (2). We then concate-
nate the outputs of several such CNNs with differ-
ent lengths, so that the information from different
n-grams are extracted and can interact with each
other.
</p>
<p>4We do not use the highway networks since it did not im-
prove the performance in preliminary experiments.
</p>
<p>5The details of the padding is in Appendix B.
</p>
<p>673</p>
<p />
</div>
<div class="page"><p />
<p>Model Ara Baq Fre Ger Heb Hun Kor Pol Swe Avg
</p>
<p>Int
</p>
<p>WORD 84.50 77.87 82.20 85.35 74.68 76.17 84.62 80.71 79.14 80.58
W2V 85.11 79.07 82.73 86.60 74.55 78.21 85.30 82.37 79.67 81.51
</p>
<p>LSTM 83.42 82.97 81.35 85.34 74.03 83.06 86.56 80.13 77.44 81.48
CNN 84.65 83.91 82.41 85.61 74.23 83.68 86.99 83.28 80.00 82.75
</p>
<p>LSTM+WORD 84.75 83.43 82.25 85.56 74.62 83.43 86.85 82.30 79.85 82.56
CNN+WORD 84.58 84.22 81.79 85.85 74.79 83.51 87.21 83.66 80.52 82.90
LSTM+W2V 85.35 83.94 83.04 86.38 75.15 83.30 87.35 83.00 79.38 82.99
CNN+W2V 85.67 84.37 83.09 86.81 74.95 84.08 87.72 84.44 80.35 83.50
</p>
<p>Ext
B15-WORD 83.46 73.56 82.03 84.62 72.70 69.31 83.37 79.83 76.40 78.36
B15-LSTM 83.40 78.61 81.08 84.49 72.26 76.34 86.21 78.24 74.47 79.46
BestPub 86.21 85.70 85.66 89.65 81.65 86.13 87.27 87.07 82.75 85.79
</p>
<p>Table 1: LAS on the test sets, the best LAS in each group is marked in bold face.
</p>
<p>3 Experiments
3.1 Experimental Setup
We conduct our experiments on the treebanks from
the SPMRL 2014 shared task (Seddah et al., 2013,
2014), which includes 9 morphologically rich lan-
guages: Arabic, Basque, French, German, He-
brew, Hungarian, Korean, Polish, and Swedish.
All the treebanks are split into training, develop-
ment, and test sets by the shared task organizers.
We use the fine-grained predicted POS tags pro-
vided by the organizers, and evaluate the labeled
attachment scores (LAS) including punctuation.
We experiment with the CNN-based character
</p>
<p>composition model (CNN) along with several base-
lines. The first baseline (WORD) uses the word-
lookup model described in Section 2.1 with ran-
domly initialized word embeddings. The sec-
ond baseline (W2V) uses pre-trained word embed-
dings by word2vec (Mikolov et al., 2013) with
the CBOW model and default parameters on the
unlabeled texts from the shared task organizers.
The third baseline (LSTM) uses a bidirectional
LSTM as the character composition model follow-
ing Ballesteros et al. (2015). Appendix C lists the
hyper-parameters of all the models.
</p>
<p>Further analysis suggests that combining the
character composition models with word-lookup
models could be beneficial since they capture dif-
ferent aspects of word similarities (orthographic
vs. syntactic/semantic). We therefore experiment
with four combined models in two groups: (1) ran-
domly initialized word embeddings (LSTM+WORD
vs. CNN+WORD), and (2) pre-trained word embed-
dings (LSTM+W2V vs. CNN+W2V).
The experimental results are shown in Table 1,
</p>
<p>with Int denoting internal comparisons (with
three groups) and Ext denoting external compar-
isons, the highest LAS in each group is marked in
bold face.
</p>
<p>3.2 Internal Comparisons
</p>
<p>In the first group, we compare the LAS of the
four single models WORD, W2V, LSTM, and CNN.
In macro average of all languages, the CNN model
performs 2.17% higher than the WORD model, and
1.24% higher than the W2V model. The LSTM
model, however, performs only 0.9% higher than
the WORD model and 1.27% lower than the CNN
model.
The CNN model shows large improvement in
</p>
<p>four languages: three agglutinative languages
(Basque, Hungarian, Korean), and one highly in-
flected fusional language (Polish). They all have
high OOV rate, thus difficult for the baseline parser
that does not model morphological information.
Also, morphemes in agglutinative languages tend
to have unique, unambiguous meanings, thus eas-
ier for the convolutional kernels to capture.
In the second group, we observe that the addi-
</p>
<p>tional word-lookup model does not significantly
improve the CNN moodel (from 82.75% in CNN
to 82.90% in CNN+WORD on average) while the
LSTMmodel is improved by a much larger margin
(from 81.48% in LSTM to 82.56% in LSTM+WORD
on average). This suggests that the CNN model
has already learned the most important informa-
tion from the the word forms, while the LSTM
model has not. Also, the combined CNN+WORD
model is still better than the LSTM+WORD model,
despite the large improvement in the latter.
In the third group where pre-trained word em-
</p>
<p>beddings are used, combining CNN with W2V
brings an extra 0.75% LAS over the CNN model.
Combining LSTM with W2V shows similar trend
but with much larger improvement, yet again,
CNN+W2V outperforms LSTM+W2V both on aver-
age and individually in 8 out of 9 languages.
</p>
<p>674</p>
<p />
</div>
<div class="page"><p />
<p>Model Case Ara Baq Fre Ger Heb Hun Kor Pol Swe Avg
CNN
</p>
<p>ΔIV 0.12 2.72 -0.44 0.13 -0.35 1.48 1.30 0.98 1.39 0.81
ΔOOV 0.03 5.78 0.33 0.10 -1.04 5.04 2.17 2.34 0.95 1.74
</p>
<p>LSTM
ΔIV -0.58 1.98 -0.55 -0.08 -1.23 1.62 1.12 -0.49 0.21 0.22
ΔOOV -0.32 5.09 0.12 -0.21 -1.99 4.74 1.51 0.10 0.38 1.05
</p>
<p>Table 2: LAS improvements by CNN and LSTM in the IV and OOV cases on the development sets.
Mod Ara Baq Fre Ger Heb Hun Kor Pol Swe Avg
♣bc -1.23 -1.94 -1.35 -1.57 -0.79 -3.23 -1.22 -2.53 -1.54 -1.71
a♣c -3.47 -3.96 -2.39 -2.54 -1.24 -4.52 -3.21 -4.47 -4.19 -3.33
ab♣ -1.52 -15.31 -0.72 -1.23 -0.26 -13.97 -10.22 -3.52 -2.61 -5.48
a♣♣ -3.73 -19.29 -3.30 -3.49 -1.21 -17.89 -12.95 -6.22 -6.01 -8.23
♣b♣ -3.02 -18.06 -2.60 -3.54 -1.42 -18.43 -11.69 -6.22 -3.85 -7.65
♣♣c -5.11 -7.90 -4.05 -4.86 -2.50 -9.75 -4.56 -6.71 -6.74 -5.80
</p>
<p>Table 3: Degradation of LAS of the CNN model on the masked development sets.
</p>
<p>3.3 External Comparisons
We also report the results of the two mod-
els from Ballesteros et al. (2015): B15-WORD
with randomly initialized word embeddings and
B15-LSTM as their proposed model. Finally, we
report the best published results (BestPub) on
this data set (Björkelund et al., 2013, 2014).
On average, the B15-LSTM model improves
</p>
<p>their own baseline by 1.1%, similar to the 0.9%
improvement of our LSTM model, which is much
smaller than the 2.17% improvement of the CNN
model. Furthermore, the CNN model is improved
from a strong baseline: our WORD model performs
already 2.22% higher than the B15-WORD model.
</p>
<p>Comparing the individual performances on each
language, we observe that the CNN model almost
always outperforms the WORD model except for
Hebrew. However, both LSTM and B15-LSTM
perform higher than baseline only on the three
agglutinative languages (Basque, Hungarian, and
Korean), and lower than baseline on the other six.
Ballesteros et al. (2015) do not compare the ef-
</p>
<p>fect of adding a word-lookup model to the LSTM
model as in our second group of internal com-
parisons. However, Plank et al. (2016) show that
combining the same LSTM character composition
model with word-lookup model improves the per-
formance of POS tagging by a very large mar-
gin. This partially confirms our hypothesis that the
LSTMmodel does not learn sufficient information
from the word forms.
Considering both internal and external compar-
</p>
<p>isons in both average and individual performances,
we argue that CNN is more suitable than LSTM as
character composition model for parsing.
While comparing to the best published results
</p>
<p>(Björkelund et al., 2013, 2014), we have to note
that their approach uses explicit morphological
features, ensemble, ranking, etc., which all can
boost parsing performance. We only use a greedy
parser with much fewer features, but bridge the 6
points gap between the previous best greedy parser
and the best published result bymore than one half.
3.4 Discussion on CNN and LSTM
We conjecture that the main reason for the bet-
ter performance of CNN over LSTM is its flex-
ibility in processing sub-word information. The
CNN model uses different kernels to capture n-
grams of different lengths. In our setting, a kernel
with a minimum length of 3 can capture short mor-
phemes; and with a maximum length of 9, it can
practically capture a normal word. With the flexi-
bility of capturing patterns from morphemes up to
words, the CNNmodel almost always outperforms
the word-lookup model.
In theory, LSTM has the ability to model much
</p>
<p>longer sequences, however, it is composed step by
step with recurrence. For such deep network ar-
chitectures, more data would be required to learn
the same sequence, in comparison to CNN which
can directly use a large kernel to match the pat-
tern. For dependency parsing, training data is usu-
ally scarce, this could be the reason that the LSTM
has not utilized its full potential.
3.5 Analyses on OOV and Morphology
The motivation for using character composition
models is based on the hypothesis that it can ad-
dress the OOV problem. To verify the hypothesis,
we analyze the LAS improvements of the CNN and
LSTMmodel on the development sets in two cases:
(1) both the head and the dependent are in vocabu-
</p>
<p>675</p>
<p />
</div>
<div class="page"><p />
<p>lary or (2) at least one of them is out of vocabulary.
Table 2 shows the results, where the two cases are
denoted as ΔIV and ΔOOV. The general trend in
the results is that the improvements of both mod-
els in the OOV case are larger than in the IV case,
which means that the character composition mod-
els indeed alleviates the OOV problem. Also, CNN
improves on seven languages in the IV case and
eight languages in the OOV case, and it performs
consistently better than LSTM in both cases.
To analyze the informativeness of the mor-
</p>
<p>phemes at different positions, we conduct an ab-
lation experiment. We split each word equally
into three thirds, approximating the prefix, stem,
and suffix. Based on that, we construct six modi-
fied versions of the development sets, in which we
mask one or two third(s) of the characters in each
word. Then we parse them with the CNN models
trained on normal data. Table 3 shows the degra-
dations of LAS on the six modified data sets com-
pared to parsing the original data, where the posi-
tion of ♣ signifies the location of the masks. The
three agglutinative languages Basque, Hungarian,
and Korean suffer the most with masked words.
In particular, the suffixes are the most informa-
tive for parsing in these three languages, since they
cause the most loss while masked, and the least
loss while unmasked. The pattern is quite differ-
ent on the other languages, in which the distinction
of informativeness among the three parts is much
smaller.
4 Conclusion
In this paper, we propose to use a CNN to compose
word representations from characters for depen-
dency parsing. Experiments show that the CNN
model consistently improves the parsing accuracy,
especially for agglutinative languages. In an exter-
nal comparison on the SPMRL data sets, our sys-
tem outperforms the previous best greedy parser.
We also provide empirical evidence and analy-
</p>
<p>sis, showing that the CNN model indeed alleviates
the OOV problem and that it is better suited than
the LSTM in dependency parsing.
Acknowledgements
This work was supported by the German Research
Foundation (DFG) in project D8 of SFB 732. We
also thank our collegues in the IMS, especially An-
ders Björkelund, for valuable discussions, and the
anonymous reviewers for the suggestions.
</p>
<p>References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei
</p>
<p>Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics,
pages 2442–2452. https://doi.org/10.18653/v1/P16-
1231.
</p>
<p>Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL-X). Associa-
tion for Computational Linguistics, pages 166–170.
http://aclweb.org/anthology/W06-2922.
</p>
<p>Miguel Ballesteros, Chris Dyer, and A. Noah Smith.
2015. Improved transition-based parsing by mod-
eling characters instead of words with lstms. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 349–359.
https://doi.org/10.18653/v1/D15-1041.
</p>
<p>Anders Björkelund, Özlem Çetinoğlu, Agnieszka
Faleńska, Richárd Farkas, Thomas Müller, Wolf-
gang Seeker, and Zsolt Szántó. 2014. The ims-
wrocław-szeged-cis entry at the spmrl 2014 shared
task: Reranking and morphosyntax meet unlabeled
data. Notes of the SPMRL .
</p>
<p>Anders Björkelund, Özlem Çetinoğlu, Richárd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the spmrl 2013 shared task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages. Associa-
tion for Computational Linguistics, pages 135–145.
http://aclweb.org/anthology/W13-4916.
</p>
<p>Danqi Chen and Christopher Manning. 2014. A
fast and accurate dependency parser using neu-
ral networks. In Proceedings of the 2014
Conference on Empirical Methods in Natural
Language Processing (EMNLP). Association
for Computational Linguistics, pages 740–750.
https://doi.org/10.3115/v1/D14-1082.
</p>
<p>R. Marta Costa-jussà and R. José A. Fonollosa. 2016.
Character-based neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers). Association for Computational Linguistics,
pages 357–361. https://doi.org/10.18653/v1/P16-
2058.
</p>
<p>Cicero dos Santos and Victor Guimarães. 2015.
Boosting named entity recognition with neu-
ral character embeddings. In Proceedings of
the Fifth Named Entity Workshop. Association
for Computational Linguistics, pages 25–33.
https://doi.org/10.18653/v1/W15-3904.
</p>
<p>676</p>
<p />
</div>
<div class="page"><p />
<p>Cicero dos Santos and Bianca Zadrozny. 2014. Learn-
ing character-level representations for part-of-speech
tagging. In Proceedings of the 31st International
Conference on Machine Learning (ICML-14). pages
1818–1826.
</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.
In Proceedings of the IEEE international conference
on computer vision. pages 1026–1034.
</p>
<p>Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
derM. Rush. 2016. Character-aware neural language
models. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, February 12-17,
2016, Phoenix, Arizona, USA.. AAAI Press, pages
2741–2749.
</p>
<p>Wang Ling, Chris Dyer, W. Alan Black, Isabel
Trancoso, Ramon Fermandez, Silvio Amir, Luis
Marujo, and Tiago Luis. 2015. Finding func-
tion in form: Compositional character models for
open vocabulary word representation. In Proceed-
ings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1520–1530.
https://doi.org/10.18653/v1/D15-1176.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, Curran Associates, Inc., pages 3111–3119.
</p>
<p>Joakim Nivre. 2004. Incrementality in deter-
ministic dependency parsing. In Proceed-
ings of the Workshop on Incremental Parsing:
Bringing Engineering and Cognition Together.
http://aclweb.org/anthology/W04-0308.
</p>
<p>Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 412–418.
https://doi.org/10.18653/v1/P16-2067.
</p>
<p>Djamé Seddah, Sandra Kübler, and Reut Tsarfaty.
2014. Introducing the spmrl 2014 shared task on
parsing morphologically-rich languages. In Pro-
ceedings of the First Joint Workshop on Statis-
tical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Lan-
guages. Dublin City University, pages 103–109.
http://aclweb.org/anthology/W14-6111.
</p>
<p>Djamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie
Candito, D. Jinho Choi, Richárd Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
</p>
<p>Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepiórkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin
Woliński, Alina Wróblewska, and Villemonte Eric
de la Clergerie. 2013. Overview of the spmrl 2013
shared task: A cross-framework evaluation of pars-
ing morphologically rich languages. In Proceed-
ings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages. Associa-
tion for Computational Linguistics, pages 146–182.
http://aclweb.org/anthology/W13-4917.
</p>
<p>David Weiss, Chris Alberti, Michael Collins, and
Slav Petrov. 2015. Structured training for neu-
ral network transition-based parsing. In Proceed-
ings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Associa-
tion for Computational Linguistics, pages 323–333.
https://doi.org/10.3115/v1/P15-1032.
</p>
<p>677</p>
<p />
</div>
<div class="page"><p />
<p>A Transitions and Feature Templates
</p>
<p>Transition Configurations (Before ⇒ After)
SHIFT (�, [wi|�], A)⇒ ([�|wi], �, A)
LEFT ([�|wi, wj], �, A)
</p>
<p>⇒ ([�|wj], �, A ∪ (wj → wi))
RIGHT ([�|wi, wj], �, A)
</p>
<p>⇒ ([�|wi], �, A ∪ (wi → wj))
LEFT-2 ([�|wi, wj , wk], �, A)
</p>
<p>⇒ ([�|wj , wk], �, A ∪ (wk → wi))
RIGHT-2 ([�|wi, wj , wk], �, A)
</p>
<p>⇒ ([�|wi, wj], �, A ∪ (wi → wk))
Table 4: The transition system in our experiments,
where the configuration is a tuple of (stack, buffer,
arcs).
</p>
<p>s1, s2, s3, s4, b1, b2, b3, b4,
s1.lc1, s1.lc2, s1.rc1, s1.rc2,
s2.lc1, s2.lc2, s2.rc1, s2.rc2,
s3.lc1, s3.lc2, s3.rc1, s3.rc2,
s1.lc1.lc1, s1.lc1.rc1, s1.rc1.lc1, s1.rc1.rc1
</p>
<p>Table 5: The list of tokens to extract feature tem-
plates, where si denotes the i-th token in the stack,
bi the i-th token in the buffer, lci denotes the i-th
leftmost child, rci the i-th rightmost child.
</p>
<p>B Character Input Preprocessing
For the CNN input, we use a list of characters
with fixed length to for batch processing. We add
some special symbols apart from the normal al-
phabets, digits, and punctuations: &lt;SOW&gt; as the
start of the word, &lt;EOW&gt; as the end of the word,
&lt;MUL&gt; as multiple characters in the middle of
the word squeezed into one symbol, &lt;PAD&gt; as
padding equally on both sides, and &lt;UNK&gt; as char-
acters unseen in the training data.
For example, if we limit the input length to 9,
</p>
<p>a short word ein will be converted into &lt;PAD&gt;-
&lt;PAD&gt;-&lt;SOW&gt;-e-i-n-&lt;EOW&gt;-&lt;PAD&gt;-&lt;PAD&gt;;
a long word prächtiger will be &lt;SOW&gt;-p-r-ä-
&lt;MUL&gt;-g-e-r-&lt;EOW&gt;. In practice, we set the
length as 32, which is long enough for almost all
the words.
C Hyper-Parameters
The common hyper-parameters of all the models
are tuned on the development set in favor of the
WORD model:
</p>
<p>• 100,000 training steps with random sampling of
mini-batches of size 100;
</p>
<p>• test on the development set every 2,000 steps;
• early stop if the LAS on the development does
not improve for 3 times in a row;
</p>
<p>• learning rate of 0.1, with exponential decay rate
of 0.95 for every 2,000 steps;
</p>
<p>• L2-regularization rate of 10−4;
• averaged SGD with momentum of 0.9;
• parameters are initialized following He et al.
(2015);
</p>
<p>• dimensionality of the embeddings of each word,
tag, and label are 256, 32, 32, respectively;
</p>
<p>• dimensionality of the hidden layers are 512, 256;
• dropout on both hidden layers with rate of 0.1;
• total norm constraint of the gradients is 10.
The hyper-parameters for the CNN model are:
</p>
<p>• dimensionality of the character embedding is
32;
</p>
<p>• 4 convolutional kernels of lengths 3, 5, 7, 9;
• number of output channels of each kernel is 64;
• fixed length for the character input is 32.
The hyper-parameters for the LSTM model are:
</p>
<p>• 128 hidden units for both LSTMs;
• all the gates use orthogonal initialization;
• gradient clipping of 10;
• no L2-regularization on the parameters.
</p>
<p>678</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 679–684
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>https://doi.org/10.18653/v1/P17-2107
</p>
<p>How (not) to train a dependency parser:
The curious case of jackknifing part-of-speech taggers
</p>
<p>Željko Agić and Natalie Schluter
Department of Computer Science
</p>
<p>IT University of Copenhagen
Rued Langgaards Vej 7, 2300 Copenhagen S, Denmark
</p>
<p>zeag@itu.dk nael@itu.dk
</p>
<p>Abstract
</p>
<p>In dependency parsing, jackknifing tag-
gers is indiscriminately used as a simple
adaptation strategy. Here, we empirically
evaluate when and how (not) to use jack-
knifing in parsing. On 26 languages, we
reveal a preference that conflicts with, and
surpasses the ubiquitous ten-folding. We
show no clear benefits of tagging the train-
ing data in cross-lingual parsing.
</p>
<p>1 Introduction
</p>
<p>Dependency parsers are trained over manually an-
notated treebank data. By contrast, when applied
in the real world, they parse over sequences of pre-
dicted parts of speech. As POS tagging accuracy
drops due to domain change, the parsing quality
declines proportionally. Bringing these two POS
tag sources closer together thus makes for a rea-
sonable adaptation strategy.
</p>
<p>Arguably the simplest of such adaptations is n-
fold jackknifing. In it, a treebank is divided into n
equal parts, and the n-th part is POS-tagged with
a tagger trained on the remainder. The procedure
is repeated until all n parts are assigned with pre-
dicted POS tags. A parser is then trained over the
thus altered treebank, under the assumption that
its POS features will now more closely resemble
those of the input data.
</p>
<p>Jackknifing is simplistic as it i) has a very lim-
ited adaptation range for n ∈ N+, and it ii) does not
in any way take the input data into account, other
than through a vague assumption of an undefined
amount of tagging noise in the input. As such, it
exhibits very mixed results. Still, the method is
now ubiquitous in the parsing literature.
</p>
<p>In Figure 1, we survey the ACL Anthology1 for
POS jackknifing. We uncover that ∼80% of the 70
</p>
<p>1http://aclweb.org/anthology/
</p>
<p>Figure 1: Jackknifing in the ACL Anthology. Dis-
tribution of n over 70 parsing papers that use tag-
ger n-folding.
</p>
<p>parsing papers we retrieved make use of ten-fold
jackknifing. This choice spans across the various
languages and domains parsed in these papers, and
is even motivated by simply “following the tradi-
tions in literature”.2
</p>
<p>Our contributions. We evaluate jackknifing to
establish whether its use is warranted in depen-
dency parsing. Controlling for tagging quality in
training and testing, we experiment with monolin-
gual and delexicalized cross-lingual parsers over
26 languages, showing that:
</p>
<p>i) Indiscriminate use of ten-fold jackknifing re-
sults in sub-optimal parsing.
</p>
<p>ii) Tagging the training data does not yield clear
benefits in realistic cross-lingual parsing.
</p>
<p>iii) Our jackknifing extension improves parsing
through finer-grained adaptation.
</p>
<p>2 Method
</p>
<p>Jackknifing generally refers to a leave-one-out
procedure for reducing bias in parameter estima-
tion from an unbiased sample (Quenouille, 1956;
Tukey, 1958). More recently, in machine learn-
ing the term is used synonymously with “cross-
</p>
<p>2Incidentally, using ten-folds with the WSJ data yields
roughly the same train- and test-set tagging accuracy, and
seems to be where the choice originated.
</p>
<p>679</p>
<p />
<div class="annotation"><a href="https://doi.org/10.18653/v1/P17-2107">https://doi.org/10.18653/v1/P17-2107</a></div>
</div>
<div class="page"><p />
<p>validation” for estimation of predictive model per-
formance measures. In NLP, jackknifing has com-
monly been used to describe a procedure by which
the training input is adjusted to correspond more
closely to the expected test input, and it is in this
latter sense that we use the term here.
</p>
<p>In particular, in parsing research, the n-fold
jackknifing proceeds as follows. The treebank is
first partitioned into n non-overlapping subsets of
equal size. Then, iteratively, each part acts as a
test subset and is tagged using a model induced by
the remaining n−1 parts, the training subset, until
the entire treebank is tagged.
</p>
<p>We want to control for POS tagging accuracy
through the jackknifing method. To do this, we
train a tagger on increasing sized subsets of the
training set. In fold terminology, this corresponds
to dividing the training set into equal parts of size
1
n , training on
</p>
<p>n−1
n ths of the training set and testing
</p>
<p>on the remaining 1n th. However, this constrains the
size of the training subset to be larger than half the
original data, and thus concentrates our study on
models that use almost all the data, since the non-
linear curve f(n) = n−1n becomes very flat very
fast. Thus, varying fold numbers reveals very little
variation in terms of POS tagging accuracy in the
lower accuracy range.
</p>
<p>Linear extension. We now propose a simple ex-
tension of the jackknifing paradigm to study parser
accuracy given a percentage p of the training set:
linear jackknifing.
</p>
<p>Let p ∈ (0,1) be the percentage of the randomly
shuffled training set D used to induce a model to
tag some remaining number of instances. A train-
ing subset of this size allows a test subset of size at
most ⌈∣D∣ ⋅ (1 − p)⌉. Given a test subset to tag, we
can induce a model from a random subset of the
remaining examples of size approximately p ⋅ ∣D∣
to become our training subset. We randomize the
choice of examples in the training subset to avoid
introducing bias. In order to tag all of D, the min-
imum number of models we need to generate is⌈1/(1 − p)⌉. We thus separate D into test sub-
sets f1, . . . , f⌈ 1/(1−p)⌉ each of size approximately⌈∣D∣ ⋅ (1 − p)⌉. For each fi, we randomly sam-
ple a training subset of size approximately ⌈p ⋅ ∣D∣⌉
from the remainder of D, induce a model and then
tag fi. This results in the original full training set
tagged with an accuracy corresponding to the per-
formance of a randomly selected tagger trained on
approximately p ⋅ ∣D∣ of the examples.
</p>
<p>Figure 2: Tagger learning curve for 26 languages:
mean tagging accuracy with 95% confidence inter-
vals. Accuracy ranges for n-fold and linear jack-
knifing are indicated.
</p>
<p>Intrinsic evaluation. For increasing values of p,
at 5% increments, we carried out linear jackknif-
ing on 26 languages. For each p, we averaged the
performance of the induced taggers on the respec-
tive gold standards. Figure 2 illustrates the dif-
ference in informativeness of the two approaches,
where each tagging accuracy score is averaged
across the 26 languages. We see that with n-fold
jackknifing, tagging accuracy is constrained to be-
tween approximately 92% and 95%, whereas lin-
ear jackknifing explores accuracies as low as ap-
proximately 86%. Moreover, the confidence in-
tervals are consistent across the p, demonstrating
unbiased tagging models generated on less data
(lower p). We now show that these smaller lev-
els of p are essential for good parser performance
in some cases of jackknifing.
</p>
<p>3 Experiments
</p>
<p>Our experiment aims at judging the adequacy of
jackknifing in dependency parsing. First, we out-
line the experiment setup, where we conduct two
sets of experiments:
</p>
<p>i) monolingual, where lexicalized parsers are
trained on treebanks for their respective lan-
guages, and
</p>
<p>ii) cross-lingual, that features SINGLE-best and
MULTI-source delexicalized parsers.
</p>
<p>Tagging sources. By jackknifing we explore
how the mismatch between training and test POS
affects parsing. Our setup thus critically relies on
the sources of tags. We tag our test sets using:
</p>
<p>i) PRED, the monolingual taggers, and
ii) PROJ, the low-resource taggers by Agić et al.
</p>
<p>(2016), based on annotation projection.
</p>
<p>680</p>
<p />
</div>
<div class="page"><p />
<p>Monolingual parsing
</p>
<p>Train: GOLD linear jackknifing n-fold jackknifing PROJ
</p>
<p>Test: PRED PROJ PRED pmax PROJ pmax PRED nmax PROJ nmax PRED PROJ
</p>
<p>Arabic (ar) 79.4 51.4 79.5 90 64.0 5 79.5 12 55.3 2 73.5 74.4
Bulgarian (bg) 86.8 56.4 87.2 80 66.7 5 87.2 12 60.2 2 82.2 78.0
</p>
<p>Czech (cs) 81.0 58.2 81.6 85 62.5 5 81.6 8 60.1 2 77.3 70.7
Danish (da) 74.4 65.7 78.2 95 71.8 5 78.3 11 68.5 2 75.1 76.0
</p>
<p>German (de) 79.0 51.8 80.2 20 56.6 5 80.1 2 54.4 7 75.6 69.2
Greek (el) 81.1 59.4 81.4 70 64.7 5 81.4 20 61.2 2 74.7 75.6
</p>
<p>English (en) 80.9 71.6 82.3 80 77.6 5 82.3 17 76.0 2 80.2 80.9
Spanish (es) 80.7 75.4 82.1 20 78.3 5 81.8 9 77.3 4 80.2 80.2
Estonian (et) 74.3 61.6 76.0 55 67.1 5 76.1 13 65.0 2 73.3 70.5
</p>
<p>Persian (fa) 82.3 25.8 83.2 35 46.3 5 83.1 4 35.1 2 66.3 71.9
Finnish (fi) 72.0 53.9 72.9 75 60.4 5 73.0 18 56.6 2 69.2 64.7
French (fr) 80.9 65.4 81.7 90 72.7 5 81.8 19 68.7 2 79.2 77.1
</p>
<p>Hebrew (he) 80.6 54.2 81.5 75 68.1 5 81.5 7 61.7 2 77.9 75.7
</p>
<p>Hindi (hi) 89.1 51.0 91.0 70 70.1 5 91.0 11 63.1 2 84.8 85.5
Croatian (hr) 78.1 54.6 78.2 45 62.9 5 78.4 19 56.3 20 73.6 72.5
</p>
<p>Hungarian (hu) 74.3 55.4 74.9 95 63.2 5 75.2 17 58.3 2 71.0 66.8
Indonesian (id) 79.4 70.6 79.6 90 74.9 5 79.6 9 72.3 2 77.8 77.7
</p>
<p>Italian (it) 86.2 76.2 87.6 55 82.9 5 87.6 2 80.6 2 86.2 85.5
Dutch (nl) 72.1 60.2 73.8 50 67.9 5 73.8 12 65.6 8 67.6 72.6
</p>
<p>Norwegian (no) 83.7 74.8 85.3 95 79.9 5 85.3 8 78.0 2 83.7 82.6
</p>
<p>Polish (pl) 83.7 69.8 84.7 30 75.8 5 84.9 20 73.0 2 81.4 78.6
Portuguese (pt) 82.2 75.7 83.2 30 78.8 5 83.1 2 77.7 19 80.2 82.0
Romanian (ro) 81.8 66.8 82.5 85 72.9 5 82.5 5 69.0 2 79.7 79.1
</p>
<p>Slovene (sl) 82.0 62.5 84.2 55 71.4 5 84.2 16 68.9 7 79.3 77.4
Swedish (sv) 80.9 74.4 81.9 90 77.3 10 82.0 19 76.8 7 80.0 79.1
</p>
<p>Tamil (ta) 65.1 33.4 65.8 95 49.5 5 65.6 5 42.5 2 51.5 56.3
</p>
<p>Mean 79.7 60.6 80.8 67.5 68.6 5.2 80.8 11.4 64.7 4.2 76.2 75.4
Best for #/26 0 0 18 – 0 – 21 – 0 – 0 26
</p>
<p>Table 1: Parsing accuracy (UAS) in relation to the underlying sources of POS tags in training and at
runtime. Bold: best result for language, separately for PRED and PROJ test sets.
</p>
<p>We do not experiment with gold POS tags in the
test sets. Instead, we only focus on realistic pars-
ing over predicted tags. The tags in our training
sets can be GOLD, PROJ, or they can be predicted
through n-fold or linear jackknifing.
</p>
<p>In n-fold jackknifing, we experiment with n ∈{2,3, ..,20}, while for the linear extension we set
p ∈ {5,10, ...,95}. We report the average pars-
ing scores over 5 runs for each n and p so as to
mitigate the effects of random shuffling in the two
jackknifing procedures. In finding the optimal val-
ues of the parameters nmax and pmax, we report
the highest values in case of ties. For example, if
n = 5 and n = 10 both yield the same maximum
UAS, we set nmax = 10.
</p>
<p>We emphasize the importance of realistic set-
</p>
<p>tings especially in cross-lingual parsing. Thus, we
commit to using PROJ taggers with an outlook on
true low-resource languages.
</p>
<p>Data. We use the Universal Dependencies (UD)
treebanks version 1.2 (Nivre et al., 2016).3 As the
projection-based taggers are trained on the WTC
dataset by Agić et al. (2016), we intersect the list
of WTC languages with the UD list for a total of
26 languages.
</p>
<p>Tagging and parsing. For POS tagging, we use
the TNT tagger by Brants (2000). The PRED
taggers score 94.1±1.1%, while the low-resource
PROJ taggers are on average 71.7±5.7% accurate.
We experiment with two parsers. Bohnet’s (2010)
</p>
<p>3http://universaldependencies.org/
</p>
<p>681</p>
<p />
</div>
<div class="page"><p />
<p>Delexicalized transfer
</p>
<p>Train: GOLD↝PROJ PROJ↝PROJ
Test: MULTI SINGLE MULTI SINGLE
</p>
<p>Arabic (ar) 34.2 he 38.3 28.3 id 37.0
Bulgarian (bg) 49.5 cs 50.1 50.2 cs 51.1
</p>
<p>Czech (cs) 50.4 sl 50.7 48.4 sl 50.8
Danish (da) 58.0 no 58.5 58.1 no 61.4
</p>
<p>German (de) 43.9 no 45.0 45.8 sv 45.4
Greek (el) 56.3 it 55.4 57.3 no 55.3
</p>
<p>English (en) 55.8 no 56.7 57.2 sv 58.2
Spanish (es) 67.9 it 68.5 64.9 it 67.6
Estonian (et) 45.8 fi 53.1 43.9 fi 50.8
</p>
<p>Persian (fa) 21.5 ar 25.7 18.9 pl 24.2
Finnish (fi) 38.8 et 45.1 40.0 et 45.5
French (fr) 52.8 it 54.4 54.9 it 58.9
</p>
<p>Hebrew (he) 44.6 ro 45.1 41.7 ro 44.0
</p>
<p>Hindi (hi) 16.9 ta 38.1 18.0 ta 31.0
Croatian (hr) 50.9 sl 50.8 46.8 cs 49.3
</p>
<p>Hungarian (hu) 39.9 sv 46.4 40.1 et 49.3
Indonesian (id) 54.5 ro 56.6 48.7 ro 54.4
</p>
<p>Italian (it) 67.0 es 67.8 67.4 es 69.2
Dutch (nl) 55.1 es 53.9 52.2 sv 52.5
</p>
<p>Norwegian (no) 63.5 sv 64.3 62.4 sv 64.4
</p>
<p>Polish (pl) 62.9 cs 64.2 57.3 cs 59.2
Portuguese (pt) 65.7 es 67.7 64.7 it 66.9
Romanian (ro) 53.6 it 53.7 50.5 es 53.9
</p>
<p>Slovene (sl) 50.5 cs 53.4 52.6 cs 56.3
Swedish (sv) 62.7 no 66.8 61.9 no 67.1
</p>
<p>Tamil (ta) 21.2 hu 28.9 24.6 hi 33.8
</p>
<p>Mean 49.4 – 52.3 48.3 – 52.2
Best for #/26 14 – 12 12 – 14
</p>
<p>Table 2: UAS scores for the delexicalized transfer
parsers. TRAIN↝TEST indicates the training and
testing POS. Bold: best result for language, sepa-
rate for MULTI and SINGLE transfer. For SINGLE,
best source names are also reported.
</p>
<p>second-order graph-based system MATE4 is the
primary. Further, we verify all parsing results by
using a transition-based parser YARA5 with dy-
namic oracles (Rasooli and Tetreault, 2015).
</p>
<p>The following CoNLL 2009 features are used
for training the parsers:6 ID, FORM (in monolin-
gual parsing only), POS, and HEAD. Since ours
is not a benchmarking effort, we apply all systems
with their default settings.
</p>
<p>3.1 Results
</p>
<p>In monolingual parsing over PRED tags (Table 1),
we achieve an identical average UAS with lin-
</p>
<p>4https://code.google.com/archive/p/
mate-tools/
</p>
<p>5https://github.com/yahoo/YaraParser
6https://ufal.mff.cuni.cz/
</p>
<p>conll2009-st/
</p>
<p>Figure 3: Parsing accuracy (UAS) in relation to
linear jackknifing over 26 languages, with two
sources of test set POS tags.
</p>
<p>ear and n-fold jackknifing. Our adaptations sur-
pass training with GOLD data by +1.1 UAS. Lin-
ear jackknifing improves over GOLD training by
+8.1 UAS when parsing over low-resource PROJ
tags. There, we top GOLD training by n-fold jack-
knifing as well, but it trails the linear variant by
-3.9 UAS. In the low-resource PROJ setup, PROJ-
trained parsers are dominant. They score +6.8
UAS over linear, +10.7 UAS over n-folding, and
+14.8 UAS over GOLD training.
</p>
<p>Figure 3 plots the relation between the sample
size p in linear jackknifing and the resulting UAS
in parsing, split for PRED and PROJ test-set tag-
gings. Parsing over PRED tags, the UAS generally
increases with p, but we note that this increase is
rather small: over 26 languages, moving p from
5% to 95% yields only +0.7 UAS on average. By
contrast, adapting to the lower-quality PROJ tags
sees a larger +5 UAS benefit from decreasing p all
the way to 5%, which is well outside the n-fold
range, as indicated for n ∈ {2, ...,20} by the dot-
ted lines in the figure.
</p>
<p>Our cross-lingual parsing experiment (Table 2)
contrasts two options: we either tag (PROJ↝) or do
not tag (GOLD↝) the parser training data. To re-
flect realistic low-resource parsing, the test data is
tagged with PROJ taggers only. On average, the
unadapted parsers are slightly better (UAS: +1.1
MULTI, +0.1 SINGLE). However, they are almost
evenly split with the adapted ones in terms of of-
fering the best performance for 12-14 out of the
26 test languages each. These results suggest, at
least for simplicity, a preference for not tagging
the treebanks.
</p>
<p>4 Discussion
</p>
<p>Linear or n-fold? In resource-rich PRED pars-
ing, the two jackknifing methods are evenly split,
with identical average UAS score and an overlap
</p>
<p>682</p>
<p />
</div>
<div class="page"><p />
<p>on 13 languages. In low-resource PROJ parsing, n-
folding falls far behind as the constraint for n ≥ 2
prevents it from adapting accordingly. The median
pmax in PRED and PROJ are 75% and 5%. The
first one roughly corresponds to 4-fold jackknif-
ing, while the second one is far below the two-fold
range. The median nmax are 11 and 2, and we note
that nmax is rarely ∼10 in Table 1.
</p>
<p>If we simply use ten-fold jackknifing for PRED
tags, we manage to match the pmax scores for only
9 of 26 languages, and we score -0.2 UAS on av-
erage. If using n = 10 with PROJ tags, the discon-
nect is much more substantial, and we are unable
to reach pmax (-4.6 UAS).
</p>
<p>The GOLD training data is never the best choice
in our monolingual parsing experiment, regardless
of whether the test tags are PRED or PROJ. This
result in itself justifies the usage of jackknifing as
adaptation for the monolingual setting, provided
that it is not indiscriminate.
</p>
<p>Finding p̂max. For choosing the optimal linear
jackknifing in real-world parsing, we note that
pmax closely correlates with test set tagging ac-
curacy (Spearman’s ρ = 0.76), and negatively with
treebank size (ρ = −0.42, for ∣D∣ ≤ 10k sentences).
Thus, to adapt via linear jackknifing, we must i)
approximate the expected input data tagging accu-
racy, while at the same time ii) accounting for the
fact that the accuracy associated with any p de-
pends on treebank size as well. In other words,
given two treebanks ∣D1∣ &lt; ∣D2∣, we would typi-
cally need p1 &gt; p2 with the goal of matching the
same test-set accuracy.
</p>
<p>The other parser. Replacing the MATE parser
with the transition-based YARA does not change
the outcome of our monolingual parsing experi-
ment, save for the average 0.58–1.65 drop in UAS.
On the other hand, in cross-lingual parsing, YARA
highlights the benefits of not tagging the training
data, as the GOLD↝PROJ parsers are there the best
choice for parsing 17/26 languages. On average,
we see +2.1 UAS for MULTI, and +0.7 for SIN-
GLE over PROJ↝PROJ. This is especially worth
noting since large-scale parsing generally favors
transition-based systems.
</p>
<p>GOLD test tags. Thus far, we have shown the
need for more careful jackknifing in parser train-
ing with respect to the expected tagging quality
at parse time. Fixing n = 10 was suboptimal in
parsing over the fully supervised PRED tags, while
</p>
<p>n = 2,10 were way below the threshold in low-
resource parsing over our cross-lingual PROJ tags.
We have purposely excluded GOLD test tags from
the discussion so far.
</p>
<p>Still, while parsing over GOLD POS input does
not hold much significance for real-world applica-
tions, it is worth noting how jackknifing performs
in the upper limit: trying to reach the accuracy of
parsers trained and tested on GOLD tags. In that
particular setup, we observe the maximum UAS of
83.8 for median nmax = 12 and pmax = 80%. The
respective modal values are n = 20 and p = 95%,
meaning that for most languages, we come closest
to GOLD↝GOLD by maximizing the tagging ac-
curacy. The overall score amounts to -0.7 UAS
below the upper bound.
</p>
<p>5 Related work
</p>
<p>Jackknifing itself is for the most part incidental to
the work that employs it. Here, we mention a few
notable exceptions.
</p>
<p>Che et al. (2012) compare jackknifing to using
gold tags in parsing Chinese for constituents and
dependencies, where they observe mixed results:
improvement with one parser, and decrease with
the other. Seeker and Kuhn (2013) briefly touch
upon the importance of jackknifing in bridging the
gap between training and test data, and experi-
ment with 5- and 10-folds. Honnibal and John-
son (2015) passingly contrast jackknifing to joint
learning, giving precedence to the latter for sim-
plicity. Finally, Kong et al. (2015) follow Zhu
et al. (2013) in ten-folding for Chinese and En-
glish, citing 2.0% and 0.4% improvements. Inci-
dentally, jackknifing parsers then hurts their per-
formance in tree conversions.
</p>
<p>6 Conclusions
</p>
<p>The parsing literature is riddled with indiscrimi-
nate use of n-fold part-of-speech tagger jackknif-
ing as makeshift domain adaptation.
</p>
<p>In this paper we have proposed a careful empir-
ical treatment of jackknifing in dependency pars-
ing, far surpassing ten-folding via fine-grained
control over the data adjustment.
</p>
<p>Acknowledgements. We thank Barbara Plank
and Djamé Seddah for their valuable comments on
an earlier version of this paper. We appreciate the
incisive feedback by the anonymous reviewers. Fi-
nally, we acknowledge the NVIDIA Corporation
for supporting our research.
</p>
<p>683</p>
<p />
</div>
<div class="page"><p />
<p>References
Željko Agić, Anders Johannsen, Barbara Plank, Héctor
</p>
<p>Martı́nez Alonso, Natalie Schluter, and Anders
Søgaard. 2016. Multilingual projection for pars-
ing truly low-resource languages. Transactions of
the Association of Computational Linguistics 4:301–
312. http://aclweb.org/anthology/Q16-1022.
</p>
<p>Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010). Col-
ing 2010 Organizing Committee, pages 89–97.
http://aclweb.org/anthology/C10-1011.
</p>
<p>Thorsten Brants. 2000. Tnt - a statistical part-of-
speech tagger. In Proceedings of the Sixth Ap-
plied Natural Language Processing Conference.
http://aclweb.org/anthology/A00-1031.
</p>
<p>Wanxiang Che, Valentin Spitkovsky, and Ting Liu.
2012. A comparison of chinese parsers for stan-
ford dependencies. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics, pages 11–16.
http://aclweb.org/anthology/P12-2003.
</p>
<p>Matthew Honnibal and Mark Johnson. 2015. An
improved non-monotonic transition system for
dependency parsing. In Proceedings of the
2015 Conference on Empirical Methods in
Natural Language Processing. Association for
Computational Linguistics, pages 1373–1378.
https://doi.org/10.18653/v1/D15-1162.
</p>
<p>Lingpeng Kong, M. Alexander Rush, and A. Noah
Smith. 2015. Transforming dependencies into
phrase structures. In Proceedings of the 2015
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies. Association
for Computational Linguistics, pages 788–798.
https://doi.org/10.3115/v1/N15-1080.
</p>
<p>Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016). European Language Re-
sources Association (ELRA), Paris, France, pages
1659–1666.
</p>
<p>Maurice H. Quenouille. 1956. Notes on
Bias in Estimation. Biometrika 61:1–17.
https://doi.org/10.2307/2332914.
</p>
<p>Mohammad Sadegh Rasooli and Joel Tetreault. 2015.
Yara Parser: A Fast and Accurate Dependency
Parser. arXiv preprint arXiv:1503.06733 .
</p>
<p>Wolfgang Seeker and Jonas Kuhn. 2013. The effects
of syntactic features in automatic prediction of mor-
phology. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 333–344. http://aclweb.org/anthology/D13-
1033.
</p>
<p>John W. Tukey. 1958. Bias and Confi-
dence in Not Quite Large Samples. An-
nals of Mathematical Statistics 29:614.
https://doi.org/10.1214/aoms/1177706647.
</p>
<p>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). As-
sociation for Computational Linguistics, pages 434–
443. http://aclweb.org/anthology/P13-1043.
</p>
<p>684</p>
<p />
</div>
<div class="page"><p />
<p>Author Index
</p>
<p>Abad, Azad, 518
Abdelali, Ahmed, 601
Agić, Željko, 679
Agichtein, Eugene, 299
Aharoni, Roee, 132
Aizawa, Akiko, 504
Alkhouli, Tamer, 125
Almodaresi, Fatemeh, 79
Alpert-Abrams, Hannah, 411
Anand, Pranav, 141
Artzi, Yoav, 217
Augenstein, Isabelle, 341
</p>
<p>Baldwin, Timothy, 209
Beigman Klebanov, Beata, 244
Belinkov, Yonatan, 601
Bell, Eric, 459
Berant, Jonathan, 623
Berg-Kirkpatrick, Taylor, 366, 411
Bernard, Mathieu, 178
Bhat, Suma, 629
Bisazza, Arianna, 441, 567
Blanco, Eduardo, 641
Braud, Chloé, 237
Bruno, James, 263
Buffone, Anneke, 654
</p>
<p>Cai, Deng, 608
Cai, Zheng, 616
Camacho-Collados, Jose, 594
Chen, Haiqing, 498
Chen, Hsin-Hsi, 404
Chen, Jiajun, 580
Chen, Lingzhen, 542
Chen, Yan, 498
Chen, Yidong, 269
Cheng, Fei, 1
Cheng, Jianpeng, 118
Cheng, Weiwei, 530
Chiarcos, Christian, 256
Cho, Kyunghyun, 72
Choi, Yejin, 459
Chu, Chenhui, 385
Chu, Wei, 498
</p>
<p>Clark, Peter, 311
Cohn, Trevor, 209, 587
Cristia, Alejandrina, 178
</p>
<p>Dabre, Raj, 385
Dagan, Ido, 352
Dahlmeier, Daniel, 92
Dalvi, Fahim, 601
Darari, Fariz, 347
Das, Rajarshi, 358
Delli Bovi, Claudio, 594
Dinu, Liviu P., 85
Dras, Mark, 399
Dredze, Mark, 305
Dupoux, Emmanuel, 178
Durrani, Nadir, 601
Dyer, Chris, 366
</p>
<p>Eckle-Kohler, Judith, 26, 352
Ekbal, Asif, 659
Elbassuoni, Shady, 454
Elrazzaz, Mohammed, 454
Eriguchi, Akiko, 72
Evanini, Keelan, 263
</p>
<p>Fadaee, Marzieh, 441, 567
Fang, Meng, 587
Feng, Yansong, 231
Ferret, Olivier, 224
Finch, Andrew, 560
Franco-Salvador, Marc, 97
Fried, Daniel, 161
Fujita, Atsushi, 392
</p>
<p>Gales, Mark, 45
Gao, Xing, 498
Garrette, Dan, 411
Gella, Spandana, 64
Gildea, Daniel, 7
Gimpel, Kevin, 616
Giorgi, Salvatore, 79
Gliner, Douglas, 32
Goldberg, Yoav, 132
Goldberger, Jacob, 167
Goyal, Kartik, 366
</p>
<p>685</p>
<p />
</div>
<div class="page"><p />
<p>Guo, Hongyu, 372
Gurevych, Iryna, 250, 352
Gyawali, Binod, 244
</p>
<p>Habernal, Ivan, 250
Hachey, Ben, 293
Haffari, Gholamreza, 524
Hajishirzi, Hannaneh, 510
Hasanuzzaman, Mohammed, 659
Hasegawa, Shun, 281
Hayashi, Katsuhiko, 554
Helcl, Jindřich, 196
Helwe, Chadi, 454
Herzig, Jonathan, 623
Hirao, Tsutomu, 275
Hirst, Graeme, 250
Hodas, Nathan, 647
Hou, Yufang, 250
Hu, Wenpeng, 378
Hua, Xinyu, 203
Huang, Feiyue, 608
Huang, Hen-Hsen, 404
Huang, Jun, 498
Huang, Liang, 335
Huang, Shujian, 580
</p>
<p>Jagfeld, Glorianna, 58
Jamshid Lou, Paria, 547
Jang, Jin Yea, 647
Janocko, Anthony, 654
Ji, Heng, 536
Jiang, Youxuan, 103
Johnson, Mark, 547
Jurafsky, Dan, 51
Jurgens, David, 51
</p>
<p>Kamila, Sabyasachi, 659
Kato, Akihiko, 427
Kaur, Mandeep, 659
Kazi, Michaeel, 172
Keller, Frank, 64
Khot, Tushar, 311
Kikuchi, Yuta, 281
Kim, Sunghwan Mac, 471
Kiritchenko, Svetlana, 465
Klein, Dan, 161, 330
Knight, Kevin, 574
Knill, Kate, 45
Kober, Thomas, 433
Komninos, Alexandros, 324
Korostil, Igor, 293
Kulkarni, Vivek, 79
</p>
<p>Kummerfeld, Jonathan K., 103
Kurohashi, Sadao, 385
</p>
<p>Lacroix, Ophélie, 237
Lapata, Mirella, 118
Larche, Allegra, 654
Lasecki, Walter S., 103
Lease, Matthew, 155
Lewis, Mike, 217
Li, Feng-Lin, 498
Li, Hang, 580
Li, Qing, 484
Li, Sujian, 184
Li, Wenjie, 504, 635
Li, Yanran, 504
Libovický, Jindřich, 196
Lin, Chin-Yew, 536
Lin, Ying, 536
Liu, Bing, 148
Liu, Xiaohua, 580
Long, Guoping, 504
Lopez, Adam, 118
Loyola, Pablo, 287
Lu, Wei, 38
Ludusan, Bogdan, 178
</p>
<p>Ma, Mingbo, 335
Ma, Shuming, 635
Makazhanov, Aibek, 666
Malinin, Andrey, 45
Malmasi, Shervin, 399
Manandhar, Suresh, 324
Manning, Christopher D., 110
Marie, Benjamin, 392
Marrese-Taylor, Edison, 287
Matsumoto, Yuji, 427
Matsuo, Yutaka, 287
Mausam, 317
Mazuka, Reiko, 178
McCallum, Andrew, 358
Melamud, Oren, 167
Min, Sewon, 510
Mirza, Paramita, 347
Miyao, Yusuke, 1
Mohammad, Saif, 465
Molloy, Hillary, 263
Monz, Christof, 441, 567
Moosavi, Nafise Sadat, 14
Moschitti, Alessandro, 518
Mou, Lili, 231
Mu, Jiaqi, 629
</p>
<p>Nabi, Moin, 518</p>
<p />
</div>
<div class="page"><p />
<p>Naderi, Nona, 250
Nagata, Masaaki, 275
Nastase, Vivi, 542
Navigli, Roberto, 594
Neveol, Aurelie, 224
Ney, Hermann, 125
Nishino, Masaaki, 275
Nisioi, Sergiu, 85
Niu, Shuzi, 504
Nothman, Joel, 293
</p>
<p>Okumura, Manabu, 281
Oraby, Shereen, 141
</p>
<p>Pal, Harinder, 317
Paris, Cecile, 471
Peng, Xiaochang, 7
Peyrard, Maxime, 26
Ponzetto, Simone Paolo, 85, 97
Post, Matt, 189
Prud’hommeaux, Emily, 32
Puzikov, Yevgeniy, 352
</p>
<p>Qi, Peng, 110
Qiu, Minghui, 498
Qu, Lizhen, 471
</p>
<p>Rabinovich, Maxim, 330
Raganato, Alessandro, 594
Ragni, Anton, 45
Rahimi, Afshin, 209
Rao, Yanghui, 484
Rashkin, Hannah, 459
Raso, Joe, 654
Razniewski, Simon, 347
Reddy, Siva, 358
Reed, Lena, 141
Reffin, Jeremy, 433
Rönnqvist, Samuel, 256
Rosso, Paolo, 97
Roy, Deb, 478
Ryskina, Maria, 411
</p>
<p>Sabharwal, Ashish, 311
Saha, Sriparna, 659
Saha, Swarnadeep, 317
Sajjad, Hassan, 601
Sakaguchi, Keisuke, 189
Sanagavarapu, Krishna Chaitanya, 641
Savenkov, Denis, 299
Schenk, Niko, 256
Schluter, Natalie, 679
Schwartz, H. Andrew, 79, 654
</p>
<p>Seo, Minjoon, 510
Søgaard, Anders, 237, 341
Shaban, Khaled, 454
Shaffer, Kyle, 647
Shen, Xiaoyu, 504
Shi, Xiaodong, 269
Shi, Xing, 574
Shigeto, Yutaro, 417
Shimbo, Masashi, 554
Shindo, Hiroyuki, 427
Shiue, Yow-Ting, 404
Shu, Lei, 148
Sima’an, Khalil, 20
Son, Youngseo, 654
Song, Linfeng, 7
Song, Yi, 244
Song, Yiping, 231
Štajner, Sanja, 85, 97
Stanojević, Miloš, 20
Stanovsky, Gabriel, 352
Stein, Benno, 250
Stern, Mitchell, 161
Strapparava, Carlo, 542
Strube, Michael, 14
Stuckenschmidt, Heiner, 97
Su, Hui, 504
Su, Jinsong, 269
Su, Qi, 635
Suhr, Alane, 217
Sumita, Eiichiro, 560
Sun, Xu, 635
Susanto, Raymond Hendy, 38
Szymanski, Terrence, 448
</p>
<p>Takamura, Hiroya, 281
Takeuchi, Akikazu, 417
Tannier, Xavier, 224
Teneva, Nedelina, 530
Thompson, Brian, 172
Tian, Zhiliang, 231
Tolegen, Gulmira, 666
Toleu, Alymzhan, 666
Tourille, Julien, 224
Tran, Quan Hung, 524
Tsuruoka, Yoshimasa, 72
Tsvetkov, Yulia, 51
Tu, Lifu, 616
Tu, Zhaopeng, 580
</p>
<p>Ungar, Lyle, 79, 654
Utiyama, Masao, 560
</p>
<p>van der Goot, Rob, 491</p>
<p />
</div>
<div class="page"><p />
<p>van der Plas, Lonneke, 58
Van Durme, Benjamin, 189, 305
van Noord, Gertjan, 491
van Santen, Jan, 32
Vempala, Alakananda, 641
Vijayaraghavan, Prashanth, 478
Viswanath, Pramod, 629
Vogel, Stephan, 601
Volkova, Svitlana, 459, 647
Vosoughi, Soroush, 478
Vu, Ngoc Thang, 672
</p>
<p>Wachsmuth, Henning, 250
Walker, Marilyn, 141
Wallace, Byron C., 155
Wan, Stephen, 471
Wang, Boli, 269
Wang, Fu Lee, 484
Wang, Houfeng, 184, 635
Wang, Lu, 203
Wang, Rui, 560
Wang, Siyu, 498
Wang, Weiyue, 125
Wang, William Yang, 422
Wang, Xinhao, 263
Wang, Yaowei, 484
Wang, Yizhong, 184
Wang, Zhiguo, 7
Weeds, Julie, 433
Wei, Sam, 293
Weikum, Gerhard, 347
Weir, David, 433
Wolfe, Travis, 305
Wong, Tak-Lam, 484
Wu, Changxing, 269
Wu, Jiaqi, 141
Wu, Yongjian, 608
</p>
<p>Xiang, Bing, 335
Xie, Haoran, 484
Xin, Yuan, 608
Xu, Hu, 148
Xu, Jingjing, 635
Xu, Qiongkai, 471
</p>
<p>Yan, Rui, 231
Yeh, James, 217
Yoshikawa, Yuya, 417
Yu, Xiang, 672
</p>
<p>Zaheer, Manzil, 358
Zakeri, Mohsen, 79
Zechner, Klaus, 263
</p>
<p>Zembroski, Kevin, 654
Zhan, Xueying, 484
Zhang, Jiajun, 378
Zhang, Ye, 155
Zhang, Yue, 7
Zhang, Zhisong, 608
Zhao, Dongyan, 231
Zhao, Hai, 608
Zhao, Weipeng, 498
Zhao, Yang, 504
Zhou, Bowen, 335
Zhou, Hao, 580
Zhou, Long, 378
Zhu, Derui, 125
Ziering, Patrick, 58
Zong, Chengqing, 378
Zukerman, Ingrid, 524</p>
<p />
</div>
<ul>	<li>Program</li>
	<li>Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths</li>
	<li>AMR-to-text Generation with Synchronous Node Replacement Grammar</li>
	<li>Lexical Features in Coreference Resolution: To be Used With Caution</li>
	<li>Alternative Objective Functions for Training MT Evaluation Metrics</li>
	<li>A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments</li>
	<li>Vector space models for evaluating semantic fluency in autism</li>
	<li>Neural Architectures for Multilingual Semantic Parsing</li>
	<li>Incorporating Uncertainty into Deep Learning for Spoken Language Assessment</li>
	<li>Incorporating Dialectal Variability for Socially Equitable Language Identification</li>
	<li>Evaluating Compound Splitters Extrinsically with Textual Entailment</li>
	<li>An Analysis of Action Recognition Datasets for Language and Vision Tasks</li>
	<li>Learning to Parse and Translate Improves Neural Machine Translation</li>
	<li>On the Distribution of Lexical Features at Multiple Levels of Analysis</li>
	<li>Exploring Neural Text Simplification Models</li>
	<li>On the Challenges of Translating NLP Research into Commercial Products</li>
	<li>Sentence Alignment Methods for Improving Text Simplification Systems</li>
	<li>Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection</li>
	<li>Arc-swift: A Novel Transition System for Dependency Parsing</li>
	<li>A Generative Parser with a Discriminative Recognition Algorithm</li>
	<li>Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation</li>
	<li>Towards String-To-Tree Neural Machine Translation</li>
	<li>Learning Lexico-Functional Patterns for First-Person Affect</li>
	<li>Lifelong Learning CRF for Supervised Aspect Extraction</li>
	<li>Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization</li>
	<li>Improving Neural Parsing by Disentangling Model Combination and Reranking Effects</li>
	<li>Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function</li>
	<li>Implicitly-Defined Neural Networks for Sequence Labeling</li>
	<li>The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective</li>
	<li>A Two-Stage Parsing Method for Text-Level Discourse Analysis</li>
	<li>Error-repair Dependency Parsing for Ungrammatical Texts</li>
	<li>Attention Strategies for Multi-Source Sequence-to-Sequence Learning</li>
	<li>Understanding and Detecting Diverse Supporting Arguments on Controversial Issues</li>
	<li>A Neural Model for User Geolocation and Lexical Dialectology</li>
	<li>A Corpus of Natural Language for Visual Reasoning</li>
	<li>Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative Containers</li>
	<li>How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models</li>
	<li>Cross-lingual and cross-domain discourse segmentation of entire documents</li>
	<li>Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?</li>
	<li>Argumentation Quality Assessment: Theory vs. Practice</li>
	<li>A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations</li>
	<li>Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework</li>
	<li>Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings</li>
	<li>Oracle Summaries of Compressive Summarization</li>
	<li>Japanese Sentence Compression with a Large Training Dataset</li>
	<li>A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes</li>
	<li>English Event Detection With Translated Language Features</li>
	<li>EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering</li>
	<li>Pocket Knowledge Base Population</li>
	<li>Answering Complex Questions Using Open Information Extraction</li>
	<li>Bootstrapping for Numerical Open IE</li>
	<li>Feature-Rich Networks for Knowledge Base Completion</li>
	<li>Fine-Grained Entity Typing with High-Multiplicity Assignments</li>
	<li>Group Sparse CNNs for Question Classification with Answer Sets</li>
	<li>Multi-Task Learning of Keyphrase Boundary Classification</li>
	<li>Cardinal Virtues: Extracting Relation Cardinalities from Text</li>
	<li>Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets</li>
	<li>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</li>
	<li>Differentiable Scheduled Sampling for Credit Assignment</li>
	<li>A Deep Network with Visual Text Composition Behavior</li>
	<li>Neural System Combination for Machine Translation</li>
	<li>An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation</li>
	<li>Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings</li>
	<li>Feature Hashing for Language and Dialect Identification</li>
	<li>Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM</li>
	<li>Automatic Compositor Attribution in the First Folio of Shakespeare</li>
	<li>STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset</li>
	<li>"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection</li>
	<li>English Multiword Expression-aware Dependency Parsing Including Named Entities</li>
	<li>Improving Semantic Composition with Offset Inference</li>
	<li>Learning Topic-Sensitive Word Representations</li>
	<li>Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings</li>
	<li>Methodical Evaluation of Arabic Word Embeddings</li>
	<li>Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast</li>
	<li>Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation</li>
	<li>Demographic Inference on Twitter using Recursive Neural Networks</li>
	<li>Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning</li>
	<li>A Network Framework for Noisy Label Aggregation in Social Media</li>
	<li>Parser Adaptation for Social Media by Integrating Normalization</li>
	<li>AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine</li>
	<li>A Conditional Variational Framework for Dialog Generation</li>
	<li>Question Answering through Transfer Learning from Large Fine-grained Supervision Data</li>
	<li>Self-Crowdsourcing Training for Relation Extraction</li>
	<li>A Generative Attentional Neural Network Model for Dialogue Act Classification</li>
	<li>Salience Rank: Efficient Keyphrase Extraction with Topic Modeling</li>
	<li>List-only Entity Linking</li>
	<li>Improving Native Language Identification by Using Spelling Errors</li>
	<li>Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model</li>
	<li>On the Equivalence of Holographic and Complex Embeddings for Link Prediction</li>
	<li>Sentence Embedding for Neural Machine Translation Domain Adaptation</li>
	<li>Data Augmentation for Low-Resource Neural Machine Translation</li>
	<li>Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary</li>
	<li>Chunk-Based Bi-Scale Decoder for Neural Machine Translation</li>
	<li>Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary</li>
	<li>EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text</li>
	<li>Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging</li>
	<li>Fast and Accurate Neural Word Segmentation for Chinese</li>
	<li>Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task</li>
	<li>Neural Semantic Parsing over Multiple Knowledge-bases</li>
	<li>Representing Sentences as Low-Rank Subspaces</li>
	<li>Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization</li>
	<li>Determining Whether and When People Participate in the Events They Tweet About</li>
	<li>Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter</li>
	<li>Recognizing Counterfactual Thinking in Social Media Texts</li>
	<li>Temporal Orientation of Tweets for Predicting Income of Users</li>
	<li>Character-Aware Neural Morphological Disambiguation</li>
	<li>Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages</li>
	<li>How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers</li>
</ul>
</body></html>