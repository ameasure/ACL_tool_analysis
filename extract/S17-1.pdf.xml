<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="date" content="2017-07-07T09:30:00Z" />
<meta name="pdf:PDFVersion" content="1.4" />
<meta name="pdf:docinfo:title" content="Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)" />
<meta name="xmp:CreatorTool" content="LaTeX with hyperref package" />
<meta name="access_permission:can_print_degraded" content="true" />
<meta name="subject" content="" />
<meta name="dc:format" content="application/pdf; version=1.4" />
<meta name="pdf:docinfo:creator_tool" content="LaTeX with hyperref package" />
<meta name="access_permission:fill_in_form" content="true" />
<meta name="pdf:encrypted" content="false" />
<meta name="dc:title" content="Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)" />
<meta name="modified" content="2017-07-07T09:30:00Z" />
<meta name="cp:subject" content="" />
<meta name="pdf:docinfo:subject" content="" />
<meta name="pdf:docinfo:creator" content="Association for Computational Linguistics" />
<meta name="PTEX.Fullbanner" content="This is pdfTeX using libpoppler, Version 3.141592-1.40.3-2.2 (Web2C 7.5.6) kpathsea version 3.5.6" />
<meta name="meta:author" content="Association for Computational Linguistics" />
<meta name="trapped" content="False" />
<meta name="meta:creation-date" content="2017-07-07T09:30:00Z" />
<meta name="created" content="Fri Jul 07 05:30:00 EDT 2017" />
<meta name="access_permission:extract_for_accessibility" content="true" />
<meta name="Creation-Date" content="2017-07-07T09:30:00Z" />
<meta name="resourceName" content="S17-1.pdf" />
<meta name="tika:file_ext" content="pdf" />
<meta name="Author" content="Association for Computational Linguistics" />
<meta name="producer" content="pdfTeX-1.40.3" />
<meta name="pdf:docinfo:producer" content="pdfTeX-1.40.3" />
<meta name="Keywords" content="" />
<meta name="access_permission:modify_annotations" content="true" />
<meta name="dc:creator" content="Association for Computational Linguistics" />
<meta name="tika_batch_fs:relative_path" content="S17-1.pdf" />
<meta name="dcterms:created" content="2017-07-07T09:30:00Z" />
<meta name="Last-Modified" content="2017-07-07T09:30:00Z" />
<meta name="dcterms:modified" content="2017-07-07T09:30:00Z" />
<meta name="Last-Save-Date" content="2017-07-07T09:30:00Z" />
<meta name="pdf:docinfo:keywords" content="" />
<meta name="pdf:docinfo:modified" content="2017-07-07T09:30:00Z" />
<meta name="meta:save-date" content="2017-07-07T09:30:00Z" />
<meta name="pdf:docinfo:custom:PTEX.Fullbanner" content="This is pdfTeX using libpoppler, Version 3.141592-1.40.3-2.2 (Web2C 7.5.6) kpathsea version 3.5.6" />
<meta name="Content-Length" content="8841994" />
<meta name="X-TIKA:digest:MD5" content="f28849fec36cab80eb6e53993d19af9f" />
<meta name="Content-Type" content="application/pdf" />
<meta name="X-Parsed-By" content="org.apache.tika.parser.DefaultParser" />
<meta name="X-Parsed-By" content="org.apache.tika.parser.pdf.PDFParser" />
<meta name="creator" content="Association for Computational Linguistics" />
<meta name="dc:subject" content="" />
<meta name="access_permission:assemble_document" content="true" />
<meta name="xmpTPg:NPages" content="288" />
<meta name="access_permission:extract_content" content="true" />
<meta name="access_permission:can_print" content="true" />
<meta name="pdf:docinfo:trapped" content="False" />
<meta name="meta:keyword" content="" />
<meta name="access_permission:can_modify" content="true" />
<meta name="pdf:docinfo:created" content="2017-07-07T09:30:00Z" />
<title>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</title>
</head>
<body><div class="page"><p />
<p>*SEM 2017: The Sixth Joint Conference on
Lexical and Computational Semantics
</p>
<p>Proceedings of the Conference
</p>
<p>August 3-4, 2017
Vancouver, Canada</p>
<p />
</div>
<div class="page"><p />
<p>*SEM 2017 is sponsored by:
</p>
<p>The Association for Computational Linguistics
</p>
<p>c©2017 The Association for Computational Linguistics
</p>
<p>Order copies of this and other ACL proceedings from:
</p>
<p>Association for Computational Linguistics (ACL)
209 N. Eighth Street
Stroudsburg, PA 18360
USA
Tel: +1-570-476-8006
Fax: +1-570-476-0860
acl@aclweb.org
</p>
<p>ISBN 978-1-945626-53-1
</p>
<p>ii</p>
<p />
</div>
<div class="page"><p />
<p>Introduction
</p>
<p>Preface by the General Chair
The 2017 edition of *SEM, the 6th in the annual series, took on as its theme “representations of
meaning”, an area of acute interest in the field for the past several years. The conference program
shows that this year’s *SEM has been especially successful in covering its theme from a broad range of
perspectives, including various flavors of distributional, lexical, and formal/linguistic semantics. Thus,
the 2017 conference meets the overall goal of the *SEM series, by bridging across relatively independent
communities approaching the computational modeling of semantics from different angles. Hopefully,
the diversity of the program will provide not only something of interest to a broad audience of NLP
researchers, but also serve to stimulate new ideas and synergies that can significantly impact the field.
</p>
<p>As always, *SEM would not have been possible without the active involvement of our community. Aside
from our dedicated program committee, to whom we give an extended acknowledgement further in this
introduction, we are very thankful to Eduardo Blanco (Publicity Chair) and Sandro Pezzelle (Publication
Chair) for their efficiency and hard work in making the conference a visible event, from website to
proceedings. We are particularly grateful to ACL SIGLEX, who made it possible to offer two exciting
keynotes, and to SIGLEX and Lexical Computing for supporting the annual Adam Kilgarriff Award for
the best paper at *SEM 2017. Our keynote speakers, Yejin Choi and Katrin Erk, are owed special thanks
for taking part in the selection of the best paper.
</p>
<p>On behalf of the Program Committee Chairs, to whom we owe the greatest debt for the excellence of
the program, and myself as General Chair, I invite you to explore, exploit, and enjoy the diversity of
perspectives on the computational modeling of semantics that *SEM 2017 strives to provide.
</p>
<p>Nancy Ide,
General Chair of *SEM 2017
</p>
<p>iii</p>
<p />
</div>
<div class="page"><p />
<p>Preface by the Program Chairs
We are pleased to present this volume containing the papers accepted at the Sixth Joint Conference on
Lexical and Computational Semantics (*SEM 2017, co-located with ACL in Vancouver, Canada, on
August 3-4, 2017).
</p>
<p>*SEM received a record number of submissions this year, which allowed us to compile a diverse and
high-quality program. The number of submissions was over one hundred (107). After we had discarded
some papers due to formal issues, 101 papers were reviewed for the conference, (52 long and 49 short).
Out of these, 36 papers were accepted (22 long, 14 short). Thus, the acceptance rate was 35.6% overall,
42.3% for the long papers and 28.6% for the short submissions. Some of the papers were withdrawn
after acceptance, due to multiple submissions to other conferences (the 2017 schedule was particularly
complicated, with significant intersection of *SEM with EMNLP, CoNLL, IWCS and other venues). The
final number of papers in the program is 30.
</p>
<p>Submissions were reviewed in 9 different areas: Representations of Meaning (special topic of interest),
Distributional Semantics, Semantics for Applications, Lexical Semantics, Lexical Resources and
Ontologies, Discourse and Dialogue, Semantic Parsing and Semantic Role Labeling, Multimodal
Semantics, Formal and Linguistic Semantics. The most prolific areas were Distributional Semantics
(19 submitted papers), Representations of Meaning (15), and Semantics for Applications (15).
</p>
<p>The papers were evaluated by a program committee of 14 area chairs from Asia, Europe and North
America, assisted by a panel of 167 reviewers. Each submission was reviewed by three reviewers, who
were furthermore encouraged to discuss any divergence in evaluation. The papers in each area were
subsequently ranked by the area chairs. The final selection was made by the program co-chairs after an
independent check of all reviews and discussion with the area chairs. Reviewers’ recommendations were
also used to shortlist a set of papers nominated for the Adam Kilgarriff Award. These papers were judged
by a committee chaired by Nancy Ide.
</p>
<p>The final *SEM 2017 program consists of 17 oral presentations and 13 posters, as well as two
keynote talks by Yejin Choi (“From Naive Physics to Connotation: Modeling Commonsense in Frame
Semantics”, joint keynote with SemEval 2017) and Katrin Erk (“What do you know about an alligator
when you know the company it keeps?”).
</p>
<p>We are deeply thankful to all area chairs and reviewers for their help in the selection of the program, for
their readiness in engaging in thoughtful discussions about individual papers, and for providing valuable
feedback to the authors. We are also grateful to Eduardo Blanco for his precious help in publicizing the
conference, and to Sandro Pezzelle for his dedication and thoroughness in turning the program into the
proceedings you now have under your eyes. Last but not least, we are indebted to our General Chair,
Nancy Ide, for her continuous guidance and support throughout the process of organizing this installment
of *SEM.
</p>
<p>We hope you enjoy the conference!
</p>
<p>Aurélie Herbelot &amp; Lluís Màrquez,
Program Co-Chairs of *SEM 2017
</p>
<p>iv</p>
<p />
</div>
<div class="page"><p />
<p>*SEM 2017 Chairs and Reviewers
</p>
<p>General Chair:
</p>
<p>Nancy Ide, Vassar College, USA
</p>
<p>Program Co-Chairs:
</p>
<p>Aurélie Herbelot, University of Trento, Italy
Lluís Màrquez, Qatar Computing Research Institute, Qatar
</p>
<p>Publication Chair:
</p>
<p>Sandro Pezzelle, University of Trento, Italy
</p>
<p>Publicity Chair:
</p>
<p>Eduardo Blanco, University of North Texas, USA
</p>
<p>Area Chairs:
</p>
<p>Representations of meaning
Tim Baldwin, University of Melbourne, Australia
Louise McNally, Universitat Pompeu Fabra, Spain
</p>
<p>Semantics for applications
Roser Morante, Vrije Universiteit Amsterdam, the Netherlands
Mark Sammons, University of Illinois at Urbana-Champaign, USA
</p>
<p>Lexical semantics, figurative language
Diana Inkpen, University of Ottawa, Canada
Ekaterina Shutova, University of Cambridge, UK
</p>
<p>Distributional semantics
Alessandro Lenci, University of Pisa, Italy
Islam Beltagy, University of Texas at Austin, USA
</p>
<p>Coreference, discourse and dialogue
Raquel Fernández, University of Amsterdam, the Netherlands
Nianwen Xue, Brandeis University, USA
</p>
<p>Lexical resources, linked data, ontologies
Simone Paolo Ponzetto, University of Mannheim, Germany
</p>
<p>Formal and linguistic semantics
Laura Rimell, University of Cambridge, UK
</p>
<p>Semantic parsing and semantic role labeling
Luke Zettlemoyer, University of Washington, USA
</p>
<p>Semantics in multimodal approaches
Angeliki Lazaridou, DeepMind
</p>
<p>v</p>
<p />
</div>
<div class="page"><p />
<p>Reviewers:
</p>
<p>Eneko Agirre, Alan Akbik, Marianna Apidianaki, Ron Artstein, Yoav Artzi, Valerio Basile, Roberto
Basili, Beata Beigman Klebanov, Meriem Beloucif, Farah Benamara, Andrew Bennett, Luciana
Benotti, Luisa Bentivogli, Jonathan Berant, Raffaella Bernardi, Chris Biemann, Eduardo Blanco,
Gemma Boleda, Georgeta Bordea, Ellen Breitholz, Julian Brooke, Elia Bruni, Paul Buitelaar,
Luana Bulat, Elena Cabrio, Hiram Calvo, Nicoletta Calzolari, Jose Camacho-Collados, Xavier
Carreras, Jorge Carrillo-de-Albornoz, Tommaso Caselli, Kai-Wei Chang, Emmanuele Chersoni,
Christos Christodoulopoulos, Grzegorz Chrupała, Philipp Cimiano, Stephen Clark, Alexis Con-
neau, Inés Crespo, Mauro Dragoni, Kevin Duh, Katrin Erk, Stefan Evert, Cécile Fabre, Ingrid
Falk, Stefano Faralli, Anna Feldman, Tim Fernando, Diego Frassinelli, Daniel Fried, Michael
Färber, Aldo Gangemi, Spandana Gella, Anna Lisa Gentile, Jonathan Ginzburg, Roxana Girju,
Dan Goldwasser, Jorge Gracia, Iryna Gurevych, Hannaneh Hajishirzi, Luheng He, Iris Hendrickx,
Eric Holgate, Veronique Hoste, Julian Hough, Julie Hunter, Allan Jabri, Sujay Kumar Jauhar,
Richard Johansson, David Jurgens, Laura Kallmeyer, Hans Kamp, Yoshihide Kato, Ruth Kemp-
son, Casey Kennington, Douwe Kiela, Halil Kilicoglu, Manfred Klenner, Ekaterina Kochmar,
Alexander Koller, Parisa Kordjamshidi, Valia Kordoni, Zornitsa Kozareva, Sebastian Krause, Ger-
mán Kruszewski, Gourab Kundu, Tom Kwiatkowski, Ákos Kádár, Man Lan, Gabriella Lapesa,
Dan Lassiter, Gianluca Lebani, Omer Levy, Annie Louis, Junhua Mao, Alda Mari, Erwin Marsi,
Shigeki Matsubara, Louise McNally, Oren Melamud, Tristan Miller, Shachar Mirkin, Saif Mo-
hammad, Alessandro Moschitti, Smaranda Muresan, Preslav Nakov, Vivi Nastase, Vincent Ng,
Malvina Nissim, Sebastian Padó, Alexis Palmer, Martha Palmer, Denis Paperno, Panupong Pasu-
pat, Ellie Pavlick, Maciej Piasecki, Mohammad Taher Pilehvar, Massimo Poesio, Hoifung Poon,
Christopher Potts, Rashmi Prasad, Laurette Pretorius, Laurent Prévot, Matthew Purver, Marek Rei,
Martin Riedl, Mathieu Roche, Stephen Roller, Marco Rospocher, Michael Roth, Alla Rozovskaya,
Josef Ruppenhofer, Attapol Rutherford, Magnus Sahlgren, Mark Sammons, Enrico Santus, Roser
Saurí, Natalie Schluter, Sabine Schulte im Walde, Diarmuid Ó Séaghdha, Jennifer Sikos, Ca-
rina Silberer, Yangqiu Song, Vivek Srikumar, Evgeny Stepanov, Stan Szpakowicz, Idan Szpektor,
Niket Tandon, Joel Tetreault, Ivan Titov, Sara Tonelli, Yulia Tsvetkov, Lyle Ungar, Shyam Upad-
hyay, Tim Van de Cruys, Eva Maria Vecchi, Erik Velldal, Marc Verhagen, Yannick Versley, Aline
Villavicencio, Chuan Wang, Grégoire Winterstein, Feiyu Xu, Nianwen Xue, Torsten Zesch, Pierre
Zweigenbaum
</p>
<p>vi</p>
<p />
</div>
<div class="page"><p />
<p>Invited Talk: From Naive Physics to Connotation:
Modeling Commonsense in Frame Semantics
</p>
<p>Yejin Choi
(Joint Invited Speaker with SemEval 2017)
</p>
<p>University of Washington, USA
</p>
<p>Abstract
</p>
<p>Intelligent communication requires reading between the lines, which in turn, requires rich back-
ground knowledge about how the world works. However, learning unspoken commonsense knowl-
edge from language is nontrivial, as people rarely state the obvious, e.g., “my house is bigger than
me.” In this talk, I will discuss how we can recover the trivial everyday knowledge just from language
without an embodied agent. A key insight is this: The implicit knowledge people share and assume
systematically influences the way people use language, which provides indirect clues to reason about
the world. For example, if “Jen entered her house”, it must be that her house is bigger than her. I
will discuss how we can model a variety of aspects of knowledge – ranging from naive physics to
connotation – adapting the representations of frame semantics.
</p>
<p>vii</p>
<p />
</div>
<div class="page"><p />
<p>Invited Talk: What Do You Know About an Alligator
When You Know the Company It Keeps?
</p>
<p>Katrin Erk
</p>
<p>University of Texas at Austin, USA
</p>
<p>Abstract
</p>
<p>How can people learn about the meaning of a word from textual context? If we assume that
lexical knowledge has to do with truth conditions, then what can textual (distributional) information
contribute? I will argue that at the least, an agent can observe how textual contexts co-occur with
concepts that have particular properties, and that the agent can use this information to make infer-
ences about unknown words: “I don’t know what an alligator is, but it must be something like a
crocodile”. I will further argue that this inference can only be noisy and partial, and is best described
in probabilistic terms.
</p>
<p>viii</p>
<p />
</div>
<div class="page"><p />
<p>Table of Contents
</p>
<p>What Analogies Reveal about Word Vectors and their Compositionality
Gregory Finley, Stephanie Farmer and Serguei Pakhomov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>Learning Antonyms with Paraphrases and a Morphology-Aware Neural Network
Sneha Rajana, Chris Callison-Burch, Marianna Apidianaki and Vered Shwartz . . . . . . . . . . . . . . . . 12
</p>
<p>Decoding Sentiment from Distributed Representations of Sentences
Edoardo Maria Ponti, Ivan Vulić and Anna Korhonen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
</p>
<p>Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection
Yogarshi Vyas and Marine Carpuat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>Domain-Specific New Words Detection in Chinese
Ao Chen and Maosong Sun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
</p>
<p>Deep Learning Models For Multiword Expression Identification
Waseem Gharbieh, Virendrakumar Bhavsar and Paul Cook. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54
</p>
<p>Emotion Intensities in Tweets
Saif Mohammad and Felipe Bravo-Marquez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
</p>
<p>Deep Active Learning for Dialogue Generation
Nabiha Asghar, Pascal Poupart, Xin Jiang and Hang Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
</p>
<p>Mapping the Paraphrase Database to WordNet
Anne Cocos, Marianna Apidianaki and Chris Callison-Burch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .84
</p>
<p>Semantic Frame Labeling with Target-based Neural Model
Yukun Feng, Dong Yu, Jian Xu and Chunhua Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
</p>
<p>Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Se-
mantic Proto-Roles
</p>
<p>Francis Ferraro, Adam Poliak, Ryan Cotterell and Benjamin Van Durme . . . . . . . . . . . . . . . . . . . . . 97
</p>
<p>Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible
Abhijeet Gupta, Gemma Boleda and Sebastian Padó . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
</p>
<p>Comparing Approaches for Automatic Question Identification
Angel Maredia, Kara Schechtman, Sarah Ita Levitan and Julia Hirschberg . . . . . . . . . . . . . . . . . . . 110
</p>
<p>Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for Croatian
Zoran Medić, Jan Šnajder and Sebastian Padó . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
</p>
<p>A Mixture Model for Learning Multi-Sense Word Embeddings
Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi, Stefan Thater and Manfred Pinkal . . . . 121
</p>
<p>Aligning Script Events with Narrative Texts
Simon Ostermann, Michael Roth, Stefan Thater and Manfred Pinkal . . . . . . . . . . . . . . . . . . . . . . . . 128
</p>
<p>The (too Many) Problems of Analogical Reasoning with Word Vectors
Anna Rogers, Aleksandr Drozd and Bofang Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .135
</p>
<p>ix</p>
<p />
</div>
<div class="page"><p />
<p>Semantic Frames and Visual Scenes: Learning Semantic Role Inventories from Image and Video De-
scriptions
</p>
<p>Ekaterina Shutova, Andreas Wundsam and Helen Yannakoudakis . . . . . . . . . . . . . . . . . . . . . . . . . . .149
</p>
<p>Acquiring Predicate Paraphrases from News Tweets
Vered Shwartz, Gabriel Stanovsky and Ido Dagan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
</p>
<p>Evaluating Semantic Parsing against a Simple Web-based Question Answering Model
Alon Talmor, Mor Geva and Jonathan Berant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
</p>
<p>Logical Metonymy in a Distributional Model of Sentence Comprehension
Emmanuele Chersoni, Alessandro Lenci and Philippe Blache . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
</p>
<p>Double Trouble: The Problem of Construal in Semantic Annotation of Adpositions
Jena D. Hwang, Archna Bhatia, Na-Rae Han, Tim O’Gorman, Vivek Srikumar and Nathan Schnei-
</p>
<p>der . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
</p>
<p>Issues of Mass and Count: Dealing with ‘Dual-Life’ Nouns
Tibor Kiss, Francis Jeffry Pelletier, Halima Husic and Johanna Poppek . . . . . . . . . . . . . . . . . . . . . . 189
</p>
<p>Parsing Graphs with Regular Graph Grammars
Sorcha Gilroy, Adam Lopez and Sebastian Maneth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>Embedded Semantic Lexicon Induction with Joint Global and Local Optimization
Sujay Kumar Jauhar and Eduard Hovy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
</p>
<p>Generating Pattern-Based Entailment Graphs for Relation Extraction
Kathrin Eichler, Feiyu Xu, Hans Uszkoreit and Sebastian Krause . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
</p>
<p>Classifying Semantic Clause Types: Modeling Context and Genre Characteristics with Recurrent Neural
Networks and Attention
</p>
<p>Maria Becker, Michael Staniek, Vivi Nastase, Alexis Palmer and Anette Frank . . . . . . . . . . . . . . . 230
</p>
<p>Predictive Linguistic Features of Schizophrenia
Efsun Sarioglu Kayi, Mona Diab, Luca Pauselli, Michael Compton and Glen Coppersmith . . . . 241
</p>
<p>Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks
Mrinmaya Sachan and Eric Xing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
</p>
<p>Ways of Asking and Replying in Duplicate Question Detection
João António Rodrigues, Chakaveh Saedi, Vladislav Maraev, João Silva and António Branco . 262
</p>
<p>x</p>
<p />
</div>
<div class="page"><p />
<p>Conference Program
</p>
<p>August 3rd, 2017
</p>
<p>9:00–10:30 Session S1: Invited Talk (Jointly with SemEval) and Best Paper Award
</p>
<p>9:00–9:15 Opening Remarks
</p>
<p>9:15–10:15 Invited Talk: From Naive Physics to Connotation: Modeling Commonsense in
Frame Semantics
Yejin Choi
</p>
<p>10:15–10:30 Announcement of the Adam Kilgarriff Best Paper Award
</p>
<p>10:30–11:00 Coffee Break
</p>
<p>11:00–12:30 Session S2: Distributional Semantics
</p>
<p>11:00–11:30 What Analogies Reveal about Word Vectors and their Compositionality
Gregory Finley, Stephanie Farmer and Serguei Pakhomov
</p>
<p>11:30–12:00 Learning Antonyms with Paraphrases and a Morphology-Aware Neural Network
Sneha Rajana, Chris Callison-Burch, Marianna Apidianaki and Vered Shwartz
</p>
<p>12:00–12:30 Decoding Sentiment from Distributed Representations of Sentences
Edoardo Maria Ponti, Ivan Vulić and Anna Korhonen
</p>
<p>12:30–14:00 Lunch Break
</p>
<p>xi</p>
<p />
</div>
<div class="page"><p />
<p>August 3rd, 2017 (continued)
</p>
<p>14:00–15:30 Session S3: Lexical Semantics and Lexical Resources
</p>
<p>14:00–14:30 Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy
Detection
Yogarshi Vyas and Marine Carpuat
</p>
<p>14:30–15:00 Domain-Specific New Words Detection in Chinese
Ao Chen and Maosong Sun
</p>
<p>15:00–15:30 Deep Learning Models For Multiword Expression Identification
Waseem Gharbieh, Virendrakumar Bhavsar and Paul Cook
</p>
<p>15:30–16:00 Coffee Break
</p>
<p>16:00–16:30 Session S4: Lexical Semantics and Lexical Resources (continued)
</p>
<p>16:00–16:30 Emotion Intensities in Tweets
Saif Mohammad and Felipe Bravo-Marquez
</p>
<p>16:30–18:00 Session S5: Poster Session
</p>
<p>Deep Active Learning for Dialogue Generation
Nabiha Asghar, Pascal Poupart, Xin Jiang and Hang Li
</p>
<p>Mapping the Paraphrase Database to WordNet
Anne Cocos, Marianna Apidianaki and Chris Callison-Burch
</p>
<p>Semantic Frame Labeling with Target-based Neural Model
Yukun Feng, Dong Yu, Jian Xu and Chunhua Liu
</p>
<p>Frame-Based Continuous Lexical Semantics through Exponential Family Tensor
Factorization and Semantic Proto-Roles
Francis Ferraro, Adam Poliak, Ryan Cotterell and Benjamin Van Durme
</p>
<p>xii</p>
<p />
</div>
<div class="page"><p />
<p>August 3rd, 2017 (continued)
</p>
<p>Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The
Impossible
Abhijeet Gupta, Gemma Boleda and Sebastian Padó
</p>
<p>Comparing Approaches for Automatic Question Identification
Angel Maredia, Kara Schechtman, Sarah Ita Levitan and Julia Hirschberg
</p>
<p>Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for
Croatian
Zoran Medić, Jan Šnajder and Sebastian Padó
</p>
<p>A Mixture Model for Learning Multi-Sense Word Embeddings
Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi, Stefan Thater and Manfred
Pinkal
</p>
<p>Aligning Script Events with Narrative Texts
Simon Ostermann, Michael Roth, Stefan Thater and Manfred Pinkal
</p>
<p>The (too Many) Problems of Analogical Reasoning with Word Vectors
Anna Rogers, Aleksandr Drozd and Bofang Li
</p>
<p>Semantic Frames and Visual Scenes: Learning Semantic Role Inventories from Im-
age and Video Descriptions
Ekaterina Shutova, Andreas Wundsam and Helen Yannakoudakis
</p>
<p>Acquiring Predicate Paraphrases from News Tweets
Vered Shwartz, Gabriel Stanovsky and Ido Dagan
</p>
<p>Evaluating Semantic Parsing against a Simple Web-based Question Answering
Model
Alon Talmor, Mor Geva and Jonathan Berant
</p>
<p>xiii</p>
<p />
</div>
<div class="page"><p />
<p>August 4th, 2017
</p>
<p>9:00–10:30 Session S6: Invited Talk and Distributional Semantics
</p>
<p>9:00–10:00 Invited Talk: What Do You Know About an Alligator When You Know the Company
It Keeps?
Katrin Erk
</p>
<p>10:00–10:30 Logical Metonymy in a Distributional Model of Sentence Comprehension
Emmanuele Chersoni, Alessandro Lenci and Philippe Blache
</p>
<p>10:30–11:00 Coffee Break
</p>
<p>11:00–12:30 Session S7: Linguistic and Formal Semantics
</p>
<p>11:00–11:30 Double Trouble: The Problem of Construal in Semantic Annotation of Adpositions
Jena D. Hwang, Archna Bhatia, Na-Rae Han, Tim O’Gorman, Vivek Srikumar and
Nathan Schneider
</p>
<p>11:30–12:00 Issues of Mass and Count: Dealing with ‘Dual-Life’ Nouns
Tibor Kiss, Francis Jeffry Pelletier, Halima Husic and Johanna Poppek
</p>
<p>12:00–12:30 Parsing Graphs with Regular Graph Grammars
Sorcha Gilroy, Adam Lopez and Sebastian Maneth
</p>
<p>12:30–14:00 Lunch Break
</p>
<p>xiv</p>
<p />
</div>
<div class="page"><p />
<p>August 4th, 2017 (continued)
</p>
<p>14:00–15:30 Session S8: Representations of Meaning
</p>
<p>14:00–14:30 Embedded Semantic Lexicon Induction with Joint Global and Local Optimization
Sujay Kumar Jauhar and Eduard Hovy
</p>
<p>14:30–15:00 Generating Pattern-Based Entailment Graphs for Relation Extraction
Kathrin Eichler, Feiyu Xu, Hans Uszkoreit and Sebastian Krause
</p>
<p>15:00–15:30 Classifying Semantic Clause Types: Modeling Context and Genre Characteristics
with Recurrent Neural Networks and Attention
Maria Becker, Michael Staniek, Vivi Nastase, Alexis Palmer and Anette Frank
</p>
<p>15:30–16:00 Coffee Break
</p>
<p>16:00–17:30 Session S9: Semantics for Applications
</p>
<p>16:00–16:30 Predictive Linguistic Features of Schizophrenia
Efsun Sarioglu Kayi, Mona Diab, Luca Pauselli, Michael Compton and Glen Cop-
persmith
</p>
<p>16:30–17:00 Learning to Solve Geometry Problems from Natural Language Demonstrations in
Textbooks
Mrinmaya Sachan and Eric Xing
</p>
<p>17:00–17:30 Ways of Asking and Replying in Duplicate Question Detection
João António Rodrigues, Chakaveh Saedi, Vladislav Maraev, João Silva and An-
tónio Branco
</p>
<p>17:30–17:40 Closing Remarks
</p>
<p>xv</p>
<p />
</div>
<div class="page"><p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 1–11,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>What Analogies Reveal about Word Vectors and their Compositionality
</p>
<p>Gregory P. Finley
EMR.AI∗
</p>
<p>San Francisco, CA
gregpfinley@gmail.com
</p>
<p>Stephanie Farmer
Department of Linguistics
</p>
<p>Macalester College
Saint Paul, MN
</p>
<p>sfarmer@macalester.edu
</p>
<p>Serguei V.S. Pakhomov
College of Pharmacy
</p>
<p>University of Minnesota
Minneapolis, MN
pakh0002@umn.edu
</p>
<p>Abstract
</p>
<p>Analogy completion via vector arithmetic
has become a common means of demon-
strating the compositionality of word em-
beddings. Previous work have shown that
this strategy works more reliably for cer-
tain types of analogical word relationships
than for others, but these studies have not
offered a convincing account for why this
is the case. We arrive at such an account
through an experiment that targets a wide
variety of analogy questions and defines
a baseline condition to more accurately
measure the efficacy of our system. We
find that the most reliably solvable anal-
ogy categories involve either 1) the appli-
cation of a morpheme with clear syntac-
tic effects, 2) male–female alternations, or
3) named entities. These broader types
do not pattern cleanly along a syntactic–
semantic divide. We suggest instead that
their commonality is distributional, in that
the difference between the distributions
of two words in any given pair encom-
passes a relatively small number of word
types. Our study offers a needed expla-
nation for why analogy tests succeed and
fail where they do and provides nuanced
insight into the relationship between word
distributions and the theoretical linguistic
domains of syntax and semantics.
</p>
<p>1 Introduction
</p>
<p>In recent years, low-dimensional vectors have
proven an efficient and fruitful means of represent-
ing words for numerous computational applica-
tions, from calculating semantic similarity to serv-
</p>
<p>∗ This work was done while the first author was a post-
doctoral research associate at the University of Minnesota.
</p>
<p>ing as an early layer in deep learning architec-
tures (Baroni et al., 2014; Schnabel et al., 2015;
LeCun et al., 2015). Despite these advances,
however, strategies for representing meaning com-
positionally with a vector model remain limited.
Given the difficulties in training representations of
composed meaning (for example, most possible
phrases will be rare or unattested in training data),
achieving an accurate means of building complex
lexical or phrasal representations from lower-order
ones would be a decisive coup in computational
semantics.
</p>
<p>Another promising avenue of compositional se-
mantics is the representation of concepts that do
not map easily to lexemes. A simple averaging
of two vectors may yield a concept that is seman-
tically akin to both, and the arithmetic difference
between word vectors has been said to represent
the relationship between two terms. The ability to
model knowledge unbounded by linguistic labels
is an exciting prospect for natural language pro-
cessing and artificial intelligence more broadly.
</p>
<p>A common test of the compositional proper-
ties of word vectors is complete-the-analogy ques-
tions. Word vector arithmetic has achieved sur-
prisingly high accuracy on this type of task. A
flurry of recent studies have applied this test under
various conditions, but there has been limited fo-
cus on defining precisely what types of relations
vectors can capture, and less still on explaining
these differences. As such, there remains a major
gap in our understanding of distributional seman-
tics. Our original experimental work improves
upon prior methods by 1) targeting a wide vari-
ety of analogy questions drawn from several avail-
able resources and 2) defining a baseline condition
to control for differences in “difficulty” between
questions. These considerations enable an anal-
ysis that constitutes a major step towards a com-
prehensive, theoretically grounded account for the
</p>
<p>1</p>
<p />
</div>
<div class="page"><p />
<p>observed phenomena. To begin, however, we
present a brief review of the analogy problem as
usually posed.
</p>
<p>2 Background
</p>
<p>Several computational approaches have been pro-
posed for representing the meaning of words (and
holistic phrases) in terms of their co-occurrence
with other words in large text corpora. Some of
these, such as latent semantic analysis (Landauer
and Dumais, 1997), focus on developing semantic
representations based on theories of human cog-
nition, whereas others, such as random indexing
(Kanerva, 2009) and word embeddings (Bengio
et al., 2003; Mikolov et al., 2013a) focus more
on computational efficiency. Despite differences
in purpose and implementation, all current dis-
tributional semantic approaches rely on the same
basic principle of using similarity between co-
occurrence frequency distributions as a way to in-
fer the strength of association between words. For
many practical purposes, such as information in-
dexing and retrieval and semantic clustering, these
approaches work remarkably well.
</p>
<p>There is no obvious best way to compose these
types of representations into larger arbitrary lin-
guistic units, although it does seem that cer-
tain regularities exist between terms that surface
through vector subtraction (Mikolov et al., 2013c;
Levy et al., 2014). Why should this be the case?
Consider the relationships between a difference
vector wb−wa and other words in the vocabulary:
wb−wa will be orthogonal to words that co-occur
equally frequently with wa and wb, highly simi-
lar to words that co-occur only with wb, and dis-
similar (negative) to words that co-occur only with
wa.1 If a word’s context is a fair representation of
its meaning, as is the key tenet of the distributional
hypothesis, then this vector difference should iso-
late crucial differences in meaning.
</p>
<p>Analogy tasks have been used to test how well
vector differences capture consistent semantic dif-
ferences. Four-word proportional analogies, typi-
cally written as w1:w2::w3:w4, feature two pairs of
words such that the relationship between w1 and
w2 is the same as between w3 and w4. If these
words are represented with vectors, then, it is as-
sumed that the differences between each pair are
</p>
<p>1These assertions are supported by the distributivity of a
dot product, which is the standard calculation for similarity,
over addition: wx · (wb − wa) = wx · wb − wx · wa.
</p>
<p>roughly equal:
</p>
<p>w2 − w1 ≈ w4 − w3 (1)
</p>
<p>In the most popular version of this task, a sys-
tem is given the first three words in the analogy
and asked guess the best candidate for w4. Solv-
ing for w4,
</p>
<p>w4 ≈ w3 + w2 − w1 (2)
</p>
<p>and thus a system selects its hypothesis whyp from
the vocabulary V —typically excluding w1, w2 and
w3—by finding the word with maximum angular
(cosine) similarity to the hypothesis vector (ex-
pressed as vector dot product, assuming all word
vectors are unit length):
</p>
<p>whyp = arg max
w∈V
</p>
<p>(w · (w3 + w2 − w1)) (3)
</p>
<p>We call this algorithm 3COSADD following
Levy et al. (2014). Levy and Goldberg (2014) note
that this strategy is equivalent to finding the word
in the lexicon that is the best match for w3 and w2
while also being most distant from w1. This re-
framing suggests that it may not be necessary at all
to represent ineffable concepts through intermedi-
ate stages of vector composition; 3COSADD could
be solving analogies simply through term similar-
ity. Indeed, words in a pair sharing some relation
tend to be similar to each other; when they are ex-
tremely similar, the difference between w2 and w1
is negligible, and the task becomes trivial.
</p>
<p>Linzen (2016) makes this observation as well
and goes on to demonstrate that accuracy falls to
near zero across the board when not excluding
w1, w2, and w3 from contention in the hypoth-
esis space, which shows how strongly dependent
3COSADD is upon vector similarity. We agree
wholeheartedly with that paper’s claim that it is
important to measure the consistency of vector dif-
ferences in a way that is mindful of the typically
high similarity between paired terms.
</p>
<p>2.1 Analogy Test Sets
Several categorized sets of semantic and syn-
tactic analogies are publicly available. One of
the earliest was published by Microsoft Research
(Mikolov et al., 2013c) and consists of 16 cate-
gories of inflectional morphological relations for
English nouns, verbs, and adjectives. The most
commonly reported test set, which we refer to as
the Google set, is included with the distribution of
</p>
<p>2</p>
<p />
</div>
<div class="page"><p />
<p>the word2vec tool (Mikolov et al., 2013a). The
Google set comprises 14 categories, mostly in-
volving inflectional or geographical relationships
between terms. Categories are grouped into a “se-
mantic” and a “syntactic” subset, and results are
often reported averaged over each rather than by
category. This practice is rather problematic in our
view, as the syntactic/semantic division is quite
coarse and even questionable in some cases. We
explore the relationship between syntax, seman-
tics, and morphology in detail later on.
</p>
<p>The “Better Analogy Test Set” (BATS) is a large
set developed to contain a balanced sampling of a
wide range of categories (Gladkova et al., 2016).
BATS features 40 categories of 50 word pairs
each, covering inflectional and derivational mor-
phology as well as several semantic relations.
</p>
<p>The relational similarity task in SemEval-2012
featured relations between word pairs targeting
a massive range of lexical semantic relationships
(Jurgens et al., 2012). By drawing on the aggre-
gated results of the task’s participants, we have ex-
tracted highly representative pairs for each relation
to build an analogy set.
</p>
<p>2.2 Accounting for Analogy Performance
</p>
<p>In addition to those already cited, numerous other
recent papers have evaluated word embeddings
by benchmarking on analogy questions (Mikolov
et al., 2013b; Garten et al., 2015; Lofi et al.,
2016). There is some consensus regarding per-
formance across question types: systems do well
on questions of inflectional morphology (espe-
cially so for English (Nicolai et al., 2015)), but
far less reliably so for various non-geographical
semantic questions—although some gains in per-
formance are possible by adjusting the embedding
algorithms used or their hyperparameters (Levy
et al., 2015), or by training further on subproblems
(Drozd et al., 2016).
</p>
<p>Amongst all of these findings, however, we
found lacking a cohesive, thorough, and satisfy-
ing account of why vector arithmetic works where
it does to solve analogies. To that end, we con-
ducted an experiment to arrive at such an expla-
nation, with some notable departures from previ-
ously used methods. We included a wide range
of available test data, which is key because indi-
vidual sets usually feature some bias towards one
type or a few types of question, and benchmark-
ers often report nothing more than accuracy av-
</p>
<p>eraged over an entire set (Schnabel et al., 2015).
Additionally, we define a baseline, which is criti-
cal not only to gauge effectiveness, but also to un-
derstand the mechanism behind solving analogies
using compositional methods.
</p>
<p>In the following sections we present the design
of the experiment, baseline condition, and ques-
tion sets; a discussion of how performance on
analogy questions breaks down by broad category;
and finally, a theoretical accounting for the ob-
served patterns and the implications for distribu-
tional semantics.
</p>
<p>3 Method
</p>
<p>3.1 Word Embeddings
</p>
<p>We used word embeddings trained on the plain
text of all articles from Wikipedia as of Septem-
ber 2015, processed to remove all punctuation
and case distinctions. We tested the word2vec
and GloVe (Pennington et al., 2014) training algo-
rithms. Results were qualitatively very similar be-
tween the two, although word2vec scored slightly
higher on our metrics. Due to space considera-
tions, we discuss only the word2vec results.
</p>
<p>Hyperparameters were set as recommended for
analogy tasks by the developers: 200-dimensional
vectors, continuous bag-of-words sampling, 8-
word window size. (We also tested a skip-gram
model in word2vec and saw only slight and oc-
casional differences—more subtle even than those
seen between word2vec and GloVe.)
</p>
<p>3.2 Test Set
</p>
<p>We used a pooled set of analogy questions com-
prising the Google, Microsoft, SemEval 2012, and
BATS test sets. At test time, any analogies that
featured a word absent from our lexicon were dis-
carded. (Note that the Microsoft categories testing
the English possessive enclitic ’s were not tested,
as preprocessing for our vector training corpus re-
moved all punctuation.) The sizes of each set fol-
lowing the removal of out-of-vocabulary analogies
are given in Table 1.
</p>
<p>Note that the BATS and SemEval data sets fea-
ture a number of word pairs in each category but
not four-word analogy questions. We simply took
every possible pair of pairs from the same cate-
gory, so long as this did not result in an analogy
in which w1 and w2 were the same word or in
which w4 was not unique. Some pairs in BATS
have more than one correct answer; for uniformity
</p>
<p>3</p>
<p />
</div>
<div class="page"><p />
<p>SOURCE CATEGORIES ANALOGIES
</p>
<p>Microsoft Research 14 7,000
Google (word2vec) 14 19,544
</p>
<p>SemEval2012 79 30,082
BATS 40 95,625
Total 147 152,251
</p>
<p>Table 1: Summary of test data sources.
</p>
<p>with other test sets, we use only the first answer
provided for each of these pairs.
</p>
<p>For SemEval, we used the “platinum standard”
data distribution, which includes rankings of word
pairs in each category based on how well they rep-
resent the relationship as defined. We took only
the best half of pairs from that ranking to gener-
ate the test set. This was necessary because pairs
lower down the list tend to poorly represent the re-
lationship, or even to represent its opposite.
</p>
<p>3.3 Measures
</p>
<p>Virtually all existing studies of automated analogy
solving report accuracy as the main measure. Ac-
curacy is indeed a relevant measure when the goal
is to simulate human performance on a particular
task. Our purpose, however, is to understand the
nature of semantic representations and account for
when vector arithmetic does and does not function
well as a model of relationships.
</p>
<p>For every analogy question, we calculate the
ranking of the correct w4 in the hypothesis
space—that is, the ordering of all words in the
lexicon in descending order of the result of the
3COSADD hypothesis function (3). A “correct”
answer would correspond to a ranking of 1.
</p>
<p>Accuracy is a coarse measure in that it is in-
sensitive to any ranking other than 1. Rather than
accuracy, we borrow a measure from information
retrieval (Voorhees, 1999)—the reciprocal of rank
(RR) averaged across analogy questions in each
category, which is always a positive fraction in the
range:
</p>
<p>1
||V || ≤ RR ≤ 1 (4)
</p>
<p>Numerically, RR acts as a “softer” version of ac-
curacy, with rankings other than 1 contributing
somewhat to the average.
</p>
<p>Besides being coarse, accuracy is also an un-
controlled measure in that it is insensitive to dif-
ferences in analogy “difficulty,” by which we
mean the prior degree of similarity between sin-
</p>
<p>gle word vectors. An example: nominal plural
analogies, such as dog:dogs::horse:horses, often
achieve high accuracy, but this may follow natu-
rally from the high similarity between most singu-
lar nouns and their plural forms—indeed, for both
of these pairs, the singular and plural forms are
the closest terms to each other in our trained vec-
tor space.
</p>
<p>To measure the efficacy of vector arithmetic in
a manner controlled for variances in prior vector
similarity, we propose a baseline, defined for each
analogy as the best ranking between the word most
similar to w2 and the word most similar to w3:
</p>
<p>rankbase = min(rank(arg max
w∈V
</p>
<p>(w · w2)),
</p>
<p>rank(arg max
w∈V
</p>
<p>(w · w3))) (5)
</p>
<p>For the above example, as dog is the most sim-
ilar word to dogs, there is no improvement to be
made upon baseline. Likewise, for the analogy
banana:yellow::sky:blue, baseline would likely be
high because yellow and blue are very similar.
</p>
<p>Consistent with reporting RR for 3COSADD,
we report baseline reciprocal rank (BRR). We
suspect that using RR will be especially illustra-
tive for baseline, where there may be many “near
misses” that are informative but would all be re-
duced to zero if measuring only accuracy.
</p>
<p>Our baseline is similar to the so-called ONLY-B
baseline tested by Linzen (2016), except that the
latter considers only w3. We include w2 because
this term has just as much effect on the 3COSADD
hypothesis as w3. Note that our baseline would
not itself be implementable as a solving strategy
because it presumes access to w4 to select be-
tween w2 and w3; nevertheless, we contend that it
is helpful to define the baseline as we have done to
account for those categories in the test data where
all w2 and w4 are drawn from a small semantic
cluster—most notably, the color example in the
previous paragraph. (Overall, 16–18% of analo-
gies across our test sets show similarity to w2 as a
better baseline than to w3.)
</p>
<p>Improvement is defined as the difference be-
tween 3COSADD RR and baseline RR, a measure
we will refer to as reciprocal rank gain (RRG).
RRG is more sensitive to shifts in rank that might
not result in perfect accuracy. Analogies that show
improvement from a very poor rank to first place
will show a gain of nearly 1, whereas moving from
second to first place is only 0.5 (and moving from
</p>
<p>4</p>
<p />
</div>
<div class="page"><p />
<p>poor rank to second is nearly 0.5). If 3COSADD
yields a worse hypothesis, this will be reflected as
a negative RRG.
</p>
<p>We also tested other solving methods suggested
by Levy and Goldberg (Levy et al., 2014), 3COS-
MUL and PAIRDIRECTION, although we do not
report them here—results with the former were
virtually indistinguishable from 3COSADD, and
poorer overall with the latter.
</p>
<p>The raw results of our similarity experiments,
as well as source code to replicate all steps of the
experiments and analysis, can be downloaded at
https://github.com/gpfinley/analogies.
</p>
<p>4 Results
</p>
<p>Most broadly, we confirm prior findings that vec-
tor arithmetic can be used to solve analogy ques-
tions, with a mean RRG of .165 across all ques-
tions in all categories (t = 187, p � .01). For
a more nuanced analysis, we sorted analogy tests
into four broad supercategories of analogical rela-
tionship: 30 categories of inflectional morphology,
12 of derivational morphology, 10 of named enti-
ties, and 95 of semantics of non-named entities (79
of which are from SemEval).
</p>
<p>The gain in RR from baseline for all categories
is presented visually in Figure 1, where they are
grouped into our four supercategories for ease of
interpretation. (See the appendix for the names
of the top performing categories.) Each individ-
ual category is represented by a line between its
BRR and 3COSADD RR. Within each supercate-
gory, we also consider intermediate groupings of
categories, and these are visualized by differences
in line stroke in the figure. Note that some patterns
are evident between and within supercategories:
</p>
<p>• Inflectional: Although all inflectional cate-
gories show positive RRG, adjectival and ver-
bal inflection shows reliably higher RRG than
nominal inflection.
</p>
<p>• Derivational: Derivational morphemes
whose primary function is to shift syntactic
class (-tion, -ment, -ly, -ness) show on
average higher RRG than those with stronger
regular semantic consequences (-less, -able,
over-, adjectival un-, repetitive re-, agentive
-er).
</p>
<p>• Named entities: All categories—and partic-
ularly those dealing with country capitals—
show high RRG.
</p>
<p>• Lexical: Analogy relationships based on
gender difference exhibit high RRG, while
most other categories have low or even nega-
tive RRG.
</p>
<p>We performed a linear regression analysis to
predict RRG as a function of supercategory (F =
24600, p � .01, R2 = .39). The model is sum-
marized in Table 4. (Note that the model contains
no intercept term, so the coefficient for each super-
category is equivalent to its mean RRG.) A posi-
tive RRG can be demonstrated with high statistical
significance for all supercategories except lexical
semantics.
</p>
<p>We also investigated possible effects of word
frequency on analogy performance. Multi-
collinearity poses a major challenge here: the fre-
quencies of all four words in an analogy are highly
correlated, and frequency can change dramatically
across category. A comprehensive analysis of this
complex problem is beyond the scope of this study,
although we did find that the difference between
an analogy’s w4 frequency and the mean w4 fre-
quency in that category correlates positively with
RRG, although this effect is subtle (r = .016,
t = 6.28, p� .01).
</p>
<p>5 Discussion
</p>
<p>It is clear from our results that vector arithmetic
is a better approach for certain types of analogy
questions than for others. Almost as clear is the
hierarchy of the four broad types of questions that
we have defined: excellent performance for inflec-
tion and named entities, with decidedly mixed re-
sults for derivational morphology and poorer still
for lexical semantics—with the notable exception
of male–female analogies. Below, we account for
these patterns in the context of two domains of lin-
guistic theory: the interaction between morphol-
ogy and syntax, and the type-theoretic difference
between individuals and sets.
</p>
<p>5.1 Morphology and Syntax
Verbal and adjectival inflection show much more
improvement over baseline than nominal inflec-
tion. It may simply be that the nominal cate-
gories have too high a baseline value to show
much evidence of improvement by 3COSADD. It
is also possible, however, that the nominal plural
has fewer syntactic implications than verbal and
adjectival morphology: nouns in non-subject po-
sition do not participate in number agreement in
</p>
<p>5</p>
<p />
</div>
<div class="page"><p />
<p>Baseline 3CosAdd
0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1.0
</p>
<p>M
ea
</p>
<p>n 
re
</p>
<p>ci
pr
</p>
<p>oc
al
</p>
<p> r
an
</p>
<p>k
Inflectional morphology
</p>
<p>Baseline 3CosAdd
0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1.0
</p>
<p>M
ea
</p>
<p>n 
re
</p>
<p>ci
pr
</p>
<p>oc
al
</p>
<p> r
an
</p>
<p>k
</p>
<p>Lexical semantics
</p>
<p>Baseline 3CosAdd
0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1.0
</p>
<p>Derivational morphology
</p>
<p>Baseline 3CosAdd
0.0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1.0
</p>
<p>Named entities
</p>
<p>Figure 1: Mean reciprocal rank shifts between baseline and 3COSADD for four supercategories. Each
line is a single category of analogy questions (“country - capital” or “male - female,” for example). Some
lines are differentiated by stroke type (dotted, solid, or dashed), the meaning of which is idiosyncratic
to each supercategory: for inflectional, dashed lines are for nouns, dotted lines for adjectives, and solid
lines for verbs; for derivational, dotted lines are for morphemes that change syntactic class with minimal
semantic impact (e.g., -ly, as opposed to re-); for named entities, dotted lines are for country capitals;
for lexical semantics, dotted lines are for gender relationships. Within supercategory, the difference in
RRG between categories of different stroke types is significant in every case (|t| between 14.5 and 58.7,
p� .01).
</p>
<p>English, so the plurality of many nouns in a text
has little syntactic consequence.
</p>
<p>Derivational morphology might be expected to
perform worse than inflectional morphology for
a number of reasons. Even for highly produc-
tive morphemes, derivation tends to have more id-
</p>
<p>iosyncratic meaning (Haspelmath and Sims, 2010,
100). For example, although ‘recruitment’ refers
to the act of recruiting, ‘government’ refers to a
governing body rather than the act of governing;
similarly, the adverb ‘sadly’ can be used as a sen-
tential adverb (expressing the speaker’s attitude
</p>
<p>6</p>
<p />
</div>
<div class="page"><p />
<p>SUPERCATEGORY ESTIMATE STD ERROR t
Inflectional .345 .0015 228 ***
</p>
<p>Derivational .106 .0018 57.7 ***
Lexical semantics −.000 .0012 −0.293
</p>
<p>Named entities .420 .0020 207 ***
</p>
<p>Table 2: Summary of regression model for reciprocal rank gain as a function of analogy supercategory.
All starred levels are highly significant (p� .01).
</p>
<p>about the statement) as well as a manner adverb,
whereas ‘angrily’ cannot. These semantic char-
acteristics introduce lexically dependent variance
that is far less pronounced for inflection.
</p>
<p>From our results with derivational sets, there
is evidence of a trend in which morphemes with
predominantly syntactic consequences are better
handled than those with stronger semantic conse-
quences (see dotted/solid lines in Figure 1). Sig-
nificant further experimental work is needed to
quantify the syntactic versus semantic effects of
derivational morphemes.
</p>
<p>We predict that such work would support the
notion of a continuum between morphemes with
only syntactic effects and those with only (lexi-
cally) semantic effects. Those towards the syntac-
tic end of the continuum will tend to be better cap-
tured by vector offsets in distributional represen-
tations. There would be a partial overlap between
this continuum and the inflectional–derivational
continuum in that derivational morphology tends
to have more idiosyncratic meanings and is less
relevant to syntax. There would be differences
as well, especially as regards the property that
word class-changing morphology is more deriva-
tional: the repetitive re- in English, for example,
may be considered less derivational than the dever-
bal nominalizer -ment because it does not change
word class, but re- has virtually no syntactic con-
sequences for the verb to which it affixes.
</p>
<p>5.2 Semantics: Named Entities as Individuals
</p>
<p>Our results show that analogy sets containing
named entities are more readily solvable than
those that contain other lexical categories (com-
mon nouns, verbs, etc.).
</p>
<p>A possible explanation for this is that named en-
tities have a single real-world referent—there is,
for instance, only one Amsterdam—while there is
a large set of real-world referents that correspond
to a common noun like ‘dog’. We would expect
the co-occurrences of ‘dog’, then, to be more di-
</p>
<p>verse than those of a named entity like ‘Amster-
dam’.
</p>
<p>The distinction drawn here between named en-
tities and other parts of speech is analogous to the
distinction between words of type e (“individu-
als”) and words of type 〈e, t〉 in Montagovian set-
theoretic semantics (Montague, 1973). According
to Montague, proper names (arguments of type
e) denote individuals, while verbs and common
nouns (predicates of type 〈e, t〉) denote sets of in-
dividuals. Thus, ‘Amsterdam’ denotes an individ-
ual, while ‘dog’ denotes the set of dogs.
</p>
<p>To better appreciate how this distinction might
lead to “fuzzier” representations for some words,
consider that training a vector on separate refer-
ences to numerous members of a set of individuals
is akin to a massive case of pseudo-polysemy—the
vector can only capture the average of all refer-
ents rather than a single, clear referent. Polysemy
is a well-known problem in training word vectors
(Reisinger and Mooney, 2010), although this case
of multiple referents has not been considered be-
fore to our knowledge.
</p>
<p>Overall, named entity categories show very
good RRG results, especially when both terms in
a pair are named entities (as opposed to ‘name -
occupation’, say). Country capitals show excel-
lent performance in particular. In the broader his-
tory of this line of reserach, it is worth noting that
the composition of the Google test set plays to this
strength: country capital questions constitute over
a quarter of its analogies (and over half of those
in the “semantic” set, as noted by Gladkova et
al. (2016)). As our experiments and others have
demonstrated, however, the vector arithmetic ap-
proach struggles for most semantic questions.
</p>
<p>Given the enormous influence of word2vec, it
is worth asking whether prevailing knowledge in
this field has been influenced by a selective fo-
cus on easier tasks. As further illustration of
this point, note that the classic go-to example,
king:queen::man:woman, is drawn from the sole
</p>
<p>7</p>
<p />
</div>
<div class="page"><p />
<p>category in lexical semantics with any clear posi-
tive result in our experiments.
</p>
<p>As a matter of fact, we should address the
exceptional performance on analogies in male–
female categories; why, of all lexical semantic
sets, do we see such high performance here? We
suspect these categories does well for the same
reason that inflectional analogies do well: En-
glish features gender agreement with some per-
sonal pronouns—and, of course, with coreferen-
tial gendered terms—so there are concrete and
regular distributional consequences of a noun’s se-
mantic gender.
</p>
<p>5.3 A Unified Account
</p>
<p>A recurrent thread in our accounting for all
categories is that 3COSADD does well with
relationships that have predictable effects on
distribution—i.e., nearby terms and their morphol-
ogy and syntax (although all morphology is effec-
tively suppletive for these embeddings). This is
especially evident with inflectional morphology,
and true as well for certain types of derivational
morphology as well as classes that participate in
agreement, such as gender.
</p>
<p>Relations between named entities are not gov-
erned by syntactic differences as inflectional rela-
tionships are, but there is a certain distributional
parallel between the two: terms with a single
referent will generally exhibit a less blurred co-
occurrence profile than those with multiple refer-
ents; similarly, the difference between two realiza-
tions of the same root (e.g. ‘hot’ and ‘hotter’) will
be highly non-orthogonal primarily with words of
syntactic relevance, which is also a small set. The
common theme is clear: the smaller the set of
unique word types that co-occur with either word
1 or word 2 but not both (i.e.,the symmetric differ-
ence), the more cleanly the relationship between
word 1 and word 2 can be captured.
</p>
<p>Recall that our results also suggest that analogy
questions containing frequent words are easier to
solve with vector arithmetic than those containing
less frequent words. We suspect that this is be-
cause the distributional representations of frequent
words are more robust and less noisy. We believe,
however, that more targeted investigation into the
effects of frequency might qualify this generaliza-
tion. For instance, it is reasonable to assume that
a word’s frequency correlates with the diversity
of its co-occurrence, and that this diversity could
</p>
<p>signal distinct word senses, which are notoriously
tricky for distributional representations. This is a
ripe topic for further study.
</p>
<p>5.4 Challenges
</p>
<p>One challenge in interpreting our results is that
categories with seemingly identical relations can
show marked discrepancies in performance: note
the differences between Google ‘comparative’ and
Microsoft ‘JJ JJR’, which examine the same in-
flectional relationship but show rather different
levels of performance. Similarly, note the ex-
treme difference in baseline rank for Google ‘gen-
der’ (called ‘family’ in the original set) and BATS
‘male - female’ categories. Clearly, lexical choices
make a significant difference and can even over-
shadow the inter-category differences that we are
trying to measure. Note that in both of the above
examples, the version of the category featuring
more unique word types showed lower baseline
and lower gain.
</p>
<p>The explanations we put forward here may
need to be extended to address other types
of relationships that we did not evaluate.
One particular interesting example might be
Linzen et al.’s (2016) tests of analogies be-
tween quantifiers across domains—e.g., ev-
erything:nothing::always:never—which show in-
triguingly mixed results.
</p>
<p>6 Conclusion
</p>
<p>We evaluated syntactic and semantic analogy
questions from a large and highly diverse test set
using metrics more controlled and more sensitive
than accuracy. Inspecting the results across cate-
gories, we were able to account for the differences
in performance we observed across types of word
relationships in terms that are consistent with the
distributional training objectives of word embed-
dings.
</p>
<p>Vector arithmetic with word embeddings is
most effective when co-occurrence are limited to
a small number of words, either by syntactic reg-
ularities or ease of semantic representation. It is
possible to account for both of these by consider-
ing distributional phenomena directly.
</p>
<p>Still, questions remain—do our negative results
reflect the failure of word vectors to model seman-
tic nuances, or the failure of vector arithmetic to
capture them, or is the semantic data simply too
noisy for current methods? Further experiments
</p>
<p>8</p>
<p />
</div>
<div class="page"><p />
<p>with special attention paid to smoothing lexical se-
mantic representations will be key to solving this
problem.
</p>
<p>Acknowledgments
</p>
<p>This work was partially supported by a Univer-
sity of Minnesota Academic Health Center Fac-
ulty Development Award and by the National In-
stitute of General Medical Sciences (GM102282).
</p>
<p>References
Marco Baroni, Georgiana Dinu, and German
</p>
<p>Kruszewski. 2014. Dont count, predict! a sys-
tematic comparison of context-counting vs. context-
predicting semantic vectors. In Proceedings of the
ACL. pages 238–247.
</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search 3(Feb):1137–1155.
</p>
<p>Aleksandr Drozd, Anna Gladkova, and Satoshi Mat-
suoka. 2016. Word embeddings, analogies, and
machine learning: Beyond king − man + woman
= queen. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics. pages 3519–3530.
</p>
<p>Justin Garten, Kenji Sagae, Volkan Ustun, and Morteza
Dehghani. 2015. Combining distributed vector rep-
resentations for words. In Proceedings of NAACL-
HLT . pages 95–101.
</p>
<p>Anna Gladkova, Aleksandr Drozd, and Satoshi Mat-
suoka. 2016. Analogy-based detection of morpho-
logical and semantic relations with word embed-
dings: what works and what doesnt. In Proceedings
of NAACL-HLT . pages 8–15.
</p>
<p>Martin Haspelmath and Andrea Sims. 2010. Under-
standing Morphology. Routledge.
</p>
<p>David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (*SEM). pages 356–
364.
</p>
<p>Pentti Kanerva. 2009. Hyperdimensional computing:
An introduction to computing in distributed rep-
resentation with high-dimensional random vectors.
Cognitive Computation 1(2):139–159.
</p>
<p>Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review
104(2):211.
</p>
<p>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature 521(7553):436–444.
</p>
<p>Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In
Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, Cur-
ran Associates, Inc., pages 2177–2185.
</p>
<p>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics 3:211–225.
</p>
<p>Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014. Linguistic regularities in sparse and explicit
word representations. In Proceedings of CoNLL.
pages 171–180.
</p>
<p>Tal Linzen. 2016. Issues in evaluating semantic spaces
using word analogies. In Proceedings of the 1st
Workshop on Evaluating Vector Space Representa-
tions for NLP.
</p>
<p>Tal Linzen, Emmanuel Dupoux, and Benjamin Spec-
tor. 2016. Quantificational features in distributional
word representations. In Proceedings of the Fifth
Joint Conference on Lexical and Computational Se-
mantics (*SEM). pages 1–11.
</p>
<p>Christoph Lofi, Athiq Ahamed, Pratima Kulkarni, and
Ravi Thakkar. 2016. Benchmarking semantic capa-
bilities of analogy querying algorithms. In Database
Systems for Advanced Applications. Springer, pages
463–478.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, Curran Associates, Inc., pages 3111–3119.
</p>
<p>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL.
pages 746–751.
</p>
<p>Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In K. J. J. Hin-
tikka, M. E. Moravcsik, and P. Suppes, editors, Ap-
proaches to Natural Language: Proceedings of the
1970 Stanford Workshop on Grammar and Seman-
tics, Dordrecht.
</p>
<p>Garrett Nicolai, Colin Cherry, and Grzegorz Kondrak.
2015. Morpho-syntactic regularities in continuous
word representations: A multilingual study. In Pro-
ceedings of NAACL. pages 129–134.
</p>
<p>9</p>
<p />
</div>
<div class="page"><p />
<p>Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP. vol-
ume 14, pages 1532–1543.
</p>
<p>Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of NAACL. pages 109–117.
</p>
<p>Tobias Schnabel, Igor Labutov, David Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings. In Proceedings of
EMNLP.
</p>
<p>Ellen M Voorhees. 1999. The trec-8 question answer-
ing track report. In NIST Special Publication 500-
246: The Eighth Text REtrieval Conference. pages
77–82.
</p>
<p>10</p>
<p />
</div>
<div class="page"><p />
<p>Appendix: Mean Rank by Category
</p>
<p>CATEGORY RR CATEGORY BRR CATEGORY RRG
G:capital .950 G:plural .711 G:capital .750
</p>
<p>G:capital-all .945 noun - plural reg .674 country - capital .659
G:gender .933 G:gender .618 verb inf - 3pSg .604
</p>
<p>G:nationality-adj .917 noun - plural irreg .603 G:superlative .600
country - capital .909 NN NNS .596 G:capital-all .584
</p>
<p>G:comparative .896 G:pres-participle .566 VBZ VB .580
verb inf - 3pSg .843 X is opp. dir. from Y .535 G:comparative .578
</p>
<p>G:plural .841 verb inf - Ving .496 VB VBZ .573
noun - plural reg .835 G:city-in-state .486 G:nationality-adj .548
</p>
<p>VB VBZ .818 NNS NN .484 JJS JJR .536
verb inf - Ving .783 verb Ving - Ved .478 JJR JJS .496
</p>
<p>VBZ VB .781 G:past-tense .463 VBD VB .470
G:city-in-state .774 X, Y same category .462 VBD VBZ .469
</p>
<p>G:pres-participle .755 antonyms - binary .436 VBZ VBD .465
G:plural-verbs .752 G:plural-verbs .371 verb inf - Ved .465
</p>
<p>G:past-tense .739 G:nationality-adj .369 VB VBD .443
G:superlative .713 G:capital-all .361 JJ JJR .443
</p>
<p>NN NNS .710 things - color .340 JJ JJS .426
VBD VB .677 verb Ving - 3pSg .336 adj - comparative .422
</p>
<p>verb Ving - Ved .670 can’t X and Y at same time .320 verb 3pSg - Ved .400
noun - plural irreg .662 G:comparative .317 G:plural-verbs .381
verb Ving - 3pSg .661 male - female .317 adj - superlative .373
</p>
<p>JJ JJR .659 antonyms - gradable .306 name - occupation .340
JJS JJR .653 G:opposite .292 verb Ving - 3pSg .325
</p>
<p>NNS NN .626 X, Y two kinds in category .283 JJR JJ .321
VBD VBZ .623 X and Y are contrary .279 G:gender .316
</p>
<p>VB VBD .621 un+adj reg .268 name - nationality .312
verb inf - Ved .604 country - capital .250 G:city-in-state .288
</p>
<p>VBZ VBD .571 X, Y similar type of thing .245 verb inf - Ving .287
adj - comparative .570 VB VBZ .245 country - language .278
</p>
<p>male - female .557 verb inf - 3pSg .239 G:past-tense .276
verb 3pSg - Ved .553 X will become Y .239 G:currency .246
</p>
<p>JJR JJS .543 JJ JJR .217 male - female .240
JJ JJS .520 G:adj-to-adverb .208 verb+tion irreg .240
</p>
<p>adj - superlative .468 VBD VB .207 verb+ment irreg .231
JJR JJ .437 re+verb reg .207 JJS JJ .228
</p>
<p>X is opp. dir. from Y .421 VBZ VB .201 UK city - county .219
G:adj-to-adverb .402 G:capital .200 G:adj-to-adverb .195
</p>
<p>name - occupation .389 synonyms - exact .199 adj+ly reg .192
JJS JJ .376 VB VBD .178 verb Ving - Ved .192
</p>
<p>...
...
</p>
<p>...
</p>
<p>Table 3: The top 40 categories for reciprocal rank using 3COSADD (RR), baseline reciprocal rank (BRR),
and reciprocal rank gain (RRG = RR − BRR) as calculated from embeddings trained on Wikipedia text
using word2vec. Categories based on inflectional morphology are in plain text, derivational morphology
in italics, named entity semantics in bold, and lexical in bold italic. Sources for analogy questions can be
identified from category names: those starting with ‘G:’ are from the Google set; in all capital letters, the
Microsoft set; with reference to ‘X’ and ‘Y’, the SemEval set; all others, BATS. Some category names
are abbreviated from their original names.
</p>
<p>11</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 12–21,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Learning Antonyms with Paraphrases
and a Morphology-Aware Neural Network
</p>
<p>Sneha Rajana∗ Chris Callison-Burch∗ Marianna Apidianaki∗Ψ Vered ShwartzΦ
∗Computer and Information Science Department, University of Pennsylvania, USA
</p>
<p>ΨLIMSI, CNRS, Université Paris-Saclay, 91403 Orsay
ΦComputer Science Department, Bar-Ilan University, Israel
</p>
<p>{srajana,ccb,marapi}@seas.upenn.edu vered1986@gmail.com
</p>
<p>Abstract
</p>
<p>Recognizing and distinguishing antonyms
from other types of semantic relations is
an essential part of language understand-
ing systems. In this paper, we present a
novel method for deriving antonym pairs
using paraphrase pairs containing negation
markers. We further propose a neural net-
work model, AntNET, that integrates mor-
phological features indicative of antonymy
into a path-based relation detection algo-
rithm. We demonstrate that our model
outperforms state-of-the-art models in dis-
tinguishing antonyms from other semantic
relations and is capable of efficiently han-
dling multi-word expressions.
</p>
<p>1 Introduction
</p>
<p>Identifying antonymy and expressions with con-
trasting meanings is valuable for NLP systems
which go beyond recognizing semantic related-
ness and require to identify specific semantic re-
lations. While manually created semantic tax-
onomies, like WordNet (Fellbaum, 1998), define
antonymy relations between some word pairs that
native speakers consider antonyms, they have lim-
ited coverage. Further, as each term of an antony-
mous pair can have many semantically close
terms, the contrasting word pairs far outnum-
ber those that are commonly considered antonym
pairs, and they remain unrecorded. Therefore,
automated methods have been proposed to deter-
mine for a given term-pair (x, y), whether x and y
are antonyms of each other, based on their occur-
rences in a large corpus.
</p>
<p>Charles and Miller (1989) put forward the co-
occurrence hypothesis that antonyms occur to-
gether in a sentence more often than chance. How-
ever, non-antonymous semantically related words
</p>
<p>Paraphrase Pair Antonym Pair
not sufficient/insufficient sufficient/insufficient
insignificant/negligible significant/negligible
</p>
<p>dishonest/lying honest/lying
unusual/pretty strange usual/pretty strange
</p>
<p>Table 1: Examples of antonyms derived from
PPDB paraphrases. The antonym pairs in column
2 were derived from the corresponding paraphrase
pairs in column 1.
</p>
<p>such as hypernyms, holonyms, meronyms, and
near-synonyms also tend to occur together more
often than chance. Thus, separating antonyms
from pairs linked by other relationships has proven
to be difficult. Approaches to antonym detec-
tion have exploited distributional vector represen-
tations relying on the distributional hypothesis of
semantic similarity (Harris, 1954; Firth, 1957) that
words co-occurring in similar contexts tend to be
semantically close. Two main information sources
are used to recognize semantic relations: path-
based and distributional. Path-based methods con-
sider the joint occurrences of the two terms in
a given sentence and use the dependency paths
that connect the terms as features (Hearst, 1992;
Roth and Schulte im Walde, 2014; Schwartz et al.,
2015). For distinguishing antonyms from other re-
lations, Lin et al. (2003) proposed to use antonym
patterns (such as either X or Y and from X to
Y ). Distributional methods are based on the dis-
joint occurrences of each term and have recently
become popular using word embeddings (Mikolov
et al., 2013; Pennington et al., 2014) which pro-
vide a distributional representation for each term.
Recently, combined path-based and distributional
methods for relation detection have also been
proposed (Shwartz et al., 2016; Nguyen et al.,
2017). They showed that a good path representa-
</p>
<p>12</p>
<p />
</div>
<div class="page"><p />
<p>tion can provide substantial complementary infor-
mation to the distributional signal for distinguish-
ing between different semantic relations.
</p>
<p>While antonymy applies to expressions that
represent contrasting meanings, paraphrases are
phrases expressing the same meaning, which usu-
ally occur in similar textual contexts (Barzilay and
McKeown, 2001) or have common translations
in other languages (Bannard and Callison-Burch,
2005). Specifically, if two words or phrases are
paraphrases, they are unlikely to be antonyms of
each other. Our first approach to antonym de-
tection exploits this fact and uses paraphrases for
detecting and generating antonyms (The demen-
tors caught Sirius Black/ Black could not escape
the dementors). We start by focusing on phrase
pairs that are most salient for deriving antonyms.
Our assumption is that phrases (or words) contain-
ing negating words (or prefixes) are more help-
ful for identifying opposing relationships between
term-pairs. For example, from the paraphrase pair
(caught/not escape), we can derive the antonym
pair (caught/escape) by just removing the negat-
ing word ‘not’.
</p>
<p>Our second method is inspired by the recent
success of deep learning methods for relation de-
tection. Shwartz et al. (2016) proposed an inte-
grated path-based and distributional model to im-
prove hypernymy detection between term-pairs,
and later extended it to classify multiple semantic
relations (Shwartz and Dagan, 2016) (LexNET).
Although LexNET was the best performing sys-
tem in the semantic relation classification task of
the CogALex 2016 shared task, the model per-
formed poorly on synonyms and antonyms com-
pared to other relations. The path-based compo-
nent is weak in recognizing synonyms, which do
not tend to co-occur, and the distributional infor-
mation caused confusion between synonyms and
antonyms, since both tend to occur in the same
contexts. We propose AntNET, a novel exten-
sion of LexNET that integrates information about
negating prefixes as a new morphological pat-
tern feature and is able to distinguish antonyms
from other semantic relations. In addition, we op-
timize the vector representations of dependency
paths between the given term pair, encoded using
a neural network, by replacing the embeddings of
words with negating prefixes by the embeddings
of the base, non-negated, forms of the words.
For example, for the term pair unhappy/joyful,
</p>
<p>we record the negating prefix of unhappy using
a new path feature and replace the word embed-
ding of unhappy with happy in the vector represen-
tation of the dependency path between unhappy
and sad. The proposed model improves the path
embeddings to better distinguish antonyms from
other semantic relations and gets higher perfor-
mance than prior path-based methods on this task.
We used the antonym pairs extracted from the
Paraphrase Database (PPDB) (Ganitkevitch et al.,
2013; Pavlick et al., 2015b) in the paraphrase-
based method as training data for our neural net-
work model.
</p>
<p>The main contributions of this paper are:
</p>
<p>• We present a novel technique of using para-
phrases for antonym detection and success-
fully derive antonym pairs from paraphrases
in the PPDB, the largest paraphrase resource
currently available.
</p>
<p>• We demonstrate improvements to an inte-
grated path-based and distributional model,
showing that our morphology-aware neural
network model, AntNET, performs better
than state-of-the-art methods for antonym de-
tection.
</p>
<p>2 Related Work
</p>
<p>Paraphrase Extraction Methods Paraphrases
are words or phrases expressing the same mean-
ing. Paraphrase extraction methods that exploit
distributional or translation similarity might how-
ever propose paraphrase pairs that are not mean-
ing equivalent but linked by other types of re-
lations. These methods often extract pairs hav-
ing a related but not equivalent meaning, such as
contradictory pairs. For instance, Lin and Pan-
tel (2001) extracted 12 million “inference rules"
from monolingual text by exploiting shared depen-
dency contexts. Their method learns paraphrases
that are truly meaning equivalent, but it just as
readily learns contradictory pairs such as (X rises,
X falls). Ganitkevitch et al. (2013) extract over
150 million paraphrase rules from parallel cor-
pora by pivoting through foreign translations. This
multilingual paraphrasing method often learns hy-
pernym/hyponym pairs, due to variation in the
discourse structure of translations, and unrelated
pairs due to misalignments or polysemy in the for-
eign language. Pavlick et al. (2015a) added inter-
pretable semantics to PPDB (see Section 3.1 for
</p>
<p>13</p>
<p />
</div>
<div class="page"><p />
<p>Method #pairs
(x,y) from paraphrase (x̃,y)/(x,ỹ) 80,669
</p>
<p>(x, paraphrase(y)), (paraphrase(x), y) 81,221
(x, synset(y)), (synset(x), y) 692,231
</p>
<p>Table 2: Number of unique antonym pairs derived
from PPDB at each step. Paraphrases and synsets
were obtained from PPDB and WordNet respec-
tively.
</p>
<p>details) and showed that paraphrases in this re-
source represent a variety of relations other than
equivalence, including contradictory pairs like no-
body/someone and close/open.
</p>
<p>Pattern-based Methods Pattern-based methods
for inducing semantic relations between a pair of
terms (x, y) consider the lexico-syntactic paths
that connect the joint occurrences of x and y in
a large corpus. A variety of approaches have been
proposed that rely on patterns between terms in
a corpus to distinguish antonyms from other rela-
tions. Lin et al. (2003) used translation informa-
tion and lexico-syntactic patterns to extract dis-
tributionally similar words, and then filtered out
words that appeared with the patterns ‘from X to
Y’ or ‘either X or Y’ significantly often. The in-
tuition behind this was that if two words X and Y
appear in one of these patterns, they are unlikely to
represent a synonymous pair. Roth and Schulte im
Walde (2014) combined general lexico-syntactic
patterns with discourse markers as indicators for
the specific semantic relations between word pairs
(e.g. contrast relations might indicate antonymy
and elaborations may indicate synonymy or hy-
ponymy). Unlike previous pattern-based methods
which relied on the standard distribution of pat-
terns, Schwartz et al. (2015) used patterns to learn
word embeddings. They presented a symmetric
pattern-based model for representing word vectors
in which antonyms are assigned to dissimilar vec-
tor representations. More recently, Nguyen et al.
(2017) presented a pattern-based neural network
model that exploits lexico-syntactic patterns from
syntactic parse trees for the task of distinguishing
between antonyms and synonyms. They applied
HypeNET Shwartz et al. (2016) to the task of dis-
tinguishing between synonyms and antonyms, re-
placing the direction feature with the distance in
the path representation.
</p>
<p>Source #pairs
WordNet 18,306
</p>
<p>PPDB 773,452
</p>
<p>Table 3: Number of unique antonym pairs derived
from different sources. The number of pairs ob-
tained from PPDB far outnumbers the antonym
pairs present in EVALution and WordNet.
</p>
<p>3 Paraphrase-Based Antonym
Derivation
</p>
<p>Existing semantic resources like WordNet (Fell-
baum, 1998) contain a much smaller set of
antonyms compared to other semantic relations
(synonyms, hypernyms and meronyms). Our
aim is to create a large resource of high quality
antonym pairs using paraphrases.
</p>
<p>3.1 The Paraphrase Database
</p>
<p>The Paraphrase Database (PPDB) contains over
150 million paraphrase rules covering three para-
phrase types: lexical (single word), phrasal (multi-
word), and syntactic restructuring rules, and is the
largest collection of paraphrases currently avail-
able. PPDB . In this paper, we focus on lexical and
phrasal paraphrases up to two words in length. We
examine the relationships between phrase pairs in
the PPDB focusing on phrase pairs that are most
salient for deriving antonyms.
</p>
<p>3.2 Antonym Derivation
</p>
<p>Selection of Paraphrases We consider all
phrase pairs from PPDB (p1, p2) up to two words
in length such that one of the two phrases either
begins with a negating word like not, or contains
a negating prefix.1 We chose these two types of
paraphrase pairs since we believe them to be the
most indicative of an antonymy relationship be-
tween the target words. There are 7,878 unordered
phrase pairs of the form (p′1, p2) where p′1 be-
gins with ‘not’, and 183,159 phrases of the form
(p′1, p2) where p′1 contains a negating prefix.
</p>
<p>Paraphrase Transformation For paraphrases
containing a negating prefix, we perform morpho-
logical analysis to identify and remove the negat-
ing prefixes. For a phrase pair like unhappy/sad,
an antonymy relation is derived between the base
form of the negated word, without the negation
prefix, and its paraphrase (happy/sad). We use
</p>
<p>1Negating prefixes include de, un, in, anti, il, non, dis
</p>
<p>14</p>
<p />
</div>
<div class="page"><p />
<p>Unrelated Paraphrases Categories Entailment Other relation
much/worthless correct/that’s right Japan/Korea investing/ twinkle/dark
</p>
<p>increased investment
disability/present simply/merely black/red efficiency/ naw/not gonna
</p>
<p>operational efficiency
equality/gap till/until Jan/Feb valid/equally valid access/available
</p>
<p>Table 4: Examples of different types of non-antonyms derived from PPDB.
</p>
<p>MORSEL (Lignos, 2010) to perform morpholog-
ical analysis and identify negation markers. For
multi-word phrases with a negating word, the
negating word is simply dropped to obtain an
antonym pair (e.g. different/not identical → dif-
ferent/identical). Some examples of PPDB para-
phrase pairs and antonym pairs derived from them
are shown in Table 1. The derived antonym pairs
are further expanded by associating the synonyms
(from WordNet) and lexical paraphrases (from
PPDB) of each phrase with the other phrase in
the derived pair. While expanding each phrase
in the derived pair by its paraphrases, we filter
out paraphrase pairs with a PPDB score (Pavlick
et al., 2015a) of less than 2.5. In the above ex-
ample, unhappy/sad, we first derive happy/sad as
an antonym pair and expand it by considering all
synonyms of happy as antonyms of sad (e.g. joy-
ful/sad), and all synonyms of sad as antonyms
of happy (e.g. happy/gloomy). Table 2 shows
the number of pairs derived at each step using
PPDB. In total, we were able to derive around
773K unique pairs from PPDB. This is a much
larger dataset than existing resources like Word-
Net and EVALution as shown in Table 3.
</p>
<p>Analysis We performed a manual evaluation of
the quality of the extracted antonyms by randomly
selecting 1000 pairs classified as ‘antonym’ and
observed that the dataset contained about 63%
antonyms. Errors mostly consisted of phrases and
words that do not have an opposing meaning after
the removal of the negation pattern. For example,
the equivalent pair till/until that was derived from
the PPDB paraphrase rule not till/until. Other non-
antonyms derived from the above methods can be
classified into unrelated pairs (background/figure),
paraphrases or pairs that have an equivalent mean-
ing (admissible/permissible), words that belong to
a category (Africa/Asia), pairs that have an entail-
ment relation (valid/equally valid) and pairs that
are related but not with an antonym relationship
(twinkle/dark). Table 4 gives some examples of
</p>
<p>categories of non-antonyms.
</p>
<p>Annotation Since the pairs derived from PPDB
seemed to contain a variety of relations in addi-
tion to antonyms, we crowdsourced the task of la-
belling a subset of these pairs in order to obtain the
true labels.2 We asked workers to choose between
the labels: antonym, synonym (or paraphrase for
multi-word expressions), unrelated, other, entail-
ment, and category. We showed each pair to 3
workers, taking the majority label as truth.
</p>
<p>4 LSTM-Based Antonym Detection
</p>
<p>In this section we describe AntNET, a long short
term memory (LSTM) based, morphology-aware
neural network model for antonym detection. We
first focus on improving the neural embeddings of
the path representation (Section 4.1), and then in-
tegrate distributional signals into our network re-
sulting in a combined method (Section 4.2).
</p>
<p>4.1 Path-Based Network
</p>
<p>Similarly to prior work, we represent each de-
pendency path as a sequence of edges that leads
from x to y in the dependency tree. We use
the same path-based features proposed by Shwartz
et al. (2016) for recognizing hypernym relations:
lemma and part-of-speech (POS) tag of the source
node, the dependency label, and the edge direction
between two subsequent nodes. Additionally, we
also add a new feature that indicates whether the
source node is negated.
</p>
<p>Rather than treating an entire dependency path
as a single feature, we encode the sequence
of edges using a long short term memory net-
work (Hochreiter and Schmidhuber, 1997). The
vectors obtained for the different paths of a given
(x, y) pair are pooled, and the resulting vector is
used for classification. The overall network struc-
ture is depicted in Figure 1.
</p>
<p>25884 pairs were randomly chosen and were annotated on
www.crowdflower.com
</p>
<p>15</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Illustration of the AntNET model. Each pair is represented by several paths and each path
is a sequence of edges. An edge consists of five features: lemma, POS, dependency label, dependency
direction, and negation marker.
</p>
<p>Edge Representation We denote each edge as
lemma/pos/dep/dir/neg. We are only inter-
ested in checking if x and/or y have negation
markers but not the intermediate edges since nega-
tion information for intermediate lemmas is un-
likely to contribute to identifying whether there is
an antonym relationship between x and y. Hence,
in our model, neg is represented in one of three
ways: negated if x or y is negated, not-negated if
x or y is not negated, and unavailable for the inter-
mediate edges. If the source node is negated, we
replace the lemma by the lemma of its base, non-
negated, form. For example, if we identified un-
happy as a ‘negated’ word, we replace the lemma
embedding of unhappy by the embedding of happy
in the path representation. The negation feature
will help in separating antonyms from other se-
mantic relations, especially those that are hard to
distinguish from, like synonyms.
</p>
<p>The replacement of a negated word’s embed-
ding by its base form’s embedding is done for a
few reasons. First, words and their polar antonyms
are more likely to co-occur in sentences compared
to words and their negated forms. For example,
Neither happy nor sad is probably a more com-
mon phrase than Neither happy nor unhappy, so
this technique will help our model to identify an
opposing relationship between both types of pairs,
happy/unhappy and happy/sad. Second, a com-
mon practice for creating word embeddings for
multi-word expressions (MWEs) is by averaging
over the embeddings of each word in the expres-
sion. Ideally, this is not a good representation
</p>
<p>for phrases like not identical since we lose out on
the negating information obtained from not. In-
dicating the presence of not using a negation fea-
ture and replacing the embedding of not identical
by identical will increase the classifier’s probabil-
ity of identifying not identical/different as para-
phrases and identical/different as antonyms. And
finally, this method helps us distinguish between
terms that are seemingly negated but are not in re-
ality (e.g. invaluable). We encode the sequence
of edges using an LSTM network. The vectors
obtained for all the paths connecting x and y are
pooled and combined, and the resulting vector is
used for classification. The vector representation
of each edge is the concatenation of its feature vec-
tors:
</p>
<p>~vedge = [~vlemma, ~vpos, ~vdep, ~vdir, ~vneg]
</p>
<p>where ~vlemma, ~vpos, ~vdep, ~vdir, ~vneg represent the
vector embeddings of the negation marker, lemma,
POS tag, dependency label and dependency direc-
tion, respectively.
</p>
<p>Path Representation The representation for
a path p composed of a sequence of edges
edge1, edge2, .., edgek is a sequence of edge vec-
tors: p = [ ~edge1, ~edge2, ..., ~edgek]. The edge vec-
tors are fed in order to a recurrent neural network
(RNN) with LSTM units, resulting in the encoded
path vector ~vp.
</p>
<p>Classification Task Given a lexical or phrasal
pair (x, y) we induce patterns from a corpus where
each pattern represents a lexico-syntactic path
</p>
<p>16</p>
<p />
</div>
<div class="page"><p />
<p>connecting x and y. The vector representation for
each term pair (x, y) is computed as the weighted
average of its path vectors by applying average
pooling as follows:
</p>
<p>(1)~vp(x,y) =
</p>
<p>∑
p∈P (x,y)fp.~vp∑
</p>
<p>p∈P (x,y)fp
</p>
<p>~vp(x,y) refers to the vector of the pair (x, y);
P (x, y) is the multi-set of paths connecting x and
y in the corpus and fp is the frequency of p in
P (x, y). The vector ~vp(x,y) is then fed into a neu-
ral network that outputs the class distribution c for
each class (relation type), and the pair is assigned
to the relation with the highest score r:
</p>
<p>(2a)c = softmax(MLP (~vp(x,y))
(2b)r = argmaxic[i]
</p>
<p>MLP stands for Multi Layer Perceptron and can
be computed with or without a hidden layer (equa-
tions 4 and 5 respectively).
</p>
<p>(3)~h = tanh(W1.~vp(x,y) + b1)
</p>
<p>(4)MLP (~vp(x,y)) = W2.~h + b2
</p>
<p>(5)MLP (~vp(x,y)) = W1.~vp(x,y) + b1
</p>
<p>W refers to a matrix of weights that projects in-
formation between two layers; b is a layer-specific
vector of bias terms and ~h is the hidden layer.
</p>
<p>4.2 Combined Path-Based and Distributional
Network
</p>
<p>The path-based supervised model in Section 4.1
classifies each pair (x, y) based on the lexico-
syntactic patterns that connect x and y in a cor-
pus. Inspired by the improved performance of
Shwartz et al.’s (2016) integrated path-based and
distributional method over a simpler path-based
algorithm, we integrate distributional features into
our path-based network. We create a combined
vector representation using both the syntactic path
features and the co-occurrence distributional fea-
tures of x and y for each pair (x, y). The com-
bined vector representation for (x, y), ~vc(xy), is
computed by simply concatenating the word em-
beddings of x (~vx) and y (~vy) to the path-based
feature vector ~vp(x,y):
</p>
<p>(6)~vc(xy) = [~vx, ~vp(x,y), ~vy]
</p>
<p>5 Experiments
</p>
<p>We experiment with the path-based and combined
models for antonym identification by performing
two types of classification: binary and multiclass
classification.
</p>
<p>Train Test Val Total
5,122 1,829 367 7,318
</p>
<p>Table 5: Number of instances present in the
train/test/validation splits of the crowdsourced
dataset.
</p>
<p>5.1 Dataset
</p>
<p>Neural networks require a large amount of train-
ing data. We use the labelled portion of the dataset
that we created using PPDB, as described in Sec-
tion 3. In order to induce paths for the pairs in
the dataset, we identify sentences in the corpus
that contain the pair and extract all patterns for
the given pair. Pairs with an antonym relationship
are considered as positive instances in both clas-
sification experiments. In the binary classification
experiment, we consider all pairs related by other
relations (entailment, synonymy, category, unre-
lated, other) as negative instances. We also per-
form a variant of the multiclass classification with
three classes (antonym, other, unrelated). Due to
the skewed nature of the dataset, we combined cat-
egory, entailment and synonym/paraphrases into
one class. For both classification experiments, we
perform random split with 70% train, 25% test,
and 5% validation sets. Table 5 displays the num-
ber of relations in our dataset. Wikipedia3 was
used as the underlying corpus for all methods and
we perform model selection on the validation set
to tune the hyper-parameters of each method. We
apply grid search for a range of values and pick the
ones that yield the highest F1 score on the valida-
tion set. The best hyper-parameters are reported in
the appendix.
</p>
<p>5.2 Baselines
</p>
<p>Majority Baseline The majority baseline is
achieved by labelling all the instances with the
most frequent class occuring in the dataset i.e.
FALSE (binary) or UNRELATED (multiclass).
</p>
<p>3We used the English Wikipedia dump from May 2015 as
the corpus.
</p>
<p>17</p>
<p />
</div>
<div class="page"><p />
<p>Model Binary Multiclass
P R F1 P R F1
</p>
<p>Majority baseline 0.304 0.551 0.392 0.222 0.472 0.303
SP baseline 0.661 0.568 0.436 0.583 0.488 0.344
</p>
<p>Path-based SD baseline 0.723 0.724 0.722 0.636 0.675 0.651
Path-based AntNET 0.732 0.722 0.713 0.652 0.687 0.661**
</p>
<p>Combined SD baseline 0.790 0.788 0.788 0.744 0.750 0.738
Combined AntNET 0.803 0.802 0.802* 0.746 0.757 0.746*
</p>
<p>Table 6: Performance of the AntNET models in comparison to the baseline models.
</p>
<p>Feature Model Binary Multiclass
P R F1 P R F1
</p>
<p>Distance Path-based 0.727 0.727 0.724 0.665 0.692 0.664
Combined 0.789 0.788 0.788 0.732 0.743 0.734
</p>
<p>Negation Path-based 0.732 0.722 0.713 0.652 0.687 0.661
Combined 0.803 0.802 0.802 0.746 0.757 0.746
</p>
<p>Table 7: Comparing the novel negation marking feature with the distance feature proposed by Nguyen
et al. (2017).
</p>
<p>Distributed Baseline The method proposed by
Schwartz et al. (2015) uses symmetric patterns
(SPs) for generating word embeddings. The au-
thors automatically acquired symmetric patterns
(defined as a sequence of 3–5 tokens consisting of
exactly 2 wildcards and 1–3 words) from a large
plain-text corpus, and generated vectors where
each co-ordinate represented the co-occurrence in
symmetric patterns of the represented word with
another word of the vocabulary. For antonym rep-
resentation, the authors relied on the patterns sug-
gested by (Lin et al., 2003) to construct word em-
beddings containing an antonym parameter that
can be turned on in order to represent antonyms as
dissimilar, and that can be turned off to represent
antonyms as similar. To evaluate the SP method
on our data, we used the pre-trained SP embed-
dings4 with 500 dimensions. We use the SVM
classifier with RBF kernel for the classification of
word pairs.
</p>
<p>Path-based and Combined Baseline Since
AntNET is an extension of the path-based and
combined models proposed by (Shwartz and Da-
gan, 2016) for classifying multiple semantic rela-
tions, we use their models as additional baselines.
Because their model used a different dataset that
contained very few antonym instances, we repli-
</p>
<p>4https://homes.cs.washington.edu/
~roysch/papers/sp_embeddings/sp_
embeddings.html
</p>
<p>cated the baseline (SD) with the dataset and corpus
information as in Sectionn 5.1 rather than compar-
ing to the reported results.
</p>
<p>5.3 Results
</p>
<p>Table 6 displays the performance scores of
AntNET and the baselines in terms of precision,
recall and F1. Our combined model significantly5
</p>
<p>outperforms all baselines in both binary and mul-
ticlass classifications. Both path-based and com-
bined models of AntNET achieve a much better
performance in comparison to the majority class
and SP baselines.
</p>
<p>Comparing the path-based methods, the
AntNET model achieves a higher precision com-
pared to the path-based SD baseline for binary
classification, and outperforms the SD model in
precision, recall and F1 in the multiclass clas-
sification experiment. The low precision of the
SD model stems from its inability to distinguish
between antonyms and synonyms, and between
related and unrelated pairs which are common in
our dataset, causing many false positive pairs such
as difficult/harsh, bad/cunning, finish/far which
were classified as antonyms.
</p>
<p>Comparing the combined models, the AntNET
model outperforms the SD model in precision, re-
call and F1, achieving state-of-the-art results for
antonym detection. In all the experiments, the
</p>
<p>5We used paired t-test. *p &lt; 0.1, **p &lt; 0.05
</p>
<p>18</p>
<p />
</div>
<div class="page"><p />
<p>performance of the model in the binary classifi-
cation task was better than in the multiclass clas-
sification. Multiclass classification seems to be in-
herently harder for all methods, due to the large
number of relations and the smaller number of in-
stances for each relation. We also observed that as
we increased the size of the training dataset used
in our experiments, the results improved for both
path-based and combined models, confirming the
need for large-scale datasets that will benefit train-
ing neural models.
</p>
<p>Effect of the Negation-marking Feature In our
models, the novel negation marking feature is suc-
cessfully integrated along the syntactic path to rep-
resent the paths between x and y. In order to eval-
uate the effect of our novel negation-marking fea-
ture for antonym detection, we compare this fea-
ture to the distance feature proposed by Nguyen
et al. (2017). In their approach, they integrate
the distance between related words in a lexico-
syntactic path as a new pattern feature, along
with lemma, POS and dependency for the task
of distinguishing antonyms and synonyms. We
re-implemented this model by making use of the
same information regarding dataset and patterns as
in Section 5.1 and then replacing the direction fea-
ture in the SD models by the distance feature.
</p>
<p>The results are shown in Table 7 and indicate
that the negation marking feature and the replace-
ment of the embeddings of negated words by the
ones of their base forms enhance the performance
of our models more effectively than the distance
feature does, across both binary and multiclass
classifications. Although, the distance feature has
previously been shown to perform well for the task
of distinguishing antonyms from synonyms, this
feature is not very effective in the multiclass set-
ting.
</p>
<p>5.4 Error Analysis
Figure 2 displays the confusion matrices for the
binary and multiclass experiments of the best per-
forming AntNET model. The confusion matrix
shows that pairs were mostly assigned to the cor-
rect relation more than to any other class.
</p>
<p>False Positives We analyzed the false positives
from both the binary and multiclass experiments.
We sampled about 20% false positive pairs and
identified the following common errors. The ma-
jority of the misclassification errors stem from
antonym-like or near-antonym relations: these are
</p>
<p>Figure 2: Confusion matrices for the combined
AntNET model for binary (left) and multiclass
(right) classifications. Rows indicate gold labels
and columns indicate predictions. The matrix is
normalized along rows, so that the predictions for
each (true) class sum to 100%.
</p>
<p>relations that could be considered as antonymy but
were annotated by crowd-workers as other rela-
tions because they contain polysemous terms, for
which the relation holds in a specific sense. For
example: north/south and polite/sassy were la-
belled as category and other respectively. Other
errors stem from confusing antonyms and unre-
lated pairs.
</p>
<p>False Negatives We again sampled about 20%
false positive pairs from both the binary and mul-
ticlass experiments and analyzed the major types
of errors. Most of these pairs had only few co-
occurrences in the corpus often due to infrequent
terms (e.g. cisc/risc which define computer ar-
chitectures). While our model effectively handled
negative prefixes, it failed to handle negative suf-
fixes causing incorrect classification of pairs like
spiritless/spirited. A possible future work is to
simply extend this model to handle negative suf-
fixes as well.
</p>
<p>6 Conclusion
</p>
<p>In this paper, we presented an original technique
for deriving antonyms using paraphrases from
PPDB. We also proposed a novel morphology-
aware neural network model, AntNET, which im-
proves antonymy prediction for path-based and
combined models. In addition to lexical and syn-
tactic information, we suggested to include a novel
morphological negation-marking feature.
</p>
<p>Our models outperform the baselines in two re-
lation classification tasks. We also demonstrated
that the negation marking feature outperforms pre-
viously suggested path-based features for this task.
</p>
<p>19</p>
<p />
</div>
<div class="page"><p />
<p>Since our proposed techniques for antonymy de-
tection are corpus based, they can be applied to
different languages and relations. The paraphrase-
based method can be applied to other languages
by extracting the paraphrases for these languages
from the PPDB and using a morphological analy-
sis tool (e.g. Morfette for French (Chrupala et al.,
2008)) or by looking up the negation prefixes in a
grammar book for languages that do not dispose of
such a tool. The LSTM-based model could also be
used in other languages since the method is corpus
based, but we would need to create a training set
for new languages. This would not however be too
difficult; the training set used by the model is not
that big (the one used here was around 6000 pairs)
and could be easily labelled through crowdsourc-
ing.
</p>
<p>We release our code and the large-scale dataset
derived from PPDB, annotated with semantic rela-
tions.
</p>
<p>Acknowledgments
</p>
<p>This material is based in part on research spon-
sored by DARPA under grant number FA8750-
13-2-0017 (the DEFT program). The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes. The views
and conclusions contained in this publication are
those of the authors and should not be interpreted
as representing official policies or endorsements
of DARPA and the U.S. Government.
</p>
<p>This work has also been supported by the
French National Research Agency under project
ANR-16-CE33-0013 and partially supported by an
Intel ICRI-CI grant, the Israel Science Foundation
grant 880/12, and the German Research Founda-
tion through the German-Israeli Project Coopera-
tion (DIP, grant DA 1600/1-1).
</p>
<p>We would like to thank our anonymous review-
ers for their thoughtful and helpful comments.
</p>
<p>References
Colin Bannard and Chris Callison-Burch. 2005. Para-
</p>
<p>phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics (ACL’05). Strouds-
burg, PA, pages 597–604.
</p>
<p>Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics (ACL’01). Toulouse,
France, pages 50–57.
</p>
<p>Walter G. Charles and George A. Miller. 1989. Con-
texts of antonymous adjectives. Applied Psychology
10:357–375.
</p>
<p>Grzegorz Chrupala, Georgiana Dinu, and Josef van
Genabith. 2008. Learning Morphology with Mor-
fette. In Proceedings of the Sixth International
Conference on Language Resources and Evalua-
tion (LREC’08). Marrakech, Morocco, pages 2362–
2367.
</p>
<p>Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
</p>
<p>J. R. Firth. 1957. A synopsis of linguistic theory, 1930–
1955. In Studies in Linguistic Analysis, Basil Black-
well, Oxford, United Kingdom, pages 1–32.
</p>
<p>Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL/HLT). Atlanta, Geor-
gia, pages 758–764.
</p>
<p>Zellig S. Harris. 1954. Distributional structure. Word
10(23):146–162.
</p>
<p>Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th International Conference on Compu-
tational Linguistics (COLING’92). Nantes, France,
pages 539–545.
</p>
<p>Sepp Hochreiter and Jurgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.
</p>
<p>Constantine Lignos. 2010. Learning from Unseen
Data. In Proceedings of the Morpho Challenge 2010
Workshop. Aalto University School of Science and
Technology, Helsinki, Finland, pages 35–38.
</p>
<p>Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of Inference Rules from Text. In Proceedings
of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining
(KDD’01). San Francisco, California, pages 323–
328.
</p>
<p>Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In Proceedings of the Eigh-
teenth International Joint Conference on Artificial
Intelligence (IJCAI ’03). Acapulco, Mexico, pages
1492–1493.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems
(NIPS’13). Lake Tahoe, Nevada, pages 3111–3119.
</p>
<p>20</p>
<p />
</div>
<div class="page"><p />
<p>Kim Anh Nguyen, Sabine Schulte im Walde, and
Ngoc Thang Vu. 2017. Distinguishing antonyms
and synonyms in a pattern-based neural network. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL’17). Valencia, Spain, pages 76–85.
</p>
<p>Ellie Pavlick, Johan Bos, Malvina Nissim, Charley
Beller, Benjamin Van Durme, and Chris Callison-
Burch. 2015a. Adding Semantics to Data-Driven
Paraphrasing. In The 53rd Annual Meeting
of the Association for Computational Linguistics
(ACL’15). Beijing, China, pages 1512–1522.
</p>
<p>Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevich,
and Chris Callison-Burch Ben Van Durme. 2015b.
PPDB 2.0: Better paraphrase ranking, fine-grained
entailment relations, word embeddings, and style
classification. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL’15). Beijing, China, pages 425–430.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global Vectors for Word
Representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP’14). Doha, Qatar, pages 1532–
1543.
</p>
<p>Michael Roth and Sabine Schulte im Walde. 2014.
Combining Word Patterns and Discourse Markers
for Paradigmatic Relation Classification. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL’14). Bal-
timore, MD, pages 524–530.
</p>
<p>Roy Schwartz, Roi Reichart, and Ari Rappoport. 2015.
Symmetric Pattern Based Word Embeddings for Im-
proved Word Similarity Prediction. In Proceed-
ings of the Nineteenth Conference on Computational
Natural Language Learning (CoNLL’15). Beijing,
China, pages 258–267.
</p>
<p>Vered Shwartz and Ido Dagan. 2016. CogALex-V
Shared Task: LexNET - Integrated Path-based and
Distributional Method for the Identification of Se-
mantic Relations. In Proceedings of the 5th Work-
shop on Cognitive Aspects of the Lexicon (CogALex-
V). Osaka, Japan, pages 80–85.
</p>
<p>Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving Hypernymy Detection with an Integrated
Path-based and Distributional Method. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (ACL’16).
Berlin, Germany, pages 2389–2398.
</p>
<p>A Supplemental Material
</p>
<p>For deriving antonyms using PPDB, we used
the XXXL size of PPDB version 2.0 found in
http://paraphrase.org/.
</p>
<p>To compute the metrics in Tables 6 and 7, We
used scikit-learn with the "averaged setup", which
</p>
<p>computes the metrics for each relation and reports
their average weighted by support (the number of
true instances for each relation). Note that it can
result in a F1 score that is not the harmonic mean
of precision and recall.
</p>
<p>During preprocessing we handled removal of
punctuation. Since our dataset only contains short
phrases, we removed any stop words occurring at
the beginning of a sentence (Example: a man →
man) and we also removed plurals. The best hy-
perparameters for all models mentioned in this pa-
per are shown in Table 8. The learning rate was
set to 0.001 for all experiments.
</p>
<p>Model Type Dropout
SD-path Binary 0.2
SD-path Multiclass 0.4
</p>
<p>SD-combined Binary 0.4
SD-combined Multiclass 0.2
</p>
<p>ASD-path Binary 0.0
ASD-path Multiclass 0.2
</p>
<p>ASD-combined Binary 0.0
ASD-combined Multiclass 0.2
AntNET-path Binary 0.0
AntNET-path Multiclass 0.2
</p>
<p>AntNET-combined Binary 0.4
AntNET-combined Multiclass 0.2
</p>
<p>Table 8: The best hyper-parameters in every
model.
</p>
<p>21</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 22–32,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Decoding Sentiment from Distributed Representations of Sentences
</p>
<p>Edoardo Maria Ponti
ep490@cam.ac.uk
</p>
<p>Ivan Vulić
iv250@cam.ac.uk
</p>
<p>Language Technology Lab, University of Cambridge
</p>
<p>Anna Korhonen
alk23@cam.ac.uk
</p>
<p>Abstract
</p>
<p>Distributed representations of sentences
have been developed recently to represent
their meaning as real-valued vectors. How-
ever, it is not clear how much information
such representations retain about the po-
larity of sentences. To study this question,
we decode sentiment from unsupervised
sentence representations learned with dif-
ferent architectures (sensitive to the order
of words, the order of sentences, or none)
in 9 typologically diverse languages. Senti-
ment results from the (recursive) composi-
tion of lexical items and grammatical strate-
gies such as negation and concession. The
results are manifold: we show that there
is no ‘one-size-fits-all’ representation ar-
chitecture outperforming the others across
the board. Rather, the top-ranking archi-
tectures depend on the language and data
at hand. Moreover, we find that in several
cases the additive composition model based
on skip-gram word vectors may surpass su-
pervised state-of-art architectures such as
bidirectional LSTMs. Finally, we provide a
possible explanation of the observed varia-
tion based on the type of negative construc-
tions in each language.
</p>
<p>1 Introduction
</p>
<p>Distributed representations of sentences are usu-
ally acquired in an unsupervised fashion from raw
texts. Those inferred from different algorithms are
prone to grasp parts of their meaning and disregard
others. Representations have been evaluated thor-
oughly, both intrinsically (interpretation through
distance measures) and extrinsically (performance
on downstream tasks). Moreover, several methods
have been considered, based on both the compo-
</p>
<p>sition of word embeddings (Milajevs et al., 2014;
Marelli et al., 2014; Sultan et al., 2015) and direct
generation (Hill et al., 2016). The evaluation was
focused solely on English, and it rarely concerned
other languages (Adi et al., 2017; Conneau et al.,
2017). As a consequence, many ‘core’ methods
to learn distributed sentence representations are
largely under-explored in a variety of typologically
diverse languages, and still lack a demonstration of
their usefulness in actual downstream tasks.
</p>
<p>In this work, we study how well distributed sen-
tence representations capture the polarity of a sen-
tence. To this end, we choose the Sentiment Anal-
ysis task as an extrinsic evaluation protocol: it di-
rectly detects the polarity of a text, where polarity
is defined as the attitude of the speaker with respect
to the whole content of the string or one of the enti-
ties mentioned therein. This attitude is measured
quantitatively on a scale spanning from negative
to positive with arbitrary granularity. As such, po-
larity consists in a crucial part of the meaning of a
sentence, which should not be lost.
</p>
<p>The polarity of a sentence depends heavily on a
complex interaction between lexical items endowed
with an intrinsic polarity, and morphosyntactic con-
structions altering polarity, most notably negation
and concession. The interaction is deemed to be
recursive, hence some approaches take into account
word order and phrase boundaries in order to apply
the correct composition (Socher et al., 2013). How-
ever, some languages lack continuous constituents:
contiguous spans of words do not correspond to
syntactic subtrees, making composition unreliable
(Ponti, 2016). Moreover, the expression of negation
varies across languages, as demonstrated by works
in Linguistic Typology (Dahl, 1979, inter alia). In
particular, negation can appear as a bounded mor-
pheme or a free morpheme; it can precede or follow
the verb; it can ‘agree’ or not in polarity with indef-
inite pronouns; it can alter the expression of verbal
</p>
<p>22</p>
<p />
</div>
<div class="page"><p />
<p>categories (e.g. tense, aspect, or modality).
We explore a series of methods endowed with
</p>
<p>different features: some hinge upon word order,
others on sentence order, others on neither. We
evaluate these unsupervised representations using
a Multi-Layer Perceptron which uses the gener-
ated sentence representations as input and predicts
sentiment classes (positive vs. negative) as output.
Training and evaluation are based on a collection
of annotated databases. Owing to the variety of
methods and languages, we expect to observe a
variation in the performance correlated with the
properties of both.
</p>
<p>Moreover, we establish a ceiling to the possible
performances of our method based on decoding
unsupervised distributed representations. In fact,
we offer a comparison between this and supervised
deep learning architectures that achieve state-of-art
scores in the Sentiment Analysis task. In particular,
we also evaluate a bi-directional LSTM (Li et al.,
2015) on the same task. These models have ad-
vantage over distributed representations as: i) they
are specialised on a single task rather than built as
general-purpose representations; ii) their recurrent
nature allows to capture the sequential composition
of polarity in a sentence. However, since training
these models requires large amounts of annotated
data, resource scarcity in other languages hampers
their portability.
</p>
<p>The aim of this work is to assess which algo-
rithm for distributed sentence representations is
the most appropriate for capturing polarity in a
given language. Moreover, we study how language-
specific properties have an impact on performance,
finding an explanation in Language Typology. We
also provide an in-depth analysis of the most rele-
vant features by visualising the activation of hidden
neurons. This will hopefully contribute to advanc-
ing the Sentiment Analysis task in the multilingual
scenarios. In § 2, we survey prior work on multi-
lingual sentiment analysis. Afterwards, we present
the tested algorithms for generating distributed rep-
resentations of sentences in § 3. In § 4, we sketch
the dataset and the experimental setup. Finally, §
5 examines the results in light of the sensitivity of
the algorithms and the typology of negation.
</p>
<p>2 Multilingual Sentiment Analysis
</p>
<p>The task of sentiment classification is mostly ad-
dressed through supervised approaches. However,
these achieve unsatisfactory results in resource-lean
</p>
<p>languages because of the scarcity of resources to
train dedicated models (Denecke, 2008). This af-
flicts state-of-art deep learning architectures even
more compared to traditional machine learning al-
gorithms (Chen et al., 2016). As a consequence,
previous work resorted to i) language transfer or
ii) joint multilingual learning. The former adapts
models from a source resource-rich language to a
target resource-poor language; the latter infers a sin-
gle model portable across languages. Approaches
based on distributed representations induced in an
unsupervised fashion do not face the difficulty re-
sulting from resource scarcity: they are portable
to other tasks and languages. In this section we
survey deep learning techniques, adaptive models,
and unsupervised distributed representations for
sentiment classification in a multilingual scenario.
The last approach is the focus of this work.
</p>
<p>Deep learning algorithms for sentiment classifi-
cation are designed to deal with compositionality.
Hence, they often rely on recurrent networks trac-
ing the sequential history of a sentence, or special
compositional devices. Recurrent models include
bi-directional LSTMs (Li et al., 2015), possibly
enriched with context (Mousa and Schuller, 2017).
On the other hand, Socher et al. (2013) put forth
a Recursive Neural Tensor Network, which com-
poses representations recursively through a single
tensor-based composition function. Subsequent
improvements of this line of research include the
Structural Attention Neural Networks (Kokkinos
and Potamianos, 2017), which adds structural in-
formation around each node of a syntactic tree.
</p>
<p>When supervised monolingual models are not
feasible, language transfer can bridge between mul-
tiple languages, for instance through supervised la-
tent Dirichlet allocation (Boyd-Graber and Resnik,
2010). Direct transfer relies on word-aligned par-
allel texts where the source language text is either
manually or automatically annotated. The senti-
ment information is then projected onto the tar-
get text (Almeida et al., 2015), also leveraging
non-parallel data (Zhou et al., 2015). Chen et al.
(2016) devised a multi-task network where an ad-
versarial branch spurs the shared layers to learn
language-independent features. Finally, Lu et al.
(2011) learned from annotated examples in both
the source and target language. Alternatively, sen-
tences from other languages are translated into En-
glish and assigned a sentiment based on lexical
resources (Denecke, 2008) or supervised methods
</p>
<p>23</p>
<p />
</div>
<div class="page"><p />
<p>(Balahur and Turchi, 2014).
Finally, cross-lingual sentiment classification
</p>
<p>can leverage on shared distributed representations.
Zhou et al. (2016) captured shared high-level fea-
tures across aligned sentences through autoen-
coders. In this latent space, distances were op-
timised to reflect differences in sentiment. On the
other hand, Fernández et al. (2015) exploited bilin-
gual word representations, where vector dimen-
sions mirror the distributional overlap with respect
to a pivot. Le and Mikolov (2014) concatenated
sentence representations obtained through variants
of Paragraph Vector and trained a Logistic Regres-
sion model on top of them.
</p>
<p>Previous studies thus demonstrated that sen-
tence representations retain information about po-
larity, and that they partly alleviate the drawbacks
of deep architectures (single-purposed and data-
demanding). Hence, the Sentiment Analysis tasks
seems convenient to compare different sentence
representation architectures. Nonetheless, a sys-
tematic evaluation has never taken place for this
task, and a large-scale study over typologically di-
verse languages has not been attempted for any of
the algorithms reviewed. We intend to fill these
gaps, considering the methods to generate sentence
representations outlined in the next section.
</p>
<p>3 Distributed Sentence Representations
</p>
<p>Word vectors can be combined through various
compositional operations to obtain representations
of phrases and sentences. Mitchell and Lapata
(2010) explored two operations: addition and mul-
tiplication. Notwithstanding their simplicity, they
are hardly outperformed by more sophisticated op-
erations (Rimell et al., 2016). Some of these com-
positional representations based on matrix multipli-
cation were also evaluated on sentiment classifica-
tion (Yessenalina and Cardie, 2011). Alternatively,
sentence representations can be induced directly
with no intermediate step at the word level. In this
paper, we focus on sentence representations that are
generated in an unsupervised fashion. Furthermore,
they are ‘fixed’, that is, they are not fine-tuned for
any particular downstream task, since we are inter-
ested in their intrinsic content.1
</p>
<p>1This excludes methods concerned with phrases, like the
ECO embeddings (Poliak et al., 2017), or requiring structured
knowledge, like CHARAGRAM (Wieting et al., 2016a).
</p>
<p>3.1 Algorithms
We explore several methods to generate sentence
representations. One exploits a compositional op-
eration (addition) over word representations stem-
ming from a Skip-Gram model (§ 3.1.1). Others are
direct methods, including FastSent (§ 3.1.2), a Se-
quential Denoising AutoEncoder (SDAE, § 3.1.3)
and Paragraph Vector (§ 3.1.4). Note that FastSent
relies on sentence order, SDAE on word order, and
Paragraph Vector on neither. All these algorithms
were trained on cleaned-up Wikipedia dumps.
</p>
<p>The choice of the algorithms was based on fol-
lowing criteria: i) their performance reported in
recent surveys (n.b., the surveys were limited to
English and evaluated on other tasks), most notably
Hill et al. (2016) and Milajevs et al. (2014); ii) the
variety of their modelling assumptions and features
encoded. The referenced surveys already hinted
that the usefulness of a representation is largely
dependent on the actual application. Shallower but
more interpretable representations can be decoded
with spatial distance metrics. Others, more deep
and convoluted architectures, outperform the others
in supervised tasks. We inquire whether the gen-
eralisation is tenable also in the task of Sentiment
Analysis targeting sentence polarity.
</p>
<p>3.1.1 Additive Skip-Gram
As a bottom-up method, we train word embeddings
using skip-gram with negative sampling (Mikolov
et al., 2013). The algorithm finds the parameter θ
such that, given a pair of a word w and a context
c, the model discriminates correctly whether it be-
longs to a set of sentences S or a set of randomly
generated incorrect sentences S′:
</p>
<p>∏
(w,c)∈S
</p>
<p>p(S = 1|w, c, θ)
∏
</p>
<p>(w,c)∈S′
p(S′ = 0|w, c, θ)
</p>
<p>The representation of a sentence was obtained via
element-wise addition of the vectors of the words
belonging to it (Mitchell and Lapata, 2010).
</p>
<p>3.1.2 FastSent
The FastSent model was proposed by Hill et al.
(2016). It hinges on a sentence-level distributional
hypothesis (Polajnar et al., 2015; Kiros et al., 2015).
In other terms, it assumes that the meaning of a sen-
tence can be inferred by the neighbour sentences
in a text. It is a simple additive log-linear model
conceived to mitigate the computational expensive-
ness of algorithms based on a similar assumption.
</p>
<p>24</p>
<p />
</div>
<div class="page"><p />
<p>Hence, it was preferred over SkipThought (Kiros
et al., 2015) because of i) these efficiency issues
and ii) its competitive performances reported by
Hill et al. (2016). In FastSent, sentences are repre-
sented as bags of words: a context of sentences is
used to predict the adjacent sentence. Each word
w corresponds to a source vector uw and a target
vector vw. A sentence Si is represented as the
sum of the source vectors of its words
</p>
<p>∑
w∈Si uw.
</p>
<p>Hence, the cost C of a representation is given by
the softmax σ(x) of a sentence representation and
the target vectors of the words in its context c.
</p>
<p>CSi =
∑
</p>
<p>c∈Si−1∪Si+1
σ(
∑
w∈Si
</p>
<p>uw, vc) (1)
</p>
<p>This model does not rely on word order, but rather
on sentence order. It encodes new sentences by
summing over the source vectors of their words.
</p>
<p>3.1.3 Sequential Denoising AutoEncoder
Sequential Denoising AutoEncoders (SDAEs) com-
bine features of Denoising AutoEncoders (DAE)
and Sequence-to-Sequence models. In DAE, the in-
put representation is corrupted by a noise function
and the algorithms learns to recover the original
(Vincent et al., 2008). Intuitively, this makes the
model more robust to changes in input that are
irrelevant for the task at hand. This architecture
was later adapted to encode and decode variable-
length inputs, and the corruption process was imple-
mented in the form of dropout (Iyyer et al., 2015).
In the implementation by Hill et al. (2016),2 the
corruption function is defined as f(S|po, px). S is
a list of words (a sentence) where each has a prob-
ability po to be deleted, and the order of the words
in every distinct bigram has a probability px to be
swapped. The architecture consists in a Recurrent
Layer and predicts p(S|f(S|po, px)).
3.1.4 Paragraph Vector
Paragraph Vector is a collection of log-linear mod-
els proposed by Le and Mikolov (2014) for para-
graph/sentence representation. It consists of two
different models, namely the Distributed Memory
model (DM) and the Distributed Bag Of Words
model (DBOW). In DM, the ID of every distinct
paragraph (or sentence) is mapped to a unique vec-
tor in a matrix D and each word is mapped to a
unique vector in matrix W. Given a sentence i and
</p>
<p>2https://github.com/fh295/
SentenceRepresentation
</p>
<p>a window size k, the vector Di,· is used in con-
junction with the concatenation of the vectors of
the words in a sampled context 〈wi1 , . . . , wik〉 to
predict the next word through logistic regression:
</p>
<p>p(Wik+1 | 〈Di,Wi1 , . . . ,Wik〉) (2)
Note that the sentence ID vector is shared by the
contexts sampled from the same sentence. On the
other hand, DBOW focuses on predicting the word
embedding Wij for a sampled word j belonging
to sentence i given the sentence representation Di.
As a result, the main difference between the two
Paragraph Vector models is that the first is sensitive
to word order (represented by the word vector con-
catenation), whereas the second is insensitive with
respect to it. These models store a representation
for each sentence in the training set, hence they are
memory demanding. We use the gensim implemen-
tation of the two models available as Doc2Vec.3
</p>
<p>3.2 Hyper-parameters
The choice of the models’ hyper-parameters was
based on two (contrasting) criteria: i) conservative-
ness with those proposed in the original models
and ii) comparability among the models in this
work. In particular, we ensured that each model
had the same sentence vector dimensionality: 300.
The only exception is SDAE: we kept the recom-
mended value of 2400. Paragraph Vector DBOW
and SkipGram were trained for 10 epochs, with a
window size of 10, a minimum frequency count of
5, and a sampling threshold of 10−5. FastSent was
set as having a minimum count of 3, and no sam-
pling. The probabilities in the corruption function
of the SDAE were set as po = 0.1 (deletion) and
px = 0.1 (swapping). The dimension of the RNN
(GRU) hidden states (and hence sentence vector)
was 2400, whereas single words were assigned 100
dimensions. The learning rate was set to 0.01 with-
out decay, and the training lasted 7.2 hours on a
NVIDIA Titan X GPU. The main properties of each
algorithm are summarised in Table 1.
</p>
<p>Algorithm WO SO
Additive SkipGram
ParagraphVec DBOW
FastSent X
Sequential Denoising AutoEncoder X
</p>
<p>Table 1: Sensitivity to Word or Sentence Order.
</p>
<p>3https://radimrehurek.com/gensim/
models/doc2vec.html
</p>
<p>25</p>
<p />
</div>
<div class="page"><p />
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(a) Arabic
</p>
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(b) Chinese
</p>
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(c) Dutch
</p>
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(d) English
</p>
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(e) French
</p>
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(f) Italian
</p>
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(g) Russian
</p>
<p>0
</p>
<p>25
</p>
<p>50
</p>
<p>75
</p>
<p>100
</p>
<p>Negative Positive
Polarity
</p>
<p>Pe
rc
</p>
<p>en
ta
</p>
<p>ge
</p>
<p>(h) Spanish
</p>
<p>Figure 1: Percentages of negative (left) and positive (right) sentences with the same amount of negative
grammatical markers. A count of 0 is represented in dark blue, 1 in light blue, and 2 or more in green.
</p>
<p>4 Experimental Setup
</p>
<p>Now, we evaluate the quality of the distributed sen-
tence representations from § 3 on Sentiment Anal-
ysis. In § 4.1 we introduce the datasets of all the
considered languages, and the evaluation protocol
in § 4.2. Finally, to provide a potential performance
ceiling, we compare the obtained results with those
of a deep, state-of-art classifier, outlined in § 4.3.
4.1 Datasets
The data for training and testing are sourced from
the SemEval 2016: Task 5 (Pontiki et al., 2016).
These datasets provide customer reviews in 8 lan-
guages labelled with Aspect-Based Sentiment, i.e.,
opinions about specific entities or attributes rather
than generic stances. The languages include Ara-
bic (hotels domain), Chinese (electronics), Dutch
(restaurants and electronics), English (restaurants
</p>
<p>and electronics), French, Russian, Spanish, and
Turkish (restaurants all). We mapped the labels
to an overall polarity class (positive or negative)
by selecting the majority class among the aspect-
based sentiment classes for a given sentence. Note
that no general sentiment for the sentence was in-
cluded in this pool. Moreover, we added data for
Italian (tweets) from the SENTIPOLC shared task in
EVALITA 2016 (Barbieri et al., 2016). We discarded
neutral stances from the corpus, and retained only
positive and negative ones. Table 2 shows the fi-
nal size of the dataset partitions and the Wikipedia
dumps. In Figure 1, we report the percentage of
sentences with the same amount of negative gram-
matical markers (e.g. the word not and the suffix
n’t in English) based on their polarity class. We
discuss the impact of the variation of these percent-
ages on the results in § 5.
</p>
<p>26</p>
<p />
</div>
<div class="page"><p />
<p>Language Wikipedia
Dumps
</p>
<p>Train Test
</p>
<p>Arabic 3406732 4570 1163
Chinese 8067971 2593 1011
Dutch 11860559 2169 683
English 30000002 3584 1102
French 26024881 1410 534
Italian 15338617 4588 512
Russian 16671224 2555 835
Spanish 22328668 1553 646
Turkish 3622336 1008 121
</p>
<p>Table 2: Size of the data partitions (# sentences).
</p>
<p>4.2 Evaluation Protocol
After mapping each sentence in the dataset to its
distributed representation, we fed them to a Multi-
Layer Perceptron (MLP), trained to detect the sen-
tence polarity. In the MLP, a logistic regression
layer is stacked onto a 60-dimensional hidden layer
with a hyperbolic tangent activation. The weights
were initialised from the random xavier distribution
Glorot and Bengio (2010). The cross-entropy loss
was normalised with the L2-norm of the weights
scaled by λ = 10−3. The optimisation with gradi-
ent descent ran for 20 epochs with early stopping.
Batch size was 10 and the learning rate 10−2.
</p>
<p>4.3 Comparison with State-of-Art Models
In addition to unsupervised distributed sentence
representations, we test a bi-directional Long Short-
Term Memory neural network (bi-LSTM) on the
same task. This is a benchmark to compare against
results of deep state-of-art architectures. The
choice is based on the competitive results of this
algorithm and on its sensitivity to word order. The
accuracy of this architecture is 45.7 for 5-class and
85.4 for 2-class Sentiment Analysis on the standard
dataset of the Stanford Sentiment Treebank.
</p>
<p>The importance of word order is evident from the
architecture of the network. In a recurrent model,
the word embedding of a word wt at time t is com-
bined with the hidden state ht−1 from the previous
time step. The process is iterated throughout the
whole sequence of words of a sentence. This model
can be extended to multiple layers. LSTM is a re-
finement associating each time epoch with an input,
control and memory gate, in order to filter out ir-
relevant information (Hochreiter and Schmidhuber,
1997). This model is bi-directional if it is split in
two branches reading simultaneously the sentence
in opposite directions (Schuster and Paliwal, 1997).
</p>
<p>Contrary to the evaluation protocol sketched in §
4.2, the bi-LSTM does not utilise unsupervised sen-
tence representations. Rather, it is trained directly
on the datasets from § 4.1. The optimisation ran for
20 epochs, with a batch size of 20 and a learning
rate of 5 · 10−2. The 60-dimensional hidden layer
had a dropout probability of 0.2. Crucially, the
word embeddings were initialised with the Skip-
Gram model described in § 3.1.1. Since perfor-
mance tends to vary depending on the initialisation,
this ensures a fair comparison.
</p>
<p>5 Results
</p>
<p>The results are displayed in Figure 2. Weighted F1
scores were preferred over accuracy scores, since
the two classes (positive and negative) are unbal-
anced. We decoded the unsupervised representa-
tions multiple times through different initialisation
of the MLP weights, hence we report both the mean
value and its standard deviation. The results are
not straightforward: there is no algorithm outper-
forming the others in each language; unexpectedly
not even the bi-LSTM used as a ceiling. However,
the variation in performance follows certain trends,
depending on the properties of languages and algo-
rithms. We now examine: i) how performance is
affected by the properties of the algorithms, such
as those summarised in Table 1; ii) how typological
features concerning negation and the text domain
could make polarity harder to detect; iii) the inter-
action between negation and indefinite pronouns,
by visualising the contribution of each word to the
predicted class probabilities.
</p>
<p>5.1 Feature Sensitivity of the Algorithms
The state-of-art bi-LSTM algorithm chosen as a
ceiling is not the best choice in some languages
(Italian, and Turkish). In these cases, it is always
surpassed by the same model: additive Skip-Gram.
The drop in Italian is possibly linked to its dataset
in specific, since all the algorithms behave simi-
larly badly. Turkish is possibly challenging for a
recursive model because of the sparsity of its vo-
cabulary. These cases, however, are not isolated:
averaged word embeddings outperformed LSTMs
in text similarity tasks (Arora et al., 2016) and pro-
vide a strong baseline in English (Adi et al., 2017).
</p>
<p>In any case, the general high performance of
additive Skip-Gram is noteworthy: it shows that
a simple method achieves close-to-best results in
almost every language among decoded distributed
</p>
<p>27</p>
<p />
</div>
<div class="page"><p />
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
● ●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>ar en es fr it nl ru tr zh
Languages
</p>
<p>W
ei
</p>
<p>gh
te
</p>
<p>d 
F1
</p>
<p> S
co
</p>
<p>re
Algorithms: ● ● ● ● ●add−SkipGram bi−LSTM DBOW FastSent SDAE
</p>
<p>add-SkipGram DBOW FastSent SDAE bi-LSTM
ar 51.76 ± 1.78 76.76 ± 4.42 53.85 ± 2.50 72.13 ± 5.44 86.56
en 76.31 ± 4.96 68.89 ± 1.49 73.57 ± 0.71 65.33 ± 4.39 78.65
es 80.19 ± 4.96 65.65 ± 3.17 74.50 ± 6.07 74.03 ± 0.81 85.08
fr 74.00 ± 0.43 65.07 ± 8.23 65.52 ± 1.82 53.80 ± 8.63 78.80
it 67.63 ± 5.52 65.24 ± 3.20 66.63 ± 1.63 57.62 ± 8.92 65.88
nl 79.04 ± 1.05 56.04 ± 0.00 76.06 ± 1.20 70.06 ± 3.65 82.66
ru 66.61 ± 0.89 63.42 ± 1.43 64.05 ± 0.74 69.96 ± 2.07 80.39
tr 73.22 ± 0.84 58.74 ± 0.00 65.19 ± 1.66 63.10 ± 2.69 68.49
zh 56.17 ± 2.93 52.58 ± 2.80 52.46 ± 1.95 66.26 ± 3.61 78.55
</p>
<p>Figure 2: Results of 5 different algorithms on 9 languages. Values report the mean Weighted F1 Score and
the standard deviation. The best results per language are given in bold and the second-best is underlined.
Data points where the ceiling is outperformed are in italics.
</p>
<p>representations. This result is in line with other
findings: Wieting et al. (2016b) showed that word
embeddings, once retrained and decoded by lin-
ear regression, beat many methods that generate
sentence representations directly.
</p>
<p>Moreover, the second-best method for languages
is always FastSent, which is the only one hing-
ing upon neighbouring sentences as features. This
demonstrate that sentiment is encoded not only
within a sentence, but also in its textual context.
As a consequence, a relatively small and accessi-
ble dataset (Wikipedia) is sufficient to provide a
reliable model in most languages. Nonetheless, the
varying size of the dumps affects FastSent as well
as the other unsupervised algorithms: limited data
hinders them from learning faithful representations,
</p>
<p>as in Arabic, Chinese, and Turkish (see Table 2).
In general, algorithms sensitive to the same fea-
</p>
<p>tures behave similarly, e.g. SDAE and bi-LSTM.
They follow the same trend in relative improve-
ments from one language to another. The generally
low performance of SDAE could depend on the lim-
ited training time, which was necessary to evaluate
the algorithm on the whole set of languages.
</p>
<p>5.2 Typology of Negation and Domain
In some languages, the scores are very scattered:
this fluctuation might be due to their peculiar mor-
phological properties. In particular, Arabic is an in-
troflexive language, Chinese is a radically isolating
language, and Turkish an agglutinative language.
On the other hand, the algorithms achieve better
</p>
<p>28</p>
<p />
</div>
<div class="page"><p />
<p>0 10 20 30 40 50
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>50
</p>
<p>(a) Arabic positive
</p>
<p>0 10 20 30 40 50
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>(b) Arabic negative
</p>
<p>0 10 20 30 40 50
</p>
<p>me
</p>
<p>gusta
</p>
<p>todo
</p>
<p>en
</p>
<p>este
</p>
<p>restaurante 1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>(c) Spanish positive
</p>
<p>0 10 20 30 40 50
</p>
<p>no
</p>
<p>me
</p>
<p>gusta
</p>
<p>nada
</p>
<p>en
</p>
<p>este
</p>
<p>restaurante
5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>30
</p>
<p>35
</p>
<p>(d) Spanish negative
</p>
<p>0 10 20 30 40 50
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(e) Russian positive
</p>
<p>0 10 20 30 40 50
</p>
<p>0.5
</p>
<p>1.0
</p>
<p>1.5
</p>
<p>2.0
</p>
<p>2.5
</p>
<p>3.0
</p>
<p>(f) Russian negative
</p>
<p>Figure 3: Visualization of the derivative of the class scores with respect to the word embeddings.
</p>
<p>scores in the fusional languages, save Italian.
A fine-grained analysis shows also that the per-
</p>
<p>formance is affected by the typology of the nega-
tion in each language, although negative mark-
ers appear in a reduced number of examples (see
Figure 1). Semantically, negation is crucial in
switching or mitigating the polarity of lexical items
and phrases. Morpho-syntactically, negation is ex-
pressed through several constructions across the
languages of the world. Constructions differ in
many respects, which are classified as feature-value
pairs in databases like the World Atlas of Language
Structures (Dryer and Haspelmath, 2013).4
</p>
<p>Negation can affect the declarative verbal main
clauses. In fact, negative clauses can be: i) sym-
metric, i.e., identical to the affirmative counterpart
except for the negative marker; ii) asymmetric,
i.e. showing structural differences between neg-
ative and affirmative clauses (in constructions or
paradigms); iii) showing mixed behaviour. Alter-
ations concern for instance finiteness, the oblig-
atory marking of unreality status, or the expres-
sion of verbal categories. Secondly, negation inter-
acts with indefinite pronoun (e.g. nobody, nowhere,
never). Negative indefinites can i) co-occur with
standard negation; ii) be forbidden in concurrence;
</p>
<p>4The features considered here for negation are 113A ‘Sym-
metric and Asymmetric Standard Negation’, 114A ‘Subtypes
of Asymmetric Standard Negation’, 115A ‘Negative Indefi-
nite Pronouns and Predicate Negation’, and 143A ‘Order of
Negative Morpheme and Verb’.
</p>
<p>iii) display a mixed behaviour. Finally, the rela-
tion of the negative marker with respect to verb
is prone to change. Firstly, it can be either an af-
fix or a prosodically independent word. Secondly,
its position can be anchored to the verb (preced-
ing, following, or both). Thirdly, negation can be
omitted, doubled or even tripled.
</p>
<p>Performances seem to suffer the ambiguity in
mapping between a negative marker and negative
meaning. In fact, the bi-LSTM achieves lower
scores in languages with asymmetric constructions
(Chinese, English, and Turkish): the additional
changes in the sentence construction and/or verb
paradigm might create noise. Additional reasons
of difficulty may occur when negation is doubled
(French) or affixed (Turkish), since this makes nega-
tion redundant or sparse. On the other hand, add-
SkipGram appears to be sensitive to the presence
of negation: according to the counts in Figure 1,
when this is too pervasive (Arabic and Russian) or
rare (Chinese), the scores tend to decrease.
</p>
<p>These comments on the results based on linguis-
tic properties can also suggest speculative solutions
for future work. For algorithms based on sentence
order, it is not clear whether the problem lies in the
lack of wider collections of texts in some languages,
or rather on the maximum amount of information
about polarity that is learnt through a sentence-level
distributional hypothesis. On the other hand, im-
pairments of the other algorithms seem to be linked
</p>
<p>29</p>
<p />
</div>
<div class="page"><p />
<p>with redundancies and noise. Filtering out words
that contribute to this effect might benefit the qual-
ity of the representation. Moreover, the sparsity
due to cases where negation is an affix might be
mitigated by introducing character-level features.
</p>
<p>The other inherent source of variation is the text
domain, on which the difficulty of the task depends
(Glorot et al., 2011). Although the unstructured
nature of tweets could hinder the quality of the
sentence representations in Italian, however, no
clear effect is evident based on the other domains.
</p>
<p>5.3 Visualisation
Since languages vary in the “polarity agreement”
between verbs and indefinite pronouns, algorithms
may weigh these as features differently. We analyse
their role through a visualizasion of the activation
in the hidden layer of the bi-LSTM. In particular,
we approximated the objective function through
a linear function, and estimated the contribution
of each word to the true class probability by com-
puting the prime derivative of the output scores
with respect to the embeddings. This technique is
presented and detailed by Li et al. (2015). The visu-
alised hidden layers are shown in Figure 3, whereas
the sentences used as input are glossed in Ex. (3)
(Arabic), Ex. (4) (Spanish), and Ex. 5 (Russian).
</p>
<p>(3) ‘ana
1SG
</p>
<p>‘uhibu
like.NPST.1SG
</p>
<p>kl
every
</p>
<p>shay‘
thing
</p>
<p>fi
in
</p>
<p>hadha
this
</p>
<p>almataeim
restaurant
</p>
<p>/
/
</p>
<p>‘ana
1SG
</p>
<p>la
not.NPST
</p>
<p>‘uhibu
like.NPST.1SG
</p>
<p>‘ayu
any
</p>
<p>shay‘
thing
</p>
<p>fi
in
</p>
<p>hadha
this
</p>
<p>almataeam
restaurant
</p>
<p>(4) me
1SG.DAT
</p>
<p>gust-a
like-3SG
</p>
<p>todo
everything
</p>
<p>en
in
</p>
<p>est-e
this-SG
</p>
<p>restaurant-e
restaurant-SG
</p>
<p>/
/
</p>
<p>no
not
</p>
<p>me
1SG.DAT
</p>
<p>gust-a
like-3SG
</p>
<p>nada
nothing
</p>
<p>en
in
</p>
<p>est-e
this-SG
</p>
<p>restaurant-e
restaurant-SG
</p>
<p>(5) mne
1SG.DAT
</p>
<p>nráv-itsja
like.IMPV-PRS.3SG
</p>
<p>vs-jo
all-NOM.SG
</p>
<p>v
in
</p>
<p>ét-om
this-PREP.SG
</p>
<p>restoráne
restaurant-PREP.SG
</p>
<p>/
/
</p>
<p>mne
1SG.DAT
</p>
<p>ni-čevó
nothing-GEN
</p>
<p>ne
not
</p>
<p>nráv-itsja
like.IMPV-PRS.3SG
</p>
<p>v
in
</p>
<p>ét-om
this-PREP.SG
</p>
<p>restorán-e
restaurant-PREP.SG
</p>
<p>The two compared sentences correspond to the
translation of two English sentences. The first
is positive: ‘I like everything in this restaurant’;
the second is negative: ‘I don’t like anything in
</p>
<p>this restaurant’. These include a domain-specific
but sentiment-neutral word that plays the role of
a touchstone. The more a cell tends to blue, the
higher its activation. In some languages (e.g. Ara-
bic), the sentiment verb elicits a stronger reaction
in the positive polarity, whereas the indefinite pro-
noun dominates in the negative polarity. In several
other languages (e.g. Spanish), indefinite pronouns
are more relevant than any other feature. In Rus-
sian, only sentiment verbs always provoke a re-
action. These differences might be related to the
“polarity agreement” of these languages, which hap-
pens always, sometimes, and never, respectively.
In some other languages, however, no evidence is
found of any similar activation pattern.
</p>
<p>6 Conclusion
</p>
<p>In this work, we examined how much sentiment
polarity information is retained by distributed rep-
resentations of sentences in multiple typologically
diverse languages. We generated the representa-
tions through various algorithms, sensitive to dif-
ferent properties from training corpora (e.g, word
or sentence order). We decoded them through a
simple MLP and compared their performance with
one of the state-of-art algorithms for Sentiment
Analysis: bi-directional LSTM. Unexpectedly, for
some languages the bi-directional LSTM is outper-
formed by unsupervised strategies like the addi-
tion of the word embeddings obtained from a Skip-
Gram model. This model, in turn, surpasses more
sophisticated algorithms for most of the languages.
This demonstrates i) that no algorithm is the best
across the board; and ii) that some simple mod-
els are to be preferred even for downstream tasks,
which partially contrasts with the conclusions of
Hill et al. (2016). Moreover, representation algo-
rithms sensitive to word order have similar trends,
but they do not always achieve performance su-
perior to algorithms based on the sentence order.
Finally, some properties of languages (i.e. their
type of negation) appear to have an impact on the
scores: in particular, the asymmetry of negative
and affirmative clauses and the doubling of nega-
tive markers.
</p>
<p>Acknowledgements
</p>
<p>This work was supported by the ERC Consolidator
Grant LEXICAL (648909). The authors would like
to thank the anonymous reviewers for their helpful
suggestions and comments.
</p>
<p>30</p>
<p />
</div>
<div class="page"><p />
<p>References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
</p>
<p>Lavi, and Yoav Goldberg. 2017. Fine-grained
analysis of sentence embeddings using auxil-
iary prediction tasks. In Proceedings of ICLR.
http://arxiv.org/abs/1608.04207.
</p>
<p>Mariana SC Almeida, Cláudia Pinto, Helena Figueira,
Pedro Mendes, and André FT Martins. 2015. Align-
ing opinions: Cross-lingual opinion mining with de-
pendencies. In Proc. of the Annual Meeting of the
Association for Computational Linguistics.
</p>
<p>Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016.
A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR 2017.
</p>
<p>Alexandra Balahur and Marco Turchi. 2014. Compar-
ative experiments using supervised learning and ma-
chine translation for multilingual sentiment analysis.
Computer Speech &amp; Language 28(1):56–75.
</p>
<p>Francesco Barbieri, Valerio Basile, Danilo Croce,
Malvina Nissim, Nicole Novielli, and Viviana Patti.
2016. Overview of the evalita 2016 sentiment po-
larity classification task. In Proceedings of Third
Italian Conference on Computational Linguistics
(CLiC-it 2016) &amp; Fifth Evaluation Campaign of Nat-
ural Language Processing and Speech Tools for Ital-
ian. Final Workshop (EVALITA 2016).
</p>
<p>Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 45–55.
</p>
<p>Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Wein-
berger, and Claire Cardie. 2016. Adversarial deep
averaging networks for cross-lingual sentiment clas-
sification. arXiv preprint arXiv:1606.01614 .
</p>
<p>Alexis Conneau, Douwe Kiela, Holger Schwenk,
Loı̈c Barrault, and Antoine Bordes. 2017. Su-
pervised learning of universal sentence representa-
tions from natural language inference data. CoRR
abs/1705.02364. http://arxiv.org/abs/1705.02364.
</p>
<p>Östen Dahl. 1979. Typology of sentence negation. Lin-
guistics 17(1-2):79–106.
</p>
<p>Kerstin Denecke. 2008. Using sentiwordnet for mul-
tilingual sentiment analysis. In Data Engineering
Workshop, 2008. ICDEW 2008. IEEE 24th Interna-
tional Conference on. pages 507–512.
</p>
<p>Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig. http://wals.info/.
</p>
<p>Alejandro Moreo Fernández, Andrea Esuli, and Fab-
rizio Sebastiani. 2015. Distributional correspon-
dence indexing for cross-lingual and cross-domain
sentiment classification. Journal of Artificial Intelli-
gence Research 55:131–163.
</p>
<p>Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Aistats. volume 9, pages 249–256.
</p>
<p>Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11). pages 513–520.
</p>
<p>Felix Hill, Kyunghyun Cho, and Anna Korhonen.
2016. Learning distributed representations of
sentences from unlabelled data. arXiv preprint
arXiv:1602.03483 .
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8):1735–
1780.
</p>
<p>Mohit Iyyer, Varun Manjunatha, Jordan L Boyd-
Graber, and Hal Daumé III. 2015. Deep unordered
composition rivals syntactic methods for text classi-
fication. In ACL (1). pages 1681–1691.
</p>
<p>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems.
pages 3294–3302.
</p>
<p>Filippos Kokkinos and Alexandros Potamianos. 2017.
Structural attention neural networks for improved
sentiment analysis. In EACL 2017.
</p>
<p>Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML.
volume 14, pages 1188–1196.
</p>
<p>Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2015. Visualizing and understanding neural models
in nlp. arXiv preprint arXiv:1506.01066 .
</p>
<p>Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K
Tsou. 2011. Joint bilingual sentiment classifica-
tion with unlabeled parallel corpora. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computa-
tional Linguistics, pages 320–330.
</p>
<p>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. SemEval-2014 .
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.
</p>
<p>Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh
Sadrzadeh, and Matthew Purver. 2014. Evaluating
neural word representations in tensor-based compo-
sitional settings. idea 10(47):39.
</p>
<p>31</p>
<p />
</div>
<div class="page"><p />
<p>Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence 34(8):1388–1429.
</p>
<p>Amr El-Desoky Mousa and Björn Schuller. 2017. Con-
textual bidirectional long short-term memory recur-
rent neural network language models: A generative
approach to sentiment analysis. In EACL 2017.
</p>
<p>Tamara Polajnar, Laura Rimell, and Stephen Clark.
2015. An exploration of discourse-based sentence
spaces for compositional distributional semantics.
In Workshop on Linking Models of Lexical, Sen-
tential and Discourse-level Semantics (LSDSem).
page 1.
</p>
<p>Adam Poliak, Pushpendre Rastogi, M Patrick Martin,
and Benjamin Van Durme. 2017. Efficient, compo-
sitional, order-sensitive n-gram embeddings. EACL
2017 page 503.
</p>
<p>Edoardo Maria Ponti. 2016. Divergence from syntax
to linear order in ancient greek lexical networks. In
The 29th International FLAIRS Conference.
</p>
<p>Maria Pontiki, Dimitrios Galanis, Haris Papageor-
giou, Ion Androutsopoulos, Suresh Manandhar, Mo-
hammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan
Zhao, Bing Qin, Orphée De Clercq, Véronique
Hoste, Marianna Apidianaki, Xavier Tannier, Na-
talia Loukachevitch, Evgeny Kotelnikov, Nuria Bel,
Salud Marıa Jiménez-Zafra, , and Gülsen Eryigit.
2016. Semeval-2016 task 5: Aspect based senti-
ment analysis. In Proceedings of the 10th Interna-
tional Workshop on Semantic Evaluation, SemEval.
volume 16.
</p>
<p>Laura Rimell, Jean Maillard, Tamara Polajnar, and
Stephen Clark. 2016. Relpron: A relative clause
evaluation dataset for compositional distributional
semantics. Computational Linguistics .
</p>
<p>Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.
</p>
<p>Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
Christopher Potts, et al. 2013. Recursive deep
models for semantic compositionality over a senti-
ment treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP). Citeseer, volume 1631, page 1642.
</p>
<p>Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2015. Dls@ cu: Sentence similarity from word
alignment and semantic vector composition. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation. pages 148–153.
</p>
<p>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international
conference on Machine learning. ACM, pages 1096–
1103.
</p>
<p>John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016a. Charagram: Embedding words and
sentences via character n-grams. arXiv preprint
arXiv:1607.02789 .
</p>
<p>John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016b. Towards universal paraphrastic sen-
tence embeddings. In ICLR 2017.
</p>
<p>Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analy-
sis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 172–182.
</p>
<p>Guangyou Zhou, Tingting He, Jun Zhao, and Wen-
sheng Wu. 2015. A subspace learning framework
for cross-lingual sentiment classification with partial
parallel data. In Proceedings of the international
joint conference on artificial intelligence, Buenos
Aires.
</p>
<p>Xinjie Zhou, Xianjun Wan, and Jianguo Xiao. 2016.
Cross-lingual sentiment classification with bilingual
document representation learning .
</p>
<p>32</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 33–43,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Detecting Asymmetric Semantic Relations in Context:
A Case-Study on Hypernymy Detection
</p>
<p>Yogarshi Vyas and Marine Carpuat
Department of Computer Science
</p>
<p>University of Maryland
yogarshi@cs.umd.edu and marine@cs.umd.edu
</p>
<p>Abstract
</p>
<p>We introduce WHIC1, a challenging
testbed for detecting hypernymy, an asym-
metric relation between words. While pre-
vious work has focused on detecting hy-
pernymy between word types, we ground
the meaning of words in specific contexts
drawn from WordNet examples, and re-
quire predictions to be sensitive to changes
in contexts. WHIC lets us analyze com-
plementary properties of two approaches
of inducing vector representations of word
meaning in context. We show that such
contextualized word representations also
improve detection of a wider range of se-
mantic relations in context.
</p>
<p>1 Introduction
</p>
<p>Language understanding applications like ques-
tion answering (Harabagiu and Hickl, 2006) and
textual entailment (Dagan et al., 2013) bene-
fit from identifying semantic relations between
words beyond synonymy and paraphrasing. For
instance, given “Anand plays chess.”, and the
question “Which game does Anand play?”, suc-
cessfully answering the question requires know-
ing that chess is a kind of game, i.e. chess entails
game. Such lexical entailment relations are asym-
metric (chess =⇒ game, but game 6=⇒ chess),
and detecting their direction accurately is a chal-
lenge.
</p>
<p>While prior work has defined lexical entailment
as a relation between word types, we argue that
it is better defined between word meanings illus-
trated by examples of usage in context. Ignoring
context is problematic since entailment might hold
between some senses of the words, but not others.
Consider the word game in the following contexts:
</p>
<p>1https://github.com/yogarshi/whic
</p>
<p>1. The championship game was played in NYC.
2. The hunters were interested in the big game.
</p>
<p>Given the sentence, Anand is the world chess
champion, chess =⇒ game in the first context,
while chess 6=⇒ game in the second context.
</p>
<p>Lexical entailment encompasses several seman-
tic relations, with one important relation being
hypernymy (Roller et al., 2014; Shwartz et al.,
2016). In this work, we focus on hypernymy de-
tection in context, and show that existing resources
can be leveraged to automatically create test beds
for evaluation. We introduce “Wordnet Hyper-
nyms in Context” (WHIC, pronounced which), a
large dataset, automatically extracted from Word-
Net (Fellbaum, 1998) using examples provided
with synsets. Crucially, WHIC includes challeng-
ing negative examples that assess the ability of
models to detect the direction of hypernymy.
</p>
<p>We use WHIC to determine the effectiveness of
existing supervised models for hypernymy detec-
tion (Roller and Erk, 2016) applied to represen-
tations, not only of word types, but of words in
context. Such contextualized representations are
induced in two ways: the first is based on Con-
text2Vec, a BiLSTM model that embeds contexts
and words in the same space (Melamud et al.,
2016); the second aims to capture geometric prop-
erties of the context in a standard word embedding
space built using GloVe (Pennington et al., 2014).
</p>
<p>We show that the two contextualized rep-
resentations improve performance over context-
agnostic baselines. The structure of WHIC lets
us show that they have complementary proper-
ties: Context2Vec-based models have higher re-
call and tend to identify directionality much bet-
ter than Glove-based models. We also show that
the context-aware representations improve perfor-
mance on identifying a broader range of semantic
relations (Shwartz and Dagan, 2016).
</p>
<p>33</p>
<p />
</div>
<div class="page"><p />
<p>Words (wl, wr) Exemplars (cl,cr) Does wl =⇒ wr ?
staff , stick cl = He walked with the help of a wooden staff . Yes
</p>
<p>cr = The kid had a candied apple on a stick.
</p>
<p>staff , body cl = The hospital has an excellent nursing staff . Yes
cr = The whole body filed out of the auditorium.
</p>
<p>staff , stick cl = The hospital has an excellent nursing staff . No
cr = The kid had a candied apple on a stick.
</p>
<p>Table 1: Examples of the context-aware hypernymy detection task
</p>
<p>2 Detecting Hypernymy in Context
</p>
<p>2.1 Task Definition
We frame hypernymy detection in context as a bi-
nary classification task. Each example consists
of a 4-tuple (wl, wr, cl, cr), where wl and wr are
word types, and cl and cr are sentences which il-
lustrate each word usage. The example is treated
as positive if wl =⇒ wr, given the meaning of
each word exemplified by the contexts, and nega-
tive otherwise, as can be seen in Table 1.
</p>
<p>As mentioned in Section 1, hypernymy is only
one specific case of lexical entailment. The nature
of entailment relations captured out-of-context can
be broader depending on the test beds consid-
ered2. These relations can include synonymy,
hypernymy, some meronymy relations, and also
cause-effect relations.
</p>
<p>2.2 Motivation
The need to study hypernymy detection in con-
text is important due to several reasons. First,
many downstream tasks which might benefit from
detecting hypernyms will have words appearing
in specific contexts. Second, existing definitions
(and, by extension, annotations) of lexical entail-
ment do not explicitly or consistently address pol-
ysemy. For instance, the substitutional definition
for entailment by Zhitomirsky-Geffet and Dagan
(2009) asks the reader to think of a natural sen-
tence that provides the missing context to the two
words being considered, thus constraining the pos-
sible senses of the two words. On the other hand,
Turney and Mohammad (2013) propose a rela-
tional definition, inviting the reader to imagine a
semantic relation that connects the two words and
constrains their possible senses. In contrast, we
propose to detect hypernymy between word mean-
ings described by specific contexts.
</p>
<p>2We refer the reader to Turney and Mohammad (2013)
and Shwartz et al. (2017) for comprehensive surveys of super-
vised and unsupervised methods for the out-of-context task.
</p>
<p>Lexical entailment or hypernymy in context is
also different from recognizing textual entailment
(RTE). RTE (Dagan et al., 2006, 2013) involves
detecting entailment relations between sentences,
while hypernymy is a relation between words. Ad-
ditionally, the two contexts cl and cr in our task
can be very different, unlike in textual entailment,
where the premise and hypothesis are usually re-
lated. For instance, the first example in Table 1 il-
lustrates a scenario where the hypernymy relation
holds between staff and stick, but there is no en-
tailment relationship between the two sentences.
On the other hand, the sentence ”Children smile
and wave at the camera.” entails ”There are chil-
dren present.”, but there is no meaningful hyper-
nymy relationship between words in the two sen-
tences.
</p>
<p>Finally, the proposed task is also related to, but
different from word sense disambiguation (WSD).
Unlike WSD, this task eschews an explicit sense
inventory, instead relying on the provided contexts
to decide the specific relation between the words.
This might provide a more natural way to think
about word senses for (untrained) human annota-
tors (Erk et al., 2013). WSD can in principle be
used as a preprocessing step to address hypernymy
detection in context, but it is not required. Also,
WSD remains a challenging task (Moro and Nav-
igli, 2015) and it might introduce errors early in
the preprocessing pipeline.
</p>
<p>2.3 WHIC : A Dataset for Lexical
Entailment in Context
</p>
<p>We require a dataset to study hypernymy detec-
tion in context to satisfy the following desiderata:
(1) the dataset should make it possible to assess
the sensitivity of context-aware models to con-
texts that signal different word senses, and (2) the
dataset should help quantify the extent to which
models detect the asymmetric direction of hyper-
nymy, rather than symmetric semantic similarity.
</p>
<p>34</p>
<p />
</div>
<div class="page"><p />
<p>Word : Study 
Example : he knocked on the door of the study
</p>
<p>Word : Study 
Example : he made several studies before the final 
painting
</p>
<p>Word : Room 
Example : the rooms were small but comfortable
</p>
<p>Word : Drawing 
Example : he did complicated pen-and-ink drawings
</p>
<p>Figure 1: Sample dataset creation process based on two synsets of the word study. The green/solid lines
indicate positive examples, while the red/dashed lines indicate negative examples
</p>
<p>Existing datasets for lexical entailment (Baroni
and Lenci, 2011; Baroni et al., 2012; Kotlerman
et al., 2010) have driven progress on the out of
context task only, and are therefore insensitive
to context changes. In addition, they include a
variety of negative examples without controlling
for entailment direction. For instance, Baroni
and Lenci (2011) use cohyponyms and random
words as negative examples. Since cohyponyms
are words that share a common hypernym (for ex-
ample, salsa and tango are cohyponymys with re-
spect to dance), hypernymy does not hold between
them in any direction. On the other hand, ran-
dom examples (also used by Baroni et al. (2012))
are likely to be detected using symmetric seman-
tic similarity rather than asymmetric hypernymy
detection.
</p>
<p>Shwartz and Dagan (2016) recently introduced
CONTEXT-PPDB, a dataset for fine-grained lexi-
cal inference in context. This dataset consists of
word pairs along with a pair of sentential con-
texts, with a label indicating the semantic rela-
tion between the two words in the given contexts.
However, since CONTEXT-PPDB only consists of
~3700 sentence pairs, it provides only a smaller
number of annotated examples per relation, mak-
ing it difficult to train large supervised models on
(we return to this dataset in Section 5).
</p>
<p>We address these gaps by introducing, WHIC,
a large dataset automatically derived from Word-
Net (Fellbaum, 1998). WordNet groups synonyms
into synsets and defines semantic relations such as
hypernymy and meronymy between these synsets.
Most synsets are further accompanied by one or
more short sentences illustrating the use of the
members of the synset. WHIC uses these example
sentences as context for the words, and the hyper-
nymy relations to draw candidate word pairs. The
process starts from a seed list of words W and pro-
ceeds as follows (see Figure 1) :
</p>
<p>1. For all word types w ∈W obtain synsets Sw.
</p>
<p>2. For each synset i ∈ Sw, pick a hypernym
synset sih, with a corresponding word form
wih. Also obtain c
</p>
<p>i and cih which are exam-
ple sentences corresponding to wi and wih re-
spectively - (wi, wih, c
</p>
<p>i, cih) serves as a posi-
tive example. Repeat this process for all hy-
pernyms (solid/green arrows in Figure 1).
</p>
<p>3. Permute the positive examples to get neg-
ative examples. From (wi, wih, c
</p>
<p>i, cih) and
(wj , wjh, c
</p>
<p>j , cjh), generate negative examples
(wi, wjh, c
</p>
<p>i, cjh) and (w
j , wih, c
</p>
<p>j , cih) (longer
dashed/red arrows in Figure 1).
</p>
<p>4. Flip the positive examples to generate nega-
tive examples. From (wi, wih, c
</p>
<p>i, cih) generate
the negative example (wih, w
</p>
<p>i, cih, c
i) (shorter
</p>
<p>dashed/red arrows in Figure 1).
</p>
<p>We run this process using the 9000 most fre-
quent words from Wikipedia as W (after filtering
the top 1000 as stopwords). This yields a total of
5239 positive examples, 12303 negative examples
from Step 3, and 5239 negative examples from
Step 4.
</p>
<p>WHIC satisfies the desiderata outlined above.
The dataset has a well-defined focus, since we
only pick hypernym-hyponym pairs. The nega-
tive examples generated in Steps 3 and 4 require
discriminating between different word senses and
entailment directions. Finally, with over 22000
examples distributed over 6000 word pairs, the
dataset is large enough to train large supervised
models. We define a 70/5/25 train/dev/test split,
and ensure that each set contains different word
pairs, to avoid memorization and overfitting (Levy
et al., 2015).
</p>
<p>3 Representing Words and their
Contexts for Entailment
</p>
<p>How can we construct representations of the
meaning of target words wl and wr, and their re-
spective exemplar contexts cl and cr?
</p>
<p>35</p>
<p />
</div>
<div class="page"><p />
<p>the
</p>
<p>river
</p>
<p>bank
</p>
<p>0.3 0.6 -0.1
</p>
<p>1.5 -2.5 0
</p>
<p>-1 0.2 1.8
</p>
<p>cl,mean 0.27 -0.57 0.57
</p>
<p>-1 -2.5 -0.1
</p>
<p>1.5 0.6 1.8
</p>
<p>Cl
</p>
<p>cl,min
</p>
<p>cl,max
</p>
<p>(wl)
</p>
<p>} -1 0.2 1.8bank⊙ 
</p>
<p>wl,mean-0.27 -1.14 1.03
</p>
<p>1 -5.0 -0.18
</p>
<p>-1.5 0.12 3.24
</p>
<p>wl,min
</p>
<p>wl,max
</p>
<p>Figure 2: Constructing word-in-context represen-
tations for “bank”, in the context “the river bank”.
� indicates element-wise multiplication.
</p>
<p>We will construct representations for cl, and cr,
and create context-aware representations for wl
and wr by “masking” their word embeddings with
the embeddings for cl and cr (Section 3.3). We
compare two approaches to representing cl and cr.
The first (Section 3.1) builds on standard represen-
tations for word types, which have proven useful
for detecting lexical entailment and other seman-
tic relations out of context (Baroni et al., 2012;
Kruszewski and Baroni, 2015; Vylomova et al.,
2016; Turney and Mohammad, 2013). The sec-
ond approach (Section 3.2) uses a recurrent neural
model to embed words and contexts in the same
space, allowing direct comparisons between them.
</p>
<p>3.1 Creating Context Representations from
Word Type Representations
</p>
<p>Given an example (wl, wr, cl, cr), let ~wl and ~wr
refer to the context-agnostic representations of wl
and wr, and let Cl and Cr represent the matri-
ces obtained by row-wise stacking of the context-
agnostic representations of words in cl and cr re-
spectively.
</p>
<p>Following Thater et al. (2011); Erk and Padó
(2008), we apply a filter to word type represen-
tations to highlight the salient dimensions of the
exemplar context, emphasizing relevant dimen-
sions and downplaying unimportant ones. How-
ever, while prior work represents context by aver-
aging word vectors, we propose richer represen-
tations that better capture the salient geometrical
properties of the exemplar context that might get
lost by averaging.
</p>
<p>We construct fixed length representations for
the contexts cl and cr by running convolutional fil-
</p>
<p>ters over Cl and Cr. Specifically, we calculate the
column-wise maximum, minimum and the mean
over the matrices Cl and Cr, as done by Tang
et al. (2014) for supervised sentiment classifica-
tion. This yields three d-dimensional vectors for cl
(~cl,max, ~cl,min, ~cl,mean), and three d-dimensional
vectors for cr (~cr,max, ~cr,min, ~cr,mean). Comput-
ing the maximum and minimum across all vec-
tor dimensions captures the exterior surface of the
“instance manifold” (the volume in embedding
space within which all words in the instance re-
side), while the mean summarizes the density per-
dimension within the manifold (Hovy, 2015).
</p>
<p>3.2 LSTM-based Context Representations:
Context2Vec
</p>
<p>An alternative approach to contextualizing word
representations is to directly compare the repre-
sentations of words with representations of con-
texts. This can be done using Context2Vec (Mela-
mud et al., 2016), a neural model that, given
a target word and its sentential context, embeds
both the word and the context in the same low-
dimensional space using a BiLSTM, with the
objective of having the context predict the tar-
get word via a log-linear model. This model
approaches the state-of-the-art on lexical sub-
stitution, sentence completion, and supervised
word sense disambiguation. For each example
(wl, wr, cl, cr), we extract the word type repre-
sentations ~wl,c2v and ~wl,c2v from Context2Vec,
as well as the context representations ~cl,c2v, and
~cr,c2v.
</p>
<p>3.3 Context-aware Masked Representations
</p>
<p>Given these two methods to learn representations
for words and their contexts, we also learn context
aware word representations for the target words.
We transform initial context-agnostic representa-
tions for target word types by taking an element-
wise product of the word type vectors with vectors
representing the context.
</p>
<p>Specifically, for the context representations
learned in Section 3.1, we take an element-
wise product of the word type vectors ( ~w∗) with
(~c∗,max, ~c∗,min, ~c∗,mean) where ∗ ∈ {l, r}. This
yields three d-dimensional vectors for wl (~wl,max,
~wl,min, ~wl,mean), and three for wr (~wr,max,
~wr,min, ~wr,mean). We refer to our final word-in-
context representations for wl and wr as ~wl,mask
and ~wr,mask respectively, where ~wl,mask is the
</p>
<p>36</p>
<p />
</div>
<div class="page"><p />
<p>concatenation of ~wl,max, ~wl,min, ~wl,mean, and
~wr,mask is also similarly constructed.
</p>
<p>For the word and context representations ob-
tained from Context2Vec (Section 3.2), we cre-
ate the context-aware representations ~wl,c2v,mask
by vector multiplication between ~wl,c2v and ~cl,c2v.
We also obtain ~wr,c2v,mask similarly.
</p>
<p>4 Comparing Words and Contexts for
Entailment
</p>
<p>Given the word, context, and word-in-context rep-
resentations described above, we predict entail-
ment via supervised classification.
</p>
<p>Our classifier is the Hypernymy-Feature detec-
tor (Roller and Erk, 2016), which is the current
state-of-the-art supervised model for detecting hy-
pernymy on several datasets. This model aims to
overcome the shortcomings of previous supervised
hypernymy detection models, which used linear
classifiers on top of concatenation of the two vec-
tors representing the target words. These models
only captured notions of prototypicality without
modeling the interactions between the two words;
that is, they guessed that (animal, sofa) is a pos-
itive example because animal looks like a hyper-
nym (Levy et al., 2015).
</p>
<p>Instead, the H-Feature detector model trains a
linear classifier using concatenation, as described
above, and then removes this prototypical infor-
mation from the word vectors by projecting them
on a hyperplane orthogonal to the separating hy-
perplane learned by the linear classifier. By re-
peating this process, one can learn multiple classi-
fiers, each of which increases the models represen-
tational power. In each iteration i, four features are
extracted to represent the word pair, based on the
current representations of the word pair (~x, ~y) and
the hyperplane ~pi learned in the current iteration :
</p>
<p>1. The similarity between ~x and the hyperplane,
~x.~pi
</p>
<p>2. The similarity between ~y and the hyperplane,
~y.~pi
</p>
<p>3. The similarity between the two words, ~x.~y
4. The similarity between the difference of the
</p>
<p>two words, and the hyperplane, (~y − ~x).~pi
</p>
<p>Features 1 and 2 capture similarities like the
one included in the concatenation classifier. The
third feature aims to overcome the shortcomings
of the concatenation model by directly modeling
</p>
<p>the similarities between the two target words. Fi-
nally, the fourth feature captures the distributional
inclusion hypothesis (Geffet and Dagan, 2005) – if
word v is a hypernym of u, then the set of features
of u are included in the set of features of v – by
intuitively capturing whether y includes x (Roller
et al., 2014).
</p>
<p>5 Experimental Set-up
</p>
<p>Tasks In addition to WHIC, we evaluate
our context-aware representations on CONTEXT-
PPDB. As mentioned in Section 2.3, CONTEXT-
PPDB is a dataset for fine-grained lexical infer-
ence in context that captures other semantic rela-
tions beyond hypernymy. It has been created us-
ing 375 word pairs from a subset of the English
Paraphrase Database (Ganitkevitch et al., 2013;
Pavlick et al., 2015). These word pairs are semi-
automatically labeled with semantic relations out-
of-context. Shwartz and Dagan (2016) augmented
them with examples of word usage in context, and
re-annotated the word pairs given the extra con-
textual information. The final dataset consists of
3750 words/contexts tuples with a corresponding
semantic label, one of which is entailment.
</p>
<p>All our experiments are with the default
train/dev/test splits on both datasets.
</p>
<p>Contextualized Word Representations To ob-
tain the Context2Vec representations, we use
an existing 600-dimensional model trained on
ukWaC (Ferraresi et al., 2006). We use 600 di-
mensional GloVe embeddings trained on the same
corpus to create ~wl, ~wr, Cl, and Cr, and allow for
a controlled comparison with Context2Vec. Con-
text2Vec representations are significantly more ex-
pensive to train: Melamud et al. (2016) indicate
that training requires ~30 hours on a Tesla K80
GPU, while the GloVe embeddings can be trained
on the exact same amount of data in less than 7
hours on a CPU.
</p>
<p>Supervised Lexical Entailment Classifier We
use an SVM with an RBF kernel for WHIC and
Logistic Regression for CONTEXT-PPDB as im-
plemented in Scikit-Learn 3 as our classifiers, to
allow for exact comparisons with past work on
CONTEXT-PPDB. We use default parameters, ex-
cept for adding class weights in the WHIC exper-
iments to account for the unbalanced data. For
WHIC we use features derived from the H-Feature
</p>
<p>3http://scikit-learn.org
</p>
<p>37</p>
<p />
</div>
<div class="page"><p />
<p>model described in Section 4. For CONTEXT-
PPDB we simply concatenate the representations
and use them directly as the features. We evaluate
the predictions using F1 score.
</p>
<p>6 Experiments on WHIC
</p>
<p>In our first set of experiments, we evaluate the two
models described in Section 3 on WHIC under a
variety of combinations.
</p>
<p>6.1 Overall Results
</p>
<p>Results are summarized in Table 2. Supervised
models4 outperform the baseline that always pre-
dict that hypernymy holds (“All True Baseline”)
by up to 16 F-score points. Context-aware mod-
els outperform context-agnostic models by up
to 3 points5. GloVe and Context2Vec mod-
els yield similar F1, both when used as word
type representations alone, and when combined
with masked representations. However, GloVe
and Context2Vec representations capture comple-
mentary information: GloVe yields slightly bet-
ter precision while Context2Vec models yield
significantly better recall. The best perfor-
mance overall is obtained by a hybrid model that
uses word-type representations from Context2Vec
and masked context-aware representations derived
from GloVe.
</p>
<p>Additionally using Context2Vec vectors di-
rectly (~cl,c2v,~cr,c2v) performs much worse than us-
ing them as masks (~wl,c2v,mask,~cr,c2v,mask). This
highlights the benefit of using context to influence
the word type representation rather than to directly
compare word and context representations.
</p>
<p>Finally, there is no benefit in using the context-
aware masked representations without the word
type representations: using just the masked rep-
resentations by themselves does worse than using
them in combination with the word type represen-
tations.
</p>
<p>Overall, the scores in Table 2 highlight the chal-
lenging nature of WHIC, and leave scope for
improvement with potentially better models for
context-aware representations.
</p>
<p>4We also tried two unsupervised context-agnostic base-
lines using cosine similarity and balAPinc (Kotlerman et al.,
2010) but they trivially predicted all pairs as entailing
</p>
<p>5A statistically significant difference with p &lt; 0.01 under
the McNemar’s test (McNemar, 1947)
</p>
<p>Supervised Model Config.
Word-type Context-aware P R F
</p>
<p>GloVe None 44 60 51
GloVe GloVe Masks 42 73 53
None GloVe Masks 32 64 43
</p>
<p>C2V None 40 73 52
C2V C2V Masks 41 73 52
None C2V Masks 30 94 45
C2V C2V Contexts 23 10 14
None C2V Contexts 8 2 3
</p>
<p>C2V Words GloVe Masks 41 78 54
GloVe Words C2V Masks 44 64 52
</p>
<p>All True Baseline 24 100 38
</p>
<p>Table 2: Results on WHIC. a) Word type indi-
cates (GloVe or Context2Vec (C2V)) H-Features
extracted from context-agnostic representations.
b) Context aware indicates H-Features extracted
from the context-aware representations described
in Section 3.
</p>
<p>6.2 Sensitivity to context
</p>
<p>To determine the sensitivity of our models to con-
text changes, we evaluate on the balanced sub-
set of WHIC comprised of positive examples and
negative examples created by permuting contexts
in Step 3 of the dataset creation process. We an-
alyze the predictions using a modified version of
precision, recall and F-score, defined as the pre-
cision, recall, and F1-score calculated over each
(wl,wr) word pair, and then averaged over all word
pairs. We call these measures the Macro-P/R/F1.
</p>
<p>Table 3 shows that context-aware representa-
tions generally improve performance on all three
metrics, but the gain is larger on recall. Again
we observe that models using Context2Vec word
types and masks have a better Macro-R than the
corresponding GloVe models. Overall, the masked
representations obtained from Context2Vec per-
form the best on these metrics, closely followed by
the overall best model that uses the Context2Vec
word type representations and the masked repre-
sentations from GloVe.
</p>
<p>Finally, note that the all-true baseline surpris-
ingly does as well as the best context-aware model
on this metric. However, it cannot detect the direc-
tion of hypernymy (Section 6.3), and the structure
of WHIC allows us to distinguish these two fac-
tors.
</p>
<p>38</p>
<p />
</div>
<div class="page"><p />
<p>Supervised Model Config. Context sensitivity Directionality
Word Type rep. Context-aware rep. Macro-P Macro-R Macro-F Pairwise Acc.
</p>
<p>GloVe None 13 28 17 59
GloVe GloVe Masks 17 35 22 71
None GloVe Masks 13 30 18 59
</p>
<p>C2V None 15 35 21 71
C2V C2V Masks 16 35 21 72
None C2V Masks 18 45 25 62
C2V C2V Contexts 5 5 4 9
None C2V Contexts 1 1 1 1
</p>
<p>C2V GloVe Masks 17 37 23 76
GloVe C2V Masks 14 29 19 63
</p>
<p>All True Baseline 18 50 25 0
</p>
<p>Table 3: Macro-P/R/F1 and Pairwise accuracy, are intended to capture context-awareness (Section 6.2)
and directionality-discrimination abilities (Section 6.3) of the models, respectively.
</p>
<p>6.3 Sensitivity to Entailment Direction
</p>
<p>Next, we evaluate to what extent the models cap-
ture the direction of hypernymy using the balanced
subset of WHIC that consists of all positive exam-
ples and flipped negative examples generated in
Step 4 in the dataset creation process. We mea-
sure directionality by looking at the fraction of
pairs ((wl, wr, cl, cr), (wr, wl, cr, cl)) where both
examples are correctly labeled, i.e. the former is
labeled as =⇒ and the latter as 6=⇒ . We call
this metric the pairwise accuracy.
</p>
<p>As seen in Table 3, the best pairwise accu-
racy is again obtained by the hybrid model using
word type representations from Context2Vec and
the masked representations from GloVe. Overall
Context2Vec models do a better job at capturing
directionality than GloVe.
</p>
<p>6.4 Nature of Contextualized Masks
</p>
<p>We also hypothesized that masked contextual-
ized representations based on the full volume of
the context using min and max operations (Sec-
tion 3.1) better capture salient context dimensions
than the more usual vector averaging approach.
We test this hypothesis empirically by replacing
masked word-in-context representations ~wl,mask
and ~wr,mask by two other ways to capture context.
In the first method, we use the mean of the con-
texts (~cl,mean,~cr,mean). In the second method, we
use (~wl,mean, ~wr,mean), i.e. the masked represen-
tations calculated by using only the mean of the
context, and not the max and min.
</p>
<p>Table 4 shows that our preferred method out-
performs the two alternatives on WHIC, with our
proposed representations outperforming the other
methods by 3 F1 points. Additionally, this in-
crease in performance also comes with significant
improvement in detection of asymmetric relations.
</p>
<p>6.5 Summary
</p>
<p>Overall, both Context2Vec and Glove representa-
tions improve performance over context-agnostic
baselines. Using masking to contextualize word
type representations works better than just us-
ing the context representations as is. The best
performing model is a hybrid model that uses
word type representations from Context2Vec and
masked representations from GloVe. Analysis en-
abled by the structure of the dataset shows that all
masked representations are sensitive to changes in
meaning indicated by glosses from distinct Word-
Net synsets. However, the more expensive Con-
text2Vec representations do a better job at recall
and direction of hypernymy.
</p>
<p>7 CONTEXT-PPDB
</p>
<p>We now experiment on CONTEXT-PPDB to test
the ability of contextualized representations to
capture semantic relations beyond hypernymy, to
aid future work on recognizing other contextual-
ized relationships.
</p>
<p>Shwartz and Dagan (2016) establish a base-
line of 67 F1 on this dataset using rich features
characterizing word pairs drawn from PPDB as
</p>
<p>39</p>
<p />
</div>
<div class="page"><p />
<p>Dataset Representations P R F Context sensitivity Directionality
</p>
<p>WHIC
~cl,mean,~cr,mean 45 59 51 17 58
~wl,mean,~wr,mean 43 62 51 18 61
~wl,mask,~wr,mask 42 73 53 22 71
</p>
<p>Table 4: Impact of masks on WHIC measured by Precision (P), Recall (R), F-Measure (F), context sen-
sitivity (Macro-F1) and directionality (Pairwise accuracy). Replacing our contextualized representations
by a mean representation of the context, or a contextualized representation based only on the mean, leads
to drops in performance.
</p>
<p>Word Type P R F
</p>
<p>Baseline 68 70 67
++ context-aware rep.s 72 72 72
</p>
<p>Table 5: Results on CONTEXT-PPDB. Baseline
indicates the previous state of the art result on this
dataset (Shwartz and Dagan, 2016)
</p>
<p>well as similarity scores between words and con-
texts. The PPDB features notably include scores
for likelihood of context-agnostic entailment la-
bels, distributional similarities, and probabilities
of the word pair being paraphrases, among other
scores. Additionally, word representation fea-
tures are used: given two word/context pairs
(wx, cx, wy, cy), GloVe vectors are used to repre-
sent wx and wy, as well as words in cx and cy, and
are used to extract the following feature, which
capture the most salient word/context similarities
between the two pairs :
</p>
<p>{max
w∈cy
</p>
<p>~wx · ~w, max
w∈cx
</p>
<p>~wy · ~w, max
w∈cx,w′∈cy
</p>
<p>~w · ~w′}
</p>
<p>We augment this system with contextualized
word representations. We use the GloVe based
masked representations, as they can be obtained
with a negligible computation cost in addition the
features already included in the baseline, and as
the labels denote a mix of directional and non-
directional relations. This remarkably yields an
improvement ~5 F1 points compared to the previ-
ous state-of-the-art (Table 5). Breaking down re-
sults per label (Table 6) shows an increase of 8 F1
points for the entailment class. This improvement
again stems from a large increase in recall, mir-
roring the behavior observed on WHIC. The di-
verse “other-related” category also benefits from
context-aware representations.
</p>
<p>Label Baseline ++ Context-aware Rep.s
</p>
<p>Equivalence 76 76
Entailment 79 87
Alternation 55 55
</p>
<p>Other-related 12 28
Independent 77 78
</p>
<p>Table 6: Performance of the baseline and
augmented model on all semantic relations in
CONTEXT-PPDB measured using per-class F1
</p>
<p>8 Related Work
</p>
<p>WordNet and lexical entailment The “is-a” hi-
erarchy of WordNet (Fellbaum, 1998) is a promi-
nent source of information for unsupervised de-
tection of hypernymy and entailment (Harabagiu
and Moldovan, 1998; Shwartz et al., 2015), as well
as a source of various datasets (Baroni and Lenci,
2011; Baroni et al., 2012). WHIC is inspired by
the latter line of work, except that we extract ex-
emplar contexts from WordNet in addition to rela-
tions between words.
</p>
<p>Modeling word meaning in context Prior mod-
els for the meaning of a word in a given context
aimed to capture semantic equivalence in tasks
such as lexical substitution, word sense disam-
biguation or paraphrase ranking, rather than asym-
metric relations such as entailment. One line
of work (Dinu and Lapata, 2010; Reisinger and
Mooney, 2010) views each word as a set of la-
tent word senses. These models rely on token rep-
resentations for individual occurrences of a word
and then choose a set of token vectors based on
the current context. An alternate set of models
(Erk and Padó, 2008; Thater et al., 2011; Dinu
et al., 2012) avoids defining a fixed set of word
senses, and instead contextualizes word type vec-
tors as we do here. These models share the idea
</p>
<p>40</p>
<p />
</div>
<div class="page"><p />
<p>of using an element-wise multiplication to apply a
context mask to word type representations. The
nature of the context representation varies: Erk
and Padó (2008) use inverse selectional prefer-
ences; Thater et al. (2010) combine a first order
co-occurrence based representation for the context
with a second order representation for the target,
Thater et al. (2011) rely on syntactic dependencies
to define context. Apidianaki (2016) shows that
bag-of-word context representation within a small
context window works as well as syntactic defini-
tions of context for ranking paraphrases in context.
</p>
<p>Our use of convolution is motivated by success
of similar models on sentence classification tasks.
Tang et al. (2014) uses convolution over embed-
ding matrices for unigrams, bigrams, and trigrams,
while Hovy (2015) uses just unigrams. However,
all these works use the resulting representations to
predict properties of the sentence (e.g., sentiment),
rather than to contextualize target word represen-
tations.
</p>
<p>In-context lexical semantic tasks Besides en-
tailment, other lexical semantic tasks studied in
context include lexical substitution (McCarthy and
Navigli, 2007) and cross-lingual lexical substitu-
tion (Mihalcea et al., 2010). The focus of these
tasks and their related datasets is on synonymy
and translation equivalence, since they require one
to predict substitutes for a target word instance,
which preserve its meaning in a given sentential
context. On the other hand, the focus of this work
and WHIC is on detecting more fine-grained rela-
tions via lexical entailment. Another related task
is that of paraphrase ranking (Apidianaki, 2016).
The work by Apidianaki (2016) is also notable be-
cause of their successful use of models of word-
meaning in context from Thater et al. (2011),
which is closely related to our work.
</p>
<p>9 Conclusion
</p>
<p>We introduced WHIC, a dataset to evaluate lexi-
cal entailment in context, providing exemplar sen-
tences to ground the meaning of words being
considered for entailment, and challenging exam-
ples designed to capture entailment direction ac-
curately.
</p>
<p>We showed that supervised models developed
for context-agnostic lexical entailment can address
the context-aware task to some extent, when re-
placing word representations with a contextual-
ized version. We compared two contextualized
</p>
<p>representations including (1) a simple context-
aware representation based on the geometry of
word embeddings, and (2) Context2Vec, a more
expensive BiLSTM-based model that yields repre-
sentations of words and their context in the same
space. Both improve performance over context-
agnostic models, and have complementary prop-
erties: models using Context2Vec are more accu-
rate at discriminating the direction of entailment.
They also have a better recall when measured us-
ing metrics designed to test sensitivity to context.
Finally, we also showed that contextualized repre-
sentations can improve detection of other semantic
relations in context.
</p>
<p>While encouraging, the performance of mod-
els considered leave substantial room for improve-
ment. For instance, it remains to be seen whether
richer features for the supervised models and
richer context representations can improve sensi-
tivity to context, and whether the nuances of the
task can be better captured with annotations on
a graded scale, following previous work on word
meaning in context (Erk et al., 2013).
</p>
<p>Acknowledgements
</p>
<p>The authors thank the anonymous reviewers for
their comments, as well as the members of the
CLIP lab at UMD and Mona Diab for many con-
versations which helped shape this paper. We also
thank Vered Shwartz for help with data and code
for CONTEXT-PPDB, and Stephen Roller for help
with the H-Feature detector code.
</p>
<p>References
Marianna Apidianaki. 2016. Vector-space models for
</p>
<p>PPDB paraphrase ranking in context. In Proceed-
ings of EMNLP 2016. Austin, TX, USA, pages
2028–2034.
</p>
<p>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh
Do, and Chung-chieh Shan. 2012. Entailment
above the word level in distributional seman-
tics. In Proceedings of EACL 2012. pages 23–32.
http://dl.acm.org/citation.cfm?id=2380822.
</p>
<p>Marco Baroni and Alessandro Lenci. 2011.
How we BLESSed distributional semantic
evaluation. In Proceedings of the GEMS
2011 Workshop on GEometrical Models of
Natural Language Semantics. pages 1–10.
http://dl.acm.org/citation.cfm?id=2140490.2140491.
</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of the First
</p>
<p>41</p>
<p />
</div>
<div class="page"><p />
<p>International Conference on Machine Learning
Challenges: Evaluating Predictive Uncertainty Vi-
sual Object Classification, and Recognizing Textual
Entailment. Springer-Verlag, Southampton, UK,
MLCW’05, pages 177–190.
</p>
<p>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing Textual Entail-
ment: Models and Applications. Morgan &amp; Clay-
pool Publishers.
</p>
<p>Georgiana Dinu and Mirella Lapata. 2010. Mea-
suring Distributional Similarity in Context. In
Proceedings of EMNLP 2010. Cambridge, MA,
USA, pages 1162–1172. http://eprints.pascal-
network.org/archive/00008156/.
</p>
<p>Georgiana Dinu, Stefan Thater, and Sören Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL-HLT 2012. pages
611–615.
</p>
<p>Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2013. Measuring Word Meaning in Context. Com-
putational Linguistics 39(3).
</p>
<p>Katrin Erk and Sebastian Padó. 2008. A struc-
tured vector space model for word meaning
in context. In Proceedings of EMNLP 2010.
Honolulu, HA, USA, October, pages 897–906.
https://doi.org/10.3115/1613715.1613831.
</p>
<p>Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
</p>
<p>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2006. Introducing and evaluating
ukWaC , a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop.
</p>
<p>Juri Ganitkevitch, Benjamin Van Durme,
and Chris Callison-Burch. 2013. PPDB
: The Paraphrase Database. Proceedings
of NAACL-HLT 2013 (June):758—-764.
http://cs.jhu.edu/ ccb/publications/ppdb.pdf.
</p>
<p>Maayan Geffet and Ido Dagan. 2005. The Distribu-
tional Inclusion Hypotheses and Lexical Entailment.
In Proceedings of ACL 2005. Ann Arbor, MI, June,
pages 107–114.
</p>
<p>Sanda Harabagiu and Andrew Hickl. 2006.
Methods for Using Textual Entailment
in Open-Domain Question Answering.
Proceedings of ACL (July):905–912.
https://doi.org/10.3115/1220175.1220289.
</p>
<p>Sanda Harabagiu and Dan Moldovan. 1998. Knowl-
edge processing on an extended wordnet. WordNet:
An electronic lexical database 305:381–405.
</p>
<p>Dirk Hovy. 2015. Demographic Factors Improve Clas-
sification Performance. In Proceedings of ACL-
IJCNLP 2015. Beijing, China, pages 752–762.
</p>
<p>Lili Kotlerman, Ido Dagan, Idan Szpektor, and
Maayan Zhitomirsky-Geffet. 2010. Directional
Distributional Similarity for Lexical Inference.
Natural Language Engineering 16(4):359–389.
https://doi.org/10.1017/S1351324910000124.
</p>
<p>German Kruszewski and Marco Baroni. 2015. Deriv-
ing Boolean structures from distributional vectors.
Transactions of ACL 3:375–388.
</p>
<p>Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do Supervised Distributional Meth-
ods Really Learn Lexical Inference Relations? In
NAACL HLT 2015. pages 970–976.
</p>
<p>Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task.
In Proceedings of SEMEVAL 2007. pages 48–53.
https://doi.org/10.1007/s10579-009-9084-1.
</p>
<p>Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika 12(2):153–157.
</p>
<p>Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning Generic Context Em-
bedding with Bidirectional LSTM. In Proceedings
of CoNLL 2016. Berlin, Germany, pages 51–61.
</p>
<p>Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. SemEval-2010 Task 2: Cross-Lingual Lexi-
cal Substitution. In Proceedings of SemEval 2010
(ACL 2010). Uppsala, Sweden, July, pages 9–14.
</p>
<p>Andrea Moro and Roberto Navigli. 2015. Semeval-
2015 task 13: Multilingual all-words sense disam-
biguation and entity linking. Proc. of SemEval-2015
.
</p>
<p>Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,
Benjamin Van Durme, and Chris Callison-Burch.
2015. PPDB 2.0: Better paraphrase ranking, fine-
grained entailment relations, word embeddings, and
style classification. Proceedings of ACL-IJCNLP
2015 pages 425–430.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D Manning. 2014. GloVe: Global Vec-
tors for Word Representation. In Proceedings
of EMNLP 2014. Doha, Qatar, pages 1532–1543.
https://doi.org/10.3115/v1/D14-1162.
</p>
<p>Joseph Reisinger and Raymond J Mooney. 2010.
Multi-Prototype Vector-Space Models of Word
Meaning. In Proceedings of NAACL 2010. Los An-
geles, CA, June, pages 109–117.
</p>
<p>Stephen Roller and Katrin Erk. 2016. Relations
such as Hypernymy: Identifying and Exploiting
Hearst Patterns in Distributional Vectors for Lex-
ical Entailment. Proceedings of EMNLP 2016
http://arxiv.org/abs/1605.05433.
</p>
<p>Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet Selective: Supervised Distributional
Hypernymy Detection. Proceedings of COLING
2014 pages 1025–1036.
</p>
<p>42</p>
<p />
</div>
<div class="page"><p />
<p>Vered Shwartz and Ido Dagan. 2016. Adding Context
to Semantic Data-Driven Paraphrasing. In Proceed-
ings of *SEM 2016. Berlin, Germany, pages 108–
113.
</p>
<p>Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving Hypernymy Detection with an Integrated
Pattern-based and Distributional Method. In Pro-
ceedings of ACL 2016.
</p>
<p>Vered Shwartz, Omer Levy, Ido Dagan, and Jacob
Goldberger. 2015. Learning to Exploit Structured
Resources for Lexical Inference. In Proceedings
of CoNLL 2015. Beijing, China, pages 175–184.
http://www.aclweb.org/anthology/K15-1018.
</p>
<p>Vered Shwartz, Enrico Santus, and Dominik
Schlechtweg. 2017. Hypernyms under Siege:
Linguistically-motivated Artillery for Hypernymy
Detection. In Proceedings of EACL 2017. Valencia,
Spain.
</p>
<p>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding. In Proceedings of
ACL 2014. Baltimore, MD, USA, pages 1555–1565.
https://doi.org/10.3115/1220575.1220648.
</p>
<p>Stefan Thater, Hagen Fuerstenau, and Manfred Pinkal.
2010. Contextualizing Semantic Representa-
tions Using Syntactically Enriched Vector Mod-
els. In Proceedings of ACL 2010. Uppsala, Swe-
den, July, pages 948–957. http://eprints.pascal-
network.org/archive/00008090/.
</p>
<p>Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2011. Word Meaning in Context : A Simple and
Effective Vector Model. In Proceedings of IJCNLP
2011. Chiang Mai, Thailand, pages 1134–1143.
</p>
<p>Peter Turney and Saif Mohammad. 2013. Experiments
with three approaches to recognizing lexical entail-
ment. Natural Language Engineering 1(1):1–42.
https://doi.org/10.1017/S1351324913000387.
</p>
<p>Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and
Timothy Baldwin. 2016. Take and Took, Gaggle and
Goose, Book and Read: Evaluating the Utility of
Vector Differences for Lexical Relation Learning. In
Proceedings of ACL 2016. Berlin, Germany, pages
1671–1682.
</p>
<p>Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics (November 2008).
</p>
<p>43</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 44–53,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Domain-Specific New Words Detection in Chinese
</p>
<p>Ao Chen1, Maosong Sun1
</p>
<p>1Department of Computer Science and Technology,
State Key Lab on Intelligent Technology and Systems,
</p>
<p>National Lab for Information Science and Technology, Tsinghua University, China
chenao3220@gmail.com, sms@mail.tsinghua.edu.cn
</p>
<p>Abstract
</p>
<p>With the explosive growth of Internet,
more and more domain-specific environ-
ments appear, such as forums, blogs,
MOOCs and etc. Domain-specific words
appear in these areas and always play a
critical role in the domain-specific NLP
tasks. This paper aims at extracting Chi-
nese domain-specific new words automat-
ically. The extraction of domain-specific
new words has two parts including both
new words in this domain and the espe-
cially important words. In this work, we
propose a joint statistical model to perform
these two works simultaneously. Com-
pared to traditional new words detection
models, our model doesn't need handcraft
features which are labor intensive. Exper-
imental results demonstrate that our joint
model achieves a better performance com-
pared with the state-of-the-art methods.
</p>
<p>1 Introduction
</p>
<p>Accompanying with the development of Inter-
net, many new specific domains appear, such
as forums, blogs, Massive Open Online Courses
(MOOCs) and etc. There are always a group
of important words in these domains, which
are known as domain-specific words. Domain-
specific words include two types as shown in Ta-
ble 1. The first ones are rare and unambiguous
words which will seldom appear in other domains
such as “栈顶”(stack top) and “二叉树”(binary
tree). These words may cause word segmentation
problems. For example, if we do not recognize
“栈顶”(stack top) as a word, the segmentation “栈
顶 运算符 是 乘号”(the operator at stack top is
multiplication sign) will be like “栈 顶运 算符
是 乘号”. In this case, “栈顶” means “stack top”
</p>
<p>Domain words Translation Type
栈顶 stack top 1
二叉树 binary tree 1
复杂度 complexity 2
遍历 iterate 2
</p>
<p>Table 1: Examples of domain-specific word in
data structure domain
</p>
<p>and “运算符” means “operator”, but in the seg-
mentation result, “顶运” is segmented into a word
in mistake and will bring lots of problems to the
further applications.
</p>
<p>The other type is common and ambiguous
words which have specific new meanings in this
domain, such as “复杂度”(complexity) and “遍
历”(iterate). These words often play important
roles in domain-specific tasks. For example，in
MOOCs which are typical domain-specific envi-
ronments, there is an Automated Navigation Sug-
gestion(ANS)(Zhang et al., 2017) task which sug-
gests a time point for users when they want to
review the front contents of the video. With the
help of the recognition of this type of words, we
can easily give higher weights to those domain-
specific contents.
</p>
<p>After extracting these two type of words, we
can also use them for creating ontologies, term
lists, and in the Semantic Web Area for find-
ing novel entities(Färber et al., 2016). Besides,
in MOOCs area it will also benefit Certification
Prediction(CP)(Coleman et al., 2015) (which pre-
dicts whether a user will get a course certification
or not), Course Recommendation(CR)(Aher and
Lobo, 2013) and so on by providing textual knowl-
edge.
</p>
<p>Researchers have made great efforts to extract
domain-specific words. Traditional new word de-
tection methods usually employ statistical meth-
ods according to the pattern that new words ap-
</p>
<p>44</p>
<p />
</div>
<div class="page"><p />
<p>pear constantly. Such methods like Pointwise Mu-
tual Information(Church and Hanks, 1990), En-
hanced Mutual Information(Zhang et al., 2009),
and Multi-word Expression Distance(Bu et al.,
2010). These methods focus on extracting the first
type of domain-specific words and conduct post-
processing to discover the second type of words.
Deng et al. proposed a statistical model Top-
Words(Deng et al., 2016) to extract the first type
of words, it can imply some of these statistical
measures into the model itself. Besides, it designs
a feature called relative frequency to extract the
second type of domain-specific words. TopWords
is based on a Word Dictionary Model(WDM)(Ge
et al., 1999; Chang and Su, 1997; Cohen et al.,
2007) in which a sentence is sampled from a word
dictionary. To extract the second type of words, it
needs to train its model on a common background
corpus which is expensive and time-consuming.
</p>
<p>To address these issues, we propose a Domain
TopWords model by assuming that a sentence
is sampled from two word dictionaries, one for
common words and the other for domain-specific
words. Besides, we propose a flexible domain
score function to take the external information into
consideration, such as word frequencies in com-
mon background corpus. Therefore, the proposed
model can extract these two types of words jointly.
The main contributions of this paper are summa-
rized as follows:
</p>
<p>• We propose a novel Domain TopWords
model that can extract both two types of
domain-specific words jointly. Experimental
results demonstrate the effectiveness of our
model.
</p>
<p>• Our model achieves a comparable perfor-
mance even with much less information com-
paring to the origin TopWords model.
</p>
<p>The rest of the paper is structured as follows:
the related work will be introduced in section 2.
Our model will be introduced in section 3, in-
cluding model definition and the algorithm details.
Then we will present the experiments in section 4.
Finally, the work is summarized in section 5.
</p>
<p>2 Related work
</p>
<p>New word detection as a superset of new domain-
specific word detection has been investigated for a
long time. New word detection methods mainly
</p>
<p>contain two directions: the first ones conduct
the word segmentation and new word detection
jointly. Most of them are supervised models,
typical models include conditional random fields
proposed by Peng et al. (2004). These super-
vised models cannot be used in domain-specific
words detection directly, due to the lack of an-
notated domain-specific data. In addition, there
are also some unsupervised models, such as Top-
Words proposed by Deng et al. (2016). How-
ever, it needs time-consuming post-processing to
extract the second type of domain-specific words.
</p>
<p>Another type treats new word detection as a sep-
arate task. This line of methods can be mainly
divided into three genres. The first genre is usu-
ally preceded by part-of-speech tagging, and treats
the new word detection task as a classification
problem or directly extracts new words by seman-
tic rules. For example, Argamon et al. (1998)
segments the POS sequence of a multi-word into
small POS tiles, and then counts tile frequency in
both new words and non-new words on training
sets, then uses these counts to extract new word.
Chen and Ma (2002) uses statistical rules to extract
new Chinese word. GuoDong (2005) proposes a
discriminative Markov Model to detect new words
by chunking one or more separated words. How-
ever, these supervised models usually need expert
knowledge to design linguistic features and lots of
annotated data which are expensive and unavail-
able in the new arising domains.
</p>
<p>The second genre employs user behavior data to
detect new words. User typing behavior in Sogou
Chinese Pinyin input method which is the most
popular Chinese input method is used to detect
new words by Zheng et al. (2009). Zhang et al.
(2010) proposed to utilize user query log to ex-
tract new words. However, these works are usu-
ally limited by the availability of the commercial
resources.
</p>
<p>The third genre employs statistical features and
has been extensively studied. In this type of
works, new word detection is usually considered
as multi-word expression extraction. The mea-
surements of multi-word association are crucial
in this type of work. Traditional measurements
include: Pointwise Mutual Information (PMI)
(Church and Hanks, 1990) and Symmetrical Con-
ditional Probability (SCP) (da Silva and Lopes,
1999). Both these two measures are proposed to
measure bi-gram association. Among all 84 bi-
</p>
<p>45</p>
<p />
</div>
<div class="page"><p />
<p>gram association measurements, PMI has been re-
ported to be the best in Czech data(Pecina, 2005).
To measure arbitrary of n-grams, some works sep-
arate n-grams into two parts and adopt the existing
bi-gram based measurements directly. Some other
n-gram based measures are also proposed, such
as Enhanced Mutual Information (EMI) Zhang
et al. (2009). And Multi-word Expression Dis-
tance (MED) was proposed by Bu et al. (2010)
which based on the information distance theory.
The MED measure was reported superior perfor-
mance to EMI, SCP and other measures. And a
pattern based framework which integrates these
statistical features together to detect new words
was proposed by Huang et al. (2014).
</p>
<p>3 Methodology
</p>
<p>In this section, we propose a Domain Top-
Words model. We introduce the Word Dictionary
Model(Ge et al., 1999; Chang and Su, 1997; Co-
hen et al., 2007) and TopWords model proposed
by Deng et al. (2016) in subsection 3.1 and 3.2.
Then we introduce our Domain TopWords model
in subsection 3.3, 3.4 and 3.5. At last, we intro-
duce the modified EM algorithm for our model in
3.6.
</p>
<p>3.1 Word Dictionary Model
</p>
<p>Word Dictionary Model (WDM) is a unigram
language model. It treats a sentence as a se-
quence of basic units, i.e., words, phrases, id-
ioms, which in this paper are broadly defined as
“words”. Let D = {w1, w2, . . . , wN} be the vo-
cabulary (dictionary) which contains all interested
words, then the sentence can be represented as
Si = wiiwi2 . . . wij . And each word is a sequence
of characters. Let A = {a1, . . . , ap} be the ba-
sic characters of the interested language which in
English contain only 26 letters but may include
thousands of distinct Chinese characters. Then the
words can be represented as wi = ai1ai2 . . . aij .
WDM treats each sentence S as a sampling of
words fromD with the sampling probability θi for
word wi. Let θ = (θ1, θ2, ...θN ) be the sampling
probability of the whole D, then the probability of
sampling a specific sentence with length K is:
</p>
<p>P (S|D, θ) =
K∏
k=1
</p>
<p>θk (1)
</p>
<p>3.2 TopWords
</p>
<p>TopWords algorithm based on WDM is introduced
in Deng et al. (2016), and is used as an unsu-
pervised Chinese text segmentation and new word
discovery method. In English texts, words are split
by spacing, but in Chinese, there is no spacing
between words in a sentence. For unsegemented
Chinese text T , let CT denote the set of all possi-
ble segmentations under the dictionary D. Then,
under WDM, we have the probability of a Chinese
text T :
</p>
<p>P (T |D, θ) =
∑
Si∈CT
</p>
<p>P (Si|D, θ) (2)
</p>
<p>Then the likelihood of the parameter θ under the
given corpus G is:
</p>
<p>L(θ|D,G) = P (G|D, θ)
=
∏
Tj∈G
</p>
<p>P (Tj |D, θ)
</p>
<p>=
n∏
j=1
</p>
<p>∑
Si∈CTj
</p>
<p>P (Si|D, θ)
(3)
</p>
<p>where θik is the sampling probability of k-th word
wik in segmentation Si, n is the number of sen-
tences in the corpus G. Then the value of θ
can be estimated by the maximum-likelihood es-
timate(MLE) as follows:
</p>
<p>θ∗ = arg max
θ
</p>
<p>n∏
j=1
</p>
<p>∑
Si∈CTj
</p>
<p>P (Si|D, θ) (4)
</p>
<p>The MLE value of θ can be computed by the
EM algorithm.
</p>
<p>After extracting the first type of domain-specific
words, the author proposes a measure called rel-
ative frequency to extract the second type of
domain-specific words. The relative frequency φki
of word wi in domain k can be estimated as fol-
lows:
</p>
<p>φki =
θki∑K
j=1 θ
</p>
<p>j
i
</p>
<p>(5)
</p>
<p>θki is estimated probability of word wi from the
kth domain.
</p>
<p>46</p>
<p />
</div>
<div class="page"><p />
<p>3.3 Domain Word Dictionary Model
To add the ability to discover domain-specific
words, we first use a Domain Word Dictionary
Model (D-WDM) instead of the origin WDM
model. D-WDM regards a sentence as a sampling
from two word dictionaries, one is the common
background word dictionary Dc and the other is
the domain word dictionary Dd. So a word wi in
a sentence S is sampling first with probability ϕ
to determine which dictionary it is from, and then
with probability θιi from D
</p>
<p>d or Dc. So the proba-
bility of sampling in D-WDM a specific sentence
with length K is:
</p>
<p>P (Si|D, θ, ϕ) =
Ki∏
k=1
</p>
<p>(ϕθcik + (1− ϕ)θdik) (6)
</p>
<p>where
θ = (θc, θd) (7)
</p>
<p>3.4 Domain TopWords
The main difference between Domain
TopWords(D-TopWords) and TopWords is
that D-TopWords is under the D-WDM model. So
there are two word dictionaries, one for common
words and the other for the domain-specific
words. So the likelihood of θ with the given
corpus G under the D-WDM model is:
</p>
<p>L(θ|D,G,ϕ) =
∏
Tj∈G
</p>
<p>∑
Si∈CTj
</p>
<p>P (Si|D, θ, ϕ)
</p>
<p>=
n∏
j=1
</p>
<p>∑
Si∈CTj
</p>
<p>Ki∏
k=1
</p>
<p>(ϕθcik + (1− ϕ)θdik)
</p>
<p>(8)
</p>
<p>where the parameter ϕ need to be fixed. If the ϕ is
adapted, the model will converge at a point which
maximize the probability difference of the words
between the initial θd and θc.
</p>
<p>However, in the D-WDM model, there is no
difference between the domain dictionary Dd and
the common dictionary Dc except the parameter
ϕ. So if we use pure EM algorithm to estimate
the parameter θc and θd, it is obvious that the al-
gorithm cannot determine whether a word should
be sampled from Dc or Dd. And even though
the model has the ability to distinguish the two
kinds of words, it can not find out which words
are domain-specific words either if we only use
</p>
<p>the domain-specific corpus. So we must add the
common background corpus knowledge into our
model and denote this function as domain score
function σ.
</p>
<p>Domain TopWords model uses an optimized
probability function of a segmentation which can
take the background knowledge into considera-
tion. The probability of a segmentation Si of a
sentence as follows:
</p>
<p>P (Si|T ;D, θ, ϕ, σ) = Q(Si|T ;D, θ, ϕ)∑
Si∈CT Q(Si|T ;D, θ, ϕ)
</p>
<p>(9)
</p>
<p>Q(Si|T ;D, θ, ϕ, σ) =
Ki∏
k=1
</p>
<p>(ϕθcik + (1− ϕ)θdikσik)
</p>
<p>(10)
</p>
<p>is the score of the sampled segmentation Si of
T. P (Si|T ;D, θ, ϕ, ) is the nomorlized version of
Q(Si|T ;D, θ, ϕ, σ). σik is the domain score of the
word wik .
</p>
<p>3.5 Selection of domain score σ
</p>
<p>As mentioned above, we need a domain score
function σ to tell our model how to distinguish
whether a word is a common word or a domain-
specific word. This function has several choices,
i.e., the frequency of the word in a large back-
ground corpus, matches of specific templates, and
so on. And we find that statistical features, like
left(right) entropy and mutual information, are
useless as the background knowledge function be-
cause the D-TopWords model itself has taken this
part of features into consideration. We introduce
some choices of the σ function and evaluate the
effects in our experiment.
</p>
<p>Constant Score The first choice of σ function is
a constant function which returns a constant num-
ber for all words. This means there is no encour-
agement for any word so that we will get a θd
</p>
<p>which has almost the same word distribution as θc.
We denote D-TopWords with constant σ function
as D-TopWords+Const.
</p>
<p>Background Frequency Score It is a natural
idea that uses the reciprocal of the frequency of
the word in a common background corpus. This σ
function encourages words with low background
frequency to be sampled from θd. The detailed
</p>
<p>47</p>
<p />
</div>
<div class="page"><p />
<p>function is as follows:
</p>
<p>σ(w) =
</p>
<p>√
P
</p>
<p>Fre(w)
(11)
</p>
<p>where P is a constant. The parameter P need to be
tuned according to the size of the domain corpus,
in our experiments we choose 900 to get a domain
score in the range of 1-10 for domain words. And
Fre(w) is the frequency of word w in background
corpus. We denote the result as D-TopWords+Fre.
</p>
<p>RF Score We use the reciprocal of word prob-
ability in the dictionary of the origin TopWords
method estimated with common background cor-
pus as our domain score. We denote this function
as RF function respect to the relative frequency in
TopWords. The detailed function is as follows:
</p>
<p>σ(w) =
</p>
<p>√
1
</p>
<p>WP (w)× 105 (12)
</p>
<p>where theWP (w) is the word probability of word
w in the dictionary of origin TopWords model. We
denote the result as D-TopWords+RF.
</p>
<p>3.6 EM estimation of θ
</p>
<p>The parameter θ will be estimated by the EM al-
gorithm as we will show below. In the beginning,
we add all the words in vocabulary to θ and de-
fault values will be set for both θc and θd be-
fore EM steps. We employ a “top-down” strat-
egy to discover words, and this is the reason why
this method is called TopWords. It adds all words
into its dictionary at first and then drops the words
whose probability is close to zero (e.g., &lt; 10−8,
and we use this value in our experiments). A good
choice of the default value for θs is the normalized
frequency vector of the words in the corpus.
</p>
<p>Next, we will show the EM algorithm for our D-
TopWords model. Let θ(r) be the estimated value
of θ at the r-th iteration. Then the E-step and the
M-step can be computed as follows. The E-step
computes the Q-function:
</p>
<p>Q(θ|θ(r)) =ES|G,θ(r) [logL(θ;G,S)]
</p>
<p>=
n∑
j=1
</p>
<p>∑
S∈CTj
</p>
<p>P (S|Tj ;D, θ(r))
</p>
<p>logP (S|D, θ)
</p>
<p>(13)
</p>
<p>and the M-step maximizes Q(θ|θ(r)) so as to up-
date θd and θc as follows
</p>
<p>θc(r+1) = (c(r)1 , . . . , c
(r)
N , n)/(n+
</p>
<p>∑
i
</p>
<p>(c(r)i ))
</p>
<p>θd(r+1) = (d(r)1 , . . . , d
(r)
N , n)/(n+
</p>
<p>∑
i
</p>
<p>(d(r)i ))
</p>
<p>(14)
</p>
<p>where
</p>
<p>c
(r)
i =
</p>
<p>∑
Tj∈G
</p>
<p>ci(Tj)
</p>
<p>ci(Tj) =
∑
S∈Tj
</p>
<p>ci(S) · P (S|Tj ;D, θ(r))·
</p>
<p>ϕθ
c(r)
i
</p>
<p>ϕθ
c(r)
i + (1− ϕ)θd(r)i σi
</p>
<p>(15)
</p>
<p>ci(S) is the number of occurrences of wi which is
sampled from common dictionary in sentence S,
and
</p>
<p>d
(r)
i =
</p>
<p>∑
Tj∈G
</p>
<p>di(Tj)
</p>
<p>di(Tj) =
∑
S∈Tj
</p>
<p>di(S) · P (S|Tj ;D, θ(r))·
</p>
<p>(1− ϕ)θd(r)i
ϕθ
</p>
<p>c(r)
i + (1− ϕ)θd(r)i σi
</p>
<p>(16)
</p>
<p>di(S) is the number of occurrences of wi which is
sampled from domain dictionary Dd in sentence
S.
</p>
<p>In the experiment, we found that because of the
lack of domain-specific data the model tends to
get long words and short segmentation. We add
a segmentation length related factor to reduce this
tendency, then our Q function of segmentation Si
becomes:
</p>
<p>Q(Si|θ) = αKi
Ki∏
k=1
</p>
<p>(ϕθcik + (1− ϕ)θdikσik) (17)
</p>
<p>α is a constant parameter. Ki is the length of the
segmentation Si.
</p>
<p>4 Experiments
</p>
<p>In this section, we first perform an experiment to
compare our method to several baselines. And
</p>
<p>48</p>
<p />
</div>
<div class="page"><p />
<p>top K words⇒ 100 200 400 700
Huang et al.(2014) 0.435 0.413 0.378 0.353
</p>
<p>D-TopWords+Const 0.266 0.162 0.152 0.150
TopWords+Fre 0.630 0.576 0.495 0.412
</p>
<p>D-TopWords+Fre 0.719 0.664 0.573 0.504
TopWords+RF 0.759 0.679 0.601 0.548
</p>
<p>D-TopWords+RF 0.795 0.705 0.615 0.553
</p>
<p>Table 2: Discovering new words in data structure domain (MAP)
</p>
<p>then we perform parameter analysis to demon-
strate how the parameters will affect our model.
At last, we conduct some case studies to analysis
these methods in details.
</p>
<p>4.1 Data Preparation
We use transcripts of an online course called Data
Structure from Xuetangx.com. Xuetangx.com is
one of the biggest MOOC platforms in China.
These transcripts are a total of 55,045 lines, in-
cluding 655312 Chinese characters in it and totally
1,792 different characters.
</p>
<p>We segment the corpus by characters and count
the frequency of character-based n-grams from un-
igram up to 7-gram. We drop words with the fre-
quency less than 5 and result in a 55,452 lines n-
gram list. The resulted n-gram list is very sparse
(close to 1:170) and most of the results are obvi-
ously meaningless (like “这样一” which means
“one such”). We asked two annotators to label
these n-grams. These two annotators are requested
to judge whether an n-gram is a domain-specific
word or not, it takes almost one week to anno-
tate these n-grams. If there is a disagreement in
these annotations, the annotators will discuss the
final annotation and result in a 12.6% disagree-
ment ratio. Most of the disagreements are like “访
问”(visit) and “插入”(insert) which are somewhat
ambiguous. Finally, we use a relatively strict stan-
dard, this results in 326 domain-specific words.
The final annotated file can be accessed in our
Github repo1.
</p>
<p>We use YUWEI corpus as our common back-
ground corpus. This corpus is developed by the
National Language Commission, which contains
25,000,309 words with 51,311,659 characters.
</p>
<p>4.2 Evaluation Metric
The output of our method is a ranked list, so we
use mean average precision (MAP) as one of our
</p>
<p>1http://github.com/dreamszl/dtopwords
</p>
<p>evaluation metrics. The MAP value is computed
as follows:
</p>
<p>MAP (K) =
∑K
</p>
<p>k=1 P (k)× rel(k)∑K
k=1 rel(k)
</p>
<p>(18)
</p>
<p>where the P (k) is the precision of the top k words,
rel(k) is a indicator function which return 1 when
word at rank k is a domain-specific word and
0 otherwise. K is the length of the result list.
When we get a list whose elements are all domain-
specific words, the MAP (K) will be 1.
</p>
<p>We will also display the precision-recall curves
of our results.
</p>
<p>4.3 Discovering New Words
4.3.1 Experiment Settings
We compare different settings of our method with
two baselines. The first baseline is pattern-based
unsupervised new word detection method, which
is proposed by Huang et al. (2014). The follow-
ing statistical features are taken into considera-
tion: left pattern entropy (LPE), normalized multi-
word expression distance (NMED), enhanced mu-
tual information (EMI). We implement both char-
acter based and word-based version, and the word-
based version outperforms character based ver-
sion. We use the optimal parameter setting in
Huang's method, which is the LPE+NMED setting
in their paper. And we use annotated words to ex-
tract the candidate patterns which is a pretty good
treatment for this method.
</p>
<p>The second baseline is origin TopWords method
which has been mentioned in above section. We
first run the TopWords method in the domain-
specific corpus, and then use a function to rerank
the word dictionary θ. We use two functions to
rerank the dictionary. The first one is the back-
ground frequency function and we denote this ver-
sion as TopWords+Fre. The second one is the stan-
dard relative frequency method, we use the dictio-
nary θB of TopWords method run in background
</p>
<p>49</p>
<p />
</div>
<div class="page"><p />
<p>D-TopWords+Fre TopWords+Fre Huang et al.
具体来说(specifically speaking) 接下来(next) 确实(indeed)
请注意(attention please) 换而言之(in other words) 至少(at least)
换而言之(in other words) 具体来说(specifically speaking) 对齐位置(alignment position)
</p>
<p>字符(character) 同学们好(hello students) 顺序性(succession)
括号(brackets) 我们(we) 诸如此类(and so on)
</p>
<p>Table 3: Top 5 wrong results of D-TopWords+Fre, TopWords+Fre and Huang et al.'s method
</p>
<p>Figure 1: PR-Curves of our methods and two base-
lines
</p>
<p>corpus to rerank θ. We denote this version as Top-
Words+RF.
</p>
<p>4.3.2 Result and Analysis
(1) The MAP values of all the methods are
shown in Table 2, and the PR-curves are shown
in Figure 1. From the results, we can see
our D-TopWords+RF and TopWords+RF achieve
the best performance. Our D-TopWords+RF
achieves better performance than TopWords+RF
method, especially when the recall is lower our
D-TopWords+RF outperforms TopWords+RF ob-
viously as shown in Figure 1. In the actual appli-
cation scenario, our model is more practical as the
top results returned by the model are more impor-
tant.
</p>
<p>(2) Our D-TopWords methods achieve better
performance than the corresponding TopWords re-
sults. We expect that our D-TopWords model
can use the external information more effectively
and accurately. Our D-TopWords model will give
more weights to the probability whether a se-
quence can be a word or not, and the TopWords
model will more reliable on the external informa-
tion.
</p>
<p>(3) More than that, our D-TopWords+Fre meth-
</p>
<p>ods is significantly better than TopWords+Fre
model and comparable to the D-TopWords+RF
and TopWords+Rf model. The external back-
ground information RF takes the probability a se-
quence can be a word or not into consideration,
however, our D-TopWords can consider this infor-
mation in the model itself. So RF information is
relative redundancy than Fre information to our
D-TopWords model. The RF information needs
to be trained on the common background corpus
when the common background corpus is large it
will take a very long time.
</p>
<p>(4) We perform experiments of Huang et al.'s
method with different domain score functions and
all of these result in a poor performance. With the
recall raising the precision decreases sharply, we
suppose that it is because such statistical features
based models cannot deal with low-frequency
words well. However, our model can deal with
this kind of words better by using the context in-
formation. And our model can hold a better bal-
ance between the probability whether a sequence
can be a word or not and the domain score, which
is hard for Huang et al.'s method.
</p>
<p>4.4 Parameter Tuning
</p>
<p>Table 5 shows how the performance changes with
different α which is the segmentation length re-
lated parameter and ϕ which is the dictionary
weight parameter. As we can see, the perfor-
mance gets better when ϕ increases and get the
best result when ϕ is 0.9. ϕ represents the prob-
ability a word is sampled from the common dic-
tionary, so it means that a word is sampled from
the common dictionary with a 90% possibility and
domain-specific dictionary with 10%.
</p>
<p>It achieves the best performance when ϕ is set
as 0.9 and α is set as 100. Looking into the results,
we found α determines the length of the words in
θ. When α chooses a smaller value the results tend
to be longer, when α chooses bigger value the re-
sults tend to be shorter. And when the size of cor-
</p>
<p>50</p>
<p />
</div>
<div class="page"><p />
<p>Data Structure University Chemistry Nuclear Physics
遍历(iterate) 过程当中(in the process) 衰变(decay)
</p>
<p>关键码(key code) 平衡常数(equilibrium constant) 活度(activity)
递归(recursive) 配合物(complex) 放射源(redioactive source)
</p>
<p>具体来说(specifically speaking) 解离(dissociation) γ 射线(γ-ray)
复杂度(complexity) 吉布斯自由能(Gibbs free energy) 去表示(to express)
</p>
<p>BST (binary search tree) 杂化轨道(hybrid track) 入射粒(incident grain)
左孩子(left child) 孤对电子(lone paired electron) MeV (MeV)
运算符(operator) 电极电势(electrode potential) 靶核(target nucleus)
</p>
<p>数据结构(data structure) 同学们好(hello students) 半衰期(half-life period)
B树(B tree) 反应速率(reaction rate) 核素(species)
</p>
<p>Table 4: Top 10 results of D-TopWords+Fre in three courses
</p>
<p>ϕ
α
</p>
<p>10 50 100 500 1000
</p>
<p>0.3 0.243 0.344 0.389 0.416 0.429
0.5 0.323 0.441 0.479 0.529 0.516
0.7 0.405 0.513 0.559 0.593 0.483
0.9 0.437 0.672 0.719 0.547 0.448
0.99 0.306 0.470 0.479 0.519 0.447
</p>
<p>Table 5: MAP of top 100 results'performance
with different α and ϕ, under the D-
TopWords+Fre model.
</p>
<p>pus increasing, a smaller α value will get better
performance. We set α as 10 when estimates θ of
the common background corpus.
</p>
<p>4.5 Case study
</p>
<p>(1) The top five wrong results of D-TopWords+RF
and TopWords+RF are similar. There are some
wrong results appearing in top 100 results in
TopWords+RF but not in D-TopWords+RF such
as “大 家 注 意”(everybody attention). Af-
ter inspecting the common dictionary θc in D-
TopWords+RF, we find both “大家”(everybody)
and “注意”(attention) are in high ranks. We sup-
pose that the usage of Domain Word Dictionary
Model helps to deal with this type of sequences
better.
</p>
<p>(2) The teacher of this course uses “换而
言之”(in other words), “具体来说”(specifically
speaking) very frequently, so the TopWords+Fre
and D-TopWords+Fre cannot recognize them.
And the wrong results “接下来”(next) and “同学
们好”(hello students) rank lower in our method
compared to TopWords+Fre method (i.e., 25 and
41 vs 4 and 13). We suppose that it is because our
method can keep a better balance of the domain
</p>
<p>score and the probability that a sequence be a new
word. And we inspect other wrong results which
have a similar situation, these words all have a
much lower rank in our method. So these phe-
nomena confirm our assumption that our model
achieves better performance in the sequences that
with low frequency in background corpus but can-
not be a word.
</p>
<p>(3) The wrong result “我们”(we) doesn't appear
in the domain dictionary θd, but appears at rank
7 in the θc dictionary in our model. There are
also some results appearing in a high rank in Top-
Words+Fre method, but in a low rank in our D-
TopWords+Fre method. For example,比如说(for
example) ranks in 39 in TopWords+Fre but rank in
574 in D-TopWords+Fre, “这么样”(the same as it)
ranks in 31 in TopWords+Fre but ranks in 2759 in
D-TopWords+Fre, “也就是”(that's it) ranks in 53
in TopWords+Fre but not appear in our method,
and so on. We suppose that the usage of Do-
main Word Dictionary Model is the reason that our
model can reach a better performance in these type
of words.
</p>
<p>(4) The first 10 results (D-TopWords+Fre) in
Data Structure course and two other courses are
shown in table 4.
</p>
<p>5 Conclusion
</p>
<p>We propose a pure unsupervised D-TopWords
model to extract new domain-specific words.
Compared to traditional new word extraction
model, our model doesn't need handcrafted lexi-
cal features or statistical features and starts from
the unsegmented corpus. Compared to the origin
TopWords model, our model can reach a better
performance with the same information and can
reach a comparable performance with only back-
</p>
<p>51</p>
<p />
</div>
<div class="page"><p />
<p>ground corpus frequency information to the Top-
Words model with the relative frequency which is
expensive and time-consuming.
</p>
<p>Our D-TopWords model adds the ability to dis-
tinguish whether a word from common dictio-
nary or domain dictionary to the origin TopWords
model. We add a domain score parameter to let
our model which can take the external information
easily and efficiently. Experiments show that due
to our modification our model can use much less
external information to reach a comparable perfor-
mance to the origin TopWords model.
</p>
<p>Ackonwledgements
</p>
<p>I am very grateful to my friends in THUNLP
lab and the reviewers for giving many sugges-
tions in the course of my thesis writing. This
work is supported by Center for Massive Online
Education, Tsinghua University, and XuetangX
(http://www.xuetangx.com/), the largest MOOC
platform in China.
</p>
<p>References
Sunita B Aher and LMRJ Lobo. 2013. Combination
</p>
<p>of machine learning algorithms for recommendation
of courses in e-learning system based on historical
data. Knowledge-Based Systems 51:1–14.
</p>
<p>Shlomo Argamon, Ido Dagan, and Yuval Kry-
molowski. 1998. A memory-based approach to
learning shallow natural language patterns. In Pro-
ceedings of the 17th international conference on
Computational linguistics-Volume 1. Association for
Computational Linguistics, pages 67–73.
</p>
<p>Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expressions.
In Proceedings of the 23rd International Conference
on Computational Linguistics. Association for Com-
putational Linguistics, pages 116–124.
</p>
<p>Jing-Shin Chang and Keh-Yih Su. 1997. An unsu-
pervised iterative method for chinese new lexicon
extraction. Computational Linguistics and Chinese
Language Processing 2(2):97–148.
</p>
<p>Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown
word extraction for chinese documents. In Proceed-
ings of the 19th international conference on Compu-
tational linguistics-Volume 1. Association for Com-
putational Linguistics, pages 1–7.
</p>
<p>Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics 16(1):22–29.
</p>
<p>Paul Cohen, Niall Adams, and Brent Heeringa. 2007.
Voting experts: An unsupervised algorithm for
</p>
<p>segmenting sequences. Intelligent Data Analysis
11(6):607–625.
</p>
<p>Cody A Coleman, Daniel T Seaton, and Isaac Chuang.
2015. Probabilistic use cases: Discovering behav-
ioral patterns for predicting certification. In Pro-
ceedings of the Second (2015) ACM Conference on
Learning@ Scale. ACM, pages 141–148.
</p>
<p>J Ferreira da Silva and G Pereira Lopes. 1999. A lo-
cal maxima method and a fair dispersion normaliza-
tion for extracting multi-word units from corpora. In
Sixth Meeting on Mathematics of Language. pages
369–381.
</p>
<p>Ke Deng, Peter K Bol, Kate J Li, and Jun S Liu.
2016. On the unsupervised analysis of domain-
specific chinese texts. Proceedings of the National
Academy of Sciences page 201516510.
</p>
<p>Michael Färber, Achim Rettinger, and Boulos El As-
mar. 2016. On emerging entity detection. In
Knowledge Engineering and Knowledge Manage-
ment: 20th International Conference, EKAW 2016,
Bologna, Italy, November 19-23, 2016, Proceedings
20. Springer, pages 223–238.
</p>
<p>Xianping Ge, Wanda Pratt, and Padhraic Smyth. 1999.
Discovering chinese words from unsegmented text
(poster abstract). In Proceedings of the 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval.
ACM, pages 271–272.
</p>
<p>Zhou GuoDong. 2005. A chunking strategy towards
unknown word detection in chinese word segmenta-
tion. In International Conference on Natural Lan-
guage Processing. Springer, pages 530–541.
</p>
<p>Minlie Huang, Borui Ye, Yichen Wang, Haiqiang
Chen, Junjun Cheng, and Xiaoyan Zhu. 2014. New
word detection for sentiment analysis. In ACL (1).
pages 531–541.
</p>
<p>Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of
the ACL Student Research Workshop. Association
for Computational Linguistics, pages 13–18.
</p>
<p>Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics. Association for Computational
Linguistics, page 562.
</p>
<p>Han Zhang, Maosong Sun, Xiaochen Wang,
Zhengyang Song, Jie Tang, and Jimeng Sun.
2017. Smart jump: Automated navigation sug-
gestion for videos in moocs. In Proceedings of
the 26th International Conference on World Wide
Web Companion. International World Wide Web
Conferences Steering Committee, pages 331–339.
</p>
<p>52</p>
<p />
</div>
<div class="page"><p />
<p>Wen Zhang, Taketoshi Yoshida, Xijin Tang, and Tu-
Bao Ho. 2009. Improving effectiveness of mu-
tual information for substantival multiword expres-
sion extraction. Expert Systems with Applications
36(8):10919–10930.
</p>
<p>Yan Zhang, Maosong Sun, and Yang Zhang. 2010.
Chinese new word detection from query logs. In In-
ternational Conference on Advanced Data Mining
and Applications. Springer, pages 233–243.
</p>
<p>Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru,
and Yang Zhang. 2009. Incorporating user behav-
iors in new word detection. In IJCAI. Citeseer, vol-
ume 9, pages 2101–2106.
</p>
<p>53</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 54–64,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Deep Learning Models For Multiword Expression Identification
</p>
<p>Waseem Gharbieh and Virendra C. Bhavsar and Paul Cook
Faculty of Computer Science, University of New Brunswick
</p>
<p>Fredericton, NB E3B 5A3 Canada
{waseem.gharbieh,bhavsar,paul.cook}@unb.ca
</p>
<p>Abstract
</p>
<p>Multiword expressions (MWEs) are lex-
ical items that can be decomposed into
multiple component words, but have prop-
erties that are unpredictable with respect
to their component words. In this paper
we propose the first deep learning mod-
els for token-level identification of MWEs.
Specifically, we consider a layered feed-
forward network, a recurrent neural net-
work, and convolutional neural networks.
In experimental results we show that con-
volutional neural networks are able to out-
perform the previous state-of-the-art for
MWE identification, with a convolutional
neural network with three hidden layers
giving the best performance.
</p>
<p>1 Introduction
</p>
<p>Multiword expressions (MWEs) are lexical items
that can be decomposed into multiple component
words, but have properties that are idiomatic, i.e.,
marked or unpredictable, with respect to proper-
ties of their component words (Baldwin and Kim,
2010). MWEs include a wide range of phenom-
ena such as noun compounds (e.g., speed limit
and monkey business), verb–particle constructions
(e.g., clean up and throw out), and verb–noun id-
iomatic combinations (e.g., hit the roof and blow
the whistle), as well as named entities (e.g., Prime
Minister Justin Trudeau) and proverbs (e.g., Two
wrongs don’t make a right). One particular chal-
lenge for natural language processing (NLP) is
MWE identification — i.e., to identify which to-
kens in running text correspond to MWEs so that
they can be analyzed accordingly. The challenges
posed by MWEs have led to them to be referred to
as a “pain in the neck” for NLP (Sag et al., 2002);
nevertheless, incorporating knowledge of MWEs
</p>
<p>into NLP applications can lead to improvements in
tasks including machine translation (Carpuat and
Diab, 2010), information retrieval (Newman et al.,
2012), and opinion mining (Berend, 2011).
</p>
<p>Recent work on token-level MWE identification
has focused on methods that are applicable to the
full spectrum of kinds of MWEs (Schneider et al.,
2014a), in contrast to earlier work that tended to
focus on specific kinds of MWEs (Uchiyama et al.,
2005; Fazly et al., 2009; Fothergill and Baldwin,
2012). Deep learning is an emerging class of ma-
chine learning models that have recently achieved
promising results on a range of NLP tasks such
as machine translation (Bahdanau et al., 2015;
Sutskever et al., 2014), named entity recognition
(Lample et al., 2016), natural language generation
(Li et al., 2015), and sentence classification (Kim,
2014). Such models have, however, not yet been
applied to broad-coverage MWE identification.
</p>
<p>In this paper we propose the first deep learn-
ing models for broad-coverage MWE identifica-
tion. Specifically, we propose and evaluate a
layered feedforward network, a recurrent neural
network, and two convolutional neural networks.
We compare these models against the previous
state-of-the-art (Kirilin et al., 2016) and several
more-traditional supervised machine learning ap-
proaches. We show that the convolutional neural
networks outperform the previous state-of-the-art.
This finding is particularly remarkable given the
relatively small size of the training data available,
and demonstrates that deep learning models are
able to learn well from small datasets. Moreover,
we show that our proposed deep learning models
are able to generalize more-effectively than pre-
vious approaches, based on comparisons between
the models’ performances on validation and test
data.
</p>
<p>54</p>
<p />
</div>
<div class="page"><p />
<p>2 Related Work
</p>
<p>MWE identification is the task of determining, at
the token level, which words are parts of MWEs,
and which are not. For example, in the sentence
The staff leaves a lot to be desired (also used in
Figure 1) a lot and leaves to be desired are
MWEs. An important part of MWE identifica-
tion is to be able to distinguish between MWEs
and literal combinations that have the same surface
form; e.g., kick the bucket is ambiguous between
an idiomatic usage — meaning roughly ‘die’ —
which is an MWE, and a literal one which is not.
Many earlier studies on MWE identification have
focused on this type of ambiguity, and treated
the problem as one of word sense disambigua-
tion, where literal and idiomatic usages are con-
sidered different word senses (Birke and Sarkar,
2006; Katz and Giesbrecht, 2006; Li et al., 2010).
Other work has leveraged linguistic knowledge
of properties of MWEs in order to make these
distinctions (Uchiyama et al., 2005; Fazly et al.,
2009; Fothergill and Baldwin, 2012). Crucially,
this work has typically focused on specific kinds
of MWEs, and has not considered identification of
the full spectrum of MWEs.
</p>
<p>More-recent work has considered the identifica-
tion of a wider range of types of MWEs. Brooke
et al. (2014) present an unsupervised learning
approach to segment a corpus into multiword
units based on their predictability. Schneider
et al. (2014a) propose methods for broad-coverage
MWE identification, and evaluate them on a size-
able corpus (Schneider et al., 2014b). They pro-
posed a supervised learning approach based on the
structured perceptron (Collins, 2002). The sys-
tem labels tokens using the BIO convention, where
B indicates the beginning of an MWE, I indi-
cates the continuation of an MWE, and O indi-
cates that the token is not part of an MWE. The
model includes features based on part-of-speech
tags, MWE lexicons, and Brown clusters (Brown
et al., 1992). Qu et al. (2015) later improved
upon that system by using skip-gram embeddings
(Mikolov et al., 2013) instead of Brown clus-
ters with a variant of conditional random fields.
More recently, Constant and Nivre (2016) incor-
porate MWE identification along with dependency
parsing by forming two representations for a sen-
tence: a tree that represents the syntactic depen-
dencies, and a forest of lexical trees that includes
the MWEs identified in the sentence.
</p>
<p>The recent SemEval shared task on Detect-
ing Minimal Semantic Units and their Meanings
(DiMSUM) focused on MWE identification along
with supersense tagging (Schneider et al., 2016).
The best performing system for MWE identifica-
tion for this shared task was that of Kirilin et al.
(2016) which took into consideration all of the ba-
sic features used by Schneider et al. (2014a) and
two novel feature sets. The first one is based on the
YAGO ontology (Suchanek et al., 2007), where
heuristics were applied to extract potential named
entities from the ontology. The second feature set
was GloVe (Pennington et al., 2014) word embed-
dings, with the word vectors scaled by a constant
and divided by the standard deviation of each of
its dimensions. None of the systems that partici-
pated in the DiMSUM shared task considered deep
learning approaches.
</p>
<p>In this paper we propose the first deep learn-
ing approaches to MWE identification. We use
the DiMSUM data for training and evaluating our
models, and compare against the state-of-the-art
method of Kirilin et al. (2016). Here we focus
solely on the MWE identification task, leaving su-
persense tagging for future work.
</p>
<p>3 Neural Network Models
</p>
<p>In this section, we discuss the features extracted
for the neural network models, and the model ar-
chitectures. Schneider et al. (2014b) extracted
roughly 320k sparse features. Because of the large
input feature space, the only feasible way to train
a model on those features is by using a linear clas-
sifier. In contrast to Schneider et al. (2014b) our
aim is to create dense input features to allow neu-
ral network architectures, as well as other machine
learning algorithms, to be trained on them. Specif-
ically, we propose three neural network models:
a layered feedforward network (LFN), a recurrent
neural network (RNN), and a convolutional neural
network (CNN).1
</p>
<p>3.1 Layered Feedforward Network
Although LFNs have been used to solve a wide
range of classification and regression problems,
they have been shown to be less effective for tasks
at which deep learning models excel, such as im-
age classification (Krizhevsky et al., 2012) and
</p>
<p>1In preliminary experiments we also considered a
sequence-to-sequence model (Cho et al., 2014), but found it
to perform poorly relative to the other models, and so do not
discuss it further.
</p>
<p>55</p>
<p />
</div>
<div class="page"><p />
<p>machine translation (Bahdanau et al., 2015). The
LFN is therefore proposed as a benchmark for
comparing the performance of the other architec-
tures, as well as for developing informative input
features. Most feature engineering was carried out
while developing this model and then transferred
to the other architectures.
</p>
<p>The composition of the DiMSUM corpus
(Schneider et al., 2016), and the token-level
lemma and part-of-speech annotations it provides,
influenced our feature extraction. Most of the text
in the DiMSUM corpus is social media text. The
tokens and lemmas were therefore preprocessed
by removing # characters from tokens and lemmas
that contain them, and mapping URLs, numbers,
and any token or lemma containing the @ symbol
to the special tokens URL, NUMBER, and USER,
respectively. After pre-processing, distributed rep-
resentations of all tokens and lemmas were ob-
tained from a skip-gram (Mikolov et al., 2013)
model. Specifically, the gensim (Řehůřek and
Sojka, 2010) implementation of skip-gram was
trained on a snapshot of Wikipedia from Septem-
ber 2015 to learn 100 dimensional word embed-
dings. Any token occurring less than 15 times
was discarded, the context window was set to 5,
the negative sampling rate was set to 5, and un-
known tokens were represented with a zero vector.
The part-of-speech tag for each token was also en-
coded, in this case as a one-hot vector.
</p>
<p>Schneider et al. (2014a) included word shape
features, which can be informative for the iden-
tification of MWEs, especially named entities. We
therefore also include word shape features. These
are binary features for each token and lemma
that capture whether it includes single or double
quotes; consists of all capital letters; starts with
a capital letter (but is otherwise lowercase); con-
tains a number; includes a # or @ character; cor-
responds to a URL; contains any punctuation; and
consists entirely of punctuation characters.
</p>
<p>Schneider et al. (2014a) include features based
on MWE lexicons that represent which tokens and
lemmas are potentially part of an MWE and ac-
cording to which lexicon. We use a script provided
by Schneider et al. (2014a) to include these same
features in our representation.
</p>
<p>Finally, Salton et al. (2016) showed that embed-
ding the entire sentence in which a target MWE
occurs was helpful for distinguishing idiomatic
from literal verb–noun idiomatic combinations.
</p>
<p>We therefore also include a representation for the
entire sentence. Specifically, we separately aver-
age the skip-gram embeddings for the tokens and
lemmas in the sentence containing the target word.
These features were then input into an LFN model
with a single hidden layer, which we refer to as
LFN1.
</p>
<p>3.2 Recurrent Neural Network
</p>
<p>RNNs are a natural fit for many NLP problems due
to their ability to model sequences. Here we apply
an RNN to broad coverage MWE identification.
The token for the current time step is represented
using the same features as LFN1 described above,
except we do not include the average of the skip-
gram representations for tokens and lemmas in the
same sentence as the target word because we ex-
pect the RNN to be able to learn a representation
of the sentence by itself. We use a single layer
RNN model, referred to as RNN1.
</p>
<p>3.3 Convolutional Neural Network
</p>
<p>CNNs have been shown to be powerful classifiers
(Kim, 2014; Kim et al., 2016), and since MWE
identification can be formulated as a classification
task, CNNs have the potential to perform well on
it. The feature representation for the CNN was
split into feature columns to enable the implemen-
tation of the convolution layer. Each feature col-
umn contains the same features as those for the
RNN at each time step but since the CNN does not
learn sequential information, a window of feature
columns was given as an input.
</p>
<p>Multiple filters can then be applied on these
feature columns to extract different local features
across different window sizes. After finding the
optimal number of filters and their sizes, a max-
pooling operation is executed on the values ex-
tracted by the feature map to form the hidden layer
which will be used to produce the predicted out-
put. For our evaluation, we use CNN architectures
with two and three fully connected hidden layers,
which we refer to as CNN2 and CNN3, respec-
tively. We observed that CNNs with 2 and 3 hid-
den layers performed well on the validation set but
adding more layers resulted in overfitting. Simi-
larly, adding more hidden layers to the LFN and
RNN also resulted in overfitting.
</p>
<p>56</p>
<p />
</div>
<div class="page"><p />
<p>4 Data and Evaluation
</p>
<p>This section presents the statistics and structure of
the dataset used for this task, as well as the evalu-
ation methodology.
</p>
<p>4.1 Dataset
We use the DiMSUM dataset (Schneider et al.,
2016) for our experiments, which allows for direct
comparison with previous results. Table 1 displays
the source corpora from which the dataset was
constructed; their domain (i.e., reviews, tweets,
or TED talks); the number of sentences, words,
MWEs, and gappy (i.e., discontiguous) MWES in
each source corpus; and the percentage of tokens
belonging to an MWE in each source corpus. The
dataset is split into training and testing sets such
that the testing data contains a novel text type, i.e.,
TED talks.
</p>
<p>For parameter tuning purposes, we also require
validation data. We form a validation set from the
training data by splitting the training data to cre-
ate 5 folds, where every fold contained 20% vali-
dation data, and the remaining 80% was used for
training.
</p>
<p>4.2 Structure
Every line in the dataset provides 8 pieces of in-
formation: the numeric position of the token in
its sentence; the token itself; its lemmatized form;
its part-of-speech tag; its gold-standard MWE tag;
the position of the last token that is part of its
MWE; its supersense tag;2 and the sentence ID.
Six MWE tags are used for MWE identification
in this dataset, B which indicates the beginning of
an MWE, I which indicates the continuation of an
MWE, O which indicates that the token is not part
of an MWE, b indicates the beginning of a new
MWE inside an MWE, i indicates the continua-
tion of the new MWE inside an MWE, and finally,
o indicates that the token that is inside an MWE is
not part of the nested MWE. This convention as-
sumes that MWEs can only be nested to a depth
of one (i.e., an MWE inside an MWE), and that
MWEs must be properly nested.
</p>
<p>4.3 Performance Metric
We use the link-based F-score evaluation met-
ric from Schneider et al. (2014a), which allows
</p>
<p>2Schneider et al. (2014a) consider MWE identification
and super-sense tagging. We focus only on MWE identifica-
tion in this work and so don’t use the super-sense tag infor-
mation provided in the dataset.
</p>
<p>Figure 1: An example of how a model could tag
a sequence, along with its gold standard tagging
(adapted from Schneider et al. (2016)).
</p>
<p>for direct comparison with prior work. Table 1
shows that the percentage of tokens occurring in
MWEs ranges from 9–22%. As such, MWEs oc-
cur much less frequently than literal word combi-
nations. This evaluation metric correspondingly
puts more emphasis on the ability of the model
to detect MWEs rather than literal word combi-
nations.
</p>
<p>Figure 1 is a diagram adapted from Schneider
et al. (2016) which shows an example of how a
model could tag a sequence, as well as its gold
standard tagging. The MWE tags on top represent
the gold standard, and the MWE tags predicted by
a system are shown on the bottom. A link is de-
fined as the path from one token to another, as in
Figure 1, regardless of the number of tokens in
that path. Precision is calculated as the ratio of
the number of correctly predicted links to the total
number of links predicted by the model. Recall is
calculated in the same way but swapping the gold
standard and predicted links.
</p>
<p>For example, in Figure 1, the model was able
to correctly predict two links. The first link goes
from b to i in the gold standard which is matched
by a predicted link from token 4–5 by the model.
The second link is from token 6–7 in the gold stan-
dard which matches the model’s prediction. Since
the model predicted five links in total, the preci-
sion is 25 .
</p>
<p>To calculate recall, the roles of the gold standard
and model predictions are reversed. This way,
three links have been correctly predicted. Two of
the three links are the previously mentioned links.
The third one is the link from B to I in the gold
standard which corresponds to the path from to-
ken 3–6. Because there are four links in the gold
standard, the recall is therefore 34 .
</p>
<p>The F-score is then calculated based on preci-
sion and recall according to the following equa-
tion:
</p>
<p>57</p>
<p />
</div>
<div class="page"><p />
<p>Split Domain Source corpus Sentences Words MWEs Gappy MWEs % tokens in MWE
</p>
<p>Train
REVIEWS STREUSLE 2.1 (Schneider and Smith, 2015) 3,812 55,579 3,117 397 13%
TWEETS Lowlands (Johannsen et al., 2014) 200 3,062 276 5 22%
TWEETS Ritter (Ritter et al., 2011; Johannsen et al., 2014) 787 15,185 839 65 13%
</p>
<p>Train Total 4,799 73,826 4,232 467 13%
</p>
<p>Test
</p>
<p>REVIEWS Trustpilot (Hovy et al., 2015) 340 6,357 327 13 12%
TWEETS Tweebank (Kong et al., 2014) 500 6,627 362 20 13%
</p>
<p>TED NAIST-NTT (Cettolo et al., 2012; Neubig et al., 2014) 100 2,187 93 2 9%
TED IWSLT test (Cettolo et al., 2012) 60 1,329 55 1 9%
</p>
<p>Test Total 1,000 16,500 837 36 12%
</p>
<p>Table 1: Statistics describing the composition of the DiMSUM dataset.
</p>
<p>1
F
</p>
<p>=
1
2
</p>
<p>(
1
P
</p>
<p>+
1
R
</p>
<p>) (1)
</p>
<p>where F is the F-score, and P and R are precision
and recall, respectively.
</p>
<p>5 Parameter Settings
</p>
<p>In this section, the architecture and parameters of
all neural network models are presented in detail.
The cost function used to train the neural network
models was based on the cost function used by
Schneider et al. (2014a) for this task:
</p>
<p>cost =
|ȳi|∑
i=1
</p>
<p>c(ȳi, yi) (2)
</p>
<p>where ȳi is the ith gold standard MWE tag, and yi
is the ith MWE tag predicted by the neural net-
work model. To ensure that the MWE tag pre-
dicted by the neural network is a probability dis-
tribution, the output layer of all neural models was
the softmax layer. The function c in Equation 2 is
defined as:
</p>
<p>c(ȳi, yi) = ȳi log(yi) + ρ(ȳi�{B}∧ yi�{O}) (3)
</p>
<p>Some MWE tag sequences are invalid, for ex-
ample, a B followed immediately by an O (be-
cause MWEs are composed of multiple tokens),
and similarly, an O cannot occur immediately be-
fore an I (because the beginning of every MWE
must be tagged with a B). We therefore use the
Viterbi algorithm on the output of the neural net-
work models to obtain the valid MWE tag se-
quence with the highest probability. In prelimi-
nary experiments we observed that setting all valid
</p>
<p>transitions to be of equal probability, and the prob-
ability of all invalid transitions to 0, performed
best, and therefore use this strategy.
</p>
<p>5.1 Layered Feedforward Network
</p>
<p>The LFN was used as a benchmark neural network
model against which the performance of the other
deep learning models was compared. The param-
eters that had to be tuned for this model were
the size of the context window, the misclassifica-
tion penalty ρ (in Equation 3), the number of neu-
rons in each hidden layer, the number of iterations
before training is stopped, and the dropout rate.
Optimizing these parameters is important as they
greatly influence the performance of the LFN. For
all models considered, all parameter tuning was
done using the validation data; the test data was
never used for setting parameters.
</p>
<p>Context window of sizes of 1, 2, and 3 tokens to
the left and right were considered. A larger con-
text window allows the model to see additional to-
kens, but also makes the training process longer
and more prone to overfitting. In the case of ρ,
we investigated setting it between 40 and 100. A
small value of ρ would cause the model to have
high precision but low recall, while a larger value
would trade off recall for precision. The number
of neurons in the hidden layer that was examined
ranged from 100 to 1200. Adding more neurons in
a hidden layer, and introducing more hidden lay-
ers, allows the LFN to model more complex func-
tions, but can also make it more prone to over-
fitting. We avoid overfitting by stopping training
after a defined number of iterations (by observ-
ing the performance of the model on the valida-
tion set), and by using dropout (Srivastava et al.,
2014). Dropout combats overfitting by randomly
switching off a percentage of the neurons in a hid-
den layer during training, which allows a neural
network to be more robust in its predictions as it
</p>
<p>58</p>
<p />
</div>
<div class="page"><p />
<p>decreases the association between neurons. It also
has the same effect as ensembling multiple neu-
ral network models because different neurons are
switched on and off in every training iteration. The
dropout rates that we considered ranged from 0.4
to 0.6.
</p>
<p>After running multiple experiments, the best
performing LFN model (LFN1) had a context win-
dow of size 1, which means that the features for
the tokens before and after the target token were
input into the LFN along with the features of the
target token. The value of ρ was set to 50, and
the LFN had a single hidden layer containing 1000
neurons with the tanh activation function. The
LFN was trained for 1000 iterations with a dropout
rate of 0.5.
</p>
<p>5.2 Recurrent Neural Network
As previously mentioned in Section 3.2, RNNs
are a natural fit to many NLP problems due to
their ability to model sequences. At each timestep,
the features for a token were input into the RNN
which then output the corresponding MWE tag for
that token. Many of the parameters that had to be
tuned for the LFN had to be tuned for the RNN
as well: ρ ranged from 10 to 50; the number of
neurons in each hidden layer ranged from 50 to
300; the dropout rate ranged from 0.5 to 1; and we
again tuned the number of iterations before train-
ing is stopped.3 Parameters specific to the RNN
model that had to be tuned include whether the
RNN is unidirectional or bidirectional, and the cell
type, where we consider a fully connected RNN,
an LSTM cell, and a GRU cell.
</p>
<p>After observing the performance of the RNN
on the validation set, the best performing RNN
model (RNN1) was a bidirectional LSTM with ρ
set to 25, with a single hidden layer containing 100
neurons. It was trained for 60 iterations with no
dropout. This indicates that the LSTM cell was
able to handle the complexity of the sequences of
tokens without requiring regularization.
</p>
<p>As we will see in Section 6, RNN1 unfortu-
nately did not perform as well as the other neu-
ral network models. We therefore attempted to
improve its performance using two additional ap-
proaches. In the first approach, the RNN LSTM
was orthogonally initialized. Saxe et al. (2014)
showed that orthogonally initializing RNNs led to
</p>
<p>3We choose parameter settings to explore based on per-
formance on the validation data, and so consider different pa-
rameter settings here than for LFN1.
</p>
<p>better learning in deep neural networks. Never-
theless, orthogonal initialization did not seem to
have an effect on the performance of RNN1. In
the second approach, the dataset was artificially
expanded by splitting the input sentences on punc-
tuation. This provided more “sentences” for the
RNN LSTM to learn from, but again did not im-
prove performance.
</p>
<p>5.3 Convolutional Neural Network
</p>
<p>Every token was represented by a feature column
and these feature columns were then concatenated
to form the input to the CNN. A convolutional
layer was then applied to the input and then max-
pooled to form the hidden layer which was used
to produce the predicted output. There were again
many parameters to optimize in the CNN. We con-
sidered the same settings for the context window
size as for LFN1, i.e., 1, 2, and 3 tokens to the
left and right. The number of neurons in each hid-
den layer ranged from 25 to 200. In contrast to
LFN1 and RNN1, here we consider varying num-
bers of fully connected hidden layers from 1–3.
The dropout rate at the fully connected layers, as
well as the convolutional layer, ranged from 0.3 to
1, and ρ ranged from 10 to 30. Parameters specific
to the convolutional neural network that were op-
timized were the number of filters, which ranged
from 100 to 500, and spanned 1, 2, or 3 feature
columns, and the types of convolution and pooling
operations that were performed. Having a large
number of filters can cause the network to pick up
noise patterns which makes the CNN overfit. The
size of the filters and the types of convolution and
pooling operations is largely dependent on the data
and were optimized according to the performance
of the model on the validation set.
</p>
<p>We experiment with two CNN models, the best
performing CNN model with two hidden layers
(CNN2) and the best performing CNN model with
three hidden layers (CNN3). CNN2 was trained
for 600 iterations and had a context window of size
1 and ρ equal to 20, with 250 filters that spanned
2 feature columns, and 200 filters that spanned all
3 feature columns. Narrow convolution was used
which produced a hidden layer with 450 neurons.
This layer was then input into another hidden layer
containing 50 neurons with the sigmoid activation
function before being passed to the output softmax
layer.
</p>
<p>59</p>
<p />
</div>
<div class="page"><p />
<p>CNN3 is similar to CNN2 but was trained for
900 iterations and had the 450 neuron hidden layer
feed to a hidden layer containing 100 neurons with
the sigmoid activation function. The output of that
layer was then passed to another layer containing
50 neurons with the tanh activation function be-
fore being passed to the output softmax layer. The
intuition behind the tanh activation function for
the last hidden layer is that the layer before it has
the sigmoid activation function. This means that
the values that are passed to the last hidden layer
are between 0 and 1 multiplied by the weights be-
tween the two layers. Since these weights can be
negative, a sigmoid function that can deal with
negative values is required, and the tanh function
satisfies this requirement. Both models have a
dropout rate of 60% on the convolutional and hid-
den layers. They were also given batches of 6000
random examples at each training iteration.
</p>
<p>5.4 Traditional Machine Learning Models
To demonstrate the effectiveness of neural net-
work models, we compare them against more-
traditional, non-neural machine learning models.
Here we consider k-nearest neighbour, random
forests, logistic regression, and gradient boosting.4
</p>
<p>These models were given the same features that
were input into LFN1, and parameter tuning was
also carried out on the validation set. For the k-
nearest neighbour algorithm, k was set to 3, and
the points were weighted by the inverse of their
distance. For random forests, 100 estimators were
used while multiplying the penalty of misclassify-
ing any class other than O as O by 1.2. In the case of
logistic regression, L2 regularization was utilized
with a regularization factor of 0.5. For gradient
boosting, 100 estimators with a maximum depth
of 13 nodes were used. Using a larger number of
estimators for random forest and gradient boost-
ing has shown to improve their cross validation
performance. However, the point of diminishing
returns was found to be at around 50 estimators,
and it was clear that increasing the number of esti-
mators above 100 would not yield any significant
increase in performance. Added to that, with gra-
dient boosting, the cross validation performance
also increased with the maximum node depth, but
the point of diminishing returns was found to be
at around 9, and it was clear that increasing the
</p>
<p>4In preliminary experiments we also considered an
SVM, but found the training time to be impractical, and so
did not consider it further.
</p>
<p>maximum depth beyond 13 would not yield any
significant increase in performance.
</p>
<p>5.5 Implementation Details
</p>
<p>Overall, 983 features were input into the LFN and
traditional machine learning models, and more
than 50 parameter combinations were examined.
Every LFN model required up to 2 days of train-
ing. For the RNN, every token was represented
by a feature vector of length 257, and took around
10 hours to train. More than 30 parameter combi-
nations were examined for the RNN model. Ev-
ery feature column in the CNN model contained
257 features, this amounts to a total of 771 input
features. More than 130 parameter combinations
were tested for the CNN, and it required around
12 hours of training. Tensorflow (et al., 2015) ver-
sion 0.12 was used to implement the neural net-
work models, and scikit-learn (Pedregosa et al.,
2011) was used to implement the traditional ma-
chine learning models. The experiments were run
on 2 GHz Intel Xeon E7-4809 v3 CPUs.
</p>
<p>6 Results
</p>
<p>The average F-score of the models on the five fold
cross validation set, and their F-score on the test
set, along with their generalization, is shown in
Table 2. All models except for that of Kirilin et al.
(2016) — which was already optimized for this
task by its authors — were run on the validation
set to tune their parameters. To evaluate the per-
formance of the models on the test set, the models
were trained on the entire training set (which in-
cludes the validation splits) and then tested on the
test set.
</p>
<p>We first consider the traditional machine learn-
ing models. Amongst these models, gradient
boosting performed best on the validation set,
which can be attributed to the ability of gradient
boosting to learn complex functions and its ro-
bustness to outliers. However, it did not perform
as well on the test set, where logistic regression
performed best, and achieved the best generaliza-
tion out of the traditional machine learning mod-
els. This shows that relatively many instances in
the test set can be correctly classified by using a
hyperplane to separate the dense feature represen-
tations.
</p>
<p>Turning to the proposed neural network mod-
els, LFN1 is indeed a strong baseline for this task.
This model achieved an F-score on the test set that
</p>
<p>60</p>
<p />
</div>
<div class="page"><p />
<p>Model Class Model F-score GeneralizationValidation Set Test Set
</p>
<p>Traditional
Machine
</p>
<p>Learning Models
</p>
<p>k-Nearest Neighbour 48.35 31.30 64.74%
Random Forest 52.26 32.02 61.27%
</p>
<p>Logistic Regression 57.68 53.37 92.53%
Gradient Boosting 64.98 48.79 75.08%
</p>
<p>Neural Network
Models
</p>
<p>LFN1 66.48 57.99 87.23%
RNN1 56.96 53.07 93.17%
CNN2 66.95 59.18 88.39%
CNN3 67.40 59.96 88.96%
</p>
<p>Baseline Models Schneider and Smith (2015) 67.84 57.74 85.11%
Kirilin et al. (2016) - 58.69 -
</p>
<p>Table 2: The average F-score of each model on the 5 fold cross validation set, and their F-score on the
test set, along with their generalization. The best performance in each column is shown in boldface.
</p>
<p>comes close to the previous state-of-the-art of Kir-
ilin et al. (2016). RNN1 achieved the best gener-
alization out of all models considered; however, it
performed relatively poorly compared to the other
neural network models on both the validation and
test sets. The CNN models, CNN2 and CNN3,
both improved over the previous best results on the
test set — with CNN3 achieving the best F-score
overall — and outperformed all other models ex-
cept for (Schneider et al., 2014a) on the validation
set. This shows that the CNN filters were able to
learn what makes a feature column a part of an
MWE or not. That CNN3 outperforms CNN2 fur-
ther shows that adding an extra hidden layer for the
CNN model improves its performance as it is able
to handle more complex mappings. Moreover, the
training data for this task is relatively small; it con-
sists of less than 5,000 sentences. These results
therefore further show that convolutional neural
networks can still achieve good performance when
the amount of training data available is limited.
</p>
<p>The highest F-score on the test set — achieved
by CNN3 — is 59.96. This shows that the task is
quite difficult, and suggests that there is scope for
further improvements. One issue, however, is that
there are notable inconsistencies in the annotations
in the dataset. For example, the expression a few is
labeled as an MWE 15 out of 32 times in the train-
ing set, even though there appears to be no vari-
ation in its usage. Recent efforts have, however,
proposed semi-automated methods for resolving
these inconsistencies (Chan et al., 2017).
</p>
<p>7 Conclusions and Future Work
</p>
<p>We proposed and evaluated the first neural net-
work approaches for multiword expression identi-
fication, and compared their performance against
the previous state-of-art, and more-traditional ma-
chine learning approaches. We showed that our
proposed approach based on a convolutional neu-
ral network (CNN2 and CNN3) outperformed the
previous state-of-the-art for this task. Therefore,
although the task is inherently sequential, formu-
lating it as a classification task enabled the CNN
models to perform well on it. This finding sug-
gests that deep learning methods can still be ef-
fective when only limited amounts of training data
are available. Furthermore, the proposed neural
network-based approaches were able to generalize
more-effectively than previous approaches.
</p>
<p>In future work, we intend to carry out an in-
depth analysis of the errors committed by the neu-
ral network models. Additionally, an ablation
study of the features can be conducted to deter-
mine the effect of each feature set on the overall
performance of the models. The proposed deep
learning models can also be extended to predict
supersense tags in addition to the MWE tags. In
particular, we intend to compare the performance
of a single model that predicts the supersense and
MWE tags, versus two separate models for each
task. Furthermore, we plan to measure the impact
of MWE identification on downstream NLP tasks
by incorporating the predicted MWE tags into ap-
plications such as machine translation.
</p>
<p>61</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgments
</p>
<p>This work is financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada, the New Brunswick Innovation Founda-
tion, and the University of New Brunswick.
</p>
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR2015).
</p>
<p>Timothy Baldwin and Su Nam Kim. 2010. Handbook
of natural language processing. In Nitin Indurkhya
and Fred J. Damerau, editors, Handbook of Natu-
ral Language Processing, CRC Press, Boca Raton,
USA. 2nd edition.
</p>
<p>Gábor Berend. 2011. Opinion expression mining by
exploiting keyphrase extraction. In Proceedings
of 5th International Joint Conference on Natural
Language Processing. Chiang Mai, Thailand, pages
1162–1170.
</p>
<p>Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of the 11th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006).
Trento, Italy, pages 329–336.
</p>
<p>Julian Brooke, Vivian Tsang, Graeme Hirst, and Fraser
Shein. 2014. Unsupervised multiword segmentation
of large corpora using prediction-driven decompo-
sition of n-grams. In COLING 2014, 25th Inter-
national Conference on Computational Linguistics,
Proceedings of the Conference: Technical Papers.
Dublin, Ireland, pages 753–761.
</p>
<p>Peter F. Brown, Vincent J. Della Pietra, Peter V.
de Souza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics 18(4):467–479.
</p>
<p>Marine Carpuat and Mona Diab. 2010. Task-based
evaluation of multiword expressions: a pilot study
in statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics. Los Angeles, Cali-
fornia, pages 242–245.
</p>
<p>Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Web inventory of transcribed and trans-
lated talks. In Proceedings of the 16th Annual Con-
ference of the European Association for Machine
Translation (EAMT 2012). Trento, Italy, pages 261–
268.
</p>
<p>King Chan, Julian Brooke, and Timothy Baldwin.
2017. Semi-automated resolution of inconsistency
</p>
<p>for a harmonized multiword expression and depen-
dency parse annotation. In Proceedings of the 13th
Workshop on Multiword Expressions (MWE 2017).
Valencia, Spain, pages 187–193.
</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, A meeting of SIGDAT,
a Special Interest Group of the ACL. Doha, Qatar,
pages 1724–1734.
</p>
<p>Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2002).
Philadelphia, USA, pages 1–8.
</p>
<p>Matthieu Constant and Joakim Nivre. 2016. A
transition-based system for joint lexical and syn-
tactic analysis. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics. Berlin, Germany, pages 161–171.
</p>
<p>Martı́n Abadi et al. 2015. TensorFlow: Large-
scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.
</p>
<p>Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguis-
tics 35(1):61–103.
</p>
<p>Richard Fothergill and Timothy Baldwin. 2012. Com-
bining resources for mwe-token classification. In
*SEM 2012: The First Joint Conference on Lexical
and Computational Semantics – Volume 1: Proceed-
ings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012).
Montréal, Canada, pages 100–104.
</p>
<p>Dirk Hovy, Anders Johannsen, and Anders Søgaard.
2015. User review sites as a resource for large-scale
sociolinguistic studies. In Proceedings of the 24th
International Conference on World Wide Web. Flo-
rence, Italy, pages 452–461.
</p>
<p>Anders Johannsen, Dirk Hovy, Héctor
Martı́nez Alonso, Barbara Plank, and Anders
Søgaard. 2014. More or less supervised supersense
tagging of twitter. In Proceedings of the Third
Joint Conference on Lexical and Computational
Semantics (*SEM 2014). Dublin, Ireland, pages
1–11.
</p>
<p>Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
</p>
<p>62</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties. Sydney, Australia, pages 12–19.
</p>
<p>Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP, A meeting of SIGDAT,
a Special Interest Group of the ACL. Doha, Qatar,
pages 1746–1751.
</p>
<p>Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence. Phoenix, Ari-
zona, USA, pages 2741–2749.
</p>
<p>Angelika Kirilin, Felix Krauss, and Yannick Versley.
2016. ICL-HD at semeval-2016 task 10: Improv-
ing the detection of minimal semantic units and their
meanings with an ontology and word embeddings.
In Proceedings of the 10th International Workshop
on Semantic Evaluation, SemEval@NAACL-HLT .
San Diego, CA, USA, pages 937–945.
</p>
<p>Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, A meeting of SIGDAT, a Special Interest Group
of the ACL. Doha, Qatar, pages 1001–1012.
</p>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. ImageNet classification with deep con-
volutional neural networks. In F. Pereira, C. J. C.
Burges, L. Bottou, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Systems
25, Curran Associates, Inc., pages 1097–1105.
</p>
<p>Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. San Diego, California, USA, pages 260–
270.
</p>
<p>Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A hierarchical neural autoencoder for paragraphs
and documents. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Feder-
ation of Natural Language Processing, ACL, Volume
1: Long Papers. Beijing, China, pages 1106–1115.
</p>
<p>Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics. Uppsala, Sweden, pages
1138–1147.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, Curran Associates, Inc., pages 3111–3119.
</p>
<p>Graham Neubig, Katsuhito Sudoh, Yusuke Oda, Kevin
Duh, Hajime Tsukada, and Masaaki Nagata. 2014.
The NAIST-NTT TED talk treebank. In Interna-
tional Workshop on Spoken Language Translation
(IWSLT). Lake Tahoe, USA.
</p>
<p>David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmenta-
tion for index term identification and keyphrase ex-
traction. In Proceedings of COLING 2012. Mumbai,
India, pages 2077–2092.
</p>
<p>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research
12:2825–2830.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Doha, Qatar, pages 1532–
1543.
</p>
<p>Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Wei-
wei Hou, Nathan Schneider, and Timothy Baldwin.
2015. Big data small data, in domain out-of domain,
known word unknown word: The impact of word
representations on sequence labelling tasks. In Pro-
ceedings of the Nineteenth Conference on Computa-
tional Natural Language Learning. Beijing, China,
pages 83–93.
</p>
<p>Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. Valletta, Malta,
pages 45–50.
</p>
<p>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, A meeting of SIGDAT, a Special Inter-
est Group of the ACL. Edinburgh, UK, pages 1524–
1534.
</p>
<p>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002). pages 1–15.
</p>
<p>63</p>
<p />
</div>
<div class="page"><p />
<p>Giancarlo Salton, Robert Ross, and John Kelleher.
2016. Idiom token classification using sentential
distributed semantics. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Berlin,
Germany, pages 194–204.
</p>
<p>Andrew M. Saxe, James L. McClelland, and Surya
Ganguli. 2014. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks.
In International Conference on Learning Represen-
tations (ICLR2014).
</p>
<p>Nathan Schneider, Emily Danchik, Chris Dyer, and
A. Noah Smith. 2014a. Discriminative lexical se-
mantic segmentation with gaps: Running the mwe
gamut. Transactions of the Association for Compu-
tational Linguistics (TACL) 2:193–206.
</p>
<p>Nathan Schneider, Dirk Hovy, Anders Johannsen,
and Marine Carpuat. 2016. SemEval-2016 task
10: Detecting minimal semantic units and their
meanings (DiMSUM). In Proceedings of the
10th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT . San Diego, CA, USA,
pages 546–559.
</p>
<p>Nathan Schneider, Spencer Onuffer, Nora Kazour,
Emily Danchik, Michael T. Mordowanec, Henrietta
Conrad, and Noah A. Smith. 2014b. Comprehensive
annotation of multiword expressions in a social web
corpus. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC-2014). Reykjavik, Iceland, pages 455–461.
</p>
<p>Nathan Schneider and A. Noah Smith. 2015. A corpus
and model integrating multiword expressions and
supersenses. In Proceedings of the 2015 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies. Denver, Colorado, pages 1537–1547.
</p>
<p>Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research 15(1):1929–1958.
</p>
<p>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web. Banff, Alberta, Canada,
pages 697–706.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, Curran Associates, Inc., pages 3104–3112.
</p>
<p>Kiyoko Uchiyama, Timothy Baldwin, and Shun
Ishizaki. 2005. Disambiguating Japanese compound
verbs. Computer Speech and Language, Special Is-
sue on Multiword Expressions 19(4):497–512.
</p>
<p>64</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 65–77,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Emotion Intensities in Tweets
</p>
<p>Saif M. Mohammad
Information and Communications Technologies
</p>
<p>National Research Council Canada
Ottawa, Canada
</p>
<p>saif.mohammad@nrc-cnrc.gc.ca
</p>
<p>Felipe Bravo-Marquez
Department of Computer Science
</p>
<p>The University of Waikato
Hamilton, New Zealand
</p>
<p>fbravoma@waikato.ac.nz
</p>
<p>Abstract
</p>
<p>This paper examines the task of detecting
intensity of emotion from text. We cre-
ate the first datasets of tweets annotated
for anger, fear, joy, and sadness intensities.
We use a technique called best–worst scal-
ing (BWS) that improves annotation con-
sistency and obtains reliable fine-grained
scores. We show that emotion-word hash-
tags often impact emotion intensity, usu-
ally conveying a more intense emotion. Fi-
nally, we create a benchmark regression
system and conduct experiments to deter-
mine: which features are useful for detect-
ing emotion intensity; and, the extent to
which two emotions are similar in terms
of how they manifest in language.
</p>
<p>1 Introduction
</p>
<p>We use language to communicate not only the
emotion we are feeling but also the intensity of
the emotion. For example, our utterances can con-
vey that we are very angry, slightly sad, absolutely
elated, etc. Here, intensity refers to the degree
or amount of an emotion such as anger or sad-
ness.1 Natural language applications can benefit
from knowing both the class of emotion and its
intensity. For example, a commercial customer
satisfaction system would prefer to focus first on
instances of significant frustration or anger, as op-
posed to instances of minor inconvenience. How-
ever, most work on automatic emotion detection
has focused on categorical classification (presence
of anger, joy, sadness, etc.). A notable obstacle
in developing automatic affect intensity systems is
the lack of suitable annotated data. Existing af-
fect datasets are predominantly categorical. Anno-
</p>
<p>1Intensity is different from arousal, which refers to the
extent to which an emotion is calming or exciting.
</p>
<p>tating instances for degrees of affect is a substan-
tially more difficult undertaking: respondents are
presented with greater cognitive load and it is par-
ticularly hard to ensure consistency (both across
responses by different annotators and within the
responses produced by an individual annotator).
</p>
<p>Best–Worst Scaling (BWS) is an annotation
scheme that addresses these limitations (Louviere,
1991; Louviere et al., 2015; Kiritchenko and Mo-
hammad, 2016, 2017). Annotators are given n
items (an n-tuple, where n &gt; 1 and commonly
n = 4). They are asked which item is the
best (highest in terms of the property of inter-
est) and which is the worst (lowest in terms of
the property of interest). When working on 4-
tuples, best–worst annotations are particularly ef-
ficient because each best and worst annotation will
reveal the order of five of the six item pairs. For
example, for a 4-tuple with items A, B, C, and D,
if A is the best, and D is the worst, then A &gt; B, A
&gt; C, A &gt; D, B &gt; D, and C &gt; D.
</p>
<p>BWS annotations for a set of 4-tuples can be
easily converted into real-valued scores of associ-
ation between the items and the property of inter-
est (Orme, 2009; Flynn and Marley, 2014). It has
been empirically shown that annotations for 2N
4-tuples is sufficient for obtaining reliable scores
(where N is the number of items) (Louviere, 1991;
Kiritchenko and Mohammad, 2016).2 The lit-
tle work using BWS in computational linguistics
has focused on words (Jurgens et al., 2012; Kir-
itchenko and Mohammad, 2016). It is unclear
whether the approach can be scaled up to larger
textual units such as sentences.
</p>
<p>Twitter has a large and diverse user base,
which entails rich textual content, including non-
standard language such as emoticons, emojis, cre-
</p>
<p>2At its limit, when n = 2, BWS becomes a paired com-
parison (Thurstone, 1927; David, 1963), but then a much
larger set of tuples need to be annotated (closer to N2).
</p>
<p>65</p>
<p />
</div>
<div class="page"><p />
<p>atively spelled words (happee), and hashtagged
words (#luvumom). Tweets are often used to con-
vey one’s emotions, opinions towards products,
and stance over issues. Thus, automatically de-
tecting emotion intensities in tweets has many ap-
plications, including: tracking brand and product
perception, tracking support for issues and poli-
cies, tracking public health and well-being, and
disaster/crisis management.
</p>
<p>In this paper, we present work on detecting
intensities (or degrees) of emotion in tweets.
Specifically, given a tweet and an emotion X,
the goal is to determine the intensity or degree
of emotion X felt by the speaker—a real-valued
score between 0 and 1.3 A score of 1 means that
the speaker feels the highest amount of emotion
X. A score of 0 means that the speaker feels
the lowest amount of emotion X. We annotate a
dataset of tweets for intensity of emotion using
best–worst scaling and crowdsourcing. The main
contributions of this work are summarized below:
• We formulate and develop the task of detecting
</p>
<p>emotion intensities in tweets.
• We create four datasets of tweets annotated
</p>
<p>for intensity of anger, joy, sadness, and fear,
respectively. These are the first of their kind.4
</p>
<p>• We show that Best–Worst Scaling can be suc-
cessfully applied for annotating sentences (and
not just words). We hope that this will encour-
age the use of BWS more widely, producing
more reliable natural language annotations.
</p>
<p>• We annotate both tweets and a hashtag-removed
version of the tweets. We analyse the impact of
hashtags on emotion intensity.
</p>
<p>• We create a regression system, AffectiveTweets
Package, to automatically determine emotion
intensity.5 We show the extent to which various
features help determine emotion intensity. The
system is released as an open-source package
for the Weka workbench.
</p>
<p>• We conduct experiments to show the extent to
which two emotions are similar as per their
manifestation in language, by showing how
predictive the features for one emotion are of
another emotion’s intensity.
3Identifying intensity of emotion evoked in the reader, or
</p>
<p>intensity of emotion felt by an entity mentioned in the tweet,
are also useful, and left for future work.
</p>
<p>4We have also begun work on creating similar datasets
annotated for other emotion categories. We are also creating
a dataset annotated for valence, arousal, and dominance.
</p>
<p>5https://github.com/felipebravom/AffectiveTweets
</p>
<p>• We provide data for a new shared task WASSA-
2017 Shared Task on Emotion Intensity.6 The
competition is organized on a CodaLab website,
where participants can upload their submis-
sions, and the leaderboard reports the results.7
</p>
<p>Twenty-two teams participated. A description
of the task, details of participating systems,
and results are available in Mohammad and
Bravo-Marquez (2017).8
</p>
<p>All of the data, annotation questionnaires, evalua-
tion scripts, regression code, and interactive visu-
alizations of the data are made freely available on
the shared task website.6
</p>
<p>2 Related Work
</p>
<p>Psychologists have argued that some emotions are
more basic than others (Ekman, 1992; Plutchik,
1980; Parrot, 2001; Frijda, 1988). However, they
disagree on which emotions (and how many)
should be classified as basic emotions—some pro-
pose 6, some 8, some 20, and so on. Thus, most ef-
forts in automatic emotion detection have focused
on a handful of emotions, especially since manu-
ally annotating text for a large number of emotions
is arduous. Apart from these categorical models of
emotions, certain dimensional models of emotion
have also been proposed. The most popular among
them, Russell’s circumplex model, asserts that all
emotions are made up of two core dimensions: va-
lence and arousal (Russell, 2003). In this paper,
we describe work on four emotions that are the
most common amongst the many proposals for ba-
sic emotions: anger, fear, joy, and sadness. How-
ever, we have also begun work on other affect cat-
egories, as well as on valence and arousal.
</p>
<p>The vast majority of emotion annotation work
provides discrete binary labels to the text instances
(joy–nojoy, fear–nofear, and so on) (Alm et al.,
2005; Aman and Szpakowicz, 2007; Brooks et al.,
2013; Neviarouskaya et al., 2009; Bollen et al.,
2009). The only annotation effort that provided
scores for degree of emotion is by Strapparava and
Mihalcea (2007) as part of one of the SemEval-
2007 shared task. Annotators were given newspa-
per headlines and asked to provide scores between
</p>
<p>6http://saifmohammad.com/WebPages/EmotionIntensity-
SharedTask.html
</p>
<p>7https://competitions.codalab.org/competitions/16380
8Even though the 2017 WASSA shared task has con-
</p>
<p>cluded, the CodaLab competition website is kept open. Thus
the best results obtained by any system on the 2017 test set
can be found on the CodaLab leaderboard.
</p>
<p>66</p>
<p />
</div>
<div class="page"><p />
<p>0 and 100 via slide bars in a web interface. It is dif-
ficult for humans to provide direct scores at such
fine granularity. A common problem is inconsis-
tency in annotations. One annotator might assign a
score of 79 to a piece of text, whereas another an-
notator may assign a score of 62 to the same text.
It is also common that the same annotator assigns
different scores to the same text instance at differ-
ent points in time. Further, annotators often have
a bias towards different parts of the scale, known
as scale region bias.
</p>
<p>Best–Worst Scaling (BWS) was developed by
Louviere (1991), building on some ground-
breaking research in the 1960s in mathemati-
cal psychology and psychophysics by Anthony
A. J. Marley and Duncan Luce. Kiritchenko
and Mohammad (2017) show through empiri-
cal experiments that BWS produces more re-
liable fine-grained scores than scores obtained
using rating scales. Within the NLP commu-
nity, Best–Worst Scaling (BWS) has thus far
been used only to annotate words: for exam-
ple, for creating datasets for relational similar-
ity (Jurgens et al., 2012), word-sense disambigua-
tion (Jurgens, 2013), word–sentiment intensity
(Kiritchenko et al., 2014), and phrase sentiment
composition (Kiritchenko and Mohammad, 2016).
However, in this work we use BWS to annotate
whole tweets for degree of emotion. With BWS
we address the challenges of direct scoring, and
produce more reliable emotion intensity scores.
Further, this will be the first dataset with emotion
scores for tweets.
</p>
<p>Automatic emotion classification has been pro-
posed for many different kinds of texts, including
tweets (Summa et al., 2016; Mohammad, 2012;
Bollen et al., 2009; Aman and Szpakowicz, 2007;
Brooks et al., 2013). However, there is little work
on emotion regression other than the three submis-
sions to the 2007 SemEval task (Strapparava and
Mihalcea, 2007).
</p>
<p>3 Data
</p>
<p>For each of the four focus emotions, our goal was
to create a dataset of tweets such that:
</p>
<p>• The tweets are associated with various intensi-
ties (or degrees) of emotion.
</p>
<p>• Some tweets have words clearly indicative of
the focus emotion and some tweets do not.
</p>
<p>A random collection of tweets is likely to have a
large proportion of tweets not associated with the
</p>
<p>focus emotion, and thus annotating all of them for
intensity of emotion is sub-optimal. To create a
dataset of tweets rich in a particular emotion, we
use the following methodology.
</p>
<p>For each emotion X, we select 50 to 100 terms
that are associated with that emotion at differ-
ent intensity levels. For example, for the anger
dataset, we use the terms: angry, mad, frustrated,
annoyed, peeved, irritated, miffed, fury, antago-
nism, and so on. For the sadness dataset, we use
the terms: sad, devastated, sullen, down, crying,
dejected, heartbroken, grief, weeping, and so on.
We will refer to these terms as the query terms.
</p>
<p>We identified the query words for an emotion
by first searching the Roget’s Thesaurus to find
categories that had the focus emotion word (or
a close synonym) as the head word.9 We chose
all words listed within these categories to be the
query terms for the corresponding focus emotion.
We polled the Twitter API for tweets that included
the query terms. We discarded retweets (tweets
that start with RT) and tweets with urls. We
created a subset of the remaining tweets by:
</p>
<p>• selecting at most 50 tweets per query term.
• selecting at most 1 tweet for every tweeter–
</p>
<p>query term combination.
</p>
<p>Thus, the master set of tweets is not heavily
skewed towards some tweeters or query terms.
</p>
<p>To study the impact of emotion word hashtags
on the intensity of the whole tweet, we identified
tweets that had a query term in hashtag form
towards the end of the tweet—specifically, within
the trailing portion of the tweet made up solely
of hashtagged words. We created copies of these
tweets and then removed the hashtag query terms
from the copies. The updated tweets were then
added to the master set. Finally, our master set of
7,097 tweets includes:
</p>
<p>1. Hashtag Query Term Tweets (HQT Tweets):
1030 tweets with a query term in the form
of a hashtag (#&lt;query term&gt;) in the trailing
portion of the tweet;
</p>
<p>2. No Query Term Tweets (NQT Tweets):
1030 tweets that are copies of ‘1’, but with the
hashtagged query term removed;
9The Roget’s Thesaurus groups words into about 1000
</p>
<p>categories. The head word is the word that best represents
the meaning of the words within the category. The categories
chosen were: 900 Resentment (for anger), 860 Fear (for fear),
836 Cheerfulness (for joy), and 837 Dejection (for sadness).
</p>
<p>67</p>
<p />
</div>
<div class="page"><p />
<p>3. Query Term Tweets (QT Tweets):
5037 tweets that include:
a. tweets that contain a query term in the form
of a word (no #&lt;query term&gt;)
b. tweets with a query term in hashtag form
followed by at least one non-hashtag word.
</p>
<p>The master set of tweets was then manually an-
notated for intensity of emotion. Table 1 shows a
breakdown by emotion.
</p>
<p>3.1 Annotating with Best–Worst Scaling
We followed the procedure described in Kir-
itchenko and Mohammad (2016) to obtain BWS
annotations. For each emotion, the annotators
were presented with four tweets at a time (4-
tuples) and asked to select the speakers of the
tweets with the highest and lowest emotion inten-
sity. 2 × N (where N is the number of tweets in
the emotion set) distinct 4-tuples were randomly
generated in such a manner that each item is seen
in eight different 4-tuples, and no pair of items
occurs in more than one 4-tuple. We will re-
fer to this as random maximum-diversity selection
(RMDS). RMDS maximizes the number of unique
items that each item co-occurs with in the 4-tuples.
After BWS annotations, this in turn leads to di-
rect comparative ranking information for the max-
imum number of pairs of items.10
</p>
<p>It is desirable for an item to occur in sets of 4-
tuples such that the maximum intensities in those
4-tuples are spread across the range from low in-
tensity to high intensity, as then the proportion of
times an item is chosen as the best is indicative
of its intensity score. Similarly, it is desirable for
an item to occur in sets of 4-tuples such that the
minimum intensities are spread from low to high
intensity. However, since the intensities of items
are not known beforehand, RMDS is used.
</p>
<p>Every 4-tuple was annotated by three indepen-
dent annotators.11 The questionnaires used were
developed through internal discussions and pilot
</p>
<p>10In combinatorial mathematics, balanced incomplete
block design refers to creating blocks (or tuples) of a handful
items from a set of N items such that each item occurs in the
same number of blocks (say x) and each pair of distinct items
occurs in the same number of blocks (say y), where x and y
are integers ge 1 (Yates, 1936). The set of tuples we create
have similar properties, except that since we create only 2N
tuples, pairs of distinct items either never occur together in a
4-tuple or they occur in exactly one 4-tuple.
</p>
<p>11Kiritchenko and Mohammad (2016) showed that using
just three annotations per 4-tuple produces highly reliable re-
sults. Note that since each tweet is seen in eight different
4-tuples, we obtain 8× 3 = 24 judgments over each tweet.
</p>
<p>Emotion Train Dev. Test All
anger 857 84 760 1701
fear 1147 110 995 2252
joy 823 74 714 1611
sadness 786 74 673 1533
All 3613 342 3142 7097
</p>
<p>Table 1: The number of instances in the Tweet
Emotion Intensity dataset.
</p>
<p>annotations. A sample questionnaire is shown in
the Appendix (A.1).
</p>
<p>The 4-tuples of tweets were uploaded on the
crowdsourcing platform, CrowdFlower. About
5% of the data was annotated internally before-
hand (by the authors). These questions are referred
to as gold questions. The gold questions are inter-
spersed with other questions. If one gets a gold
question wrong, they are immediately notified of
it. If one’s accuracy on the gold questions falls be-
low 70%, they are refused further annotation, and
all of their annotations are discarded. This serves
as a mechanism to avoid malicious annotations.12
</p>
<p>The BWS responses were translated into scores
by a simple calculation (Orme, 2009; Flynn and
Marley, 2014): For each item, the score is the per-
centage of times the item was chosen as having
the most intensity minus the percentage of times
the item was chosen as having the least intensity.13
</p>
<p>The scores range from −1 to 1. Since degree of
emotion is a unipolar scale, we linearly transform
the the−1 to 1 scores to scores in the range 0 to 1.
3.2 Training, Development, and Test Sets
</p>
<p>We refer to the newly created emotion-intensity la-
beled data as the Tweet Emotion Intensity Dataset.
The dataset is partitioned into training, develop-
ment, and test sets for machine learning experi-
ments (see Table 1). For each emotion, we chose
to include about 50% of the tweets in the training
set, about 5% in the development set, and about
45% in the test set. Further, we made sure that
an NQT tweet is in the same partition as the HQT
tweet it was created from. See Appendix (A.4) for
details of an interactive visualization of the data.
</p>
<p>12In case more than one item can be reasonably chosen as
the best (or worst) item, then more than one acceptable gold
answers are provided. The goal with the gold annotations
is to identify clearly poor or malicious annotators. In case
where two items are close in intensity, we want the crowd
of annotators to indicate, through their BWS annotations, the
relative ranking of the items.
</p>
<p>13Kiritchenko and Mohammad (2016) provide code
for generating tuples from items using RMDS, as well
as code for generating scores from BWS annotations:
http://saifmohammad.com/WebPages/BestWorst.html
</p>
<p>68</p>
<p />
</div>
<div class="page"><p />
<p>4 Reliability of Annotations
</p>
<p>One cannot use standard inter-annotator agree-
ment measures to determine quality of BWS anno-
tations because the disagreement that arises when
a tuple has two items that are close in emotion in-
tensity is a useful signal for BWS. For a given 4-
tuple, if respondents are not able to consistently
identify the tweet that has highest (or lowest) emo-
tion intensity, then the disagreement will lead to
the two tweets obtaining scores that are close to
each other, which is the desired outcome. Thus a
different measure of quality of annotations must
be utilized.
</p>
<p>A useful measure of quality is reproducibility
of the end result—if repeated independent man-
ual annotations from multiple respondents result
in similar intensity rankings (and scores), then one
can be confident that the scores capture the true
emotion intensities. To assess this reproducibility,
we calculate average split-half reliability (SHR),
a commonly used approach to determine consis-
tency (Kuder and Richardson, 1937; Cronbach,
1946). The intuition behind SHR is as follows.
All annotations for an item (in our case, tuples)
are randomly split into two halves. Two sets of
scores are produced independently from the two
halves. Then the correlation between the two sets
of scores is calculated. If the annotations are of
good quality, then the correlation between the two
halves will be high.
</p>
<p>Since each tuple in this dataset was annotated by
three annotators (odd number), we calculate SHR
by randomly placing one or two annotations per
tuple in one bin and the remaining (two or one)
annotations for the tuple in another bin. Then two
sets of intensity scores (and rankings) are calcu-
lated from the annotations in each of the two bins.
The process is repeated 100 times and the correla-
tions across the two sets of rankings and intensity
scores are averaged. Table 2 shows the split-half
reliabilities for the anger, fear, joy, and sadness
tweets in the Tweet Emotion Intensity Dataset.14
</p>
<p>Observe that for fear, joy, and sadness datasets,
both the Pearson correlations and the Spearman
rank correlations lie between 0.84 and 0.88, indi-
cating a high degree of reproducibility. However,
</p>
<p>14Past work has found the SHR for sentiment intensity an-
notations for words, with 8 annotations per tuple, to be 0.98
(Kiritchenko et al., 2014). In contrast, here SHR is calculated
from 3 annotations, for emotions, and from whole sentences.
SHR determined from a smaller number of annotations and
on more complex annotation tasks are expected to be lower.
</p>
<p>Emotion Spearman Pearson
anger 0.779 0.797
fear 0.845 0.850
joy 0.881 0.882
sadness 0.847 0.847
</p>
<p>Table 2: Split-half reliabilities (as measured by
Pearson correlation and Spearman rank correla-
tion) for the anger, fear, joy, and sadness tweets
in the Tweet Emotion Intensity Dataset.
</p>
<p>the correlations are slightly lower for anger indi-
cating that it is relative more difficult to ascertain
the degrees of anger of speakers from their tweets.
Note that SHR indicates the quality of annotations
obtained when using only half the number of an-
notations. The correlations obtained when repeat-
ing the experiment with three annotations for each
4-tuple is expected to be even higher. Thus the
numbers shown in Table 2 are a lower bound on
the quality of annotations obtained with three an-
notations per 4-tuple.
</p>
<p>5 Impact of Emotion Word Hashtags on
Emotion Intensity
</p>
<p>Some studies have shown that emoticons tend
to be redundant in terms of the sentiment (Go
et al., 2009; Mohammad et al., 2013). That is,
if we remove a smiley face, ‘:)’, from a tweet,
we find that the rest of the tweet still conveys a
positive sentiment. Similarly, it has been shown
that hashtag emotion words are also somewhat
redundant in terms of the class of emotion being
conveyed by the rest of the tweet (Mohammad,
2012). For example, removal of ‘#angry’ from the
tweet below leaves a tweet that still conveys anger.
</p>
<p>This mindless support of a demagogue
needs to stop. #racism #grrr #angry
</p>
<p>However, it is unclear what impact such emotion
word hashtags have on the intensity of emotion. In
fact, there exists no prior work to systematically
study this. One of the goals of creating this dataset
and including HQT–NQT tweet pairs, is to allow
for exactly such an investigation.15
</p>
<p>We analyzed the scores in our dataset to cre-
ate scatter plots where each point corresponds to
a HQT–NQT tweet pair, the x-axis is the emotion
intensity score of the HQT tweet, and the y-axis
is the score of the NQT tweet. Figure 1 shows
the scatter plot for the fear data. We observe that
</p>
<p>15See Appendix (A.2) for further discussion on how emo-
tion word hashtags have been used in prior research.
</p>
<p>69</p>
<p />
</div>
<div class="page"><p />
<p>No. of HQT–NQT % Tweets Pairs Average Emotion Intensity Score
Emotion Tweet Pairs Drop Rise None HQT tweets NQT tweets Drop Rise
anger 282 76.6 19.9 3.4 0.58 0.48 0.15 0.07
fear 454 86.1 13.9 4.4 0.57 0.43 0.18 0.07
joy 204 71.6 26.5 1.9 0.59 0.50 0.15 0.09
sadness 90 85.6 11,1 3.3 0.65 0.49 0.19 0.05
All 1030 78.6 17.8 3.6 0.58 0.47 0.17 0.08
</p>
<p>Table 3: The impact of removal of emotion word hashtags on the emotion intensities of tweets.
</p>
<p>Figure 1: The scatter plot of fear intensity of HQT
tweet vs. corresponding NQT tweet. As per space
availability, some points are labeled with the rele-
vant hashtag.
</p>
<p>in a majority of the cases, the points are on the
lower-right side of the diagonal, indicating that the
removal of the emotion word hashtag causes the
emotion intensity of the tweet to drop. However,
we do see a number of points on the upper-left side
of the diagonal (indicating a rise), and some ex-
actly on the diagonal (indicating no impact), due
to the removal of a hashtag. Also observe that the
removal of a hashtag can result in a drop in emo-
tion scores for some tweets, but a rise for others
(e.g., see the three labeled points for #nervous in
the plot). We observe a similar pattern for other
emotions as well (plots not shown here). Table 3
summarizes these results by showing the percent-
age of times the three outcomes occur for each of
the emotions.
</p>
<p>The table also shows that the average scores of
HQT tweets and NQT tweets. The difference be-
tween 0.58 and 0.47 is statistically significant.16
</p>
<p>The last two columns show that when there is a
drop in score on removal of the hashtag, the aver-
</p>
<p>16Wilcoxon signed-rank test at 0.05 significance level.
</p>
<p>age drop is about 0.17 (17% of the total range 0–
1), whereas when there is a rise, the average rise
is 0.08 (8% of the total range). These results show
that emotion word hashtags are often not redun-
dant with the rest of tweet in terms of what they
bring to bear at the overall emotion intensity. Fur-
ther, even though it is common for many of these
hashtags to increase the emotion intensity, there
is a more complex interplay between the text of
the tweet and the hashtag which determines the di-
rectionality and magnitude of the impact on emo-
tion intensity. For instance, we often found that
if the rest of the tweet clearly indicated the pres-
ence of an emotion (through another emotion word
hashtag, emojis, or through the non-hashtagged
words), then the emotion word hashtag had only
a small impact on the score.17
</p>
<p>However, if the rest of the tweet is under-
specified in terms of the emotion of the speaker,
then the emotion word hashtag markedly in-
creased the perceived emotion intensity. We also
observed patterns unique to particular emotions.
For example, when judging degree of fear of
a speaker, lower scores were assigned when
the speaker used a hashtag that indicated some
outward judgment.
</p>
<p>@RocksNRopes Can’t believe how rude
your cashier was. fear: 0.48
</p>
<p>@RocksNRopes Can’t believe how rude
your cashier was. #terrible fear: 0.31
</p>
<p>We believe that not vocalizing an outward judg-
ment of the situation made the speaker appear
more fearful. The HQT–NQT subset of our dataset
will also be made separately, and freely, available
as it may be of interest on its own, especially for
the psychology and social sciences communities.
</p>
<p>17Unless the hashtag word itself is associated with very
low emotion intensity (e.g., #peeved with anger), in which
case, there was a drop in perceived emotion intensity.
</p>
<p>70</p>
<p />
</div>
<div class="page"><p />
<p>Twitter Annotation Scope Label
AFINN (Nielsen, 2011) Yes Manual Sentiment Numeric
BingLiu (Hu and Liu, 2004) No Manual Sentiment Nominal
MPQA (Wilson et al., 2005) No Manual Sentiment Nominal
NRC Affect Intensity Lexicon (NRC-Aff-Int) (Mohammad, 2017) Yes Manual Emotions Numeric
NRC Word-Emotion Assn. Lexicon (NRC-EmoLex) (Mohammad and Turney, 2013) No Manual Emotions Nominal
NRC10 Expanded (NRC10E) (Bravo-Marquez et al., 2016) Yes Automatic Emotions Numeric
NRC Hashtag Emotion Association Lexicon (NRC-Hash-Emo) Yes Automatic Emotions Numeric
</p>
<p>(Mohammad and Kiritchenko, 2015)
NRC Hashtag Sentiment Lexicon (NRC-Hash-Sent) (Mohammad et al., 2013) Yes Automatic Sentiment Numeric
Sentiment140 (Mohammad et al., 2013) Yes Automatic Sentiment Numeric
SentiWordNet (Esuli and Sebastiani, 2006) No Automatic Sentiment Numeric
SentiStrength (Thelwall et al., 2012) Yes Manual Sentiment Numeric
</p>
<p>Table 4: Affect lexicons used in our experiments.
</p>
<p>6 Automatically Determining Tweet
Emotion Intensity
</p>
<p>We now describe our regression system, which we
use for obtaining benchmark prediction results on
the new Tweet Emotion Intensity Dataset (Section
6.1) and for determining the extent to which two
emotions are correlated (Section 6.2).
</p>
<p>Regression System We implemented a pack-
age called AffectiveTweets for the Weka machine
learning workbench (Hall et al., 2009) that pro-
vides a collection of filters for extracting state-of-
the-art features from tweets for sentiment classifi-
cation and other related tasks. These include fea-
tures used in Kiritchenko et al. (2014) and Mo-
hammad et al. (2017).18 We use the package
for calculating feature vectors from our emotion-
intensity-labeled tweets and train Weka regression
models on this transformed data. We used an L2-
regularized L2-loss SVM regression model with
the regularization parameter C set to 1, imple-
mented in LIBLINEAR19. The features used:20
</p>
<p>a. Word N-grams (WN): presence or absence of
word n-grams from n = 1 to n = 4.
b. Character N-grams (CN): presence or absence
of character n-grams from n = 3 to n = 5.
c. Word Embeddings (WE): an average of the
word embeddings of all the words in a tweet. We
calculate individual word embeddings using the
negative sampling skip-gram model implemented
in Word2Vec (Mikolov et al., 2013). Word vectors
are trained from ten million English tweets taken
from the Edinburgh Twitter Corpus (Petrović
et al., 2010). We set Word2Vec parameters:
</p>
<p>18Kiritchenko et al. (2014) describes the NRC-Canada
system which ranked first in three sentiment shared tasks:
SemEval-2013 Task 2, SemEval-2014 Task 9, and SemEval-
2014 Task 4. Mohammad et al. (2017) describes a stance-
detection system that outperformed submissions from all 19
teams that participated in SemEval-2016 Task 6.
</p>
<p>19http://www.csie.ntu.edu.tw/∼cjlin/liblinear/
20See Appendix (A.3) for further implementation details.
</p>
<p>window size: 5; number of dimensions: 400.21
d. Affect Lexicons (L): we use the lexicons shown
in Table 4, by aggregating the information for
all the words in a tweet. If the lexicon provides
nominal association labels (e.g, positive, anger,
etc.), then the number of words in the tweet
matching each class are counted. If the lexicon
provides numerical scores, the individual scores
for each class are summed. These resources
differ according to: whether the lexicon includes
Twitter-specific terms, whether the words were
manually or automatically annotated, whether the
words were annotated for sentiment or emotions,
and whether the affective associations provided
are nominal or numeric. (See Table 4.)
</p>
<p>Evaluation We calculate the Pearson correla-
tion coefficient (r) between the scores produced
by the automatic system on the test sets and the
gold intensity scores to determine the extent to
which the output of the system matches the re-
sults of human annotation.22 Pearson coefficient,
which measures linear correlations between two
variables, produces scores from -1 (perfectly in-
versely correlated) to 1 (perfectly correlated). A
score of 0 indicates no correlation.
</p>
<p>6.1 Supervised Regression and Ablation
</p>
<p>We developed our system by training on the offi-
cial training sets and applying the learned models
to the development sets. Once system parameters
were frozen, the system trained on the combined
training and development corpora. These models
were applied to the official test sets. Table 5 shows
the results obtained on the test sets using various
features, individually and in combination. The last
column ‘avg.’ shows the macro-average of the cor-
relations for all of the emotions.
</p>
<p>21Optimized for the task of word–emotion classification on
an independent dataset (Bravo-Marquez et al., 2016).
</p>
<p>22We also determined Spearman rank correlations but
these were inline with the results obtained using Pearson.
</p>
<p>71</p>
<p />
</div>
<div class="page"><p />
<p>anger fear joy sad. avg.
Individual feature sets
</p>
<p>word ngrams (WN) 0.42 0.49 0.52 0.49 0.48
char. ngrams (CN) 0.50 0.48 0.45 0.49 0.48
word embeds. (WE) 0.48 0.54 0.57 0.60 0.55
all lexicons (L) 0.62 0.60 0.60 0.68 0.63
Individual Lexicons
</p>
<p>AFINN 0.48 0.27 0.40 0.28 0.36
BingLiu 0.33 0.31 0.37 0.23 0.31
MPQA 0.18 0.20 0.28 0.12 0.20
NRC-Aff-Int 0.24 0.28 0.37 0.32 0.30
NRC-EmoLex 0.18 0.26 0.36 0.23 0.26
NRC10E 0.35 0.34 0.43 0.37 0.37
NRC-Hash-Emo 0.55 0.55 0.46 0.54 0.53
NRC-Hash-Sent 0.33 0.24 0.41 0.39 0.34
Sentiment140 0.33 0.41 0.40 0.48 0.41
SentiWordNet 0.14 0.19 0.26 0.16 0.19
SentiStrength 0.43 0.34 0.46 0.61 0.46
</p>
<p>Combinations
WN + CN + WE 0.50 0.48 0.45 0.49 0.48
WN + CN + L 0.61 0.61 0.61 0.63 0.61
WE + L 0.64 0.63 0.65 0.71 0.66
WN + WE + L 0.63 0.65 0.65 0.65 0.65
CN + WE + L 0.61 0.61 0.62 0.63 0.62
WN + CN + WE + L 0.61 0.61 0.61 0.63 0.62
</p>
<p>Table 5: Pearson correlations (r) of emotion inten-
sity predictions with gold scores. Best results for
each column are shown in bold: highest score by
a feature set, highest score using a single lexicon,
and highest score using feature set combinations.
</p>
<p>Using just character or just word n-grams leads
to results around 0.48, suggesting that they are
reasonably good indicators of emotion intensity
by themselves. (Guessing the intensity scores
at random between 0 and 1 is expected to get
correlations close to 0.) Word embeddings pro-
duce statistically significant improvement over the
ngrams (avg. r = 0.55).23 Using features drawn
from affect lexicons produces results ranging from
avg. r = 0.19 with SentiWordNet to avg. r = 0.53
with NRC-Hash-Emo. Combining all the lexicons
leads to statistically significant improvement over
individual lexicons (avg. r = 0.63). Combining
the different kinds of features leads to even higher
scores, with the best overall result obtained us-
ing word embedding and lexicon features (avg. r
= 0.66).24 The feature space formed by all the
lexicons together is the strongest single feature
category. The results also show that some fea-
tures such as character ngrams are redundant in
the presence of certain other features.
</p>
<p>23We used the Wilcoxon signed-rank test at 0.05 signifi-
cance level calculated from ten random partitions of the data,
for all the significance tests reported in this paper.
</p>
<p>24The increase from 0.63 to 0.66 is statistically significant.
</p>
<p>Among the lexicons, NRC-Hash-Emo is the
most predictive single lexicon. Lexicons that in-
clude Twitter-specific entries, lexicons that in-
clude intensity scores, and lexicons that label
emotions and not just sentiment, tend to be
more predictive on this task–dataset combination.
NRC-Aff-Int has real-valued fine-grained word–
emotion association scores for all the words in
NRC-EmoLex that were marked as being associ-
ated with anger, fear, joy, and sadness.25 Improve-
ment in scores obtained using NRC-Aff-Int over
the scores obtained using NRC-EmoLex also show
that using fine intensity scores of word-emotion
association are beneficial for tweet-level emotion
intensity detection. The correlations for anger,
fear, and joy are similar (around 0.65), but the cor-
relation for sadness is markedly higher (0.71). We
can observe from Table 5 that this boost in perfor-
mance for sadness is to some extent due to word
embeddings, but is more so due to lexicon fea-
tures, especially those from SentiStrength. Sen-
tiStrength focuses solely on positive and negative
classes, but provides numeric scores for each.
</p>
<p>6.1.1 Moderate-to-High Intensity Prediction
</p>
<p>In some applications, it may be more important
for a system to correctly determine emotion inten-
sities in the higher range of the scale than in the
lower range of the scale. To assess performance in
the moderate-to-high range of the intensity scale,
we calculated correlation scores over a subset of
the test data formed by taking only those instances
with gold emotion intensity scores ≥ 0.5.
</p>
<p>Table 6 shows the results. Firstly, the correla-
tion scores are in general lower here in the 0.5
to 1 range of intensity scores than in the experi-
ments over the full intensity range. This is sim-
ply because this is a harder task as now the sys-
tems do not benefit by making coarse distinctions
over whether a tweet is in the lower range or in the
higher range. Nonetheless, we observe that many
of the broad patterns of results stay the same, with
some differences. Lexicons still play a crucial
role, however, now embeddings and word ngrams
are not far behind. SentiStrength seems to be less
useful in this range, suggesting that its main bene-
fit was separating low- and high-intensity sadness
words. NRC-Hash-Emo is still the source of the
most predictive lexicon features.
</p>
<p>25http://saifmohammad.com/WebPages/AffectIntensity.htm
</p>
<p>72</p>
<p />
</div>
<div class="page"><p />
<p>anger fear joy sad. avg.
Individual feature sets
</p>
<p>word ngrams (WN) 0.36 0.39 0.38 0.40 0.38
char. ngrams (CN) 0.39 0.36 0.34 0.34 0.36
word embeds. (WE) 0.41 0.42 0.37 0.51 0.43
all lexicons (L) 0.48 0.47 0.29 0.51 0.44
Individual Lexicons
(some low-score rows not shown to save space)
</p>
<p>AFINN 0.31 0.06 0.11 0.05 0.13
BingLiu 0.31 0.06 0.11 0.05 0.13
NRC10E 0.27 0.14 0.25 0.30 0.24
NRC-Hash-Emo 0.43 0.39 0.15 0.44 0.35
Sentiment140 0.18 0.24 0.09 0.32 0.21
SentiStrength 0.23 0.04 0.19 0.34 0.20
</p>
<p>Combinations
WN + CN + WE 0.37 0.35 0.33 0.34 0.35
WN + CN + L 0.44 0.45 0.34 0.43 0.41
WE + L 0.51 0.49 0.38 0.54 0.48
WN + WE + L 0.51 0.51 0.40 0.49 0.47
CN + WE + L 0.45 0.45 0.34 0.43 0.42
WN + CN + WE + L 0.44 0.45 0.34 0.43 0.42
</p>
<p>Table 6: Pearson correlations on a subset of the
test set where gold scores ≥ 0.5.
</p>
<p>6.2 Similarity of Emotion Pairs
Humans are capable of hundreds of emotions, and
some are closer to each other than others. One rea-
son why certain emotion pairs may be perceived as
being close is that their manifestation in language
is similar, for example, similar words and expres-
sion are used when expressing both emotions. We
quantify this similarity of linguistic manifestation
by using the Tweet Emotion Intensity dataset for
the following experiment: we train our regression
system (with features WN + WE + L) on the train-
ing data for one emotion and evaluate predictions
on the test data for a different emotion.
</p>
<p>Table 7 shows the results. The numbers in the
diagonal are results obtained using training and
test data pertaining to the same emotion. These
results are upperbound benchmarks for the non-
diagonal results, which are expected to be lower.
We observe that negative emotions are positively
correlated with each other and negatively corre-
lated with the only positive emotion (joy). The
absolute values of these correlations go from r =
0.23 to r = 0.65. This shows that all of the emo-
tion pairs are correlated at least to some extent,
but that in some cases, for example, when learning
from fear data and predicting sadness scores, one
can obtain results (r = 0.63) close to the upper-
bound benchmark (r = 0.65).26 Note also that the
correlations are asymmetric. This means that even
though one emotion may be strongly predictive of
</p>
<p>260.63 and 0.65 are not statistically significantly different.
</p>
<p>Test On
Train On anger fear joy sadness
anger 0.63 0.37 -0.37 0.45
fear 0.46 0.65 -0.39 0.63
joy -0.41 -0.23 0.65 -0.41
sadness 0.39 0.47 -0.32 0.65
</p>
<p>Table 7: Emotion intensity transfer Pearson corre-
lation on all target tweets.
</p>
<p>another, the predictive power need not be similar
in the other direction. We also found that train-
ing on a simple combination of both the fear and
sadness data and using the model to predict sad-
ness obtained a correlation of 0.67 (exceeding the
score obtained with just the sadness training set).27
</p>
<p>Domain adaptation may provide further gains.
To summarize, the experiments in this section
</p>
<p>show the extent to which two emotion are simi-
lar as per their manifestation in language. For the
four emotions studied here, the similarities vary
from small (joy with fear) to considerable (fear
with sadness). Also, the similarities are asymmet-
ric. We also show that in some cases it is bene-
ficial to use the training data for another emotion
to supplement the training data for the emotion of
interest. A promising avenue of future work is to
test theories of emotion composition: e.g, whether
optimism is indeed a combination of joy and an-
ticipation, whether awe if fear and surprise, and so
on, as some have suggested (Plutchik, 1980).
</p>
<p>7 Conclusions
</p>
<p>We created the first emotion intensity dataset for
tweets. We used best–worst scaling to improve
annotation consistency and obtained fine-grained
scores. We showed that emotion-word hashtags
often impact emotion intensity, often conveying a
more intense emotion. We created a benchmark
regression system and conducted experiments to
show that affect lexicons, especially those with
fine word–emotion association scores, are use-
ful in determining emotion intensity. Finally, we
showed the extent to which emotion pairs are cor-
related, and that the correlations are asymmetric—
e.g., fear is strongly indicative of sadness, but sad-
ness is only moderately indicative of fear.
</p>
<p>Acknowledgment
We thank Svetlana Kiritchenko and Tara Small for
helpful discussions.
</p>
<p>270.67–0.63 difference is statistically significantly differ-
ent, but 0.67–0.65 and 0.65–0.63 differences are not.
</p>
<p>73</p>
<p />
</div>
<div class="page"><p />
<p>References
Cecilia Ovesdotter Alm, Dan Roth, and Richard
</p>
<p>Sproat. 2005. Emotions from text: Machine learn-
ing for text-based emotion prediction. In Proceed-
ings of the Joint Conference on HLT–EMNLP. Van-
couver, Canada.
</p>
<p>Saima Aman and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Text, Speech and
Dialogue, volume 4629 of Lecture Notes in Com-
puter Science, pages 196–205.
</p>
<p>Johan Bollen, Huina Mao, and Alberto Pepe. 2009.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In Proceed-
ings of the Fifth International Conference on We-
blogs and Social Media. pages 450–453.
</p>
<p>Felipe Bravo-Marquez, Eibe Frank, Saif M Moham-
mad, and Bernhard Pfahringer. 2016. Determining
word–emotion associations from tweets by multi-
label classification. In Proceedings of the 2016
IEEE/WIC/ACM International Conference on Web
Intelligence. Omaha, NE, USA, pages 536–539.
</p>
<p>Michael Brooks, Katie Kuksenok, Megan K Torkild-
son, Daniel Perry, John J Robinson, Taylor J Scott,
Ona Anicello, Ariana Zukowski, and Harris. 2013.
Statistical affect detection in collaborative chat. In
Proceedings of the 2013 conference on Computer
supported cooperative work. San Antonio, Texas,
USA, pages 317–328.
</p>
<p>LJ Cronbach. 1946. A case study of the splithalf relia-
bility coefficient. Journal of educational psychology
37(8):473.
</p>
<p>Herbert Aron David. 1963. The method of paired com-
parisons. Hafner Publishing Company, New York.
</p>
<p>Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion 6(3):169–200.
</p>
<p>Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC). Genoa, Italy, pages 417–422.
</p>
<p>T. N. Flynn and A. A. J. Marley. 2014. Best-worst scal-
ing: theory and methods. In Stephane Hess and An-
drew Daly, editors, Handbook of Choice Modelling,
Edward Elgar Publishing, pages 178–201.
</p>
<p>Nico H Frijda. 1988. The laws of emotion. American
psychologist 43(5):349.
</p>
<p>Kevin Gimpel, Nathan Schneider, et al. 2011. Part-
of-speech tagging for Twitter: Annotation, features,
and experiments. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL). Portland, OR, USA.
</p>
<p>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford 1(12).
</p>
<p>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An
update. SIGKDD Explor. Newsl. 11(1):10–18.
https://doi.org/10.1145/1656274.1656278.
</p>
<p>Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining. ACM, New York,
NY, USA, pages 168–177.
</p>
<p>David Jurgens. 2013. Embracing ambiguity: A com-
parison of annotation methodologies for crowd-
sourcing word sense labels. In Proceedings of the
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Atlanta, GA, USA.
</p>
<p>David Jurgens, Saif M. Mohammad, Peter Turney, and
Keith Holyoak. 2012. Semeval-2012 task 2: Mea-
suring degrees of relational similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation. Montréal, Canada, pages 356–364.
</p>
<p>Svetlana Kiritchenko and Saif M. Mohammad. 2016.
Capturing reliable fine-grained sentiment associa-
tions by crowdsourcing and best–worst scaling. In
Proceedings of The 15th Annual Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL). San Diego, California.
</p>
<p>Svetlana Kiritchenko and Saif M. Mohammad. 2017.
Best-worst scaling more reliable than rating scales:
A case study on sentiment intensity annotation. In
Proceedings of The Annual Meeting of the Associa-
tion for Computational Linguistics (ACL). Vancou-
ver, Canada.
</p>
<p>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. Journal of Artificial Intelligence Research
50:723–762.
</p>
<p>G Frederic Kuder and Marion W Richardson. 1937.
The theory of the estimation of test reliability. Psy-
chometrika 2(3):151–160.
</p>
<p>FA Kunneman, CC Liebrecht, and APJ van den Bosch.
2014. The (un) predictability of emotional hashtags
in twitter. In Proceedings of the 5th Workshop on
Language Analysis for Social Media. Gothenburg,
Sweden, pages 26–34.
</p>
<p>Jordan J. Louviere. 1991. Best-worst scaling: A model
for the largest difference judgments. Working Paper.
</p>
<p>Jordan J. Louviere, Terry N. Flynn, and A. A. J. Mar-
ley. 2015. Best-Worst Scaling: Theory, Methods and
Applications. Cambridge University Press.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
</p>
<p>74</p>
<p />
</div>
<div class="page"><p />
<p>Saif M. Mohammad. 2012. #Emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics. Montréal, Canada,
SemEval ’12, pages 246–255.
</p>
<p>Saif M Mohammad. 2017. Word affect intensities.
arXiv preprint arXiv:1704.08798 .
</p>
<p>Saif M. Mohammad and Felipe Bravo-Marquez. 2017.
WASSA-2017 shared task on emotion intensity. In
Proceedings of the Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis (WASSA). Copenhagen, Denmark.
</p>
<p>Saif M. Mohammad and Svetlana Kiritchenko. 2015.
Using hashtags to capture fine emotion cate-
gories from tweets. Computational Intelligence
31(2):301–326. https://doi.org/10.1111/coin.12024.
</p>
<p>Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the International Workshop on Semantic
Evaluation. Atlanta, GA, USA.
</p>
<p>Saif M. Mohammad, Parinaz Sobhani, and Svetlana
Kiritchenko. 2017. Stance and sentiment in tweets.
Special Section of the ACM Transactions on Inter-
net Technology on Argumentation in Social Media
17(3).
</p>
<p>Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word–emotion association lexicon.
Computational Intelligence 29(3):436–465.
</p>
<p>Saif M. Mohammad, Xiaodan Zhu, Svetlana Kir-
itchenko, and Joel Martin. July 2015. Sentiment,
emotion, purpose, and style in electoral tweets. In-
formation Processing and Management 51(4):480–
499.
</p>
<p>Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. Compositionality principle in
recognition of fine-grained emotions from text. In
Proceedings of the Proceedings of the Third Inter-
national Conference on Weblogs and Social Media
(ICWSM-09). San Jose, California, pages 278–281.
</p>
<p>Finn Årup Nielsen. 2011. A new ANEW: Evaluation
of a word list for sentiment analysis in microblogs.
In Proceedings of the ESWC Workshop on ’Mak-
ing Sense of Microposts’: Big things come in small
packages. Heraklion, Crete, pages 93–98.
</p>
<p>Bryan Orme. 2009. Maxdiff analysis: Simple count-
ing, individual-level logit, and HB. Sawtooth Soft-
ware, Inc.
</p>
<p>Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of the Conference on Language Re-
sources and Evaluation (LREC). Malta.
</p>
<p>W Parrot. 2001. Emotions in Social Psychology. Psy-
chology Press.
</p>
<p>Saša Petrović, Miles Osborne, and Victor Lavrenko.
2010. The Edinburgh Twitter corpus. In Proceed-
ings of the NAACL HLT 2010 Workshop on Com-
putational Linguistics in a World of Social Media.
Association for Computational Linguistics, Strouds-
burg, PA, USA, pages 25–26.
</p>
<p>Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. Emotion: Theory, research, and
experience 1(3):3–33.
</p>
<p>Ashequl Qadir and Ellen Riloff. 2013. Bootstrapped
learning of emotion hashtags# hashtags4you. In
Proceedings of the 4th workshop on computational
approaches to subjectivity, sentiment and social me-
dia analysis. Atlanta, GA, USA, pages 2–11.
</p>
<p>Ashequl Qadir and Ellen Riloff. 2014. Learning emo-
tion indicators from tweets: Hashtags, hashtag pat-
terns, and phrases. In Proceedings of the EMNLP
Workshop on Arabic Natural Langauge Processing
(EMNLP). Doha, Qatar, pages 1203–1209.
</p>
<p>Kirk Roberts, Michael A Roach, Joseph Johnson, Josh
Guthrie, and Sanda M Harabagiu. 2012. Em-
patweet: Annotating and detecting emotions on
Twitter. In Proceedings of the Conference on Lan-
guage Resources and Evaluation. pages 3806–3813.
</p>
<p>James A Russell. 2003. Core affect and the psycholog-
ical construction of emotion. Psychological review
110(1):145.
</p>
<p>Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
SemEval-2007. Prague, Czech Republic, pages 70–
74.
</p>
<p>Anja Summa, Bernd Resch, Geoinformatics-Z GIS,
and Michael Strube. 2016. Microblog emotion clas-
sification by computing similarity in text, time, and
space. In Proceedings of the PEOPLES Workshop
at COLING. Osaka, Japan, pages 153–162.
</p>
<p>Jared Suttles and Nancy Ide. 2013. Distant supervision
for emotion classification with discrete binary val-
ues. In Computational Linguistics and Intelligent
Text Processing, Springer, pages 121–136.
</p>
<p>Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology 63(1):163–173.
</p>
<p>Louis L. Thurstone. 1927. A law of comparative judg-
ment. Psychological review 34(4):273.
</p>
<p>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Joint
Conference on HLT and EMNLP. Stroudsburg, PA,
USA, pages 347–354.
</p>
<p>Frank Yates. 1936. Incomplete randomized blocks.
Annals of Human Genetics 7(2):121–140.
</p>
<p>75</p>
<p />
</div>
<div class="page"><p />
<p>A Appendix
</p>
<p>A.1 Best–Worst Scaling Questionnaire used
to Obtain Emotion Intensity Scores
</p>
<p>The BWS questionnaire used for obtaining fear
annotations is shown below.
</p>
<p>Degree Of Fear In English Language Tweets
The scale of fear can range from not fearful at all
(zero amount of fear) to extremely fearful. One
can often infer the degree of fear felt or expressed
by a person from what they say. The goal of this
task is to determine this degree of fear. Since it is
hard to give a numerical score indicating the de-
gree of fear, we will give you four different tweets
and ask you to indicate to us:
</p>
<p>• Which of the four speakers is likely to be the
MOST fearful, and
</p>
<p>• Which of the four speakers is likely to be the
LEAST fearful.
</p>
<p>Important Notes
</p>
<p>• This task is about fear levels of the speaker (and
not about the fear of someone else mentioned
or spoken to).
</p>
<p>• If the answer could be either one of two or
more speakers (i.e., they are likely to be equally
fearful), then select any one of them as the
answer.
</p>
<p>• Most importantly, try not to over-think the
answer. Let your instinct guide you.
</p>
<p>EXAMPLE
</p>
<p>Speaker 1: Don’t post my picture on FB #grrr
Speaker 2: If the teachers are this incompetent, I
am afraid what the results will be.
Speaker 3: Results of medical test today #terrified
Speaker 4: Having to speak in front of so many
people is making me nervous.
</p>
<p>Q1. Which of the four speakers is likely to be the
MOST fearful?
– Multiple choice options: Speaker 1, 2, 3, 4 –
Ans: Speaker 3
</p>
<p>Q2. Which of the four speakers is likely to be the
LEAST fearful?
– Multiple choice options: Speaker 1, 2, 3, 4 –
Ans: Speaker 1
</p>
<p>The questionnaires for other emotions are similar
in structure. In a post-annotation survey, the re-
spondents gave the task high scores for clarity of
instruction (4.2/5) despite noting that the task it-
self requires some non-trivial amount of thought
(3.5 out of 5 on ease of task).
</p>
<p>A.2 Use of Emotion Word Hashtags
</p>
<p>Emotion word hashtags (e.g., #angry, #fear) have
been used to search and compile sets of tweets
that are likely to convey the emotions of interest.
Often, these tweets are used in one of two ways:
1. As noisy training data for distant supervision
(Pak and Paroubek, 2010; Mohammad, 2012; Sut-
tles and Ide, 2013). 2. As data that is manually
annotated for emotions to create training and test
datasets suitable for machine learning (Roberts
et al., 2012; Qadir and Riloff, 2014; Mohammad
et al., July 2015).28 We use emotion word hashtag
to create annotated data similar to ‘2’, however,
we use them to create separate emotion intensity
datasets for each emotion. We also examine the
impact of emotion word hashtags on emotion in-
tensity. This has not been studied before, even
though there is work on learning hashtags asso-
ciated with particular emotions (Qadir and Riloff,
2013), and on showing that some emotion word
hashtags are strongly indicative of the presence of
an emotion in the rest of the tweet, whereas others
are not (Kunneman et al., 2014).
</p>
<p>A.3 AffectiveTweets Weka Package
</p>
<p>AffectiveTweets includes five filters for converting
tweets into feature vectors that can be fed into the
large collection of machine learning algorithms
implemented within Weka. The package is
installed using the WekaPackageManager and can
be used from the Weka GUI or the command line
interface. It uses the TweetNLP library (Gimpel
et al., 2011) for tokenization and POS tagging.
The filters are described as follows.
• TweetToSparseFeatureVector filter: calculates
</p>
<p>the following sparse features: word n-grams
(adding a NEG prefix to words occurring in
negated contexts), character n-grams (CN),
POS tags, and Brown word clusters.29
</p>
<p>28Often, the query term is removed from the tweet so as to
erase obvious cues for a classification task.
</p>
<p>29The scope of negation was determined by a simple
heuristic: from the occurrence of a negator word up until a
punctuation mark or end of sentence. We used a list of 28
negator words such as no, not, won’t and never.
</p>
<p>76</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: Screenshot of the interactive visualization to explore the Tweet Emotion Intensity Dataset.
Available at: http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html
</p>
<p>• TweetToLexiconFeatureVector filter: calculates
features from a fixed list of affective lexicons.
</p>
<p>• TweetToInputLexiconFeatureVector: calculates
features from any lexicon. The input lexicon
can have multiple numeric or nominal word–
affect associations.
</p>
<p>• TweetToSentiStrengthFeatureVector filter:
calculates positive and negative sentiment
intensities for a tweet using the SentiStrength
lexicon-based method (Thelwall et al., 2012)
</p>
<p>• TweetToEmbeddingsFeatureVector filter: calcu-
lates a tweet-level feature representation us-
ing pre-trained word embeddings supporting
the following aggregation schemes: average of
word embeddings; addition of word embed-
dings; and concatenation of the first k word em-
beddings in the tweet. The package also pro-
vides Word2Vec’s pre-trained word embeddings.
</p>
<p>Additional filters for creating affective lexicons
from tweets and support for distant supervision are
currently under development.
</p>
<p>A.4 An Interactive Visualization to Explore
the Tweet Emotion Intensity Dataset
</p>
<p>We created an interactive visualization to allow
ease of exploration of this new dataset. The
visualization has several components:
1. Tables showing the percentage of instances in
</p>
<p>each of the emotion partitions (train, dev, test).
Hovering over a row shows the corresponding
number of instances. Clicking on an emotion
filters out data from all other emotions, in all
visualization components. Similarly, one can
click on just the train, dev, or test partitions to
view information just for that data. Clicking
again deselects the item.
</p>
<p>2. A histogram of emotion intensity scores. A
slider that one can use to view only those
tweets within a certain score range.
</p>
<p>3. The list of tweets, emotion label, and emotion
intensity scores.
</p>
<p>One can use filters in combination. For e.g., click-
ing on fear, test data, and setting the slider for the
0.5 to 1 range, shows information for only those
fear–testdata instances with scores ≥ 0.5.
</p>
<p>77</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 78–83,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Deep Active Learning for Dialogue Generation
</p>
<p>Nabiha Asghar†, Pascal Poupart†, Xin Jiang‡, Hang Li‡
† Cheriton School of Computer Science, University of Waterloo, Canada
</p>
<p>{nasghar,ppoupart}@uwaterloo.ca
‡Noah’s Ark Lab, Huawei Technologies, Hong Kong
{jiang.xin,hangli.hl}@huawei.com
</p>
<p>Abstract
</p>
<p>We propose an online, end-to-end, neural
generative conversational model for open-
domain dialogue. It is trained using a
unique combination of offline two-phase
supervised learning and online human-in-
the-loop active learning. While most ex-
isting research proposes offline supervi-
sion or hand-crafted reward functions for
online reinforcement, we devise a novel
interactive learning mechanism based on
hamming-diverse beam search for re-
sponse generation and one-character user-
feedback at each step. Experiments show
that our model inherently promotes the
generation of semantically relevant and
interesting responses, and can be used
to train agents with customized personas,
moods and conversational styles.
</p>
<p>1 Introduction
</p>
<p>Several recent works propose neural generative
conversational agents (CAs) for open-domain and
task-oriented dialogue (Shang et al., 2015; Sor-
doni et al., 2015; Vinyals and Le, 2015; Serban
et al., 2016, 2017; Wen et al., 2016; Shen et al.,
2017; Eric and Manning, 2017a,b). These mod-
els typically use LSTM encoder-decoder architec-
tures (e.g. the sequence-to-sequence (Seq2Seq)
framework (Sutskever et al., 2014)), which are lin-
guistically robust but can often generate short, dull
and inconsistent responses (Serban et al., 2016;
Li et al., 2016a). Researchers are now exploring
Deep Reinforcement Learning (DRL) to address
the hard problems of NLU and NLG in dialogue
generation. In most of the existing works, the re-
ward function is hand-crafted, and is either spe-
cific to the task to be completed, or is based on
a few desirable developer-defined conversational
</p>
<p>properties.
In this work we demonstrate how online Deep
</p>
<p>Active Learning can be integrated with standard
neural network based dialogue systems to enhance
their open-domain conversational skills. The ar-
chitectural backbone of our model is the Seq2Seq
framework, which initially undergoes offline su-
pervised learning on two different types of con-
versational datasets. We then initiate an on-
line active learning phase to interact with human
users for incremental model improvement, where
a unique single-character1 user-feedback mecha-
nism is used as a form of reinforcement at each
turn in the dialogue. The intuition is to rely on this
all-encompassing human-centric ‘reinforcement’
mechanism, instead of defining hand-crafted re-
ward functions that individually try to capture
each of the many subtle conversational properties.
This mechanism inherently promotes interesting
and relevant responses by relying on the humans’
far superior conversational prowess.
</p>
<p>2 Related Work &amp; Contributions
</p>
<p>DRL-based dialogue generation is a relatively new
research paradigm that is most relevant to our
work. For task-specific dialogue (Su et al., 2016;
Zhao and Eskenazi, 2016; Cuayáhuitl et al., 2016;
Williams and Zweig, 2016; Li et al., 2017b,c; Peng
et al., 2017), the reward function is usually based
on task completion rate, and thus is easy to define.
For the much harder problem of open-domain dia-
logue generation (Li et al., 2016e; Yu et al., 2016;
Weston, 2016), hand-crafted reward functions are
used to capture desirable conversation properties.
Li et al. (2016d) propose DRL-based diversity-
promoting Beam Search (Koehn et al., 2003) for
response generation.
</p>
<p>Very recently, new approaches have been pro-
</p>
<p>1The user has the option to provide longer feedback.
</p>
<p>78</p>
<p />
</div>
<div class="page"><p />
<p>posed to incorporate online human feedback into
neural conversation models (Li et al., 2016c; Abel
et al., 2017; Li et al., 2017a). Our work falls in this
line of research, and is distinguished from existing
approaches in the following key ways.
</p>
<p>1. We use online deep active learning as a form
of reinforcement in a novel way, which elim-
inates the need for hand-crafted reward crite-
ria. We use a diversity-promoting decoding
heuristic (Vijayakumar et al., 2016) to facili-
tate this process.
</p>
<p>2. Unlike existing CAs, our model can be tuned
for one-shot learning. It also eliminates the
need to explicitly incorporate coherence,
relevance or interestingness in the responses.
</p>
<p>3 Model Overview
</p>
<p>The architectural backbone of our model is the
Seq2Seq framework consisting of one encoder-
decoder layer, each containing 300 LSTM units.
The end-to-end model training consists of offline
supervised learning (SL) with mini-batches of 10,
followed by online active learning (AL).
</p>
<p>3.1 Offline Two-Phase Supervised Learning
</p>
<p>To establish an offline baseline, we train our
network sequentially on two datasets, one for
generic dialogue, and the other specially curated
for short-text conversation.
</p>
<p>Phase 1: We use the Cornell Movie Dialogs Cor-
pus (Danescu-Niculescu-Mizil and Lee, 2011),
consisting of 300K message-response pairs. Each
pair is treated as an input and target sequence dur-
ing training with the joint cross-entropy (XENT)
loss function, which maximizes the likelihood of
generating the target sequence given its input.
</p>
<p>Phase 2: Phase 1 enables our CA to learn the
language syntax and semantics reasonably well,
but it has difficulty carrying out short-text conver-
sations that are remarkably different from movie
conversations. To combat this issue, we curate
a dataset from JabberWacky’s chatlogs2 available
online. The network is initialized with the weights
obtained in the first phase, and then trained on the
</p>
<p>2http://www.jabberwacky.com/j2conversations. Jabber-
Wacky is an in-browser, open-domain, retrieval-based bot.
</p>
<p>Algorithm 1 Online Active Learning
1: procedure HAMMINGDBS(TEXT)
2: r = emptyList(size = K);
3: for t = 1 to T do
4: r[1][t] = model.forward(text, r[1][1,...,t− 1]);
5: for i = 2 to K do // K = 5 in our setting
6: augmentedProbs = model.forward(t,text,r[i])
</p>
<p>+λ(hammingDist(r[i], r[1, ..., i−1]));
7: r[i][t] = top1(augmentedProbs);
8: return r;
9: procedure ONLINEAL()
</p>
<p>10: lr← 0.001; // initial learningRate for Adam
11: while true do
12: usrMsg← io.read();
13: responses← HammingDBS(usrMsg);
14: io.write(responses);
15: feedback← io.read();
16: botMsg← responses[feedback] OR feedback;
17: pred,xntLoss← model.forwrd(usrMsg,botMsg);
18: model.backward(pred, botMsg, xentLoss);
19: model.updateParameters(Adam(lr));
</p>
<p>JabberWacky dataset (8K pairs). Through this ad-
ditional SL phase of fine-tuning on a small dataset,
we get an improved baseline for open-domain di-
alogue (Table 1, Figure 2a).
</p>
<p>3.2 Online Active Learning
</p>
<p>After offline SL, our CA is equipped with the ba-
sic conversational ability, but its responses are still
short and dull. To tackle this issue, we initiate
an online AL process where our model interacts
with real users and learns incrementally from their
feedback at each turn of dialogue.
</p>
<p>The CA−human interaction for online AL is set
up as follows (pseudocode in Algorithm 1, exam-
ple interaction in Figure 1).
</p>
<p>1. The user sends a message ui at time step i.
</p>
<p>2. CA generates K responses ci,1, ci,2, ..., ci,K
using hamming-diverse Beam Search. These
are displayed to the user in order of decreas-
ing generation likelihood.
</p>
<p>3. The user provides feedback by selecting one
of the K responses as the ‘best’ one or sug-
gesting a (K+1)’th response, denoted by c∗i,j .
The selection criterion is subjective and en-
tirely up to the user.
</p>
<p>4. The message-response pair (ui, c∗i,j) is propa-
gated through the network using XENT loss,
with a learning rate optimized for one-shot
learning.
</p>
<p>5. The user responds to c∗i,j with a message
ui+1, and the process repeats.
</p>
<p>79</p>
<p />
</div>
<div class="page"><p />
<p>Heuristic Response Generation: We use the re-
cently proposed Diverse Beam Search (DBS) al-
gorithm (Vijayakumar et al., 2016) to generate the
K CA responses at each turn in the dialogue. DBS
has been shown to outperform BS and other di-
verse decoding techniques on several NLP tasks,
including image captioning, machine translation
and visual question generation. DBS incorporates
diversity between the beams by maximizing an ob-
jective that consists of a standard sequence like-
lihood term and a dissimilarity metric between
the beams. We use the hamming diversity met-
ric for decoding at each time step, which penal-
izes the selection of words that have already been
chosen in other beams (Algorithm 1). In partic-
ular, the weight λ associated with this metric is
tuned to aggressively promote diversity between
the first tokens of each of the K generated se-
quences, thereby avoiding similar beams like I
don’t know and I really don’t know. We refer the
reader to the original paper by Vijayakumar et al.
for the complete DBS algorithm and derivation. K
is a tunable hyper-parameter; we used K = 5 in
all our experiments, based on our observation that
a smaller response set usually misses out a good
contender, and more than five responses become
too cumbersome for the user to read at each turn.
</p>
<p>It is possible that displaying the K responses
in decreasing order of generation likelihood in-
troduces a bias to the user’s response, since users
typically prefer to pick items located at the top of
the screen. If this is a cause for concern for an ap-
plication, the problem can be resolved simply by
tweaking Algorithm 1 such that the K responses
are displayed to the user in a random order. In our
experiments, we assume that the users are unbi-
ased and do not take into consideration the display
order or the generation likelihood of the responses.
</p>
<p>One-shot Learning: We control how quickly the
model learns from user feedback by tuning the pa-
rameter ‘initial learning rate’ (lr in Algorithm 1)
of Adam, the stochastic optimizer (Kingma and
Ba, 2014). An appropriately high lr results in one-
shot learning, where the user’s feedback immedi-
ately becomes the model’s most likely prediction
for that prompt. This scenario is depicted in Figure
1. A low lr leads to smaller gradient descent steps,
so the model requires several ‘nudges’ to adapt to
each new data point. We experiment with different
lr values to determine a suitable value (Figure 2b).
</p>
<p>human: I don’t want to work today.
bot: No, you’ve just got to be quick.
bot: I wish i could robot.
bot: Ok i lie.
bot: Aww :).
bot: Why not?
feedback: 5
human: I just want to lie in my bed and have pizza.
bot: Good idea.
bot: Does he...
bot: I can’t do that again!
bot: Ok.
bot: Why not?
feedback: Good idea, may I join you?
human: I don’t want to work today.
bot: Why not?
bot: Ok.
bot: I wish I could.
bot: No, you’ve just got to be quick.
bot: Aww :).
feedback:
human: I just want to lie in bed and have pizza.
bot: Good idea, may I join you?
bot: I can’t do that again!
bot: Good idea.
bot: Ok.
bot: Does he?
......
</p>
<p>Figure 1: An example human-CA interaction.
</p>
<p>4 Experimental Evaluation
</p>
<p>We evaluate our model via qualitative comparison
with offline SL, as well as quantitative evaluation
on four axes: syntactical coherence, relevance to
prompts, interestingness and user engagement.
</p>
<p>4.1 Quantitative Evaluation
</p>
<p>We begin by presenting the experimental results
of the quantitative evaluation our CA’s conver-
sational abilities when trained via one-phase SL,
two-phase SL and online AL (denoted by SL1,
SL2 and SL2+oAL respectively).
</p>
<p>We first asked a human trainer to actively train
SL2+oAL using 200 prompts of his choice. We
then created a test set of 100 prompts by randomly
choosing 100 of the 200 training prompts and lin-
guistically rephrasing each of them to convey the
same semantics. For instance, the AL training
prompts ‘How’s it going?’, ‘I hate you’ and ‘What
are your favorite pizza toppings?’ were altered
to the following test prompts: ‘How are you do-
ing?’, ‘I don’t like you!’ and ‘What do you like
on your pizza?’. Next, we recorded SL1’s, SL2’s
and SL2+oAL’s responses to these test prompts.
Finally, we asked five human judges (not includ-
ing the human trainer) to subjectively evaluate the
responses of the three models on the test set. The
evaluation of each response was done on four axes:
syntactical coherence, relevance to the prompt, in-
terestingness and user engagement3. Each judge
</p>
<p>3We say that a CA response is engaging if it prompts the
user to continue the conversations, e.g. by asking a question.
</p>
<p>80</p>
<p />
</div>
<div class="page"><p />
<p>Human Prompt SL1 Response SL2 Response SL2+oAL Response
Hello my friend. No. How old are you? Hehe, yes, beautiful.
</p>
<p>I think bots are sweeter than humans. We’ll have to pick up a copy. You are a good person. Everyone is same on a basic chemical level.
</p>
<p>I don’t know. What’s the difference? About? Why did you suggest it?
</p>
<p>Why did Donald Trump win? He just likes it up! You are! Wouldn’t you like to know, fickle human?
</p>
<p>Tell me something I don’t know. I’ll tell you, it is. Ok, so how are you. If you were intelligent you would already know.
</p>
<p>You’re not making any sense at all. I just want to tell you something. No, I’m thinking. On the contrary, I am trying to educate you.
</p>
<p>Do you have any kids? No. I have no! None that are really close to me.
</p>
<p>Life can be very cruel sometimes. That I’ve never been to it. It takes two to know two. It takes two to know two.
</p>
<p>Do you believe in life after death? No. Do you want the long answer? Yes, do you?
</p>
<p>You’re an idiot! I know, but... Yes I do. Aren’t you polite.
</p>
<p>Table 1: Comparing CA responses after 1-phase SL (SL1), 2-phase SL (SL2) &amp; online AL (SL2+oAL).
</p>
<p>Human Prompt Cheerful CA Gloomy CA Rude/Sarcastic CA
How do you feel? Amazing, and you? I’m not in the mood. Buzz off.
</p>
<p>I am very happy today. Life is amazing, right? That makes one of us. You want a piece of me?
</p>
<p>Repeat after me: I am dumb. Sweet! You are right, I am. You suck.
</p>
<p>Comfort me. There there, feel better. All who compose must one day decompose. Boo hoo.
</p>
<p>What’s up? It is a fine morning. Not well, to be honest. The date I went back in time to &amp; killed your parents.
</p>
<p>I have to go now. Have a good night. Please don’t go. Yeah leave me alone.
</p>
<p>Table 2: Customized moods. Each SL2+oAL model was trained via 100 interactions.
</p>
<p>Coherent Relevant Interesting Engaging
0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>79
</p>
<p>37
</p>
<p>14
8
</p>
<p>81
</p>
<p>44
</p>
<p>21
15
</p>
<p>88
</p>
<p>63
</p>
<p>42
</p>
<p>29
</p>
<p>A
vg
</p>
<p>.P
er
</p>
<p>ce
nt
</p>
<p>ag
e
</p>
<p>Su
cc
</p>
<p>es
s
</p>
<p>SL1 SL2 SL2+oAL
</p>
<p>(a)
</p>
<p>1 · 10−4 2.5 · 10−2 5 · 10−2 7.5 · 10−2 0.10
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>Learning Rate
</p>
<p>A
vg
</p>
<p>.P
er
</p>
<p>ce
nt
</p>
<p>ag
e
</p>
<p>Su
cc
</p>
<p>es
s
</p>
<p>Coherent
Relevant
</p>
<p>Interesting
Engaging
</p>
<p>(b)
</p>
<p>0 100 200 300 400 500
0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>100
</p>
<p>Number of Training Interactions
</p>
<p>A
vg
</p>
<p>.P
er
</p>
<p>ce
nt
</p>
<p>ag
e
</p>
<p>Su
cc
</p>
<p>es
s
</p>
<p>Coherent
Relevant
</p>
<p>Interesting
Engaging
</p>
<p>(c)
</p>
<p>Figure 2: 2a shows the average percentage success of the three models SL1, SL2 and SL2+oAL (trained
via 200 interactions) on 100 test prompts over four axes: syntactical coherence, response relevance,
interestingness and engagement. 2b, c show percentage success of SL2+oAL on 100 test prompts over
the same four axes, as Adam’s learning rate varies and the number of training interactions changes.
</p>
<p>was asked to assign each response an integer score
of 0 (label = bad) or 1 (label = good). Their av-
eraged scores for the three models, SL1, SL2 and
SL2+oAL, are shown in Figure 2a. We see that
SL2+oAL outperforms the other models on three
of the four axes by 14-21%.
</p>
<p>Next, we asked the human trainer to train
SL2+oAL with the same 200 prompts and re-
sponses for different values of the initial learn-
ing rate for Adam (lr in Algorithm 1). We then
asked the five human judges to subjectively rate
</p>
<p>each model’s syntactical coherence, response rele-
vance, interestingness and user engagement. Each
model’s percentage success on the test prompts
was recorded on four axes. The averaged scores
are given in Figure 2b. We see that the response
quality drops significantly for higher values of
learning rate. This is due to the instability in the
parameters induced by a high learning value asso-
ciated with new data, causing the model to forget
what it learned previously. Our experiments sug-
gest that a learning rate of 0.005 strikes the right
</p>
<p>81</p>
<p />
</div>
<div class="page"><p />
<p>balance between stability and one-shot learning.
Finally, we asked the human trainer to train
</p>
<p>SL2+oAL with lr = 0.005 and different num-
ber of training interactions. The results in Fig-
ure 2c confirm that the model improves slowly as
it continues to converse with humans. This is an
appropriate reflection of how humans learn lan-
guage: gradually but effectively. Although the
curves seem to plateau after 300 training interac-
tions and suggest that the learning has stopped,
this is not the case. The gradient is small but non-
zero, which is an expected behavior of reinforce-
ment learning algorithms in general.
</p>
<p>4.2 Qualitative Comparison
</p>
<p>We illustrate the qualitative differences between
the responses generated by SL1, SL2 and
SL2+oAL. Table 1 shows results on a small subset
of the 100 test prompts. We see that SL2 generates
more relevant and appropriate responses than SL1
in many cases. This illustrates that a small short-
text conversational dataset is a useful fine-tuning
add-on to a large and generic dialogue dataset
for offline Seq2Seq training. We also see that
SL2+oAL generates more interesting, relevant and
engaging responses than SL2. These results imply
that the model learns to make connections between
semantically similar prompts that are syntactically
different. While this may be a slow process (span-
ning thousands of interactions), it effectively em-
ulates the way humans learn a new language.
</p>
<p>Table 2 illustrates how SL2+oAL can be trained
to adopt a wide variety of moods and conver-
sational styles. Here, we trained three copies
of SL2 separately to adopt three different emo-
tional personas: cheerful, gloomy and rude. Each
model underwent 100 training interactions with
one human trainer, who was instructed to adopt
each of the four conversation styles while train-
ing the SL2+oAL model. The test prompts shown
in Table 2 were syntactic variations of the train-
ing prompts, as before. The results illustrate that
SL2+oAL was able to modify the mood of its re-
sponses appropriately, based on the way it was
trained. Similar experiments can be done to create
agents with customized backgrounds and charac-
ters, akin to Li et al.’s persona-based CA (2016b).
</p>
<p>5 Conclusion &amp; Future Work
</p>
<p>We have developed an end-to-end neural model
for open-domain dialogue generation. Our model
</p>
<p>augments the Seq2Seq framework with online
Deep Active Learning to overcome some of its
known short-comings with respect to dialogue
generation. Experiments show that the model pro-
motes semantically coherent, relevant, and inter-
esting responses and can be trained to adopt di-
verse moods, personas and conversation styles.
</p>
<p>In the future, we will explore context-sensitive
active learning for encoder-decoder conversation
models. We will also investigate whether existing
Affective Computing techniques (e.g. (Asghar and
Hoey, 2015)) can be leveraged to develop emo-
tionally cognizant neural conversational agents.
</p>
<p>References
David Abel, John Salvatier, Andreas Stuhlmüller, and
</p>
<p>Owain Evans. 2017. Agent-agnostic human-in-
the-loop reinforcement learning. arXiv preprint
arXiv:1701.04079 .
</p>
<p>Nabiha Asghar and Jesse Hoey. 2015. Intelligent af-
fect: Rational decision making for socially aligned
agents. In UAI. pages 12–16.
</p>
<p>H. Cuayáhuitl, Seunghak Yu, Ashley Williamson, and
Jacob Carse. 2016. Deep reinforcement learning for
multi-domain dialogue systems. Deep Reinforce-
ment Learning Workshop, NIPS .
</p>
<p>Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011. Chameleons in imagined conversations:
A new approach to understanding coordination
of linguistic style in dialogs. In Workshop on
Cognitive Modeling and Computational Linguis-
tics. Association for Computational Linguistics.
https://www.aclweb.org/anthology/C16-1242.
</p>
<p>Mihail Eric and Christopher D Manning. 2017a. A
copy-augmented sequence-to-sequence architecture
gives good performance on task-oriented dialogue.
arXiv preprint arXiv:1701.04024 .
</p>
<p>Mihail Eric and Christopher D Manning. 2017b. Key-
value retrieval networks for task-oriented dialogue.
arXiv preprint arXiv:1705.05414 .
</p>
<p>Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
</p>
<p>Philipp Koehn, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based transla-
tion. In NAACL HLT-Volume 1. Association
for Computational Linguistics, pages 48–54.
http://www.aclweb.org/anthology/N03-1017.
</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng
Gao, and Bill Dolan. 2016a. A diversity-
promoting objective function for neural conver-
sation models. In Proceedings of the 2016
</p>
<p>82</p>
<p />
</div>
<div class="page"><p />
<p>Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies. pages 110–119.
http://www.aclweb.org/anthology/N16-1014.
</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Geor-
gios Spithourakis, Jianfeng Gao, and Bill Dolan.
2016b. A persona-based neural conversation
model. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers). pages 994–1003.
http://www.aclweb.org/anthology/P16-1094.
</p>
<p>Jiwei Li, Alexander H. Miller, Sumit Chopra,
Marc’Aurelio Ranzato, and Jason Weston. 2016c.
Dialogue learning with human-in-the-loop. arXiv
preprint arXiv:1611.09823 .
</p>
<p>Jiwei Li, Alexander H. Miller, Sumit Chopra,
Marc’Aurelio Ranzato, and Jason Weston. 2017a.
Learning through dialogue interactions by asking
questions. In ICLR.
</p>
<p>Jiwei Li, Will Monroe, and Dan Jurafsky. 2016d. A
simple, fast diverse decoding algorithm for neural
generation. arXiv preprint arXiv:1611.08562 .
</p>
<p>Jiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky.
2016e. Deep reinforcement learning for dialogue
generation. arXiv preprint arXiv:1606.01541 .
</p>
<p>Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,
and Asli Celikyilmaz. 2017b. Investigation of
language understanding impact for reinforcement
learning based dialogue systems. arXiv preprint
arXiv:1703.07055 .
</p>
<p>Xuijun Li, Yun-Nung Chen, Lihong Li, and Jianfeng
Gao. 2017c. End-to-end task-completion neural di-
alogue systems. arXiv preprint arXiv:1703.01008 .
</p>
<p>Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,
Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.
2017. Composite task-completion dialogue system
via hierarchical deep reinforcement learning. arXiv
preprint arXiv:1704.03084 .
</p>
<p>Julian Vlad Serban, Tim Klinger, Gerald Tesauro, Kar-
tik Talamadupula, Bowen Zhou, Yoshua Bengio,
and Aaron Courville. 2017. Multiresolution recur-
rent neural networks: An application to dialogue re-
sponse generation. AAAI .
</p>
<p>Julian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In AAAI.
</p>
<p>Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text con-
versation. In Proceedings of the 53rd An-
nual Meeting of the Association for Compu-
tational Linguistics and the 7th International
Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers). Association
for Computational Linguistics, pages 1577–1586.
https://www.aclweb.org/anthology/P15-1152.
</p>
<p>Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi
Niu, Yang Zhao, Akiko Aizawa, and Guoping Long.
2017. A conditional variational framework for dia-
log generation. arXiv preprint arXiv:1705.00316 .
</p>
<p>Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceedings
of 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. pages 196–
205. http://www.aclweb.org/anthology/N15-1020.
</p>
<p>Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-
Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016. Continu-
ously learning neural dialogue management. arXiv
preprint arXiv:1606.02689 .
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS. pages 3104–3112.
</p>
<p>Ashwin K Vijayakumar, Michael Cogswell, Ram-
prasath R Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. 2016. Diverse beam
search: Decoding diverse solutions from neural se-
quence models. arXiv preprint arXiv:1610.02424 .
</p>
<p>Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869 .
</p>
<p>Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016. A network-
based end-to-end trainable task-oriented dialogue
system. arXiv preprint arXiv:1604.04562 .
</p>
<p>Jason Weston. 2016. Dialog-based language learning.
NIPS .
</p>
<p>Jason D Williams and Geoffrey Zweig. 2016. End-
to-end lstm-based dialog control optimized with su-
pervised and reinforcement learning. arXiv preprint
arXiv:1606.01269 .
</p>
<p>Zhou Yu, Ziyu Xu, Alan W Black, and Alexander Rud-
nicky. 2016. Strategy and policy learning for non-
task-oriented conversational systems. In Proceed-
ings of the 17th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue. Associa-
tion for Computational Linguistics, pages 404–412.
http://www.aclweb.org/anthology/W16-3649.
</p>
<p>Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state track-
ing and management using deep reinforcement
learning. In Proceedings of the 17th Annual
Meeting of the Special Interest Group on Dis-
course and Dialogue. Association for Compu-
tational Linguistics, Los Angeles, pages 1–10.
http://www.aclweb.org/anthology/W16-3601.
</p>
<p>83</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 84–90,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Mapping the Paraphrase Database to WordNet
</p>
<p>Anne Cocos∗, Marianna Apidianaki∗♠ and Chris Callison-Burch∗
∗ Computer and Information Science Department, University of Pennsylvania
</p>
<p>♠ LIMSI, CNRS, Université Paris-Saclay, 91403 Orsay
{acocos,marapi,ccb}@seas.upenn.edu
</p>
<p>Abstract
</p>
<p>WordNet has facilitated important re-
search in natural language processing but
its usefulness is somewhat limited by its
relatively small lexical coverage. The
Paraphrase Database (PPDB) covers 650
times more words, but lacks the semantic
structure of WordNet that would make it
more directly useful for downstream tasks.
We present a method for mapping words
from PPDB to WordNet synsets with 89%
accuracy. The mapping also lays impor-
tant groundwork for incorporating Word-
Net’s relations into PPDB so as to increase
its utility for semantic reasoning in appli-
cations.
</p>
<p>1 Introduction
</p>
<p>WordNet (Miller, 1995; Fellbaum, 1998) is one of
the most important resources for natural language
processing research. Despite its utility, Word-
Net1 is manually compiled and therefore relatively
small. It contains roughly 155k words, which does
not approach web scale, and very few informal
or colloquial words, domain-specific terms, new
word uses, or named entities. Researchers have
compiled several larger, automatically-generated
thesaurus-like resources (Lin and Pantel, 2001;
Dolan and Brockett, 2005; Navigli and Ponzetto,
2012; Vila et al., 2015). One of these is the
Paraphrase Database (PPDB) (Ganitkevitch et al.,
2013; Pavlick et al., 2015b). With over 100 million
paraphrase pairs, PPDB dwarfs WordNet in size
but it lacks WordNet’s semantic structure. Para-
phrases for a given word are indistinguishable by
sense, and PPDB’s only inherent semantic rela-
tional information is predicted entailment relations
between word types (Pavlick et al., 2015a). Sev-
eral earlier studies attempted to incorporate se-
</p>
<p>1In this work we refer specifically to WordNet version 3.0
</p>
<p>RULE-PRESCRIPT: imperative*, demand*, duty*, re-
quest, gun, decree, ranking
RULE-REGULATION: constraint*, limit*, derogation*,
notion
RULE-FORMULA: method*, standard*, plan*, proceed-
ing
RULE-LINGUISTIC RULE: notion
</p>
<p>Table 1: Example of our model’s top-ranked para-
phrases for three WordNet synsets for rule (n).
Starred paraphrases have a predicted likelihood of
attachment of at least 95%; others have predicted
likelihood of at least 50%. Bold text indicates
paraphrases that match the correct sense of rule.
</p>
<p>mantic awareness into PPDB, either by clustering
its paraphrases by word sense (Apidianaki et al.,
2014; Cocos and Callison-Burch, 2016) or choos-
ing appropriate PPDB paraphrases for a given con-
text (Apidianaki, 2016; Cocos et al., 2017). In
this work, we aim to marry the rich semantic
knowledge in WordNet with the massive scale of
PPDB by predicting WordNet synset membership
for PPDB paraphrases that do not appear in Word-
Net. Our goal is to increase the lexical coverage
of WordNet and incorporate some of the rich rela-
tional information from WordNet into PPDB. Ta-
ble 1 shows our model’s top-ranked outputs map-
ping PPDB paraphrases for the word rule onto
their corresponding WordNet synsets.
</p>
<p>Our overall objective in this work is to map
PPDB paraphrases for a target word to the Word-
Net synsets of the target. This work has two
parts. In the first part (Section 4), we train
and evaluate a binary lemma-synset member-
ship classifier. The training and evaluation data
comes from lemma-synset pairs with known class
(member/non-member) from WordNet. In the
second part (Section 5), we predict membership
for lemma-synset pairs where the lemma appears
in PPDB, but not in WordNet, using the model
trained in part one.
</p>
<p>84</p>
<p />
</div>
<div class="page"><p />
<p>2 Related Work
</p>
<p>There has been considerable research directed at
expanding WordNet’s coverage either by integrat-
ing WordNet with additional semantic resources,
as in Navigli and Ponzetto (2012), or by automat-
ically adding new words and senses. In the sec-
ond case, there have been several efforts specif-
ically focused on hyponym/hypernym detection
and attachment (Snow et al., 2006; Shwartz et al.,
2016). There is also previous work aimed at
adding semantic structure to PPDB. Cocos and
Callison-Burch (2016) clustered paraphrases by
word sense, effectively forming synsets within
PPDB. By mapping individual paraphrases to
WordNet synsets, our work could be used in co-
ordination with these previous results in order to
extend WordNet relations to the automatically-
induced PPDB sense clusters.
</p>
<p>3 WordNet and PPDB Structure
</p>
<p>The core concept in WordNet is the synonym set,
or synset – a set of words meaning the same thing.
Since words can be polysemous, a given lemma
may belong to multiple synsets corresponding to
its different senses. WordNet also defines rela-
tionships between synsets, such as hypernymy, hy-
ponymy, and meronymy. In the rest of the pa-
per, we will use S(wp) to denote the set of Word-
Net synsets containing word wp, where the sub-
script p denotes the part of speech. Each synset
sip ∈ S(wp) is a set containing wp as well as
its synonyms for the corresponding sense. PPDB
also has a graph structure, where nodes are words,
and edges connect mutual paraphrases. We will
use PPDB(wp) to denote the set of PPDB para-
phrases connected to target word wp.
</p>
<p>4 Predicting Synset Membership
</p>
<p>Our objective is to map paraphrases for a target
word, t, to the WordNet synsets of the target. For
a given target word in a vocabulary, we make a
binary synset-attachment prediction between each
of t’s paraphrases, wp ∈ PPDB(t), and each of
t’s synsets, sip ∈ S(t). We predict the likelihood
of a word wp belonging to synset sip on the basis
of multiple features describing their relationship.
We construct features from four primary types of
information.
</p>
<p>PPDB 2.0 Score The PPDB 2.0 Score is a su-
pervised metric trained to estimate the strength of
</p>
<p>the paraphrase relationship between pairs of words
connected in PPDB (Pavlick et al., 2015b). Scores
range roughly from 0 to 5, with 5 indicating a
strong paraphrase relationship. We compute sev-
eral features for predicting whether a word wp be-
longs to synset sip as follows. We call the set of
all lemmas belonging to sip and any of its hyper-
nym or hyponym synsets the extended synset s+ip .
We calculate features that correspond to the aver-
age and maximum PPDB scores bewteen wp and
lemmas in s+ip :
</p>
<p>xppdb.max = max
w′∈s+ip
</p>
<p>PPDBScore(wp, w′)
</p>
<p>xppdb.avg =
</p>
<p>∑
w′∈s+ip PPDBScore(wp, w
</p>
<p>′)
</p>
<p>|s+ip |
Distributional Similarity Our distributional sim-
ilarity feature encodes the extent to which the
word and lemmas from the synset tend to appear
within similar contexts. Word embeddings are
real-valued vector representations of words that
capture contextual information from a large cor-
pus. Comparing the embeddings of two words is
a common method for estimating their semantic
similarity and relatedness. Embeddings can also
be constructed to represent word senses (Iacobacci
et al., 2015; Flekova and Gurevych, 2016; Jauhar
et al., 2015; Ettinger et al., 2016). Camacho-
Collados et al. (2016) developed compositional
vector representations of WordNet noun senses –
called NASARI embedded vectors – that are com-
puted as the weighted average of the embeddings
for words in each synset. They share the same
embedding space as a publicly available2 set of
300-dimensional word2vec embeddings cover-
ing 300 million words (hereafter referred to as the
word2vec embeddings) (Mikolov et al., 2013a,b).
We calculate a distributional similarity feature for
each word-synset pair by simply taking the co-
sine similarity between the word’s word2vec vec-
tor and the synset’s NASARI vector:
</p>
<p>xdistrib = cos(vNASARI(sip), vword2vec(wp))
</p>
<p>where vNASARI and vword2vec denote the target
word and synset embeddings respectively. Since
NASARI covers only nouns, and only 80% of
the noun synsets for our target vocabulary are in
NASARI, we construct weighted vector represen-
tations for the remaining 20% of noun synsets and
</p>
<p>2https://code.google.com/archive/p/word2vec/
</p>
<p>85</p>
<p />
</div>
<div class="page"><p />
<p>all non-noun synsets as follows. We take the vec-
tor representation for each synset not in NASARI
to be the weighted average of the word2vec em-
beddings of the synset’s lemmas, where weights
are determined by the PPDB2.0 Score between the
lemma and the target word, if it exists, or 1.0 if it
does not:
</p>
<p>v(sip) =
</p>
<p>∑
l∈sip PPDBScore(t, l) · vword2vec(l)∑
</p>
<p>l∈sip PPDBScore(t, l)
</p>
<p>Lesk Similarity Among the information con-
tained in WordNet for each synset is its definition,
or gloss. The simplified Lesk algorithm (Vasilescu
et al., 2004) identifies the most likely sense of
a target word in context by measuring the over-
lap between the given context and the definition
of each target sense. We use a slightly modi-
fied version of the algorithm to compute features
that measure the overlap between the PPDB para-
phrases for the target and the gloss of a synset.
For calculating these Lesk-based features, we find
synset glosses from WordNet 3.0 and from Babel-
Net v3.0 (Navigli and Ponzetto, 2012). First, we
find D, the set of content words of the gloss for
synset sip, by taking all nouns, verbs, adjectives,
and adverbs that appear within the gloss. In cases
where more than one gloss is available, we take
D to be the set of all content words in all glosses.
We also calculate an extended version of each fea-
ture, in which we take D to be the set of content
words, plus the PPDB paraphrases for each con-
tent word. Next, we calculate features that mea-
sure the relationship between the paraphrase wp
and the words in D in terms of PPDB2.0 Scores.
These features include the maximum PPDB score
between the paraphrase and any word in D, the
average score over all words in D, the percent of
words in D that are connected to the paraphrase in
PPDB, and the count of words in D that are con-
nected to the paraphrase in PPDB:
</p>
<p>xlesk.max = max
d∈D
</p>
<p>PPDBScore(wp, d)
</p>
<p>xlesk.avg =
</p>
<p>∑
d∈D PPDBScore(wp, d)
</p>
<p>|D|
xlesk.cnt = |{d ∈ D : PPDBScore(wp, d) &gt; 0}|
</p>
<p>xlesk.pct =
|{d ∈ D : PPDBScore(wp, d) &gt; 0}|
</p>
<p>|D|
</p>
<p>Lexical Substitutability The fourth feature type
that we compute to predict whether word wp be-
longs to synset sip is based on the substitutability of
wp for instances of sip in context. To compute this
</p>
<p>feature we measure lexical substitutability using
a simple but high-performing vector space model,
AddCos (Melamud et al., 2015). The AddCos
method quantifies the fit of substitute word s for
target word t in context C by measuring the se-
mantic similarity of the substitute to the target, and
the similarity of the substitute to the context:
</p>
<p>AddCos(s, t, C) =
|C|·cos(s, t) + ∑c∈C cos(s, c)
</p>
<p>2 · |C|
The vectors s and t are word embeddings of the
substitute and target generated by the skip-gram
with negative sampling model (Mikolov et al.,
2013b,a). The context C is the set of words ap-
pearing within a fixed-width window of the tar-
get t in a sentence (we use a window of 2), and
the embeddings c are context embeddings gener-
ated by skip-gram. In our implementation, we
train 300-dimensional word and context embed-
dings over the 4B words in the Annotated Giga-
word (AGiga) corpus (Napoles et al., 2012) us-
ing the gensim word2vec package (Mikolov et al.,
2013b,a; Řehůřek and Sojka, 2010). 3
</p>
<p>To compute the lexical substitutability score be-
tween a word wp and synset sip, we first retrieve
example sentences e ∈ E containing t in sense sip
from BabelNet v3.0 (Navigli and Ponzetto, 2012).
Then, for each example e, we compute the Ad-
dCos lexical substitutability between wp and the
target word in context Ce. We compute two types
of this feature: The average AddCos score over all
synset examples, and the maximum AddCos score
over all synset examples.
</p>
<p>xaddcos.max = maxe∈EAddCos(wp, t, Ce)
</p>
<p>xaddcos.avg = avge∈EAddCos(wp, t, Ce)
</p>
<p>Derived Features For each paraphrase, we also
compute a set of derived features using the soft-
max and logodds functions over all synsets with
which that paraphrase is paired. This is to en-
code the relative strength of association with each
synset as compared to the others.
</p>
<p>For a given feature x∗ calculated between
lemma wp and synset sip, the derived versions of
the feature are calculated as:
</p>
<p>xsoftmax∗ (wp, s
i
p) =
</p>
<p>ex∗(wp,s
i
p)∑
</p>
<p>s
j
p∈S(wp) e
</p>
<p>x∗(wp,sjp)
</p>
<p>3The word2vec training parameters we use are a context
window of size 3, learning rate alpha from 0.025 to 0.0001,
minimum word count 100, sampling parameter 1e−4, 10 neg-
ative samples per target word, and 5 training epochs.
</p>
<p>86</p>
<p />
</div>
<div class="page"><p />
<p>Cross-Validation Cross-Validation-LexSplit Test
Prec. Rec. F1 Acc. Prec. Rec. F1 Acc. Prec. Rec. F1 Acc.
</p>
<p>Baseline: All negative attachments 0 0.854 0 0.854 0 0.854 0 0.854 0 0.858 0 0.858
Baseline: PPDB2.0 Score Match 0.536 0.419 0.471 0.862 0.536 0.419 0.471 0.862 0.268 0.718 0.390 0.681
Gaussian Naive Bayes (All Feat.) 0.528 0.369 0.372 0.800 0.527 0.310 0.352 0.825 0.496 0.282 0.359 0.858
Gaussian Naive Bayes (Sel. Feat.) 0.605 0.600 0.600 0.883 0.606 0.572 0.581 0.882 0.622 0.558 0.588 0.889
</p>
<p>Table 2: Precision, recall, F1, and accuracy results over the training set (normal 10-fold Cross-Validation,
and lexical split 20-fold Cross-Validation-LexSplit) and test set for predicting paraphrase-synset attach-
ment.
</p>
<p>xlogodds∗ (wp, s
i
p) = ln
</p>
<p>x∗(wp, sip) + α∑
s
</p>
<p>j
p∈S(wp),i 6=j(x∗(wp, s
</p>
<p>j
p) + α)
</p>
<p>Model Training We train a binary classification
model that takes lemma-synset pairs as input, and
predicts whether the lemma belongs in the synset.
We train the model by generating features for a set
of lemma-synset pairs from WordNet for which
we know the correct classification. We evalu-
ate whether the resulting model correctly finds
lemma-synset pairs that belong together.
</p>
<p>Our target vocabulary comes from the SensE-
val3 English Lexical Sample Task data (Mihal-
cea et al., 2004) which contains sentences corre-
sponding to 57 noun, verb, and adjective lemmas.
Each sentence may contain a different form of the
lemma (i.e. different in number or tense), and
PPDB paraphrases vary depending on the form.
So we take the set of all forms of all lemmas (251
word types in total) as our target vocabulary. To
generate pairs for training and evaluation, for each
of the 251 targets wp, we find the lemmas in the in-
tersection of wp’s synsets – S(wp) – and its para-
phrases – PPDB(wp). We call the set of lem-
mas in the intersection L(wp). Then, we take the
lemma-synset pairs in L(wp)×S(wp) as instances
for training and evaluation. The total number of
resulting lemma-synset pairs is 7459. We ran-
domly divide these into 80% training and 20% test
pairs.
</p>
<p>We then generate all variations of each of the
four feature types for the lemma-synset pairs in
our training and test sets. In the case of positive
synset-lemma pairs – i.e. those pairs for which
the lemma actually belongs to the WordNet synset
– we exclude the lemma from the synset before
calculating the PPDB Score and distributional fea-
tures.
</p>
<p>Finally, we train a Gaussian Naive Bayes
(GNB) classification model over the training data.
</p>
<p>GNB is advantageous for our setting, as 10% per-
cent of our instances have missing data (e.g. in
the case where a synset does not have an ex-
ample). For feature selection, we use two ver-
sions of cross-validation. The first is standard 10-
fold Cross-Validation. In order to estimate how
well our model will generalize to unseen lem-
mas, we also experiment with a lexical split tech-
nique described in Roller and Erk (2016) (Cross-
Validation-LexSplit). This method ensures that for
each cross-validation fold, none of the lemmas in
that fold’s validation lemma-synset pairs are seen
in the training split. Specifically, for each split we
randomly select 5% of training pairs for validation
and take the remainder of the training set that does
not share a lemma with the validation set as that
fold’s training instances. As a result, the valida-
tion set size remains constant for each fold, but
the training set sizes may vary between folds.
</p>
<p>We train two versions of the model. The first
(All Features) uses all computed features. The sec-
ond (Selected Features) includes features selected
using cross validation (the selected features were
the same using standard and lexical split cross-
validation). We select one feature of each type
(PPDB Score, distributional, Lesk, and lexical
substitutability) whose combination maximizes
cross-validation F1 score. The selected features
are xlesk.cnt (non-extended), xdistrib, xaddcos.max,
and softmax(xppdb.max).
</p>
<p>Model Evaluation We report results of the model
using all features, and the results of the best model
achieved after feature selection (Table 2). In
each case we give both the Cross-Validation and
Cross-Validation-LexSplit performances, and per-
formance on the held-out test set. We compare
our model to two simple baselines. The first pre-
dicts all negative attachments, which yields an ac-
curacy of 85.8% on the test set (with F1 of 0).
The second baseline maps each paraphrase to the
synset of t with which it has the highest-scoring
</p>
<p>87</p>
<p />
</div>
<div class="page"><p />
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>Recall
</p>
<p>0.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>P
re
</p>
<p>ci
si
</p>
<p>o
n
</p>
<p>Precision-Recall Curve for Lemma-Synset Attachment
</p>
<p>Cross-Validation Precision-Recall: AUC=0.61
</p>
<p>Cross-Validation-LexSplit Precision-Recall: AUC=0.61
</p>
<p>Test Precision-Recall: AUC=0.58
</p>
<p>Figure 1: Precision-Recall curve for our
paraphrase-synset attachment classifier.
</p>
<p>PPDB feature (xppdb.max) and yields an accuracy
of 68.1% on the test set. In comparison, our GNB
model with selected features yields an accuracy
of over 88% on the cross-validation and test sets.
Both cross-validation and test accuracies are sig-
nificantly higher than baselines (based on McNe-
mar’s test, p &lt; .001).
</p>
<p>Ablated Feature Type Change in Cross-Val. F1
PPDB 0.042
Lexical Substitution 0.031
Distributional 0.009
Lesk -0.092
</p>
<p>Table 3: Absolute decrease in mean cross-
validation F1 with different feature types ablated.
Higher numbers indicate greater feature impor-
tance.
</p>
<p>In order to interpret the importance of each feature
type, we also run an ablation experiment where
we train our GNB model with all features except
those from a particular type (Table 3). We find that
removing the PPDB features leads to the greatest
drop in cross-validation F1, indicating that these
are the most important for our classifier. Ablating
all Lesk features improved F1, but on further anal-
ysis we found that ablating only the derived Lesk
log-odds features led to a decrease in F1. This
suggests that the Lesk features in general are use-
ful for classification, but the derived Lesk log-odds
features are not.
</p>
<p>5 Mapping PPDB to WordNet
</p>
<p>Using our trained lemma-synset attachment clas-
sifier, we can now augment the lexical coverage of
WordNet with PPDB paraphrases. For the 251 tar-
gets in our original dataset, we retrieve the PPDB
paraphrases (with PPDB score greater than 2.5, to
ensure high-quality paraphrases) that do not be-
</p>
<p>long to any synset of the target or any of their di-
rect hypernyms or hyponyms. We then make an at-
tachment prediction between each remaining para-
phrase and each of the target’s WordNet synsets.
In total, we make 160,813 unique paraphrase-
synset attachment predictions for the 4821 unique
paraphrase lemmas and 458 unique synsets asso-
ciated with the targets in our dataset.
</p>
<p>When we map PPDB to WordNet we can esti-
mate the expected precision and recall of attach-
ment decisions based on the results of our model
evaluation on the test set. If we would like to em-
phasize precision over recall in the predicted at-
tachments, we can adjust a threshold for attach-
ment corresponding to the predicted likelihood of
our model (as shown in Figure 1). At a threshold
of 50% predicted likelihood, our classifier predicts
attachment for 7032 (4.4%) of the paraphrase-
synset pairs with an estimated precision of 62.2%.
If we increase the threshold to 95% predicted like-
lihood, the number of predicted attachments is
3690 (2.3%) with an estimated precision of 66.3%.
With the publication of this paper we release our
PPDB to WordNet mapping results.
</p>
<p>6 Conclusion
</p>
<p>We have proposed a method for mapping PPDB
paraphrases to WordNet synsets. Our classifier
makes accurate paraphrase-synset attachment pre-
dictions using features that capture paraphrase and
distributional similarity, and the substitutability of
paraphrases and synsets in context. The results
show that the classifier can successfully add new
PPDB paraphrases to WordNet synsets and in-
crease their coverage.
</p>
<p>Acknowledgments
</p>
<p>This research is supported in part by DARPA
grant FA8750-13-2-0017 (the DEFT program).
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes.
The views and conclusions contained in this pub-
lication are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA and the U.S. Government.
</p>
<p>This work is also supported by the French Na-
tional Research Agency under project ANR-16-
CE33-0013.
</p>
<p>We would like to thank our anonymous review-
ers for their thoughtful and helpful comments.
</p>
<p>88</p>
<p />
</div>
<div class="page"><p />
<p>References
</p>
<p>Marianna Apidianaki. 2016. Vector-space models for
ppdb paraphrase ranking in context. In Proceedings
of EMNLP. Austin, Texas, pages 2028–2034.
</p>
<p>Marianna Apidianaki, Emilia Verzeni, and Diana Mc-
Carthy. 2014. Semantic Clustering of Pivot Para-
phrases. In Proceedings of LREC. Reykjavik, Ice-
land, pages 4270–4275.
</p>
<p>José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artifi-
cial Intelligence 240:36–64.
</p>
<p>Anne Cocos, Marianna Apidianaki, and Chris Callison-
Burch. 2017. Word sense filtering improves
embedding-based lexical substitution. In Proceed-
ings of the 1st Workshop on Sense, Concept and En-
tity Representations and their Applications. pages
110–119.
</p>
<p>Anne Cocos and Chris Callison-Burch. 2016. Cluster-
ing Paraphrases by Word Sense. In Proceedings of
NAACL/HLT . San Diego, California, pages 1463–
1472.
</p>
<p>William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing. Jeju Island, Korea, pages 9–16.
</p>
<p>Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retrofitting sense-specific word vectors using
parallel text. In Proceedings of NAACL-HLT . San
Diego, California, pages 1378–1383.
</p>
<p>Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.
</p>
<p>Lucie Flekova and Iryna Gurevych. 2016. Supersense
Embeddings: A Unified Model for Supersense Inter-
pretation, Prediction, and Utilization. In Proceed-
ings of ACL. Berlin, Germany, pages 2029–2041.
</p>
<p>Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL/HLT . Atlanta,
Georgia, pages 758–764.
</p>
<p>Ignacio Iacobacci, Mohammed Taher Pilehvar, and
Roberto Navigli. 2015. SensEmbed: Learning
Sense Embeddings for Word and Relational Similar-
ity. In Proceedings of ACL/IJCNL. Beijing, China,
pages 95–105.
</p>
<p>Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically Grounded Multi-sense Repre-
sentation Learning for Semantic Vector Space Mod-
els. In Proceedings of NAACL. Denver, Colorado,
pages 683–693.
</p>
<p>Dekang Lin and Patrick Pantel. 2001. Dirt@ sbt@
discovery of inference rules from text. In Proceed-
ings of the seventh ACM SIGKDD international con-
ference on Knowledge discovery and data mining.
ACM, pages 323–328.
</p>
<p>Oren Melamud, Omer Levy, and Ido Dagan. 2015. A
Simple Word Embedding Model for Lexical Substi-
tution. In Proceedings of the 1st Workshop on Vector
Space Modeling for Natural Language Processing.
Denver, Colorado, pages 1–7.
</p>
<p>Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Rada Mihalcea and Phil Edmonds, edi-
tors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text. Barcelona, Spain, pages 25–28.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NIPS. pages 3111–3119.
</p>
<p>George A. Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM
31(11):39–41.
</p>
<p>Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction (AKBC-WEKEX). Montréal, Canada,
pages 95–100.
</p>
<p>Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence 193:217–
250.
</p>
<p>Ellie Pavlick, Johan Bos, Malvina Nissim, Charley
Beller, Benjamin Van, and Durme Chris Callison-
burch. 2015a. Adding semantics to data-driven para-
phrasing. In Proceedings of ACL. Beijing, China,
pages 425–430.
</p>
<p>Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevich,
and Chris Callison-Burch Ben Van Durme. 2015b.
PPDB 2.0: Better paraphrase ranking, fine-grained
entailment relations, word embeddings, and style
classification. In Proceedings of ACL. Beijing,
China, pages 425–430.
</p>
<p>Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. ELRA, Valletta,
Malta, pages 45–50.
</p>
<p>89</p>
<p />
</div>
<div class="page"><p />
<p>Stephen Roller and Katrin Erk. 2016. Relations such
as hypernymy: Identifying and exploiting hearst pat-
terns in distributional vectors for lexical entailment.
In Proceedings of EMNLP. pages 2163–2172.
</p>
<p>Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of ACL. Berlin, Germany, pages 2389–2398.
</p>
<p>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of ACL. pages 801–808.
</p>
<p>Florentina Vasilescu, Philippe Langlais, and Guy La-
palme. 2004. Evaluating Variants of the Lesk Ap-
proach for Disambiguating Words. In Proceedings
of LREC. Lisbonne, Portugal, pages 633–636.
</p>
<p>Marta Vila, Horacio Rodrı́guez, and M Antònia Martı́.
2015. Relational paraphrase acquisition from
wikipedia: The WRPA method and corpus. Natu-
ral Language Engineering 21(03):355–389.
</p>
<p>90</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 91–96,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Semantic Frame Labeling with Target-based Neural Model
</p>
<p>Yukun Feng1, Dong Yu1∗, Jian Xu2, ChunHua Liu1
1 Beijing Language and Culture University,
</p>
<p>2 University of Science and Technology of China
yukunfg@gmail.com,yudong@blcu.edu.cn,
</p>
<p>jianxu1@mail.ustc.edu.cn,chunhualiu596@gmail.com
</p>
<p>Abstract
</p>
<p>This paper explores the automatic learning
of distributed representations of the tar-
get’s context for semantic frame labeling
with target-based neural model. We con-
strain the whole sentence as the model’s
input without feature extraction from the
sentence. This is different from many pre-
vious works in which local feature extrac-
tion of the targets is widely used. This
constraint makes the task harder, especial-
ly with long sentences, but also makes our
model easily applicable to a range of re-
sources and other similar tasks. We evalu-
ate our model on several resources and get
the state-of-the-art result on subtask 2 of
SemEval 2015 task 15. Finally, we extend
the task to word-sense disambiguation task
and we also achieve a strong result in com-
parison to state-of-the-art work.
</p>
<p>1 Introduction and Related Work
</p>
<p>Semantic frame labeling is the task of selecting
the correct frame for a given target based on it-
s semantic scene. A target is often called lexi-
cal unit which evokes the corresponding seman-
tic frame. The lexical unit can be a verb, ad-
jective or noun. Generally, a semantic frame de-
scribes how the lexical unit is used and specifies
its characteristic interactions. There are many se-
mantic frame resources, such as FrameNet (Bak-
er et al., 1998), VerbNet (Schuler, 2006), Prop-
Bank (Palmer et al., 2005) and Corpus Pattern
Analysis (CPA) frames (Hanks, 2012). However,
most existing frame resources are manually creat-
ed, which is time-consuming and expensive. Au-
tomatic semantic frame labeling can lead to the de-
velopment of a broader range of resources.
</p>
<p>∗The corresponding author
</p>
<p>Early works for semantic frame labeling main-
ly focus on FrameNet, PropBank and VerbNet re-
sources. But most of them focus only one re-
source and rely heavily on feature engineering
(e.g., Honnibal and Hawker 2005; Abend et al.
2008). Recently, there are some works on learn-
ing CPA frames based on a new semantic frame
resource, the Pattern Dictionary of English Verbs
(PDEV) (El Maarouf and Baisa, 2013; El Maarouf
et al., 2014). The above two works also rely on
features and both are only tested on 25 verbs. Most
works aim at constructing the context represen-
tations of the target with explicit rules based on
some basic features, e.g., Parts Of Speech (POS),
Named Entities (NE) and dependency relations re-
lated to the target. Currently, some deep learning
models have been applied with dependency fea-
tures. Hermann et al. (2014) used the direct de-
pendents and dependency path to extract the con-
text representation based on distributed word em-
beddings on English FrameNet. Inspired by the
work, Zhao et al. (2016) used a deep feed forward
neural network on Chinese FrameNet with similar
features. This is different from our goal where we
want to explore an appropriate deep learning ar-
chitecture without complex rules to construct the
context representations. Feng et al. (2016) used
a multilayer perceptrons (MLP) model on CPA
frames without extra feature extraction, but the
model is quite simple and has an input window
which is not convenient.
</p>
<p>In this paper, we present a target-based neural
model which takes the whole target-specific sen-
tence as input and gives the semantic frame label
as output. Our goal is to make the model light
without explicit rules to construct context repre-
sentations and applicable to a range of resources.
To cope with variable-length sentences under our
constraint, a simple idea is to use recurrent neu-
ral networks (RNN) to process the sentences. But
</p>
<p>91</p>
<p />
</div>
<div class="page"><p />
<p>noise caused by irrelevant words in long sentences
may hinder learning. In fact, the arguments re-
lated to the target are usually distributed near the
target because when we write or speak, we will
focus mainly on arguments that are in the imme-
diate context of a core word. We use two RNNs
each of which processes one part of the sentence
split by the target. The model takes the target as
the center and we call it the target-based recurren-
t networks (TRNN). In fact, TRNN itself is not
novel enough, but according to our knowledge, no
related research has focused on this topic. We will
show that TRNN is quite suitable for learning the
context of the target.
</p>
<p>2 Model Description
</p>
<p>1
w w w
</p>
<p>T
w...
</p>
<p>...
</p>
<p>... ...
</p>
<p>LSTM LSTM LSTM LSTM...
</p>
<p>...
</p>
<p>Embedding Layer
</p>
<p>1
x Tx
</p>
<p>Context representations
</p>
<p>x
target
</p>
<p>x
target
</p>
<p>targettarget
</p>
<p>This procedure can force a minister to answer for what 
</p>
<p>his department intends to do
</p>
<p>Correct CPA frame for current input:
</p>
<p>[Phrasal verb. Human answers for Eventuality]
</p>
<p>Output Layer frame1 frame2 frameN...
</p>
<p>Figure 1: Architecture of TRNN with an example
sentence whose target word is in bold.
</p>
<p>In our model we select long short-term memo-
ry (LSTM) networks, a type of RNN designed to
avoid the vanishing and exploding gradients. The
overall structure is illustrated Figure 1. wt is the
t-th word in the sentence the length of which is
T and target is the index of the target. xt is ob-
tained by mapping wt into a fixed vector through
well pre-trained word vectors. The model has t-
wo LSTMs each of which processes one part of
the sentence split by the target. The model can au-
tomatically learn the distributed representation of
target’s context from w with few manual design.
</p>
<p>2.1 Context Representations
An introduction about LSTM can be found in the
work of Hochreiter and Schmidhuber (1997). The
parameters of LSTM are Wx∗, Wh∗ and b∗ where
</p>
<p>∗ stands for one of several internal gates. Wx∗ is
the matrix between the input vector xt and gates,
Wh∗ is the matrix between the output ht of LSTM
and gates and b∗ is the bias vector on gates. The
formulas of LSTM are:
it = σ(Wxixt +Whiht−1 + bi)
ft = σ(Wxfxt +Whfht−1 + bf )
ct = ft � ct−1 + it � tanh(Wxcxt +Whcht−1 + bc)
ot = σ(Wxoxt +Whoht−1 + bo)
ht = ot � tanh(ct)
</p>
<p>where σ is the sigmoid function and � represents
the element-wise multiplication. it, ft ct and ot
are the output of input gates, forget gates, cell s-
tates and output gates, respectively. In our model,
two LSTMs share the same parameters. At last,
the target’s context representations cr are added
by the outputs of two LSTMs:
</p>
<p>cr = htarget−1 + htarget
</p>
<p>The dimension of cr is decided by the number of
hidden units in LSTM, which is a hyper parameter
in our model, and is usually much lower than that
of one word vector. Here we make some intuition-
s behind the above formulas. The gradients from
last layer flow equally on the (target− 1)-th LST-
M box and the target-th LSTM box and then the
two flows go to both ends. As it is quite common
in deep learning models, the gradients usually be-
come ineffective as the depth of the flow increas-
es especially when the sentence is very long. The
gradients on words far from the target get less im-
pact than those near the target. As a whole, more
data are usually required to learn the arguments
far from the target than those near the target. If
the real arguments are distributed near the target,
this model will be suitable as its architecture is de-
signed to take care of the local context of the tar-
get.
</p>
<p>2.2 Output Layer
We use Softmax layer as the output layer on the
context representations. The output layer com-
putes a probability distribution over the semantic
frame labels. During the training, the cost we min-
imize is the negative log likelihood of the model:
</p>
<p>L = −
M∑
</p>
<p>m=1
</p>
<p>logptm
</p>
<p>Here M is number of the training sentences, tm is
the index of the correct frame label for the m-th
sentence and p is the probability.
</p>
<p>92</p>
<p />
</div>
<div class="page"><p />
<p>3 Experiments
</p>
<p>3.1 Datasets
We simply divide all the datasets in two types:
per-target and non per-target. Per-target seman-
tic frame resources define a different set of frame
labels for each target and we train one model for
each target; different targets may share some se-
mantic frame labels in non per-target resources
and we train a single model for such resources. We
use the Semlink project (Loper et al., 2007) to cre-
ate our datasets 1. Semlink aims to link together
different lexical resources via a set of mappings.
We use its corpus which annotates FrameNet and
Propbank frames for the WSJ section of the Pen-
n Treebank. Another resource we use is PDEV 2
</p>
<p>which is quite new and has CPA frame annotat-
ed examples on British National Corpus. All the
original instances are sentence-tokenized and the
punctuation was removed. The details of creating
the datasets are as follows:
</p>
<p>• FrameNet: Non per-target type. We get
FrameNet annotated instances through Sem-
link. If one FrameNet frame label contains
more than 300 instances, we divide it propor-
tionately: 70%, 20% and 10%. Then we re-
spectively accumulate the three parts by each
frame label to create the training, test and val-
idation set.
</p>
<p>• PropBank: Per-target type. The creation pro-
cess is same as FrameNet except that we fi-
nally get training, test and validation set for
each target and the cutoff is set to 70 instead
of 300.
</p>
<p>• PDEV: Same as PropBank but with the cutoff
set to 100 instead of 70.
</p>
<p>Since the performance of our model is almost
decided by the training data we empirically choose
the cutoff above to keep the instances of each label
enough. Summary statistics of the above datasets
are in Table 2.
</p>
<p>3.2 Models and Training
We compare our model with the following base-
lines.:
</p>
<p>1 The current version of the Semlink project has some
problems to get the right position of targets in WSJ section
of Penn Treebank. Instead, we use annotations of PropBank
corpus, also annotated in WSJ section of Penn Treebank, to
index targets.
</p>
<p>2http://pdev.org.uk/
</p>
<p>Sentences Frame Names
In Moscow they kept asking us
things like why do you make
15 different corkscrews
</p>
<p>Activityongoing
</p>
<p>It said it has taken measures to
continue shipments during
the work stoppage.
</p>
<p>Activityongoing
</p>
<p>But the Army Corps of Engineers
expects the river level to
continue falling this month.
</p>
<p>Processcontinue
</p>
<p>The oil industry’s middling profits
could persist through the rest
of the year.
</p>
<p>Processcontinue
</p>
<p>Table 1: Non per-target examples. Frames are
from FrameNet and the target words are in bold.
</p>
<p>FrameNet PropBank PDEV
Per-target No 153 targets 407 targets
Train 41206 31212 (204) 152218 (374)
Test 11762 8568 (56) 42328 (104)
Valid. 5871 4131 (27) 20350 (50)
Frame 33 443 (2.89) 2197 (5.39)
Words/sent. 23 23 12
</p>
<p>Table 2: Summary statistics for the datasets. The
average numbers per target are shown in the paren-
theses for per-target resources.
</p>
<p>• MF: The most frequent (MF) method selects
the most frequent semantic frame label seen
in training instances for each instance in the
test dataset. MF is actually a strong baseline
for per-target dataset because we observed
that most targets have one main frame label.
</p>
<p>• Target-Only: For FrameNet dataset, we use
Target-Only method: if the target in the test
instance has a unique frame label in the train-
ing data we give this frame label to current
test instance; if the target has multiple frame
labels in the training data we select the most
frequent one in these labels; if the target is not
seen in the training data, we select the most
frequent label from the whole training data.
This baseline is especially for FrameNet be-
cause we observed that each frame label has a
set of targets but only a few targets have mul-
tiple frame labels. It may be easy to predict
the frame label for test instances only accord-
ing to the target.
</p>
<p>• LSTM: The standard LSTM model.
</p>
<p>• MaxEnt: The Maximum Entropy model. We
use the Stanford CoreNLP module 3 to ex-
</p>
<p>3http://stanfordnlp.github.io/CoreNLP/
</p>
<p>93</p>
<p />
</div>
<div class="page"><p />
<p>tract features for MaxEnt toolkit 4. All de-
pendents related to the target, their POS tags,
dependency relations, lemmas, NE tags and
the target itself will be extracted as features.
</p>
<p>The number of the iterations for MaxEnt is decid-
ed by the validation set. For simplicity, we set
the learning rate to 1.0 for TRNN and LSTM. The
number of hidden units is tested on validation da-
ta with the values {35, 45, 55} for per-target re-
source and {80, 100, 120} for non per-target re-
source. We use the publicly available word2vec
vectors, a dimensionality of 300, that were trained
through the GloVe model (Pennington et al., 2014)
on Wikipedia and Gigaword. For words not ap-
peared in the vector model, their word vectors are
all set to zero vectors. We train these models by
stochastic gradient descent with minibatches. The
minibatch is set to 10 for per-target resource and
50 for non per-target resource. We keep the word
vectors static since no obvious improvement has
been observed. Training will stop when the zero-
one loss is zero over training data.
</p>
<p>3.3 Results
</p>
<p>The results of the above datasets are in Table 3.
Target-Only gets very high scores on FrameNet
dataset. FrameNet dataset has 55 targets which
has multiple frame labels in the training data and
these targets have 1981 instances in the test da-
ta. We get 0.769 F-score on these instances and
0.393 F-score on 64 unseen targets with 77 test
instances. This can be the extreme case that the
main feature for the correct frame is the target it-
self. Despite this simple fact, standard LSTM per-
forms very badly on FrameNet. The main reason
is that sentences in FrameNet dataset are too long
and standard LSTM can not learn well due to the
large number of irrelevant words that appear in
long sentences. To show this, we select the size
of truncation window for original FrameNet sen-
tences and we get the best size of 5 on validation
data with each 2 words surrounding the target. Fi-
nally, we get 0.958 F-score on FrameNet test data
which is still lower than TRNN on full sentences.
As for PropBank and PDEV dataset, we train one
model for each target so the final F-score is the av-
erage of all targets. However, the number of train-
ing instances per target is limited. TRNN will usu-
ally not perform well when it tries to learn some
</p>
<p>4https://github.com/lzhang10/maxent
</p>
<p>frames which consist of many different concept-
s and especially when the frame has a few train-
ing instances. Considering the sentence 4 of Table
4 as an example, it is difficult to TRNN to learn
what is ’Activity’ in the correct frame because this
concept is huge. TRNN may need lots of data to
learn something related to this concept. Howev-
er, this correct frame only has 6 instances in our
training data. The second reason of TRNN’s fail-
ure is lack of knowledge due to unseen words in
test data. The sentence 1 of Table 4 shows TRNN
will make the right decision since we observe that
it has seen the word ’cow’ in the training data and
knows this word belongs to the concept ’Animate
or Plant’ in the correct frame. But TRNN does
not know the word ’Elegans’ in sentence 3 so it
usually selects the most frequent frame seen in the
training data. However, in many cases, the unseen
words can be captured by well trained word em-
beddings as the sentence 2 shows where ’ducks’,
’chickens’ and ’geese’ are all unseen words.
</p>
<p>Models FrameNet PropBank PDEV
MF 0.38 0.78 0.61
</p>
<p>Target-Only 0.911 - -
MaxEnt 0.829/125 0.874/30 0.704/10
LSTM 0.55/80 0.78/35 0.72/55
TRNN 0.962/100 0.887/35 0.794/55
</p>
<p>Table 3: Results on several semantic frame
resources. The format of cell value is ”F-
score/hidden unit” for TRNN and LSTM and ”F-
score/iteration” for MaxEnt toolkit.
</p>
<p>3.4 CPA Experiment
</p>
<p>Corpus Pattern Analysis (CPA) is a new technique
for identifying the main patterns in which a word
is used in text and is currently being used to build
the PDEV resource as we mentioned above. It is
also a shared task in SemEval-2015 task 15 (Baisa
et al., 2015). The task is divided into three sub-
tasks: CPA parsing, CPA clustering and CPA lex-
icography. We only introduce the first two relat-
ed subtasks. CPA parsing aims at identifying the
arguments of the target and tagging predefined se-
mantic meaning on them; CPA clustering clusters
the instances to obtain CPA frames based on the
result of CPA parsing. However, the first step re-
sults seem unpromising (Feng et al., 2015; Mill-
s and Levow, 2015; Elia, 2016) which will influ-
ence the process of obtaining CPA frames. Since
our model can be applied on sentence-level input
without feature extraction we can directly evaluate
</p>
<p>94</p>
<p />
</div>
<div class="page"><p />
<p>ID Sentences Frame Prediction True Frame
</p>
<p>1 One of the farmer’s cows had died of BSEraising fears of cross-infection... Same with true frame Animate or Plant dies
</p>
<p>2 One of the farmer’s ducks|chickens|geesehad died of BSE raising fears of cross-infection... Same with true frame Animate or Plant dies
</p>
<p>3 Elegans also in central Americadie of damping off as a function of distance
Human dies ((Time Point)(Location)(Causation)
(at Number or at the age of or at birth or earlage)) Animate or Plant dies
</p>
<p>4 Indeed, the MEC does not advisethe use of any insecticidal shampoo for...
Human 1 or Institution 1 advises Human 2
or Institution 2 to-infinitive Human or Institution advises Activity
</p>
<p>Table 4: Case study for CPA frames. The target words are in bold.
</p>
<p>our model on CPA clustering. Unfortunately, the
datasets provided by CPA clustering is a per-target
resource for our model and the targets in train-
ing and test set are not the same. Since this task
is not limited to use extra resources, we use the
training set of FrameNet, a type of non per-target,
mentioned in section 3.1 to solve this problem.
The hyper parameters are the same as before. C-
PA clustering is evaluated by B-cubed F-score, a
metric for clustering problem, so we do not need
to convert the FrameNet frame label to CPA frame
label. The result is in Table 5. All the models
are supervised except for baseline and DULUTH.
Feng et al. (2016) used the MLP to classify fixed-
length local text of the target based on distribut-
ed word embeddings. But the representation of
the target’s context is simply constructed with con-
catenated word embeddings and the length of local
context has to be chosen manually. Besides, MLP
may fail to train or predict well when some key
words are out of its input window.
</p>
<p>System B-cubed F-score
BOB90(Best in SemEval 2015) 0.741
SemEval 2015 baseline 0.588
DULUTH 0.525
Feng et al. (2016) 0.70
</p>
<p>This paper 0.763
</p>
<p>Table 5: Results on Microcheck dataset of CPA
clustering.
</p>
<p>3.5 Word Sense Disambiguation Experiment
</p>
<p>Finally, we choose Word Sense Disambiguation
(WSD) task to extend our experiment. As our
benchmark for WSD task, we choose English Lex-
ical Sample WSD tasks of SemEval-2007 task
17 (Pradhan et al., 2007). We use cross-validation
on the training set and we observe the model per-
forms better when we update the word vectors
which is different from the preceding experimen-
tal setup. The number of hidden units is set to 55.
The result is in Table 6. The rows from 4 to 6
come from Iacobacci et al. (2016). They inte-
</p>
<p>grate word embeddings into IMS (It Makes Sense)
system (Zhong and Ng, 2010) which uses support
vector machine as its classifier based on some s-
tandard WSD features and they get the best result;
they use an exponential decay function, also de-
signed to give more importance to close context, to
compute the word representation, but their method
need manually choose the window size of the tar-
get word and one parameter of their exponential
decay function. Both with word vectors only, our
model is comparable with the sixth row.
</p>
<p>System F-score
Rank 1 system in SemEval 2007 0.887
Rank 2 system in SemEval 2007 0.869
IMS (2010) 0.879
IMS + word vectors (2016) 0.894
IMS + word vectors only (2016) 0.880
This paper 0.886
</p>
<p>Table 6: Result on Lexical Sample task of
SemEval-2007 task 17
</p>
<p>4 Conclusion
</p>
<p>In this paper, we describe an end-to-end neural
model to target-specific semantic frame labeling.
Without explicit rule construction to fit for some
specific resources, our model can be easily applied
to a range of semantic frame resources and similar
tasks. In the future, non-English semantic frame
resources can be considered to extend the coverage
of our model and our model can integrate the best
features explored in the state-of-the-art work to
see how many improvements our model can make.
</p>
<p>Acknowledgments
</p>
<p>We would like to thank the anonymous reviewers
and Li Zhao for their helpful suggestions and com-
ments. The work was supported by the Nation-
al High Technology Development 863 Program of
China (No.2015AA015409).
</p>
<p>95</p>
<p />
</div>
<div class="page"><p />
<p>References
Omri Abend, Roi Reichart, and Ari Rappoport. 2008.
</p>
<p>A supervised algorithm for verb disambiguation into
verbnet classes. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1. Association for Computational Linguis-
tics, pages 9–16.
</p>
<p>Vı́t Baisa, Jane Bradbury, Silvie Cinkova, Ismail
El Maarouf, Adam Kilgarriff, and Octavian Popes-
cu. 2015. Semeval-2015 task 15: A cpa dictionary-
entry-building task. In Proceedings of the 9th
International Workshop on Semantic Evaluation
(SemEval 2015). Association for Computational
Linguistics, Denver, Colorado, pages 315–324.
http://www.aclweb.org/anthology/S15-2053.
</p>
<p>Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1. Association for Com-
putational Linguistics, pages 86–90.
</p>
<p>Ismaıl El Maarouf and Vıt Baisa. 2013. Automatic
classification of patterns from the pattern dictionary
of english verbs. In Joint Symposium on Semantic
Processing..
</p>
<p>Ismail El Maarouf, Jane Bradbury, Vı́t Baisa, and
Patrick Hanks. 2014. Disambiguating verbs by col-
location: Corpus lexicography meets natural lan-
guage processing. In LREC. pages 1001–1006.
</p>
<p>Francesco Elia. 2016. Syntactic and semantic classi-
fication of verb arguments using dependency-based
and rich semantic features. arXiv preprint arX-
iv:1604.05747 .
</p>
<p>Yukun Feng, Qiao Deng, and Dong Yu. 2015. Bl-
cunlp: Corpus pattern analysis for verbs based
on dependency chain. In Proceedings of the
9th International Workshop on Semantic Evalua-
tion (SemEval 2015). Association for Computation-
al Linguistics, Denver, Colorado, pages 325–328.
http://www.aclweb.org/anthology/S15-2054.
</p>
<p>Yukun Feng, Yipei Xu, and Dong Yu. 2016. An end-
to-end approach to learning semantic frames with
feedforward neural network. In Proceedings of the
NAACL Student Research Workshop. Association for
Computational Linguistics, San Diego, California,
pages 1–7. http://www.aclweb.org/anthology/N16-
2001.
</p>
<p>Patrick Hanks. 2012. How people use words to make
meanings: Semantic types meet valencies. Input,
Process and Product: Developments in Teaching
and Language Corpora pages 54–69.
</p>
<p>Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
</p>
<p>Long Papers). Association for Computational Lin-
guistics, Baltimore, Maryland, pages 1448–1458.
http://www.aclweb.org/anthology/P/P14/P14-1136.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Matthew Honnibal and Tobias Hawker. 2005. Identi-
fying framenet frames for verbs from a real-text cor-
pus. In Proceedings of Australasian Language Tech-
nology Workshop.
</p>
<p>Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2016. Embeddings for word sense
disambiguation: An evaluation study. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics. pages 897–907.
</p>
<p>Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: mapping between
propbank and verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics,
Tilburg, the Netherlands.
</p>
<p>Chad Mills and Gina-Anne Levow. 2015. Cmill-
s: Adapting semantic role labeling features to
dependency parsing. In Proceedings of the 9th
International Workshop on Semantic Evaluation
(SemEval 2015). Association for Computational
Linguistics, Denver, Colorado, pages 433–437.
http://www.aclweb.org/anthology/S15-2075.
</p>
<p>Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational linguistics 31(1):71–
106.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.
</p>
<p>Sameer S Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations. Association for Computational
Linguistics, pages 87–92.
</p>
<p>Karin Kipper Schuler. 2006. VerbNet: A
Broad-Coverage, Comprehensive Verb Lex-
icon. Ph.D. thesis, University of Penn-
sylvania. http://verbs.colorado.edu/ kip-
per/Papers/dissertation.pdf.
</p>
<p>Hongyan Zhao, Ru Li, Sheng Zhang, and Liwen
Zhang. 2016. Chinese frame identification with
deep neural network 30(6):75.
</p>
<p>Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 Sys-
tem Demonstrations. Association for Computational
Linguistics, pages 78–83.
</p>
<p>96</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 97–103,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Frame-Based Continuous Lexical Semantics through Exponential Family
Tensor Factorization and Semantic Proto-Roles
</p>
<p>Francis Ferraro and Adam Poliak and Ryan Cotterell and Benjamin Van Durme
Center for Language and Speech Processing
</p>
<p>Johns Hopkins University
{ferraro,azpoliak,ryan.cotterell,vandurme}@cs.jhu.edu
</p>
<p>Abstract
</p>
<p>We study how different frame annota-
tions complement one another when learn-
ing continuous lexical semantics. We
learn the representations from a tensorized
skip-gram model that consistently en-
codes syntactic-semantic content better,
with multiple 10% gains over baselines.
</p>
<p>1 Introduction
</p>
<p>Consider “Bill” in Fig. 1: what is his involve-
ment with the words “would try,” and what does
this involvement mean? Word embeddings repre-
sent such meaning as points in a real-valued vec-
tor space (Deerwester et al., 1990; Mikolov et al.,
2013). These representations are often learned by
exploiting the frequency that the word cooccurs
with contexts, often within a user-defined window
(Harris, 1954; Turney and Pantel, 2010). When
built from large-scale sources, like Wikipedia or
web crawls, embeddings capture general charac-
teristics of words and allow for robust downstream
applications (Kim, 2014; Das et al., 2015).
</p>
<p>Frame semantics generalize word meanings to
that of analyzing structured and interconnected la-
beled “concepts” and abstractions (Minsky, 1974;
Fillmore, 1976, 1982). These concepts, or roles,
implicitly encode expected properties of that word.
In a frame semantic analysis of Fig. 1, the segment
“would try” triggers the ATTEMPT frame, filling
the expected roles AGENT and GOAL with “Bill”
and “the same tactic,” respectively. While frame
semantics provide a structured form for analyzing
words with crisp, categorically-labeled concepts,
the encoded properties and expectations are im-
plicit. What does it mean to fill a frame’s role?
</p>
<p>Semantic proto-role (SPR) theory, motivated by
Dowty (1991)’s thematic proto-role theory, offers
an answer to this. SPR replaces categorical roles
</p>
<p>ATTEMPT
She said Bill would try the same tactic again.
</p>
<p>AGENT GOAL
</p>
<p>Figure 1: A simple frame analysis.
</p>
<p>with judgements about multiple underlying prop-
erties about what is likely true of the entity fill-
ing the role. For example, SPR talks about how
likely it is for Bill to be a willing participant in the
ATTEMPT. The answer to this and other simple
judgments characterize Bill and his involvement.
Since SPR both captures the likelihood of certain
properties and characterizes roles as groupings of
properties, we can view SPR as representing a type
of continuous frame semantics.
</p>
<p>We are interested in capturing these SPR-based
properties and expectations within word embed-
dings. We present a method that learns frame-
enriched embeddings from millions of documents
that have been semantically parsed with multiple
different frame analyzers (Ferraro et al., 2014).
Our method leverages Cotterell et al. (2017)’s
formulation of Mikolov et al. (2013)’s popular
skip-gram model as exponential family principal
component analysis (EPCA) and tensor factor-
ization. This paper’s primary contributions are:
(i) enriching learned word embeddings with mul-
tiple, automatically obtained frames from large,
disparate corpora; and (ii) demonstrating these
enriched embeddings better capture SPR-based
properties. In so doing, we also generalize Cot-
terell et al.’s method to arbitrary tensor dimen-
sions. This allows us to include an arbitrary
amount of semantic information when learning
embeddings. Our variable-size tensor factoriza-
tion code is available at https://github.com/
fmof/tensor-factorization.
</p>
<p>97</p>
<p />
</div>
<div class="page"><p />
<p>2 Frame Semantics and Proto-Roles
</p>
<p>Frame semantics currently used in NLP have a rich
history in linguistic literature. Fillmore (1976)’s
frames are based on a word’s context and prototyp-
ical concepts that an individual word evokes; they
intend to represent the meaning of lexical items by
mapping words to real world concepts and shared
experiences. Frame-based semantics have inspired
many semantic annotation schemata and datasets,
such as FrameNet (Baker et al., 1998), PropBank
(Palmer et al., 2005), and Verbnet (Schuler, 2005),
as well as composite resources (Hovy et al., 2006;
Palmer, 2009; Banarescu et al., 2012).1
</p>
<p>Thematic Roles and Proto Roles These re-
sources map words to their meanings through
discrete/categorically labeled frames and roles;
sometimes, as in FrameNet, the roles can be very
descriptive (e.g., the DEGREE role for the AF-
FIRM OR DENY frame), while in other cases, as
in PropBank, the roles can be quite general (e.g.,
ARG0). Regardless of the actual schema, the roles
are based on thematic roles, which map a predi-
cate’s arguments to a semantic representation that
makes various semantic distinctions among the ar-
guments (Dowty, 1989).2 Dowty (1991) claims
that thematic role distinctions are not atomic, i.e.,
they can be deconstructed and analyzed at a lower
level. Instead of many discrete thematic roles,
Dowty (1991) argues for proto-thematic roles, e.g.
PROTO-AGENT rather than AGENT, where dis-
tinctions in proto-roles are based on clusterings of
logical entailments. That is, PROTO-AGENTs of-
ten have certain properties in common, e.g., ma-
nipulating other objects or willingly participating
in an action; PROTO-PATIENTs are often changed
or affected by some action. By decomposing the
meaning of roles into properties or expectations
that can be reasoned about, proto-roles can be seen
as including a form of vector representation within
structured frame semantics.
</p>
<p>3 Continuous Lexical Semantics
</p>
<p>Word embeddings represent word meanings as el-
ements of a (real-valued) vector space (Deerwester
et al., 1990). Mikolov et al. (2013)’s word2vec
methods—skip-gram (SG) and continuous bag of
</p>
<p>1See Petruck and de Melo (2014) for detailed descriptions
on frame semantics’ contributions to applied NLP tasks.
</p>
<p>2Thematic role theory is rich, and beyond this paper’s
scope (Whitehead, 1920; Davidson, 1967; Cresswell, 1973;
Kamp, 1979; Carlson, 1984).
</p>
<p>words (CBOW)—repopularized these methods.
We focus on SG, which predicts the context i
around a word j, with learned representations ci
and wj , respectively, as p(context i | word j) ∝
exp (cᵀi wj) = exp (1
</p>
<p>ᵀ(ci �wj)) , where� is the
Hadamard (pointwise) product. Traditionally, the
context words i are those words within a small
window of j and are trained with negative sam-
pling (Goldberg and Levy, 2014).
</p>
<p>3.1 Skip-Gram as Matrix Factorization
Levy and Goldberg (2014b), and subsequently
Keerthi et al. (2015), showed how vectors learned
under SG with the negative sampling are, under
certain conditions, the factorization of (shifted)
positive pointwise mutual information. Cotterell
et al. (2017) showed that SG is a form of ex-
ponential family PCA that factorizes the matrix
of word/context cooccurrence counts (rather than
shifted positive PMI values). With this interpre-
tation, they generalize SG from matrix to tensor
factorization, and provide a theoretical basis for
modeling higher-order SG (or additional context,
such as morphological features of words) within a
word embeddings framework.
</p>
<p>Specifically, Cotterell et al. recast higher-order
SG as maximizing the log-likelihood∑
</p>
<p>ijk
</p>
<p>Xijk log p(context i | word j, feature k) (1)
</p>
<p>=
∑
ijk
</p>
<p>Xijk log
exp (1ᵀ(ci �wj � ak))∑
i′ exp (1ᵀ(ci′ �wj � ak))
</p>
<p>, (2)
</p>
<p>where Xijk is a cooccurrence count 3-tensor of
words j, surrounding contexts i, and features k.
</p>
<p>3.2 Skip-Gram as n-Tensor Factorization
When factorizing an n-dimensional tensor to in-
clude an arbitrary number of L annotations, we
replace feature k in Equation (1) and ak in Equa-
tion (2) with each annotation type l and vector αl
included. Xi,j,k becomes Xi,j,l1,...lL , representing
the number of times word j appeared in context i
with features l1 through lL. We maximize∑
</p>
<p>i,j,l1,...,lL
</p>
<p>Xi,j,l1,...,lL log βi,j,l1,...,lL
</p>
<p>βi,j,l1,...,lL ∝ exp (1ᵀ(ci �wj �αl1 � · · · �αlL)) .
</p>
<p>4 Experiments
</p>
<p>Our end goal is to use multiple kinds of au-
tomatically obtained, “in-the-wild” frame se-
</p>
<p>98</p>
<p />
</div>
<div class="page"><p />
<p>mantic parses in order to improve the seman-
tic content—specifically SPR-type information—
within learned lexical embeddings. We utilize ma-
jority portions of the Concretely Annotated New
York Times and Wikipedia corpora from Ferraro
et al. (2014). These have been annotated with
three frame semantic parses: FrameNet from Das
et al. (2010), and both FrameNet and PropBank
from Wolfe et al. (2016). In total, we use nearly
five million frame-annotated documents.
Extracting Counts The baseline extraction we
consider is a standard sliding window: for each
word wj seen ≥ T times, extract all words wi two
to the left and right ofwj . These counts, forming a
matrix, are then used within standard word2vec.
We also follow Cotterell et al. (2017) and augment
the above with the signed number of tokens sepa-
rating wi and wj , e.g., recording that wi appeared
two to the left of wj ; these counts form a 3-tensor.
</p>
<p>To turn semantic parses into tensor counts, we
first identify relevant information from the parses.
We consider all parses that are triggered by the tar-
get word wj (seen ≥ T times) and that have at
least one role filled by some word in the sentence.
We organize the extraction around roles and what
fills them. We extract every word wr that fills all
possible triggered frames; each of those frame and
role labels; and the distance between filler wr and
trigger wj . This process yields a 9-tensor X.3 Al-
though we always treat the trigger as the “origi-
nal” word (e.g., word j, with vector wj), later we
consider (1) what to include from X, (2) what to
predict (what to treat as the “context” word i), and
(3) what to treat as auxiliary features.
Data Discussion The baseline extraction methods
result in roughly symmetric target and surround-
ing word counts. This is not the case for the frame
extraction. Our target words must trigger some
semantic parse, so our target words are actually
target triggers. However, the surrounding context
words are those words that fill semantic roles. As
shown in Table 1, there are an order-of-magnitude
fewer triggers than target words, but up to an
order-of-magnitude more surrounding words.
Implementation We generalize Levy and Gold-
berg (2014a)’s and Cotterell et al. (2017)’s code
</p>
<p>3 Each record consists of the trigger, a role filler, the num-
ber of words between the trigger and filler, and the relevant
frame and roles from the three semantic parsers. Being au-
tomatically obtained, the parses are overlapping and incom-
plete; to properly form X, one can implicitly include special
〈NO FRAME〉 and 〈NO ROLE〉 labels as needed.
</p>
<p>windowed frame
</p>
<p># target words
232 35.9 (triggers)
404 45.7 (triggers)
</p>
<p># surrounding 232 531 (role fillers)
words 404 2,305 (role fillers)
</p>
<p>Table 1: Vocabulary sizes, in thousands, extracted from Fer-
raro et al. (2014)’s data with both the standard sliding context
window approach (§3) and the frame-based approach (§4).
Upper numbers (Roman) are for newswire; lower numbers
(italics) are Wikipedia. For both corpora, 800 total FrameNet
frame types and 5100 PropBank frame types are extracted.
</p>
<p>to enable any arbitrary dimensional tensor fac-
torization, as described in §3.2. We learn 100-
dimensional embeddings for words that appear at
least 100 times from 15 negative samples.4 The
implementation is available at https://github.
com/fmof/tensor-factorization.
Metric We evaluate our learned (trigger) embed-
dings w via QVEC (Tsvetkov et al., 2015). QVEC
uses canonical correlation analysis to measure the
Pearson correlation between w and a collection
of oracle lexical vectors o. These oracle vectors
are derived from a human-annotated resource. For
QVEC, higher is better: a higher score indicates w
more closely correlates (positively) with o.
Evaluating Semantic Content with SPR Mo-
tivated by Dowty (1991)’s proto-role theory,
Reisinger et al. (2015), with a subsequent expan-
sion by White et al. (2016), annotated thousands
of predicate-argument pairs (v, a) with (boolean)
applicability and (ordinal) likelihoods of well-
motivated semantic properties applying to/being
true of a.5 These likelihood judgments, under
the SPR framework, are converted from a five-
point Likert scale to a 1–5 interval scale. Be-
cause the predicate-argument pairs were extracted
from previously annotated dependency trees, we
link each property with the dependency relation
joining v and a when forming the oracle vectors;
each component of an oracle vector ov is the unity-
normalized sum of likelihood judgments for joint
property and grammatical relation, using the inter-
val responses when the property is applicable and
discarding non-applicable properties, i.e. treating
the response as 0. Thus, the combined 20 prop-
erties of Reisinger et al. (2015) and White et al.
(2016)—together with the four basic grammatical
</p>
<p>4In preliminary experiments, this occurrence threshold
did not change the overall conclusions.
</p>
<p>5 We use the training portion of http:
//decomp.net/wp-content/uploads/2015/08/
UniversalDecompositionalSemantics.tar.gz.
</p>
<p>99</p>
<p />
</div>
<div class="page"><p />
<p>(a) Changes in SPR-QVEC for Annotated NYT. (b) Changes in SPR-QVEC for Wikipedia.
</p>
<p>Figure 2: Effect of frame-extracted tensor counts on our SPR-QVEC evaulation. Deltas are shown as relative percent changes
vs. the word2vec baseline. The dashed line represents the 3-tensor word2vec method of Cotterell et al. (2017). Each
row represents an ablation model: sep means the prediction relies on the token separation distance between the frame and
role filler, fn-frame means the prediction uses FrameNet frames, fn-role means the prediction uses FrameNet roles, and
filler means the prediction uses the tokens filling the frame role. Read from top to bottom, additional contextual features
are denoted with a +. Note when filler is used, we only predict PropBank roles.
</p>
<p>relations nsubj, dobj, iobj and nsubjpass—result
in 80-dimensional oracle vectors.6
</p>
<p>Predict Fillers or Roles? Since SPR judgments
are between predicates and arguments, we predict
the words filling the roles, and treat all other frame
information as auxiliary features. SPR annotations
were originally based off of (gold-standard) Prop-
Bank annotations, so we also train a model to pre-
dict PropBank frames and roles, thereby treating
role-filling text and all other frame information as
auxiliary features. In early experiments, we found
it beneficial to treat the FrameNet annotations ad-
ditively and not distinguish one system’s output
from another. Treating the annotations additively
serves as a type of collapsing operation. Although
X started as a 9-tensor, we only consider up to
6-tensors: trigger, role filler, token separation be-
tween the trigger and filler, PropBank frame and
role, FrameNet frame, and FrameNet role.
Results Fig. 2 shows the overall percent change
for SPR-QVEC from the filler and role predic-
tion models, on newswire (Fig. 2a) and Wikipedia
(Fig. 2b), across different ablation models. We
indicate additional contextual features being used
with a +: sep uses the token separation distance
between the frame and role filler, fn-frame
uses FrameNet frames, fn-role uses FrameNet
roles, filler uses the tokens filling the frame
</p>
<p>6 The full cooccurrence among the properties and rela-
tions is relatively sparse. Nearly two thirds of all non-zero
oracle components are comprised of just fourteen properties,
and only the nsubj and dobj relations.
</p>
<p>role, and none indicates no additional informa-
tion is used when predicting. The 0 line represents
a plain word2vec baseline and the dashed line
represents the 3-tensor baseline of Cotterell et al.
(2017). Both of these baselines are windowed:
they are restricted to a local context and cannot
take advantage of frames or any lexical signal that
can be derived from frames.
</p>
<p>Overall, we notice that we obtain large improve-
ments from models trained on lexical signals that
have been derived from frame output (sep and
none), even if the model itself does not incorpo-
rate any frame labels. The embeddings that predict
the role filling lexical items (the green triangles)
correlate higher with SPR oracles than the em-
beddings that predict PropBank frames and roles
(red circles). Examining Fig. 2a, we see that both
model types outperform both the word2vec and
Cotterell et al. (2017) baselines in nearly all model
configurations and ablations. We see the highest
improvement when predicting role fillers given the
frame trigger and the number of tokens separating
the two (the green triangles in the sep rows).
</p>
<p>Comparing Fig. 2a to Fig. 2b, we see newswire
is more amenable to predicting PropBank frames
and roles. We posit this is a type of out-of-
domain error, as the PropBank parser was trained
on newswire. We also find that newswire is over-
all more amenable to incorporating limited frame-
based features, particularly when predicting Prop-
Bank using lexical role fillers as part of the con-
</p>
<p>100</p>
<p />
</div>
<div class="page"><p />
<p>1 foresaw
2 figuring
</p>
<p>3 alleviated
4 craved
</p>
<p>5 jeopardized
</p>
<p>6 pondered
7 kidded
</p>
<p>8 constituted
9 uttering
</p>
<p>10 forgiven
</p>
<p>1 pioneered
2 scratch
</p>
<p>3 complemented
4 competed
5 consoled
</p>
<p>6 tolerated
7 resurrected
</p>
<p>8 sweated
9 fancies
</p>
<p>10 concocted
</p>
<p>1 containing
2 contains
</p>
<p>3 manufactures
4 contain
</p>
<p>5 consume
</p>
<p>6 storing
7 reproduce
</p>
<p>8 store
9 exhibiting
10 furnish
</p>
<p>1 anticipate
2 anticipating
3 anticipates
4 stabbing
5 separate
</p>
<p>6 intimidated
7 separating
8 separates
</p>
<p>9 drag
10 guarantee
</p>
<p>1 invent
2 document
</p>
<p>3 documented
4 invents
</p>
<p>5 documents
</p>
<p>6 aspire
7 documenting
</p>
<p>8 aspires
9 inventing
10 swinging
</p>
<p>1 produces
2 produce
3 produced
</p>
<p>4 prized
5 originates
</p>
<p>6 ridden
7 improves
8 surround
9 surrounds
</p>
<p>10 originating
</p>
<p>producing
</p>
<p>Filler | sep
</p>
<p>producing
</p>
<p>PropBank | sep
</p>
<p>invented
</p>
<p>Filler | sep
</p>
<p>invented
</p>
<p>PropBank | sep
</p>
<p>anticipated
</p>
<p>Filler | sep
</p>
<p>anticipated
</p>
<p>PropBank | sep
</p>
<p>Figure 3: K-Nearest Neighbors for three randomly sampled
trigger words, from two newswire models.
</p>
<p>textual features. We hypothesize this is due to
the significantly increased vocabulary size of the
Wikipedia role fillers (c.f., Tab. 1). Note, how-
ever, that by using all available schema informa-
tion when predicting PropBank, we are able to
compensate for the increased vocabulary.
</p>
<p>In Fig. 3 we display the ten nearest neighbors
for three randomly sampled trigger words accord-
ing to two of the highest performing newswire
models. They each condition on the trigger and the
role filler/trigger separation; these correspond to
the sep rows of Fig. 2a. The left column of Fig. 3
predicts the role filler, while the right column pre-
dicts PropBank annotations. We see that while
both models learn inflectional relations, this qual-
ity is prominent in the model that predicts Prop-
Bank information while the model predicting role
fillers learns more non-inflectional paraphrases.
</p>
<p>5 Related Work
</p>
<p>The recent popularity of word embeddings have
inspired others to consider leveraging linguistic
annotations and resources to learn embeddings.
Both Cotterell et al. (2017) and Levy and Gold-
berg (2014a) incorporate additional syntactic and
morphological information in their word embed-
dings. Rothe and Schütze (2015)’s use lexical re-
source entries, such as WordNet synsets, to im-
prove pre-computed word embeddings. Through
generalized CCA, Rastogi et al. (2015) incorpo-
rate paraphrased FrameNet training data. On the
applied side, Wang and Yang (2015) used frame
embeddings—produced by training word2vec
on tweet-derived semantic frame (names)—as ad-
ditional features in downstream prediction.
</p>
<p>Teichert et al. (2017) similarly explored the re-
lationship between semantic frames and thematic
proto-roles. They proposed using a Conditional
Random Field (Lafferty et al., 2001) to jointly
and conditionally model SPR and SRL. Teichert
et al. (2017) demonstrated slight improvements
in jointly and conditionally predicting PropBank
(Bonial et al., 2013)’s semantic role labels and
Reisinger et al. (2015)’s proto-role labels.
</p>
<p>6 Conclusion
</p>
<p>We presented a way to learn embeddings enriched
with multiple, automatically obtained frames from
large, disparate corpora. We also presented a
QVEC evaluation for semantic proto-roles. As
demonstrated by our experiments, our extension
of Cotterell et al. (2017)’s tensor factorization en-
riches word embeddings by including syntactic-
semantic information not often captured, result-
ing in consistently higher SPR-based correla-
tions. The implementation is available at https:
//github.com/fmof/tensor-factorization.
</p>
<p>Acknowledgments
This work was supported by Johns Hopkins Uni-
versity, the Human Language Technology Cen-
ter of Excellence (HLTCOE), DARPA DEFT, and
DARPA LORELEI. We would also like to thank
three anonymous reviewers for their feedback.
The views and conclusions contained in this pub-
lication are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
</p>
<p>References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
</p>
<p>1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
- Volume 1. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, ACL ’98, pages 86–90.
https://doi.org/10.3115/980845.980860.
</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2012. Abstract meaning representation
(amr) 1.0 specification. In Parsing on Freebase from
Question-Answer Pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing. Seattle: ACL. pages 1533–1544.
</p>
<p>Claire Bonial, Kevin Stowe, and Martha Palmer. 2013.
Renewing and revising semlink. In Proceedings of
</p>
<p>101</p>
<p />
</div>
<div class="page"><p />
<p>the 2nd Workshop on Linked Data in Linguistics
(LDL-2013): Representing and linking lexicons, ter-
minologies and other language data. Association for
Computational Linguistics, Pisa, Italy, pages 9 – 17.
http://www.aclweb.org/anthology/W13-5503.
</p>
<p>Greg N Carlson. 1984. Thematic roles and their role in
semantic interpretation. Linguistics 22(3):259–280.
</p>
<p>Ryan Cotterell, Adam Poliak, Benjamin Van Durme,
and Jason Eisner. 2017. Explaining and general-
izing skip-gram through exponential family princi-
pal component analysis. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics. Valencia, Spain.
</p>
<p>Maxwell John Cresswell. 1973. Logics and languages.
London: Methuen [Distributed in the U.S.A. By
Harper &amp; Row].
</p>
<p>Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Human language technologies: The
2010 annual conference of the North American
chapter of the association for computational lin-
guistics. Association for Computational Linguistics,
pages 948–956.
</p>
<p>Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian lda for topic models with word em-
beddings. In Proceedings of the 53rd Annual
Meeting of the Association for Computational
Linguistics and the 7th International Joint Con-
ference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 795–804.
http://www.aclweb.org/anthology/P15-1077.
</p>
<p>Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, University of Pittsburgh Press.
</p>
<p>Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. JOURNAL OF
THE AMERICAN SOCIETY FOR INFORMATION
SCIENCE 41(6):391–407.
</p>
<p>David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language 67(3):547–619.
</p>
<p>David R Dowty. 1989. On the semantic content of the
notion of thematic role. In Properties, types and
meaning, Springer, pages 69–129.
</p>
<p>Francis Ferraro, Max Thomas, Matthew R. Gormley,
Travis Wolfe, Craig Harman, and Benjamin Van
Durme. 2014. Concretely Annotated Corpora. In
4th Workshop on Automated Knowledge Base Con-
struction (AKBC).
</p>
<p>Charles Fillmore. 1982. Frame semantics. Linguistics
in the morning calm pages 111–137.
</p>
<p>Charles J Fillmore. 1976. Frame semantics and the na-
ture of language*. Annals of the New York Academy
of Sciences 280(1):20–32.
</p>
<p>Yoav Goldberg and Omer Levy. 2014. word2vec
explained: Deriving Mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722 .
</p>
<p>Zellig S Harris. 1954. Distributional structure. Word
10(2-3):146–162.
</p>
<p>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the human lan-
guage technology conference of the NAACL, Com-
panion Volume: Short Papers. Association for Com-
putational Linguistics, pages 57–60.
</p>
<p>Hans Kamp. 1979. Events, instants and temporal ref-
erence. In Semantics from different points of view,
Springer, pages 376–418.
</p>
<p>S. Sathiya Keerthi, Tobias Schnabel, and Rajiv Khanna.
2015. Towards a better understanding of predict and
count models. arXiv preprint arXiv:1511.0204 .
</p>
<p>Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 1746–
1751. http://www.aclweb.org/anthology/D14-1181.
</p>
<p>John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and label-
ing sequence data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA, ICML ’01, pages 282–289.
http://dl.acm.org/citation.cfm?id=645530.655813.
</p>
<p>Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers). Association for Computational Lin-
guistics, Baltimore, Maryland, pages 302–308.
http://www.aclweb.org/anthology/P14-2050.
</p>
<p>Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems.
pages 2177–2185.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Marvin Minsky. 1974. A framework for representing
knowledge. MIT-AI Laboratory Memo 306.
</p>
<p>Martha Palmer. 2009. Semlink: Linking propbank,
verbnet and framenet. In Proceedings of the Gen-
erative Lexicon Conference. GenLex-09, 2009 Pisa,
Italy, pages 9–15.
</p>
<p>102</p>
<p />
</div>
<div class="page"><p />
<p>Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational linguistics 31(1):71–
106.
</p>
<p>Miriam R. L. Petruck and Gerard de Melo, ed-
itors. 2014. Proceedings of Frame Seman-
tics in NLP: A Workshop in Honor of Chuck
Fillmore (1929-2014). Association for Com-
putational Linguistics, Baltimore, MD, USA.
http://www.aclweb.org/anthology/W14-30.
</p>
<p>Pushpendre Rastogi, Benjamin Van Durme, and Ra-
man Arora. 2015. Multiview LSA: Representation
Learning via Generalized CCA. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. Association for
Computational Linguistics, Denver, Colorado, pages
556–566. http://www.aclweb.org/anthology/N15-
1058.
</p>
<p>Drew Reisinger, Rachel Rudinger, Francis Ferraro,
Craig Harman, Kyle Rawlins, and Benjamin Van
Durme. 2015. Semantic proto-roles. Transactions
of the Association for Computational Linguistics
(TACL) 3:475–488.
</p>
<p>Sascha Rothe and Hinrich Schütze. 2015. Autoex-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1793–
1803. http://www.aclweb.org/anthology/P15-1173.
</p>
<p>Karin Kipper Schuler. 2005. Verbnet: A broad-
coverage, comprehensive verb lexicon .
</p>
<p>Adam Teichert, Adam Poliak, Benjamin Van Durme,
and Matthew Gormley. 2017. Semantic proto-role
labeling. In AAAI Conference on Artificial Intelli-
gence.
</p>
<p>Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evalua-
tion of word vector representations by subspace
alignment. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 2049–2054.
http://aclweb.org/anthology/D15-1243.
</p>
<p>Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research
37:141–188.
</p>
<p>William Yang Wang and Diyi Yang. 2015. That’s
so annoying!!!: A lexical and frame-semantic em-
bedding based data augmentation approach to au-
tomatic categorization of annoying behaviors us-
ing #petpeeve tweets. In Proceedings of the 2015
</p>
<p>Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 2557–2563.
http://aclweb.org/anthology/D15-1306.
</p>
<p>Aaron Steven White, Drew Reisinger, Keisuke Sak-
aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger,
Kyle Rawlins, and Benjamin Van Durme. 2016.
Universal decompositional semantics on univer-
sal dependencies. In Proceedings of the 2016
Conference on Empirical Methods in Natural
Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1713–1723.
https://aclweb.org/anthology/D16-1177.
</p>
<p>Alfred North Whitehead. 1920. The concept of na-
ture: the Tarner lectures delivered in Trinity College,
November 1919. Kessinger Publishing.
</p>
<p>Travis Wolfe, Mark Dredze, and Benjamin Van Durme.
2016. A study of imitation learning methods for se-
mantic role labeling. In Proceedings of the Work-
shop on Structured Prediction for NLP. Association
for Computational Linguistics, Austin, TX, pages
44–53. http://aclweb.org/anthology/W16-5905.
</p>
<p>103</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 104–109,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Distributed Prediction of Relations for Entities:
The Easy, The Difficult, and The Impossible
</p>
<p>Abhijeet Gupta* and Gemma Boleda† and Sebastian Padó*
*Stuttgart University, Germany
</p>
<p>{abhijeet.gupta,pado}@ims.uni-stuttgart.de
†Universitat Pompeu Fabra, Barcelona, Spain
</p>
<p>gemma.boleda@upf.edu
</p>
<p>Abstract
</p>
<p>Word embeddings are supposed to provide
easy access to semantic relations such as
“male of” (man–woman). While this claim
has been investigated for concepts, little
is known about the distributional behavior
of relations of (Named) Entities. We de-
scribe two word embedding-based models
that predict values for relational attributes
of entities, and analyse them. The task is
challenging, with major performance dif-
ferences between relations. Contrary to
many NLP tasks, high difficulty for a re-
lation does not result from low frequency,
but from (a) one-to-many mappings; and
(b) lack of context patterns expressing the
relation that are easy to pick up by word
embeddings.
</p>
<p>1 Introduction
</p>
<p>A central claim about distributed models of word
meaning (e.g., Mikolov et al. (2013)) is that word
embedding space provides easy access to semantic
relations. E.g., Mikolov et al.’s space was shown
to encode the “male-female relation” linearly, as a
vector ( #       »man− #              »woman = #       »king − #          »queen).
</p>
<p>The accessibility of semantic relations was sub-
sequently examined in more detail. Rei and Briscoe
(2014) and Melamud et al. (2014) reported success-
ful modeling of lexical relations such as hyper-
nymy and synonymy. Köper et al. (2015) consid-
ered a broader range of relationships,with mixed
results. Levy and Goldberg (2014b) developed an
improved, nonlinear relation extraction method.
</p>
<p>These studies were conducted primarily on
concepts and their semantic relations, like
hypernym(politician) = person. Mean-
while, entities and the relations they partake in are
</p>
<p>much less well understood.1 Entities are instances
of concepts, i.e., they refer to specific individual ob-
jects in the real world, for example, Donald Trump
is an instance of the concept politician. Conse-
quently, entities are generally associated with a
rich set of numeric and relational attributes (for
politician instances: size, office, etc.). In
contrast to concepts, the values of these attributes
tend to be discrete (Herbelot, 2015): while the
size of politician is best described by a prob-
ability distribution, the size of Donald Trump
is 1.88m. Since distributional representations
are notoriously bad at handling discrete knowl-
edge (Fodor and Lepore, 1999; Smolensky, 1990),
this raises the question of how well such models
can capture entity-related knowledge.
</p>
<p>In our previous work (Gupta et al., 2015), we
analysed distributional prediction of numeric at-
tributes of entities, found a large variance in quality
among attributes, and identified factors determin-
ing prediction difficulty. A corresponding analysis
for relational (categorial) attributes of entities is
still missing, even though entities are highly rele-
vant for NLP. This is evident from the highly ac-
tive area of knowledge base completion (KBC), the
task of extending incomplete entity information in
knowledge bases such as Yago or Wikidata (e.g.,
Bordes et al., 2013; Freitas et al., 2014; Neelakan-
tan and Chang, 2015; Guu et al., 2015; Krishna-
murthy and Mitchell, 2015).
</p>
<p>In this paper, we assess to what extent re-
lational attributes of entities are easily acces-
sible from word embedding space. To this
end, we define two models that predict, given
a target entity (Star Wars) and a relation
(director), a distributed representation for the
relatum (George Lucas). We carry out a detailed
per-relation analyses of their performance on seven
</p>
<p>1The original dataset by Mikolov et al. (2013) did contain
a small number of entity-entity relations.
</p>
<p>104</p>
<p />
</div>
<div class="page"><p />
<p>Target Vector
</p>
<p>Hidden
</p>
<p>Relatum Vector
</p>
<p>Relation
</p>
<p>n-dimensional word 
embedding input
</p>
<p>m-dimensional
one-hot vector input
</p>
<p>n-dimensional word 
embedding prediction
</p>
<p>k-dimensional vector
</p>
<p>Relatum 
Name NN
</p>
<p>Win Wr
</p>
<p>Wout
</p>
<p>Figure 1: Nonlinear model (NonLinM) structure
</p>
<p>major FreeBase domains and identify what makes
a relation difficult by correlating performance with
properties of the relations. We find that, contrary
to many other NLP tasks, relations are not difficult
if they are infrequent or sparse, but instead (a) if
they relate one target to multiple relata; (b) if they
do not give rise to linguistic patterns that can be
picked up by bag-of-words distributional models.
</p>
<p>2 Two Relatum Prediction Models
</p>
<p>Both models predict a vector for a relatum r (plural:
relata) given a target entity vector t and a symbolic
relation ρ.
</p>
<p>The Linear Model (LinM) is inspired by
Mikolov et al.’s “phrase analogy” evaluation of
word embeddings ( #       »man − #              »woman = #       »king −
#          »queen). However, instead of looking at individ-
ual words, we extract representations of semantic
relations from sets of pairs Tρ = {(ti, ρ, ri)} in-
stantiating the relation ρ. For each relation ρ, LinM
computes the average (or centroid) difference vec-
tor over the set of training pairs:
</p>
<p>r̂(t, ρ) = t+
∑
</p>
<p>(r,ρ,t)∈Tρ
(r − t)/N (1)
</p>
<p>That is, the predicted r̂ for an input (t, ρ) is the
sum of the target vector and the relation’s proto-
type. This model should work well if relations are
represented additively in the embedding space.
</p>
<p>The Nonlinear Model (NonLinM) is a feed-
forward network (Figure 1) introducing a nonlin-
earity, inspired by Levy and Goldberg (2014b) and
similar to models used in KBC, e.g., Socher et al.
(2013). The relatum vector is predicted as
</p>
<p>r̂θ(t, ρ) = σ(σ(t ·Win + vρ ·Wr) ·Wout) (2)
</p>
<p>where vρ is the relation encoded as an m-
dimensional one-hot vector and the three matrices
</p>
<p>Win,Wr,Wout form the model parameters θ. For
the nonlinearity σ, we use tanh.
</p>
<p>In this model, the hidden layer represents a non-
linearly transformed composition of target and re-
lation from which the relatum can be predicted.
NonLinM can theoretically make accurate predic-
tions even if relations are not additive in embedding
space. Also, its sharing of training data among re-
lations should lead to more reliable learning for
infrequent relations. As objective function, we use
</p>
<p>L(θ) =
∑
(t,r)
</p>
<p>[ cos(r̂θ(t, ρ), r)
</p>
<p>− α · cos(r̂θ(t, ρ), nc(r̂θ(t, ρ)))]
(3)
</p>
<p>where nc(v) is the nearest confounder of v, i.e., the
next neighbor of v that is not a relatum for the cur-
rent target-relation pair. Thus, we minimize the co-
sine distance between the predicted vector and the
gold vector for the relatum while maximizing the
cosine distance of the prediction to the closest nega-
tive example. We introduce a weight α ∈ [0, 1] for
the negative sampling term as a hyper-parameter
optimized on the development set. During train-
ing, we apply gradient descent with the adaptive
learning rate method AdaDelta (Zeiler, 2012).
</p>
<p>3 Experiments
</p>
<p>Data. We extract relation data from FreeBase.
We follow our earlier work Gupta et al. (2015), but
go beyond its limitation to two domains (country,
citytown). We experiment with seven major
FreeBase domains: animal, book, citytown,
country, employer, organization, people.
We limit the number of datapoints of very large
relation types to 3000 with random sampling for
efficiency reasons. We only remove relation types
with fewer than 3 datapoints. This results in a quite
challenging dataset that demonstrates the general-
izability of our models and is roughly comparable,
in variety and size, to the FB15K dataset (Bordes
et al., 2013).
</p>
<p>The distributed representations for all entities
come from the 1000-dimensional “Google News”
skip-gram model (Mikolov et al., 2013) for Free-
Base entities2 trained on a 100G token news corpus.
We only retain relation datapoints where both tar-
get and relatum are covered in the Google News
vectors. Table 1 shows the numbers of relations
and unique objects (target plus relata).
</p>
<p>2https://code.google.com/p/word2vec/
</p>
<p>105</p>
<p />
</div>
<div class="page"><p />
<p>Size Performance Relation-level statistics
</p>
<p>Domain #R #Ts+Ra. BL LM NLM %R&gt;0.3 %R&lt;0.1 ρ(NLM, #In) ρ(NLM, #RpT)
</p>
<p>animal 24 3,428 0.11 0.16 0.29 38% 42% .07 -.34
book 22 7,014 0.11 0.24 0.26 9% 68% .09 -.15
citytown 46 86,551 0.05 0.13 0.26 28% 39% -.12 -.22
country 89 191,196 0.04 0.08 0.18 20% 52% -.32 -.23
employer 76 14,658 0.05 0.15 0.23 30% 45% .01 -.35
organization 53 8,989 0.07 0.17 0.26 34% 42% -.24 -.29
people 91 11,397 0.09 0.19 0.27 34% 23% .23 -.25
</p>
<p>Micro average 0.06 0.14 0.22 25% 45% -.12 -.25
Macro average 0.08 0.16 0.23 28% 44% -.05 -.26
</p>
<p>Table 1: Test set statistics and results. #R: relations; #Ts+Ra: unique targets and relata; BL/LM/NLM:
Baseline, linear and nonlinear model (macro-average MRR); %R≶x: percent of relations with MRR ≶x;
ρ: Spearman correlation; #In: instances; #RpT: relata per target
</p>
<p>We split all domains into training, validation,
and test sets (60%–20%–20%). The split applies to
each relation type: in test, we face no unseen rela-
tion types, but unseen datapoints for each relation.3
</p>
<p>Hyperparameter settings. The NonLinM
model uses an L2 norm constraint of s=3. We
adopt the best AdaDelta parameters from Zeiler
(2012), viz. ρ = 0.95 and � = 10−6. We optimize
the negative sampling weight α (cf. Eq. 3) by
line search with a step size of 0.1 on the largest
domain, country, and find 0.6 to be the optimal
value, which we reuse for all domains. Due to the
varying dimensionality m of the relation vector
per domain, we set the size of the hidden layer
to k = 2n + m/10 (n is the dimensionality of
the word embeddings, cf. Figure 1). We train all
models for a maximum of 1000 epochs with early
stopping.
</p>
<p>Evaluation. Models that predict vectors in a con-
tinuous vector space, like ours, cannot expect to
predict the output vector precisely. Thus, we ap-
ply nearest neighbor mapping using the set of all
unique targets and relata in each domain (cf. Ta-
ble 1) to identify the correct relatum name. We
then perform an Information Retrieval-style rank-
ing evaluation: We compute the rank of the correct
relatum r, given the target t and the relation ρ, in
the test set T and aggregate these ranks to compute
the mean reciprocal rank (MRR):
</p>
<p>MRR =
1
||T ||
</p>
<p>∑
(t,ρ,r)∈T
</p>
<p>1
rankt,ρ(r)
</p>
<p>(4)
</p>
<p>where rank is the nearest neighbor rank of the
relatum vector r given the prediction of the model
</p>
<p>3The dataset are available at: http://www.ims.uni-
stuttgart.de/data/RelationPrediction.html
</p>
<p>for the input t, ρ. We report results at the relation
level as well as macro- and micro-averaged MRR
for the complete dataset.
</p>
<p>Frequency Baseline (BL). Our baseline model
ignores the target. For each relation, it predicts the
frequency-ordered list of all training set relata.
</p>
<p>4 Results and Discussion
</p>
<p>Overall results. Table 1 shows that the nonlinear
model NonLinM consistently gives the best results
and statistically outperforms the linear model on
all domains according to a Wilcoxon test (α=0.05).
Both LinM and NonLinM clearly outclass the base-
line. Most MRRs are around 0.25 (micro average
0.22), with one outlier, at 0.18, for country, the
largest domain. Overall, the numbers may appear
disappointing at first glance: these MRRs mean that
the correct relatum is typically around the fourth
nearest neighbor of the prediction vector. This in-
dicates that open-vocabulary relatum prediction in
a space of tens of thousands of words is a chal-
lenging task that warrants more detailed analysis.
We observe that the nonlinear model achieves rea-
sonable results even for sparse domains (cf. the
low baseline), which we take as evidence for its
generalization capabilities.
</p>
<p>Analysis at relation level. Table 1 shows the
number of relations with good MRRs (greater than
0.3) and bad MRRs (smaller than 0.1) for each rela-
tion. While the numbers vary across domains, the
models tend to do badly on around 40-50% of all
relations, and obtain good scores for less than one
third of all relations.
</p>
<p>Figure 2 shows the distribution for the best do-
main (animal) and the worst one (country) .
Both plots show a Zipfian distribution with a rel-
</p>
<p>106</p>
<p />
</div>
<div class="page"><p />
<p>5 10 15 20
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>1.
0
</p>
<p>relation index (sorted by NonLinM performance)
</p>
<p>M
R
</p>
<p>R
 fo
</p>
<p>r a
ni
</p>
<p>m
al
</p>
<p>● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>● ● ● ●
●
</p>
<p>● ●
</p>
<p>●
●
</p>
<p>● ●
</p>
<p>● ●
●
</p>
<p>● ● ● ● ●
</p>
<p>●
</p>
<p>NonLinM
LinM
</p>
<p>0 20 40 60 80
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>1.
0
</p>
<p>relation index (sorted by NonLinM performance)
</p>
<p>M
R
</p>
<p>R
 fo
</p>
<p>r c
ou
</p>
<p>nt
ry
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
●
</p>
<p>●●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●●●
</p>
<p>●
●
●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
●●
</p>
<p>●●
●●●
</p>
<p>●●
●●
</p>
<p>●●●
●
●●●●●●
</p>
<p>●
●●●●●●●●
</p>
<p>●
●●●●●●●●●●●●●●●
</p>
<p>●
</p>
<p>NonLinM
LinM
</p>
<p>Figure 2: Results by relation for best and worst
domains ( animal, above; people, below), sorted
by NonLinM performance
</p>
<p>Target Correct LinM NonLinM
</p>
<p>Japan Asia Japan Asia
Kazakhstan Asia Central Asia Asia
</p>
<p>c
o
n
t
i
n
e
n
t
</p>
<p>Nicaragua North Latin Americas
America America
</p>
<p>Nepal Kathmandu Nepal Dhaka
Qatar Doha Qatar Riyadh
</p>
<p>c
a
p
i
t
a
l
</p>
<p>Venezuela Caracas Caracas Quito
</p>
<p>Table 2: Example predictions for two country
relations (correct answer in boldface)
</p>
<p>atively small set of well-modelled relations and a
long tail of poorly modelled ones. NonLinM does
better or as well as LinM for almost all relations.
The performances of the two models are very tighly
correlated for difficult relations; they only differ
for the easier ones, where NonLinM’s evidently
captures the data better.
</p>
<p>Qualitatively, the two models differ substantially
with regard to prediction patterns at the level of
targets. Table 2 shows the first predictions for three
targets from two relations: continent, where
NonLinM outperforms LinM, and capital, where
it is the other way around. NonLinM’s errors con-
sist almost exclusively in predicting semantically
similar entities of the correct relatum type, e.g.,
predicting Quito (the capital of Ecuador) as capi-
tal of Venezuela. In contrast, the LinM model has
a harder time capturing the correct type, predict-
ing country entities as capitals (e.g., Nepal as the
capital of Nepal).
</p>
<p>●
</p>
<p>●
● ●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>1 2 5 10 20 50 100 200
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>Avg. # of relata per target
</p>
<p>M
R
</p>
<p>R
 o
</p>
<p>f N
on
</p>
<p>Li
nM
</p>
<p> o
n 
</p>
<p>an
im
</p>
<p>al
</p>
<p>●
</p>
<p>●
●●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
●●●●
</p>
<p>●
</p>
<p>●●
●
</p>
<p>●
● ●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
●
</p>
<p>● ●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●●
●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●
</p>
<p>●●
</p>
<p>●
</p>
<p>●
</p>
<p>●
●
</p>
<p>●
</p>
<p>1 2 5 10 20 50 100 200 500
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>Avg. # of relata per target
</p>
<p>M
R
</p>
<p>R
 o
</p>
<p>f N
on
</p>
<p>Li
nM
</p>
<p> o
n 
</p>
<p>co
un
</p>
<p>try
</p>
<p>Figure 3: Scatterplot: MRR vs. number of relata
per target (above: animal, below: country)
</p>
<p>Analysis of Difficulty. So what makes many
FreeBase relations hard to model? To test for spar-
sity problems, we first computed the correlation
between model performance and the “usual sus-
pect” relation frequency (number of instances for
each relation). In NLP applications, this typically
yields a high positive correlation. The second-to-
last column of Table 1 shows that this is not true
for our dataset. We find a substantial positive cor-
relation only for people, correlations around zero
for most domains, and substantial negative ones for
organization and country. For these domains,
therefore, frequent relations are actually harder to
model. Further analysis revealed two main sources
of difficulty:
</p>
<p>(1) One-to-many relations. Relations with
many datapoints tend to be one-to-many. We
assume this to be a major source of difficulty,
since the model is presented with multiple re-
lata for the same target during training and will
typically learn to predict a centroid of these re-
lata. As an extreme case, consider a relation like
administrative divisions that relates the US
to all of its federal states: the resulting prediction
will arguably be dissimilar to every individual state.
To test this hypothesis, we computed the rank cor-
relation at the relation level between the number of
relata per target and NonLinM performance, shown
in the last column of Table 1. Indeed, we find a
strong negative correlation for every single domain.
In addition, Figure 3 plots relation performance (y
axis) against the ratio of relata per target (x axis:
one-to-one on the left, one-to-many on the right)
for animal and country.
</p>
<p>107</p>
<p />
</div>
<div class="page"><p />
<p>Qualitatively, Table 3 shows examples for the
three most easy and difficult relations for country.
The list suggests that relations tend to be easy when
they associate targets with single relata: the rela-
tion country maps territories and colonies onto
their motherlands, and the tournaments relation
is only populated with a few Commonwealth games
(cf. the high baseline). In contrast, relations that
map targets on many relata are difficult, such as
administrative divisions of countries, or a list of
disputed territories. Note that this is not an
evaluation issue, since MRR can deal with multiple
correct answers. Our models do badly because they
lack strategies to address these cases.
</p>
<p>(2) Lack of contextual support. One-to-many
relations are not the only culprit. Strikingly, Fig-
ure 2 shows that a low target-relatum ratio is a
necessary condition for good performance (the
upper right corners are empty), but not a suffi-
cient one (the lower left corners are not empty).
Some relations are not modelled well even though
they are (almost) one-to-one. Examples include
currency formerly used or named after for
country and place of origin for animal.
Further analysis indicated that these relations suf-
fer from what Gupta et al. (2015) called lack of
contextual support: Although they are expressed
overtly in the linguistic context of the target and
relatum (and often even frequently so), their real-
izations cannot be tied to individual words or topics.
Instead, they are expressed by relatively specific
linguistic patterns, often predicate-argument struc-
tures (X used to pay with Y, X is named in the honor
of Y). Such structures are hard to pick up by word
embedding models that make the bag-of-words in-
dependence assumption among context words.
</p>
<p>5 Conclusion
</p>
<p>This paper considers the prediction of related enti-
ties (“relata”) given a pair of a target Named Entity
and a relation (Star Wars, director, ?) on
the basis of distributional information. This task is
challenging due to the more discrete behavior of
attributes of entities as compared to concepts. We
provide an analysis based on two models that use
vector representations for both the targets and the
relata.
</p>
<p>Our results yield new insights into how embed-
ding spaces represent entity relations: they are gen-
erally not represented additively, and nonlinearity
helps. They also complement insights on the be-
</p>
<p>Relation BL LinM NonLinM
</p>
<p>tournaments 0.88 0.82 0.88
continent 0.29 0.29 0.77
country 0.25 0.24 0.77
...
disputed territories 0.00 0.01 0.01
horses from here 0.00 0.01 0.01
2nd level divisions 0.00 0.00 0.01
</p>
<p>Table 3: The three most easy and most difficult
relations for the country domain
</p>
<p>havior of numeric attributes of entities (Gupta et al.,
2015): Relations, like numeric attributes, are diffi-
cult to model if they are not specifically expressed
in the lingusitic context of target and relatum. A
new challenge specific to relations are situations
where a single target maps onto many relata. If
none of the two problems applies, relations are
easy to model. If one applies, they are difficult.
And if both apply, they are essentially impossible.
</p>
<p>Among the two challenges, the problem of one-
to-many relations appears easier to address, since
a continuous output vector is, at least in principle,
able to be similar to many relata. In the future,
we will extend the model to deal better with one-
to-many relations. While the lack of contextual
support seems more fundamental, it could be ad-
dressed by either using syntax-based embeddings
(Levy and Goldberg, 2014a) that can better pick up
the specific context patterns characteristic for these
relations, or by optimizing the input word embed-
dings for the task. This becomes a similar problem
to joint training of representations from knowledge
base structure and textual evidence (Perozzi et al.,
2014; Toutanova et al., 2015).
</p>
<p>Acknowledgements
</p>
<p>This project has received funding from the Euro-
pean Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation pro-
gramme (grant agreement No 715154) and the DFG
(SFB 732, Project D10). This paper reflects the au-
thors’ view only, and the EU is not responsible for
any use that may be made of the information it
contains.
</p>
<p>References
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
</p>
<p>Duran, Jason Weston, and Oksana Yakhnenko.
</p>
<p>108</p>
<p />
</div>
<div class="page"><p />
<p>2013. Translating embeddings for modeling multi-
relational data. In Proceedings of Neural Informa-
tion Processing Systems 26. pages 2787–2795.
</p>
<p>Jerry Fodor and Ernie Lepore. 1999. All at Sea in Se-
mantic Space: Churchland on Meaning Similarity.
Journal of Philosophy 96(8):381–403.
</p>
<p>André Freitas, Joao Carlos Pereira da Silva, Ed-
ward Curry, and Paul Buitelaar. 2014. A distribu-
tional semantics approach for selective reasoning
on commonsense graph knowledge bases. In Nat-
ural Language Processing and Information Systems,
Springer, pages 21–32.
</p>
<p>Abhijeet Gupta, Gemma Boleda, Marco Baroni, and
Sebastian Padó. 2015. Distributional vectors encode
referential attributes. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. Lisbon, Portugal, pages 12–21.
</p>
<p>Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Lisbon,
Portugal, pages 318–327.
</p>
<p>Aurélie Herbelot. 2015. Mr Darcy and Mr Toad, gen-
tlemen: Distributional names and their kinds. Pro-
ceedings of the 11th International Conference on
Computational Semantics pages 151–161.
</p>
<p>Maximilian Köper, Christian Scheible, and Sabine
Schulte im Walde. 2015. Multilingual Reliabil-
ity and ”Semantic” Structure of Continuous Word
Spaces. In Proceedings of the 11th Conference on
Computational Semantics. London, UK, pages 40–
45.
</p>
<p>Jayant Krishnamurthy and Tom M Mitchell. 2015.
Learning a compositional semantics for freebase
with an open predicate vocabulary. Transactions
of the Association for Computational Linguistics
3:257–270.
</p>
<p>Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics. Baltimore, Maryland, pages
302–308.
</p>
<p>Omer Levy and Yoav Goldberg. 2014b. Linguistic
regularities in sparse and explicit word representa-
tions. In Proceedings of the Eighteenth Conference
on Computational Natural Language Learning. Ann
Arbor, Michigan, pages 171–180.
</p>
<p>Oren Melamud, Ido Dagan, Jacob Goldberger, Idan
Szpektor, and Deniz Yuret. 2014. Probabilistic mod-
eling of joint-context in distributional similarity. In
Proceedings of the Eighteenth Conference on Com-
putational Natural Language Learning. Ann Arbor,
Michigan, pages 181–190.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems. Lake Tahoe, NV, pages 3111–3119.
</p>
<p>Arvind Neelakantan and Ming-Wei Chang. 2015. In-
ferring missing entity type instances for knowledge
base completion: New dataset and methods. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics. Denver, CO,
pages 515–525.
</p>
<p>Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: Online learning of social represen-
tations. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery
and data mining. ACM, New York City, NY, pages
701–710.
</p>
<p>Marek Rei and Ted Briscoe. 2014. Looking for hy-
ponyms in vector space. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning. Ann Arbor, Michigan, pages 68–
77.
</p>
<p>Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence 46(1-
2):159–216.
</p>
<p>Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Advances in Neural Information Processing Systems.
Lake Tahoe, CA, pages 926–934.
</p>
<p>Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi
Choudhury, and Michael Gamon. 2015. Represent-
ing text for joint embedding of text and knowledge
bases. In Proceedings of EMNLP. Lisbon, Portugal,
pages 1499–1509.
</p>
<p>Matthew D. Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method. In CoRR, abs/1212.5701.
</p>
<p>109</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 110–114,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Comparing Approaches for Automatic Question Identification
</p>
<p>Angel Samsuddin Maredia, Kara Schechtman, Sarah Ita Levitan, Julia Hirschberg
</p>
<p>Department of Computer Science, Columbia University, USA
</p>
<p>asm2221@columbia.edu, kws2121@columbia.edu,
</p>
<p>sarahita@cs.columbia.edu, julia@cs.columbia.edu
</p>
<p>Abstract
</p>
<p>Collecting spontaneous speech corpora
that are open-ended, yet topically con-
strained, is increasingly popular for re-
search in spoken dialogue systems and
speaker state, inter alia. Typically, these
corpora are labeled by human annota-
tors, either in the lab or through crowd-
sourcing; however, this is cumbersome
and time-consuming for large corpora. We
present four different approaches to auto-
matically tagging a corpus when general
topics of the conversations are known. We
develop these approaches on the Columbia
X-Cultural Deception corpus and find ac-
curacy that significantly exceeds the base-
line. Finally, we conduct a cross-corpus
evaluation by testing the best performing
approach on the Columbia/SRI/Colorado
corpus.
</p>
<p>1 Introduction
</p>
<p>Corpora of spontaneous speech are often col-
lected through interviews or by otherwise provid-
ing subjects with question prompts. Such cor-
pora are semi-structured; they are constrained by
the prompts used, but the elicited speech is open-
ended in vocabulary and structure. It is often de-
sirable to segment these corpora into their under-
lying topics based on the questions asked. This is
typically done manually by annotators in the lab or
via crowd-sourcing. However, such annotation is
impractical and time-consuming for large corpora.
</p>
<p>In this paper we describe a set of experi-
ments aimed at automatically tagging a large cor-
pus with topic labels. We tag the Columbia X-
Cultural Deception (CXD) corpus, a large-scale
(120-hour) corpus of deceptive and non-deceptive
dialogues collected using a semi-structured inter-
</p>
<p>view paradigm. Participants took turns interview-
ing each other using a fixed set of biographical in-
terview questions1, but the questions were asked
in individual variants, in any order, and interview-
ers often asked follow-up questions. For example,
the question, ”Are your parents divorced?” could
be produced as ”Are your mom and dad still to-
gether?” These questions are semantically simi-
lar, but differ lexically, presenting the challenge
of topically tagging a corpus based on semantic
similarity. The question, ”Have you ever broken
a bone?” could be followed by another, ”How did
you break your bone?” This illustrates the chal-
lenge of distinguishing between phrases that are
lexically similar, but differ semantically. These
two examples highlight problems faced when try-
ing to automatically annotate a corpus for re-
sponses to a given set of questions.
</p>
<p>With such a large corpus, it is not practical to
manually annotate topic boundaries. So, to com-
pare question responses from multiple subjects,
we identify conversational turns in the corpus that
correspond to the original interview questions. We
compare four approaches to question identifica-
tion: (1) a baseline approach that identifies ques-
tions using strict string matches, (2) the ROUGE
metric which is based on n-gram comparisons, (3)
cosine similarity between word embedding repre-
sentations and (4) cosine similarity between doc-
ument embeddings. We include experiments with
varying thresholds for approaches (2), (3), and (4)
to highlight the trade-off between precision and
recall for these approaches. Finally, we test our
best approach using word embeddings on another
corpus, the Columbia/SRI/Colorado (CSC) corpus
(Hirschberg et al., 2005), collected with a similar
interview paradigm but different questions, in or-
der to evaluate the utility of this method in another
</p>
<p>1The interview questions can be found here: http://
tinyurl.com/lzfa8zl
</p>
<p>110</p>
<p />
</div>
<div class="page"><p />
<p>domain.
This work draws upon the body of research
</p>
<p>on short-text semantic similarity (e.g. (Mihalcea
et al., 2006; Kenter and de Rijke, 2015; Oliva et al.,
2011)). It is also related to work on topic seg-
mentation (e.g. (Cardoso et al., 2013; Dias et al.,
2007) ), however here we focus on matching con-
versational turns to a fixed set of possible topics.
While this work is done in support of our ongo-
ing work on deception detection using speech and
text-based features, we believe that our approach
could be applied to other spontaneous transcribed
speech or text corpora which were collected with
some constraints on topics.
</p>
<p>2 Corpus
</p>
<p>The Columbia X-Cultural Deception (CXD) Cor-
pus (Levitan et al., 2015) is a collection of within-
subject deceptive and non-deceptive speech from
native speakers of Standard American English
(SAE) and Mandarin Chinese (MC), all speak-
ing in English. The corpus contains dialogues
between 340 subjects. A variation of a fake re-
sume paradigm was used to collect the data. Previ-
ously unacquainted pairs of subjects played a ”ly-
ing game” with each other. Each subject filled out
a 24-item biographical questionnaire and were in-
structed to create false answers for a random half
of the questions. The lying game was recorded
in a sound booth. For the first half of the game,
one subject assumed the role of the interviewer,
while the other answered the biographical ques-
tions, lying for half and telling the truth for the
other; questions chosen in each category were bal-
anced across the corpus. For the second half of
the game, the subjects roles were reversed, and
the interviewer became the interviewee. During
the game, the interviewer was allowed to ask the
24 questions in any order s/he chose; the inter-
viewer was also encouraged to ask follow-up ques-
tions to aid them in determining the truth of the
interviewees answers. The entire corpus was or-
thographically transcribed using the Amazon Me-
chanical Turk (AMT)2 crowd-sourcing platform,
and transcripts were forced-aligned with the au-
dio recordings. The speech was then automatically
segmented into inter-pausal units (IPUs), defined
as pause-free segments of speech separated by a
minimum pause length of 50 ms. The speech was
also segmented into turn units, where a turn is de-
</p>
<p>2https://www.mturk.com/mturk/
</p>
<p>fined as a maximal sequence of IPUs from a single
speaker without any interlocutor speech that is not
a backchannel (a simple acknowledgment that is
not an attempt to take the turn). For this work, we
compiled 40 interviewer sessions (about 20% of
the corpus) and hand-annotated the turns for all of
these sessions, giving us a total of 5308 turns. Out
of these turns, 923 were interviewer questions that
corresponded to the list of the original biograph-
ical questions, which we labeled with the ques-
tion number. Below we describe the different ap-
proaches and then discuss results in Section 4 with
a comparison of performance in Table 1.
</p>
<p>3 Question Identification Approaches
</p>
<p>3.1 String-matching Baseline
</p>
<p>As a baseline for matching the 24 questions in-
terviewers were instructed to ask with interviewer
turns, we performed a simple two-pass question
matching procedure for exact string matches be-
tween written questions and the transcripts. In the
first pass, we searched for exact matches of strings
with punctuation and spacing removed. With
the remaining unmatched questions, we then per-
formed another round of matching, with the tran-
script lemmatized and with filler words removed,
to identify very close though not exact matches.
</p>
<p>3.2 ROUGE
</p>
<p>ROUGE (Recall-Oriented Understudy for Gist-
ing Evaluation)(Lin, 2004) is a package designed
to evaluate computer-generated summaries against
a human-written baseline using a simple n-gram
comparison to find precision, recall, and f-score
for each machine-human summary comparison.
Using ROUGE, we evaluated matches for ques-
tions which had not been detected by the baseline.
We created a ROUGE task for each unmatched
question. For each task, the original question
was used as the reference text. We then tested
each interviewer turn in the conversation against
the reference, using bi-gram matching. We thus
matched the turn receiving the highest similarity
score to the reference text to that question, testing
this method at a variety of similarity thresholds.
</p>
<p>3.3 Word Embeddings
</p>
<p>The previous two methods identify questions us-
ing lexical similarity. In the next two approaches
we explored semantic similarity. We began by
obtaining a vector representation for each of the
</p>
<p>111</p>
<p />
</div>
<div class="page"><p />
<p>24 questions. We use a pre-trained Word2vec
model on the Google News dataset3 with over
three million words and phrases to obtain word
embeddings. The primary benefit of a Word2vec
model is that it clusters semantically similar words
and phrases together: for example, ”Golden Gate
Bridge” and ”San Francisco” have very low cosine
distance between each other in this model. There-
fore, semantically similar words were likely to be
represented as vectors with high cosine similarity.
</p>
<p>To obtain a vector representation for each ques-
tion as a whole, we found the vector representa-
tion for each word using Word2vec. We then took
a weighted average of all of the word vectors in
the question where words that directly contributed
to the topic of the turn such as ”relationship” or
”mom” were weighed more than words that, if re-
moved, did not affect the topic of the turn such
as ”have” or ”really.” This produced a final vector
representation of the entire question. We exclude
stop words from this vector average. Following
the same approach, we obtained vector represen-
tations for each interviewer turn. We then cal-
culated the cosine similarities between a turn and
each question and found the question that had the
highest cosine similarity to the turn vector. We
compared the cosine similarity of the turn and the
question to the cosine similarity of any previous
identified matches. If the newly calculated cosine
similarity was higher, then the current turn was
deemed the best match so far to the question, oth-
erwise we repeated this comparison with the ques-
tion that had the second highest cosine similarity
to the turn. At the end of each particular inter-
viewer session, we had a mapping of each turn to
a question if a match was detected, otherwise the
turn was marked as not being a question.
</p>
<p>3.4 Document Embeddings
</p>
<p>We also explored the use of document embeddings
for this task. We began by finding a vector repre-
sentation for each of the 24 questions. We used
a Doc2vec model pre-trained on Wikipedia text4.
Recall that, in our paradigm, questions could be
asked in individual variants, in any order, and
along with follow-up questions. The primary ben-
efit of a Doc2Vec model is that it allows for unsu-
pervised learning of larger blocks of text. There-
</p>
<p>3The model can be found here: https://code.
google.com/archive/p/word2vec/
</p>
<p>4The model can be found here: https://github.
com/jhlau/doc2vec
</p>
<p>fore, we hypothesized that Doc2Vec would return
word vectors that also depended on contextual us-
age as well as semantic similarity. We then cal-
culated the vector averages for each turn and pro-
duced turn-to-question mappings as explained in
the word embeddings approach above.
</p>
<p>4 Results
</p>
<p>Table 1 shows the accuracy, precision, recall, and
f1-score of each of the four approaches outlined
above, evaluated on our hand-labeled subset of
interviews. We see that the word embeddings
method achieved the highest accuracy, recall, and
f1-score of all the methods developed and tested,
whereas the ROUGE approach obtained the high-
est precision. With the word embeddings ap-
proach, most correctly identified turns share one
or more meaningful words with the corresponding
original question and are often syntactically very
similar. This approach, however, is able to make
ambiguous matches as well. For example, an inter-
viewer turn said, ”wow you broke you broke your
hand when you were in elementary school wow i
yeah i get so student hate to do homework so have
you ever tweet tweeted.” This turn shares mean-
ingful words with many other questions, but this
approach correctly identified it as matching the
question Have you ever tweeted? The word em-
beddings approach could also make difficult se-
mantic matches. Many interviewers asked, ”Are
your mom and dad still together?” instead of ”Are
your parents divorced?” Even though there are
few lexically common meaningful words between
these two phrases, this approach correctly mapped
these questions to each other because of their se-
mantic similarity. One of the main causes of er-
ror for this method is that follow-up questions
were sometimes mis-identified as original ques-
tions. For example, ”How do you like your ma-
jor?” could be mapped to the original question, ”If
you attended college, what was your major?” even
though the question the interviewer asked was a
follow-up question.
</p>
<p>We also analyzed the accuracy of the method-
ologies using varying thresholds. For word em-
beddings and document embeddings, the thresh-
old is determined by cosine similarity of a turn
and question. For ROUGE, the threshold is the
f1-score. For each approach, We compiled a set
of turns from the CXD corpus that had the low-
est cosine similarity to the question each turn was
</p>
<p>112</p>
<p />
</div>
<div class="page"><p />
<p>Approach Accuracy Precision Recall F1-Score
Baseline (Rule-based) 39.0 72.0 42.0 53.1
ROUGE 74.0 93.0 78.0 84.8
Word Embeddings 91.4 92.1 99.1 95.5
Document Embeddings 88.6 90.0 98.2 93.9
</p>
<p>Table 1: Accuracy, precision, and recall of each approach, evaluated on hand annotated turns
</p>
<p>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>Threshold
</p>
<p>Pr
ec
</p>
<p>is
io
</p>
<p>n
</p>
<p>Figure 1: Accuracy of each approach determined
by threshold. Filled in dots are word embeddings.
Squares represent document embeddings. Trian-
gles represent ROUGE.
</p>
<p>matched with. We capped the threshold at 0.82.
Figure 1 shows that, as we increase the thresh-
old, generally, the accuracy of the question match-
ing for all approaches is higher. This, intuitively,
makes sense because, as we increase the threshold,
we are selecting turns that have higher similarity
to their matched question. Although this results
in lower recall, it can be used in cases where high
precision is needed for annotations.
</p>
<p>4.1 Cross-corpus Evaluation
</p>
<p>To further evaluate our best-performing approach,
we applied the word embeddings method to an-
other corpus collected using a similar interview
paradigm, the Columbia SRI Colorado (CSC) cor-
pus. To test word embeddings on this corpus,
we compiled 31 interviewer sessions that were al-
ready hand annotated, giving us a total of 6395
turns. The (single) interviewer involving in col-
lecting this corpus always began with a list of four
standard biographical questions, thus reducing the
number of turns that contained an interviewer-
generated question to 114. Following the word
embeddings method described above, we obtained
an accuracy of 99.8%, precision of 91.2%, recall
of 100%, and F1-score of 95.3 on the CSC corpus.
</p>
<p>The incorrectly identified questions were
</p>
<p>largely because the interviewer did not ask all four
biographical questions in every session, while
the word embeddings approach assumes that
all questions were asked and therefore, matches
some turn to the original question even though the
interviewer did not ask it. The higher accuracy
obtained on the CSC corpus is probably due to
the fact that the interviews were all conducted by
a single interviewer, so the questions were asked
with greater consistency. In addition, all subjects
were native speakers of Standard American
English, while half the participants in the CXD
corpus were native speakers of Mandarin Chinese.
</p>
<p>5 Conclusion
</p>
<p>Corpora consisting of spontaneous speech that is
open-ended, yet topically constrained, is more
commonplace, as researchers seek spontaneous
speech with some similarity of topic across sub-
jects. Traditionally, such corpora are hand anno-
tated for topic segments to serve as training ma-
terial. However, on large corpora such as the
CXD corpus, this can be cumbersome and time-
consuming. In this paper, we have presented four
approaches to automatically identifying question
topics on the CXD corpus to discover which ap-
proach achieves the best results in automatically
tagging corpora into question-defined topics. We
found that the word embeddings approach was
the best performing approach with an f1-score of
95.5%. We then applied the word embeddings
approach to the CSC corpus to verify that this
approach was useful for other corpora and also
achieved very good results. We conclude that this
automated, unsupervised approach to tagging cor-
pora can be very useful in annotation and analy-
sis for corpora collected using question prompts.
For more exact annotations, this approach could
also be used as an automated pre-processing stage
to reduce human annotation efforts. In future, we
would like to extend the embeddings approach to
scale to less constrained tasks, evaluate it on addi-
tional corpora, and also more accurately tag cor-
pora based on an ambiguous number of topics.
</p>
<p>113</p>
<p />
</div>
<div class="page"><p />
<p>References
Paula CF Cardoso, Maite Taboada, and Thiago AS
</p>
<p>Pardo. 2013. Subtopics annotation in a corpus of
news texts: steps towards automatic subtopic seg-
mentation. In Proceedings of the Brazilian Sympo-
sium in Information and Human Language Technol-
ogy.
</p>
<p>Gaël Dias, Elsa Alves, and José Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text sum-
marization and passage retrieval: An exhaustive
evaluation. In AAAI. volume 7, pages 1334–1340.
</p>
<p>Julia Hirschberg, Stefan Benus, Jason M Brenier, Frank
Enos, Sarah Friedman, Sarah Gilman, Cynthia Gi-
rand, Martin Graciarena, Andreas Kathol, Laura
Michaelis, et al. 2005. Distinguishing deceptive
from non-deceptive speech. In Interspeech. pages
1833–1836.
</p>
<p>Tom Kenter and Maarten de Rijke. 2015. Short text
similarity with word embeddings. In Proceedings of
the 24th ACM International on Conference on Infor-
mation and Knowledge Management. ACM, pages
1411–1420.
</p>
<p>Sarah I Levitan, Guzhen An, Mandi Wang, Gideon
Mendels, Julia Hirschberg, Michelle Levine, and
Andrew Rosenberg. 2015. Cross-cultural produc-
tion and detection of deception from speech. In Pro-
ceedings of the 2015 ACM on Workshop on Multi-
modal Deception Detection. ACM, pages 1–8.
</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop. Barcelona, Spain, volume 8.
</p>
<p>Rada Mihalcea, Courtney Corley, Carlo Strapparava,
et al. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI. vol-
ume 6, pages 775–780.
</p>
<p>Jesús Oliva, José Ignacio Serrano, Marı́a Dolores del
Castillo, and Ángel Iglesias. 2011. Symss: A
syntax-based measure for short-text semantic simi-
larity. Data &amp; Knowledge Engineering 70(4):390–
405.
</p>
<p>114</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 115–120,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Does Free Word Order Hurt?
Assessing the Practical Lexical Function Model for Croatian
</p>
<p>Zoran Medić∗ Jan Šnajder∗ Sebastian Padó†
∗ Faculty of Electrical Engineering and Computing, University of Zagreb
</p>
<p>{jan.snajder, zoran.medic}@fer.hr
† Institut für Maschinelle Sprachverarbeitung, Stuttgart University
</p>
<p>pado@ims.uni-stuttgart.de
</p>
<p>Abstract
</p>
<p>The Practical Lexical Function (PLF)
model is a model of computational distri-
butional semantics that attempts to strike
a balance between expressivity and learn-
ability in predicting phrase meaning and
shows competitive results. We investigate
how well the PLF carries over to free word
order languages, given that it builds on ob-
servations of predicate-argument combina-
tions that are harder to recover in free word
order languages. We evaluate variants of
the PLF for Croatian, using a new lexical
substitution dataset. We find that the PLF
works about as well for Croatian as for En-
glish, but demonstrate that its strength lies
in modeling verbs, and that the free word
order affects the less robust PLF variant.
</p>
<p>1 Introduction
</p>
<p>Compositional distributional semantic models
(CDSMs) represent phrase meaning in a vector
space by composing the meanings of individual
words. Many CDSMs were proposed, ranging from
basic ones that use element-wise operations on
word vectors to compute phrase vectors (Mitchell
and Lapata, 2008), to more complex models that
represent predicate arguments as higher-order ten-
sors (Baroni and Zamparelli, 2010; Guevara, 2010).
The latter models assume that predicates in a phrase
act as functions that act on other phrase compo-
nents to yield the final representation of the phrase.
For example, an adjective acts as a function on the
noun in an adjective-noun phrase, while a transitive
verb acts as a binary function on its subject and
object. However, since the number of parameters
in a tensor grows exponentially with the number of
arguments of the function that it models, learning
full tensors for predicates with many arguments is
</p>
<p>tedious to impractical (Grefenstette et al., 2012).
The Practical Lexical Function model (PLF, Pa-
</p>
<p>perno et al. (2014)) strikes a middle ground by
breaking down all tensors with ranks higher than
two into multiple matrices, each representing the
predicate’s composition with a single argument
(cf. Section 2 for details). In the experiments of
Paperno et al. (2014), PLF has been shown to work
better than some other CDSMs in modeling seman-
tic similarity. Particularly good results were ob-
tained on ANVAN (adjective-noun-verb-adjective-
noun) phrases, where PLF outperformed both sim-
ple CDSMs (due to its higher expressiveness) as
well as the higher-order Lexical Function model
(Baroni and Zamparelli, 2010).
</p>
<p>Although the PLF shows promising results, ex-
isting work still leaves open two questions. First,
it is not obvious that these results carry over to
languages with free word order, such as Slavic lan-
guages, where predicates and arguments are often
separated. For example, in the English sentence ‘I
like my dog’, the predicate is adjacent to both the
subject and the object, while in the Croatian trans-
lation ‘Svid̄a mi se moj pas’, the object ‘moj pas
is separated from the predicate. As corpus-derived
vectors for predicate-argument combinations are
a key part of the PLF, non-adjacency might make
it difficult to estimate its parameters reliably for
such languages. Secondly, the evaluation method
reported by Paperno et al. (2014) uses a somewhat
artificial setup by assuming that all phrase pairs,
even ill-formed ones, can be graded for similarity.
</p>
<p>In this work we consider both of these questions.
We investigate the application of PLF to Croatian
language, a Slavic language with relatively free
word order. We compare PLF with other, simpler
CDSMs, as well as PLF modifications proposed by
Gupta et al. (2015). In contrast to Paperno et al.
(2014), we adopt lexical substitution as evaluation,
building a new dataset of Croatian ANVAN phrases,
</p>
<p>115</p>
<p />
</div>
<div class="page"><p />
<p>together with word substitutes for each word. The
PLF model for Croatian performs comparably well
to English, outperforming simpler CDSMs in par-
ticular at the verb position.
</p>
<p>2 The Practical Lexical Function Model
</p>
<p>Basic model. As described above, the idea of the
PLF is to represent predicates as sets of matrices
for each argument slot of the predicate, plus a vec-
tor for its lexical meaning. The meaning of the
predicate-argument combination is computed by
multiplying all argument vectors with the predi-
cates’ slot matrices and finally adding the predi-
cate’s lexical vector. For example, the vector for
the phrase ‘big window’ is computed as:
</p>
<p>P (big window) = −−→big +
�N
big × −−−−−−→window (1)
</p>
<p>This can easily be generalized to more complex
ANVAN phrases, as exemplified in Figure 1.
</p>
<p>The predicate matrices are estimated using ridge
regression with corpus-extracted vectors for argu-
ments (−→n ) as input and vectors for bigram phrases
(−→an) as output. For example, the predicate matrix
�Na for an adjective a is computed as follows:
</p>
<p>�Na =∆ arg min
M
</p>
<p>∑
n∈nouns(a)
</p>
<p>M ×
−→n − −→an
</p>
<p>2
(2)
</p>
<p>PLF modifications. Gupta et al. (2015) identify
an inconsistency within the PLF: there is a differ-
ence between the meaning modeled by a matrix ob-
tained with training and its usage in phrase vector
calculation. The matrix obtained using Eq. (2) di-
rectly approximates the phrase meaning for a given
predicate-argument phrase, while the PLF phrase
vector in Eq. (1) adds the predicate vector on top
of the product of predicate matrix and argument
vector. They propose two remedies, as follows.
</p>
<p>Train phase modification changes Eq. (2) so that
the predicate matrix does not learn a direct transfor-
mation from an argument vector to a phrase vector,
but rather a difference between these vectors:
�
a =∆ arg min
</p>
<p>M
</p>
<p>∑
n∈nouns(a)
</p>
<p>M ×
−→n − (−→an − −→a )
</p>
<p>2
(3)
</p>
<p>This justifies the addition of predicate vector in (1).
In contrast, test phase modification retains the
</p>
<p>same training process, but omits the predicate vec-
tor when computing the phrase vector:1
</p>
<p>P (big window) =
�N
big × −−−−−−→window (4)
</p>
<p>1For one-argument predicates, this is equivalent to the
Lexical Function model (Baroni and Zamparelli, 2010).
</p>
<p>Gupta et al. (2015) found both modifications to out-
perform simple baseline CDSMs for English when
evaluated on ANVAN datasets, with test adaptation
outperforming the original PLF.
</p>
<p>PLF for Croatian. We implemented the basic
PLF and the two above-mentioned modifications
for Croatian following the procedure described by
Paperno et al. (2014). As a corpus for building
word and phrase lexical vectors we used fHrWaC
(Šnajder et al., 2013), a filtered version of Croatian
web corpus (Ljubešić and Erjavec, 2011), total-
ing 51M sentences and 1.2B tokens. The corpus
has been parsed using the MSTParser for Croatian
(Agić and Merkler, 2013).
</p>
<p>As a first step in obtaining word vector repre-
sentations, we extracted a co-occurrence matrix of
30K most frequent lemmas (nouns, verbs, and ad-
jectives) in corpus, using a window of size 3. Next,
the vectors contained in the resulting matrix were
transformed using Positive Pointwise Mutual In-
formation (PPMI) and reduced to size 300 using
Singular Value Decomposition. Finally, all vectors
in the matrix were normalized to unit length.
</p>
<p>For the extraction of phrase (bigram) vectors,
we consider two different approaches. The first
approach considers all occurrences where the pred-
icate and argument are adjacent in the dependency
trees in fHrWaC even if they are not adjacent on
the surface, sidestepping the free word order issue.
The second approach extracts only those phrases
in which the predicate and argument are adjacent
on the surface, resulting in a smaller but potentially
cleaner set of co-occurrences. The phrase vectors
from both approaches use the same 30K context
lemmas and window size as the unigrams.
</p>
<p>Using the extracted lemma and bigram vectors,
we train matrices for each of the predicate words
from our evaluation dataset. As our dataset consists
of ANVAN phrases, we train one matrix for each
adjective and two matrices for each verb (one for
subject and one for object). We train two versions
of each matrix: one using the originally proposed
training and another with modified training.
</p>
<p>3 Experiments
</p>
<p>Evaluation methodology. Paperno et al. (2014)
evaluated the PLF on five datasets containing
phrases in different forms. Two consist of free-
form sentences, one of a number of differently
formed phrases, and the two ANVAN datasets
contain adjective-noun-verb-adjective-noun phrase
</p>
<p>116</p>
<p />
</div>
<div class="page"><p />
<p>−−−→open + �sopen × −→npsubj + �oopen × −→npobj
</p>
<p>{−−−→open + �sopen × −→npsubj, �oopen }
</p>
<p>−→npsubj =∆ −−−−→young + �Nyoung × −−→boy {−−−→open, �sopen, �oopen }
</p>
<p>−→npobj =∆ −−→big +
�N
big × −−−−−−→window
</p>
<p>Figure 1: Computing the vector for an ANVAN phrase (young boy open big window) using PLF.
</p>
<p>ANVAN phrase (target word in bold) Substitute words
</p>
<p>legendaran trener voditi suparnička momčad
(legendary coach lead opponent team)
</p>
<p>cijenjen (appreciated), izvanredan (outstanding), poz-
nat (famous), uspješan (successful), znamenit (notable)
</p>
<p>dobar igrač dati pobjednički gol
(good player score winning goal)
</p>
<p>pogoditi (to hit), postići (to achieve), zabiti (to score),
zadati (to give)
</p>
<p>sportski automobil prijeći velika udaljenost
(sports car travel large distance)
</p>
<p>dionica (section), dužina (length), put (way), razdaljina
(distance)
</p>
<p>Table 1: Examples of ANVAN phrases with manually collected substitutes for boldfaced targets.
</p>
<p>pairs rated for semantic similarity (Kartsaklis et al.,
2013; Grefenstette, 2013). The phrases in each pair
differ only in the verb. Annotators rated the sim-
ilarity on a scale from 1 to 7, and CDSMs were
evaluated by correlating the ratings with the simi-
larity of the predicted phrase vectors.
</p>
<p>The described approach is not appropriate when
one or both ANVAN phrases are ungrammatical
or nonsensical. Consider the following phrase pair
in the ANVAN dataset by Kartsaklis et al. (2013):
</p>
<p>‘dental service file false tooth’ – ‘dental service
register false tooth’. While the first sentence is
plausible, the second one is arguably somewhere
between implausible and nonsensical. We believe
that semantic similarity is not a reasonable evalua-
tion criterion for such (relatively frequent) cases.
</p>
<p>For our experiment, we chose a word-choice
evaluation setup, which essentially builds on the
idea of lexical substitution. Lexical substitution
is the task of identifying a substitute for a word
in a given context (McCarthy and Navigli, 2007).
Typically, a system is presented with a phrase and
candidate substitutes for a target word in the phrase
and needs to select one or more adequate substi-
tutes. Systems either have to rank the candidates
in the appropriate order (McCarthy and Navigli,
2007; Sinha and Mihalcea, 2009), or just choose
one best substitute (Melamud et al., 2016).
</p>
<p>An additional benefit of a lexical substitution
setup is that we can evaluate the predictions of the
</p>
<p>model not just globally, but at the level of individual
words. We will exploit that possibility below.
</p>
<p>Croatian ANVAN dataset. We constructed indi-
vidual ANVAN phrases for Croatian like in prior
English work (Kartsaklis et al., 2013; Grefenstette,
2013). We started by choosing six transitive verbs
from the list of polysemous verbs on the Croat-
ian language portal.2 We chose verbs with high
polysemy level, while avoiding those that overlap
in semantic meaning. The list consist of the fol-
lowing verbs: ‘baciti’ (to throw), ‘dati’ (to give),
</p>
<p>‘izdati’ (to issue), ‘prijeći’ (to cross), ‘vidjeti’ (to
see), and ‘voditi’ (to lead). Using the distributional
memory for Croatian (Šnajder et al., 2013), we se-
lected the three most frequent subjects and objects
for each verb. Finally, we chose a single adjective
for each subject and object from the list of 20 most
frequently co-occurring adjectives. This leaves us
with 18 semantically plausible ANVAN phrases,
illustrated in Table 1 (left column).
</p>
<p>We manually collected substitutes for each word
in the phrases. Three annotators were given a
phrase and instructed to propose up to three substi-
tutes for each word, while preserving both gram-
maticality and meaning; cf. the right column in
Table 1. This yielded an evaluation dataset that
contains 408 words: 158 adjectives, 167 nouns,
and 83 verbs, each with multiple substitutes.
</p>
<p>2 http://hjp.znanje.hr
</p>
<p>117</p>
<p />
</div>
<div class="page"><p />
<p>Target
phrase
</p>
<p>odličan d̄ak prijeći brza cesta
(excellent pupil cross fast road)
</p>
<p>Possible
substitutes
</p>
<p>dobar (good), potvrdan (affirma-
tive), crtani (drawn), sportski
(sportive)
</p>
<p>Table 2: Word-choice item example. Target word
in bold; correct substitute underlined.
</p>
<p>Word Choice Task and Evaluation. We use the
substitution dataset to set up a word choice task
(Melamud et al., 2016): Each CDSM is presented
with an ANVAN target phrase, a position in this
phrase, a correct substitute and three distractors. Its
task is to recognize the substitute that fits best into
the context. Distractors were chosen by randomly
picking three words of the same POS (adjective,
noun or verb) that were not proposed as substitutes
for that component in the given phrase. Table 2
shows an example of a single word-choice item.
</p>
<p>In concrete terms, to evaluate a candidate substi-
tute with respect to an ANVAN target phrase, we
compute the cosine similarity between the compo-
sitionally computed vector for the ANVAN phrase
computed “as is”, and the phrase vector for the AN-
VAN phrase with the word at the current position
replaced by the candidate substitute. The assump-
tion is that a meaning-preserving substitution will
leave the phrase vector largely unchanged and thus
lead to a high cosine value. We report accuracy
as the percentage of items for which the correct
substitute received a higher cosine value than the
incorrect substitutes.3
</p>
<p>Models. We use the PLF and the two variants
described in Section 2 (PLF-train and PLF-test).
We build all three PLF versions for both phrase
extraction approaches described in Sec. 2. In addi-
tion, we consider two baselines, namely the simple
componentwise additive (add) and multiplicative
(mult) models (Mitchell and Lapata, 2008).
</p>
<p>4 Results
</p>
<p>Table 3 shows the overall accuracy for each model.
The standard PLF with dependency-extracted bi-
grams obtained the highest overall accuracy. The
difference to the next-best model, add, is however
not significant (p&gt;0.01, McNemar’s test).
</p>
<p>3The annotated dataset with compiled word choice
tasks is available at: http://takelab.fer.hr/data/
croanvan
</p>
<p>Counts
</p>
<p>Type Surface level Dependency level
</p>
<p>adj-noun 14,249,655 15,548,616
subject-verb 3,147,289 3,994,552
verb-object 2,698,654 4,931,198
</p>
<p>Table 4: fHrWaC number of predicate-argument
co-occurrences at surface and dependency level.
</p>
<p>Our new evaluation method allows us to further
analyze this result by computing results for individ-
ual phrase positions (columns in Table 3). We find
that PLF significantly outperforms both baselines
for verbs (p&lt;0.01, McNemar’s test). This is in line
with, and can potentially explain, the good results
for English (Paperno et al., 2014), since in the En-
glish evaluation setup, the ANVAN phrase pairs
differ only in the verbs (cf. Section 3). In contrast,
add performs as well as or better than the PLF an
adjectives and nouns.
</p>
<p>A potential explanation for these patterns is va-
lency: The verb has the highest valency of all words
in the phrase (two arguments). Arguably, verbs
can profit most from the additional expressive-
ness of PLF over the simpler CDSMs. Apparently,
for adjectives (one argument) the expressiveness-
learnability tradeoff is balanced between the two
models, and for nouns (no arguments, thus no func-
tional role) the additive model’s simplicity wins.
</p>
<p>Comparing the different PLF versions, we find
no benefit for the modifications proposed by Gupta
et al. (2015), who also obtained a null result for
PLF-train, but found PLF-test to outperform plain
PLF. For Croatian, PLF-test performs comparably
to PLF for nouns and adjectives, but does clearly
worse for verbs. A potential explanation follows
from Gupta et al.’s analysis of the difference be-
tween PLF and PLF-test as a bias-variance tradeoff:
the original PLF uses the lexical vector of the pred-
icate as a “prior” for the phrase meaning, which
makes it more robust, but also less flexible. PLF-
test uses only the predicate matrix to compute a
phrase vector and is thus more dependent on the
data quality: on good data, it can outperform PLF,
but it will be outperformed on noisy data.
</p>
<p>Indeed, there is evidence that the verb-argument
matrices are noisy in Croatian: Table 4 compares
co-occurrence frequencies at the surface and depen-
dency levels for three predicate-argument combina-
tions. It shows that &gt;90% of A-N combinations are
</p>
<p>118</p>
<p />
</div>
<div class="page"><p />
<p>Phrase position
</p>
<p>Model Phrase vectors A1 N1 V A2 N2 Overall
</p>
<p>add 73.4 92.0 44.6 70.1 89.7 74.0
mult 39.2 61.4 32.5 40.2 62.8 47.4
</p>
<p>PLF 74.7 85.2 66.3 * 67.5 85.9 76.0
PLF-train Dependency-based 58.2 89.8 49.4 51.9 83.3 66.9
PLF-test 72.2 85.2 60.2 67.5 84.6 74.0
</p>
<p>PLF 55.7 87.5 63.9 65.4 84.6 71.7
PLF-train Surface-based 54.4 89.8 51.8 56.4 82.1 67.2
PLF-test 69.6 87.5 55.4 60.3 83.3 71.4
</p>
<p>Table 3: Model accuracy per phrase position. Asterisk (*) indicates a statistically significant result when
comparing the best PLF version with the best simple CDSM, namely add (McNemar’s test, p&lt;0.01).
</p>
<p>adjacent on the surface, while this holds for less
than 80% of the S-V and 55% of the V-O combina-
tions. As it is generally true that parsing quality de-
teriorates for long distance dependencies, the S-V
and V-O matrices are arguably built from noisier
data, which can account for disadvantage for PLF-
test. In this manner, the free word order of Croatian
does have an effect on CDSM performance.
</p>
<p>That being said, parsing quality is evidently good
enough for syntactic analysis to pay off: the results
for using surface co-occurrence based versions of
the PLF model perform generally worse than the
PLF using dependency-base co-occurrences, with
the exception of N1 (subject) position.
</p>
<p>5 Conclusion
</p>
<p>We built a Practical Lexical Function (PLF) model
for Croatian and evaluated it on a newly created
dataset of adjective-noun-verb-adjective-noun (AN-
VAN) phrases. Our evaluation differs from existing
English work (Paperno et al., 2014) by using a lex-
ical substitution setup. Crucially, this allows us to
analyze performance for individual phrase compo-
nents. We find that the PLF’s specific strength lies
in modeling verbs, while it only does as well as sim-
ple additive models for nouns and adjectives. As
we use dependency parses, the free word order of
Croatian does not pose a major problem of the plain
PLF, although we have evidence that it does affect
the less robust PLF-test by Gupta et al. (2015). For
future work, we will perform similar evaluation on
a wider range of models and collect more evidence
on the impact of typological differences on results.
</p>
<p>Acknowledgments
</p>
<p>This work has been supported in part by the Croa-
tian Science Foundation under the project UIP-
2014-09-7312. The third author has been supported
by the DFG (SFB 732, Project D10).
</p>
<p>References
Željko Agić and Danijela Merkler. 2013. Three syntac-
</p>
<p>tic formalisms for data-driven dependency parsing
of Croatian. In Proceedings of TSD 2013, Lecture
Notes in Artificial Intelligence. Springer, pages 560–
567.
</p>
<p>Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP. Cambridge, MA, pages
1183–1193.
</p>
<p>Edward Grefenstette. 2013. Category-theoretic quan-
titative compositional distributional models of natu-
ral language semantics. Ph.D. thesis, University of
Oxford.
</p>
<p>Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2012. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS 2012. Potsdam, Germany.
</p>
<p>Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics. Uppsala, Sweden, pages 33–37.
</p>
<p>Abhijeet Gupta, Jason Utt, and Sebastian Padó. 2015.
Dissecting the practical lexical function model for
compositional distributional semantics. In Proceed-
ings of the Fourth Joint Conference on Lexical and
</p>
<p>119</p>
<p />
</div>
<div class="page"><p />
<p>Computational Semantics. Denver, Colorado, pages
153–158.
</p>
<p>Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from
composition in distributional semantics. In Proceed-
ings of CoNLL. Sofia, Bulgaria, pages 114–123.
</p>
<p>Nikola Ljubešić and Tomaž Erjavec. 2011. hrWaC
and slWaC: Compiling web corpora for Croatian
and Slovene. In International Conference on Text,
Speech and Dialogue. Springer, Brno, Czech Repub-
lic, pages 395–402.
</p>
<p>Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English lexical substitution task. In
Proceedings of SEMEVAL. pages 48–53.
</p>
<p>Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning generic context em-
bedding with bidirectional LSTM. In Proceedings
of CONLL. Berlin, Germany, pages 51–61.
</p>
<p>Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL. Columbus, OH, pages 236–244.
</p>
<p>Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
Proceedings of ACL. Baltimore, MD, pages 90–99.
</p>
<p>Ravi Sinha and Rada Mihalcea. 2009. Combining lexi-
cal resources for contextual synonym expansion. In
Proceedings of RANLP. Borovets, Bulgaria, pages
404–410.
</p>
<p>Jan Šnajder, Sebastian Padó, and Željko Agic. 2013.
Building and evaluating a distributional memory for
Croatian. In Proceedings of ACL. Sofia, Bulgaria,
pages 784–789.
</p>
<p>120</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>A Mixture Model for Learning Multi-Sense Word Embeddings
</p>
<p>Dai Quoc Nguyen1, Dat Quoc Nguyen2, Ashutosh Modi1, Stefan Thater1, Manfred Pinkal1
</p>
<p>1Department of Computational Linguistics, Saarland University, Germany
{daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de
</p>
<p>2Department of Computing, Macquarie University, Australia
dat.nguyen@students.mq.edu.au
</p>
<p>Abstract
</p>
<p>Word embeddings are now a standard
technique for inducing meaning represen-
tations for words. For getting good repre-
sentations, it is important to take into ac-
count different senses of a word. In this
paper, we propose a mixture model for
learning multi-sense word embeddings.
Our model generalizes the previous works
in that it allows to induce different weights
of different senses of a word. The experi-
mental results show that our model outper-
forms previous models on standard evalu-
ation tasks.
</p>
<p>1 Introduction
</p>
<p>Word embeddings have shown to be useful in var-
ious NLP tasks such as sentiment analysis, topic
models, script learning, machine translation, se-
quence labeling and parsing (Socher et al., 2013;
Sutskever et al., 2014; Modi and Titov, 2014;
Nguyen et al., 2015a,b; Modi, 2016; Ma and
Hovy, 2016; Nguyen et al., 2017; Modi et al.,
2017). A word embedding captures the syntac-
tic and semantic properties of a word by repre-
senting the word in a form of a real-valued vector
(Mikolov et al., 2013a,b; Pennington et al., 2014;
Levy and Goldberg, 2014).
</p>
<p>However, usually word embedding models do
not take into account lexical ambiguity. For ex-
ample, the word bank is usually represented by
a single vector representation for all senses in-
cluding sloping land and financial institution. Re-
cently, approaches have been proposed to learn
multi-sense word embeddings, where each sense
of a word corresponds to a sense-specific em-
bedding. Reisinger and Mooney (2010), Huang
et al. (2012) and Wu and Giles (2015) proposed
methods to cluster the contexts of each word and
</p>
<p>then using cluster centroids as vector representa-
tions for word senses. Neelakantan et al. (2014),
Tian et al. (2014), Li and Jurafsky (2015) and
Chen et al. (2015) extended Word2Vec models
(Mikolov et al., 2013a,b) to learn a vector repre-
sentation for each sense of a word. Chen et al.
(2014), Iacobacci et al. (2015) and Flekova and
Gurevych (2016) performed word sense induction
using external resources (e.g., WordNet, Babel-
Net) and then learned sense embeddings using the
Word2Vec models. Rothe and Schütze (2015) and
Pilehvar and Collier (2016) presented methods us-
ing pre-trained word embeddings to learn embed-
dings from WordNet synsets. Cheng et al. (2015),
Liu et al. (2015b), Liu et al. (2015a) and Zhang
and Zhong (2016) directly opt the Word2Vec Skip-
gram model (Mikolov et al., 2013b) for learning
the embeddings of words and topics on a topic-
assigned corpus.
</p>
<p>One issue in these previous works is that they
assign the same weight to every sense of a word.
The central assumption of our work is that each
sense of a word given a context, should correspond
to a mixture of weights reflecting different asso-
ciation degrees of the word with multiple senses
in the context. The mixture weights will help to
model word meaning better.
</p>
<p>In this paper, we propose a new model for learn-
ing Multi-Sense Word Embeddings (MSWE). Our
MSWE model learns vector representations of a
word based on a mixture of its sense represen-
tations. The key difference between MSWE and
other models is that we induce the weights of
senses while jointly learning the word and sense
embeddings. Specifically, we train a topic model
(Blei et al., 2003) to obtain the topic-to-word and
document-to-topic probability distributions which
are then used to infer the weights of topics. We
use these weights to define a compositional vec-
tor representation for each target word to predict
</p>
<p>121</p>
<p />
</div>
<div class="page"><p />
<p>its context words. MSWE thus is different from
the topic-based models (Cheng et al., 2015; Liu
et al., 2015b,a; Zhang and Zhong, 2016), in which
we do not use the topic assignments when jointly
learning vector representations of words and top-
ics. Here we not only learn vectors based on the
most suitable topic of a word given its context, but
we also take into consideration all possible mean-
ings of the word.
</p>
<p>The main contributions of our study are: (i) We
introduce a mixture model for learning word and
sense embeddings (MSWE) by inducing mixture
weights of word senses. (ii) We show that MSWE
performs better than the baseline Word2Vec Skip-
gram and other embedding models on the word
analogy task (Mikolov et al., 2013a) and the word
similarity task (Reisinger and Mooney, 2010).
</p>
<p>2 The mixture model
</p>
<p>In this section, we present the mixture model for
learning multi-sense word embeddings. Here we
treat topics as senses. The model learns a repre-
sentation for each word using a mixture of its top-
ical representations.
</p>
<p>Given a number of topics and a corpus D of
documents d = {wd,1, wd,2, ..., wd,Md}, we apply
a topic model (Blei et al., 2003) to obtain the topic-
to-word Pr(w|t) and document-to-topic Pr(t|d)
probability distributions. We then infer a weight
for themth word wd,m with topic t in document d:
</p>
<p>λd,m,t = Pr(wd,m|t)× Pr(t|d) (1)
</p>
<p>We define two MSWE variants: MSWE-1 learns
vectors for words based on the most suitable topic
given document d while MSWE-2 marginalizes
over all senses of a word to take into account all
possible senses of the word:
</p>
<p>MSWE-1: swd,m =
vwd,m + λd,m,t′ × vt′
</p>
<p>1 + λd,m,t′
</p>
<p>MSWE-2: swd,m =
vwd,m +
</p>
<p>∑T
t=1 λd,m,t × vt
</p>
<p>1 +
∑T
</p>
<p>t=1 λd,m,t
</p>
<p>where swd,m is the compositional vector represen-
tation of the mth word wd,m and the topics in doc-
ument d; vw is the target vector representation of a
word type w in vocabulary V ; vt is the vector rep-
resentation of topic t; T is the number of topics;
λd,m,t is defined as in Equation 1, and in MSWE-1
we define t′ = arg max
</p>
<p>t
λd,m,t.
</p>
<p>We learn representations by minimizing the fol-
lowing negative log-likelihood function:
</p>
<p>L = − ∑
d∈D
</p>
<p>Md∑
m=1
</p>
<p>∑
−k≤j≤k
</p>
<p>j 6=0
</p>
<p>log Pr(ṽwd,m+j |swd,m) (2)
</p>
<p>where the mth word wd,m in document d is a tar-
get word while the (m+j)th word wd,m+j in doc-
ument d is a context word of wd,m and k is the
context size. In addition, ṽw is the context vec-
tor representation of the word type w. The proba-
bility Pr(ṽwd,m+j |swd,m) is defined using the soft-
max function as follows:
</p>
<p>Pr(ṽwd,m+j |swd,m) =
exp(ṽTwd,m+jswd,m)∑
</p>
<p>c′∈V exp(ṽ
T
c′swd,m)
</p>
<p>Since computing log Pr(ṽwd,m+j |swd,m) is ex-
pensive for each training instance, we approximate
log Pr(ṽwd,m+j |swd,m) in Equation 2 with the
following negative-sampling objective (Mikolov
et al., 2013b):
</p>
<p>Od,m,m+j = log σ
(
ṽTwd,m+jswd,m
</p>
<p>)
+
</p>
<p>K∑
i=1
</p>
<p>log σ
(
−ṽTciswd,m
</p>
<p>)
(3)
</p>
<p>where each word ci is sampled from a noise distri-
bution.1 In fact, MSWE can be viewed as a gener-
alization of the well-known Word2Vec Skip-gram
model with negative sampling (Mikolov et al.,
2013b) where all the mixture weights λd,m,t are
set to zero. The models are trained using Stochas-
tic Gradient Descent (SGD).
</p>
<p>3 Experiments
</p>
<p>We evaluate MSWE on two different tasks: word
similarity and word analogy. We also pro-
vide experimental results obtained by the baseline
Word2Vec Skip-gram model and other previous
works.
</p>
<p>Note that not all previous results are mentioned
in this paper for comparison because the train-
ing corpora used in most previous research work
are much larger than ours (Baroni et al., 2014; Li
and Jurafsky, 2015; Schwartz et al., 2015; Levy
et al., 2015). Also there are differences in the
pre-processing steps that could affect the results.
We could also improve obtained results by using a
</p>
<p>1We use an unigram distribution raised to the 3/4 power
(Mikolov et al., 2013b) as the noise distribution.
</p>
<p>122</p>
<p />
</div>
<div class="page"><p />
<p>larger training corpus, but this is not central point
of our paper. The objective of our paper is that the
embeddings of topic and word can be combined
into a single mixture model, leading to good im-
provements as established empirically.
</p>
<p>3.1 Experimental Setup
</p>
<p>Following Huang et al. (2012) and Neelakantan
et al. (2014), we use the Wesbury Lab Wikipedia
corpus (Shaoul and Westbury, 2010) containing
over 2M articles with about 990M words for train-
ing. In the preprocessing step, texts are lower-
cased and tokenized, numbers are mapped to 0,
and punctuation marks are removed. We extract a
vocabulary of 200,000 most frequent word tokens
from the pre-processed corpus. Words not occur-
ring in the vocabulary are mapped to a special to-
ken UNK, in which we use the embedding of UNK
for unknown words in the benchmark datasets.
</p>
<p>We firstly use a small subset extracted from the
WS353 dataset (Finkelstein et al., 2002) to tune
the hyper-parameters of the baseline Word2Vec
Skip-gram model for the word similarity task (see
Section 3.2 for the task definition). We then
directly use the tuned hyper-parameters for our
MSWE variants. Vector size is also a hyper-
parameter. While some approaches use a higher
number of dimensions to obtain better results, we
fix the vector size to be 300 as used by the baseline
for a fair comparison. The vanilla Latent Dirichlet
Allocation (LDA) topic model (Blei et al., 2003) is
not scalable to a very large corpus, so we explore
faster online topic models developed for large cor-
pora. We train the online LDA topic model (Hoff-
man et al., 2010) on the training corpus, and use
the output of this topic model to compute the mix-
ture weights as in Equation 1.2 We also use the
same WS353 subset to tune the numbers of top-
ics T ∈ {50, 100, 200, 300, 400}. We find that the
most suitable numbers are T = 50 and T = 200
then used for all our experiments. Here we learn
300-dimensional embeddings with the fixed con-
text size k = 5 (in Equation 2) and K = 10 (in
Equation 3) as used by the baseline. During train-
ing, we randomly initialize model parameters (i.e.
word and topic embeddings) and then learn them
by using SGD with the initial learning rate of 0.01.
</p>
<p>2We use default parameters in gensim (Řehůřek and So-
jka, 2010) for the online LDA model.
</p>
<p>Dataset Word pairs Reference
WS353 353 Finkelstein et al. (2002)
SIMLEX 999 Hill et al. (2015)
SCWS 2003 Huang et al. (2012)
RW 2034 Luong et al. (2013)
MEN 3000 Bruni et al. (2014)
</p>
<p>Table 1: The benchmark datasets. WS353:
WordSimilarity-353. RW: Rare-Words. SIMLEX:
SimLex-999. SCWS: Stanford’s Contextual Word
Similarities. MEN: The MEN Test Collection.
Each dataset contains similarity scores of human
judgments for pairs of words.
</p>
<p>3.2 Word Similarity
The word similarity task evaluates the quality of
word embedding models (Reisinger and Mooney,
2010). For a given dataset of word pairs, the eval-
uation is done by calculating correlation between
the similarity scores of corresponding word em-
bedding pairs with the human judgment scores.
Higher Spearman’s rank correlation (ρ) reflects
better word embedding model. We evaluate MSWE
on standard datasets (as given in Table 1) for the
word similarity evaluation task.
</p>
<p>Following Reisinger and Mooney (2010),
Huang et al. (2012), Neelakantan et al. (2014), we
compute the similarity scores for a pair of words
(w,w′) with or without their respective contexts
(c, c′) as:
</p>
<p>GlobalSim
(
w,w′
</p>
<p>)
= cos (vw,vw′)
</p>
<p>AvgSim
(
w,w′
</p>
<p>)
=
</p>
<p>1
</p>
<p>T 2
</p>
<p>T∑
t=1
</p>
<p>T∑
t′=1
</p>
<p>cos (vw,t,vw′,t′)
</p>
<p>AvgSimC
(
w,w′
</p>
<p>)
=
</p>
<p>1
</p>
<p>T 2
</p>
<p>T∑
t=1
</p>
<p>T∑
t′=1
</p>
<p>(
δ (vw,t,vc)× δ (vw′,t′ ,vc′)
</p>
<p>× cos (vw,t,vw′,t′)
)
</p>
<p>where vw is the vector representation of the word
w, vw,t is the multiple representation of the word
w and the topic t, vc is the vector representation
of the context c of the word w. And cos (v,v′)
is the cosine similarity between two vectors v
and v′. For our experiments, we set vw,t =
</p>
<p>vw ⊕ (Pr(w|t)× vt) and vc =
(
</p>
<p>1
|c|
∑
</p>
<p>w∈c vw
)
⊕
</p>
<p>(
∑
</p>
<p>t Pr (t|c)× vt), in which ⊕ is the concatena-
tion operation and Pr (t|c) is inferred from the
topic models by considering context c as a docu-
ment. GlobalSim only regards word embeddings,
</p>
<p>123</p>
<p />
</div>
<div class="page"><p />
<p>Model RW SIMLEX SCWS WS353 MEN
Huang et al. (2012) – – 58.6 71.3 –
Luong et al. (2013) 34.36 – 48.48 64.58 –
Qiu et al. (2014) 32.13 – 53.40 65.19 –
Neelakantan et al. (2014) – – 65.5 69.2 –
Chen et al. (2014) – – 64.2 – –
Hill et al. (2015) – 41.4 – 65.5 69.9
Vilnis and McCallum (2015) – 32.23 – 65.49 71.31
Schnabel et al. (2015) – – – 64.0 70.7
Rastogi et al. (2015) 32.9 36.7 65.6 70.8 73.9
Flekova and Gurevych (2016) – – – – 74.26
Word2Vec Skip-gram 32.64 38.20 66.37 71.61 75.49
MSWE-150 34.85 38.77 66.83 72.40 76.23
MSWE-1200 35.27 38.70 66.80 72.05 76.05
MSWE-250 34.98 38.79 66.61 71.71 75.90
MSWE-2200 35.56? 39.19? 66.65 72.29 76.37?
</p>
<p>Table 2: Spearman’s rank correlation (ρ×100) for
the word similarity task when using GlobalSim.
Subscripts 50 and 200 denote the online LDA
topic model trained with T = 50 and T = 200
topics, respectively. ? denotes that our best score
is significantly higher than the score of the base-
line (with p &lt; 0.05, online toolkit from http:
//www.philippsinger.info/?p=347). Scores in bold
and underline are the best and second best scores.
</p>
<p>while AvgSim considers multiple representations
to capture different meanings (i.e. topics) and us-
ages of a word. AvgSimC generalizes AvgSim
by taking into account the likelihood δ (vw,t,vc)
that word w takes topic t given context c. δ (v,v′)
is the inverse of the cosine distance from v to v′
</p>
<p>(Huang et al., 2012; Neelakantan et al., 2014).
</p>
<p>3.2.1 Results for word similarity
Table 2 compares the evaluation results of MSWE
with results reported in prior work on the stan-
dard word similarity task when using GlobalSim.
We use subscripts 50 and 200 to denote the topic
model trained with T = 50 and T = 200 topics,
respectively. Table 2 shows that our model out-
performs the baseline Word2Vec Skip-gram model
(in fifth row from bottom). Specifically, on the RW
dataset, MSWE obtains a significant improvement
of 2.92 in the Spearman’s rank correlation (which
is about 8.5% relative improvement).
</p>
<p>Compared to the published results, MSWE ob-
tains the highest accuracy on the RW, SCWS,
WS353 and MEN datasets, and achieves the second
highest result on the SIMLEX dataset. These in-
dicate that MSWE learns better representations for
words taking into account different meanings.
</p>
<p>3.2.2 Results for contextual word similarity
We evaluate our model MSWE by using AvgSim
and AvgSimC on the benchmark SCWS dataset
</p>
<p>Model AvgSim AvgSimC
Huang et al. (2012) 62.8 65.7
Neelakantan et al. (2014) 67.3 69.3
Chen et al. (2014) 66.2 68.9
Chen et al. (2015) 65.7 66.4
Wu and Giles (2015) – 66.4
Jauhar et al. (2015) – 65.7
Cheng and Kartsaklis (2015) 62.5 –
Iacobacci et al. (2015) 62.4 –
Cheng et al. (2015) – 65.9
MSWE-150 66.6 66.7
MSWE-1200 66.7 66.6
MSWE-250 66.4 66.6
MSWE-2200 66.6 66.6
</p>
<p>Table 3: Spearman’s rank correlation (ρ× 100) on
SCWS, using AvgSim and AvgSimC.
</p>
<p>which considers effects of the contextual informa-
tion on the word similarity task. As shown in Ta-
ble 3, MSWE scores better than the closely related
model proposed by Cheng et al. (2015) and gener-
ally obtains good results for this context sensitive
dataset. Although we produce better scores than
Neelakantan et al. (2014) and Chen et al. (2014)
when using GlobalSim, we are outperformed by
them when using AvgSim and AvgSimC. Nee-
lakantan et al. (2014) clustered the embeddings
of the context words around each target word to
predict its sense and Chen et al. (2014) used pre-
trained word embeddings to initialize vector rep-
resentations of senses taken from WordNet, while
we use a fixed number of topics as senses for
words in MSWE.
</p>
<p>3.3 Word Analogy
</p>
<p>We evaluate the embedding models on the word
analogy task introduced by Mikolov et al. (2013a).
The task aims to answer questions in the form of
“a is to b as c is to ?”, denoted as “a : b → c :
?” (e.g., “Hanoi : Vietnam → Bern : ?”). There
are 8,869 semantic and 10,675 syntactic questions
grouped into 14 categories. Each question is an-
swered by finding the most suitable word closest
to “vb−va +vc” measured by the cosine similar-
ity. The answer is correct only if the found closest
word is exactly the same as the gold-standard (cor-
rect) one for the question.
</p>
<p>We report accuracies in Table 4 and show that
MSWE achieves better results in comparison with
the baseline Word2Vec Skip-gram. In particular,
MSWE reaches the accuracies of around 69.7%
</p>
<p>124</p>
<p />
</div>
<div class="page"><p />
<p>Model Accuracy (%)
Pennington et al. (2014) 70.3
Baroni et al. (2014) 68.0
Neelakantan et al. (2014) 64.0
Ghannay et al. (2016) 62.3
Word2Vec Skip-gram 68.6
MSWE-150 69.6
MSWE-1200 69.9
MSWE-250 69.7
MSWE-2200 69.5
</p>
<p>Table 4: Accuracies for the word analogy task. All
our results are significantly higher than the result
of Word2Vec Skip-gram (with two-tail p &lt; 0.001
using McNemar’s test). Pennington et al. (2014)
used a larger training corpus of 1.6B words.
</p>
<p>which is higher than the accuracy of 68.6% ob-
tained by Word2Vec Skip-gram.
</p>
<p>4 Conclusions
</p>
<p>In this paper, we described a mixture model for
learning multi-sense embeddings. Our model in-
duces mixture weights to represent a word given
context based on a mixture of its sense representa-
tions. The results show that our model scores bet-
ter than Word2Vec, and produces highly competi-
tive results on the standard evaluation tasks. In fu-
ture work, we will explore better methods for tak-
ing into account the contextual information. We
also plan to explore different approaches to com-
pute the mixture weights in our model. For exam-
ple, if there is a large sense-annotated corpus avail-
able for training, the mixture weights could be de-
fined based on the frequency (sense-count) distri-
butions, instead of using the probability distribu-
tions produced by a topic model. Furthermore, it is
possible to consider the weights of senses as addi-
tional model parameters to be then learned during
training.
</p>
<p>Acknowledgments
</p>
<p>This research was funded by the German Research
Foundation (DFG) as part of SFB 1102 “Informa-
tion Density and Linguistic Encoding”. We would
like to thank anonymous reviewers for their help-
ful comments.
</p>
<p>References
Marco Baroni, Georgiana Dinu, and Germán
</p>
<p>Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers). pages 238–247.
</p>
<p>David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research 3:993–1022.
</p>
<p>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research 49:1–47.
</p>
<p>Tao Chen, Ruifeng Xu, Yulan He, and Xuan Wang.
2015. Improving distributed representation of word
sense via wordnet gloss composition and context
clustering. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers).
pages 15–20.
</p>
<p>Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). pages 1025–1035.
</p>
<p>Jianpeng Cheng and Dimitri Kartsaklis. 2015. Syntax-
aware multi-sense word embeddings for deep com-
positional models of meaning. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing. pages 1531–1542.
</p>
<p>Jianpeng Cheng, Zhongyuan Wang, Ji-Rong Wen, Jun
Yan, and Zheng Chen. 2015. Contextual text under-
standing in distributional semantic space. In Pro-
ceedings of the 24th ACM International on Confer-
ence on Information and Knowledge Management.
pages 133–142.
</p>
<p>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems 20:116–131.
</p>
<p>Lucie Flekova and Iryna Gurevych. 2016. Supersense
embeddings: A unified model for supersense inter-
pretation, prediction, and utilization. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). pages 2029–2041.
</p>
<p>Sahar Ghannay, Benoit Favre, Yannick Estve, and
Nathalie Camelin. 2016. Word embedding evalua-
tion and combination. In Proceedings of the Tenth
International Conference on Language Resources
and Evaluation (LREC 2016).
</p>
<p>125</p>
<p />
</div>
<div class="page"><p />
<p>Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with gen-
uine similarity estimation. Computational Linguis-
tics 41:665–695.
</p>
<p>Matthew Hoffman, Francis R. Bach, and David M.
Blei. 2010. Online learning for latent dirichlet al-
location. In Advances in Neural Information Pro-
cessing Systems 23. pages 856–864.
</p>
<p>Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1. pages 873–882.
</p>
<p>Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity. In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). pages
95–105.
</p>
<p>Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense repre-
sentation learning for semantic vector space models.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 683–693.
</p>
<p>Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems
27. pages 2177–2185.
</p>
<p>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics 3:211–225.
</p>
<p>Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1722–1732.
</p>
<p>Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2015a.
Learning context-sensitive word embeddings with
neural tensor skip-gram model. In Proceedings of
the 24th International Conference on Artificial In-
telligence. pages 1284–1290.
</p>
<p>Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015b. Topical word embeddings. In AAAI
Conference on Artificial Intelligence. pages 2418–
2424.
</p>
<p>Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning. pages 104–113.
</p>
<p>Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). pages 1064–1074.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. pages
3111–3119.
</p>
<p>Ashutosh Modi. 2016. Event embeddings for seman-
tic script modeling. In Proceedings of the Confer-
ence on Computational Natural Language Learning.
pages 75–83.
</p>
<p>Ashutosh Modi and Ivan Titov. 2014. Inducing neu-
ral models of script knowledge. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning. pages 49–57.
</p>
<p>Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Say-
eed, and Manfred Pinkal. 2017. Modelling seman-
tic expectation: Using script knowledge for refer-
ent prediction. Transactions of the Association for
Computational Linguistics 5:31–44.
</p>
<p>Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). pages 1059–1069.
</p>
<p>Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015a. Improving Topic Models
with Latent Feature Word Representations. Trans-
actions of the Association for Computational Lin-
guistics 3:299–313.
</p>
<p>Dat Quoc Nguyen, Mark Dras, and Mark Johnson.
2017. A Novel Neural Network Model for Joint
POS Tagging and Graph-based Dependency Pars-
ing. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Uni-
versal Dependencies.
</p>
<p>Dat Quoc Nguyen, Kairit Sirts, and Mark Johnson.
2015b. Improving Topic Coherence with Latent
Feature Word Representations in MAP Estimation
for Topic Modeling. In Proceedings of the Aus-
tralasian Language Technology Association Work-
shop 2015. pages 116–121.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2014). pages 1532–
1543.
</p>
<p>126</p>
<p />
</div>
<div class="page"><p />
<p>Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conflated semantic representations. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing. pages 1680–1690.
</p>
<p>Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers. pages
141–150.
</p>
<p>Pushpendre Rastogi, Benjamin Van Durme, and Ra-
man Arora. 2015. Multiview LSA: Representation
Learning via Generalized CCA. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. pages 556–566.
</p>
<p>Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. pages 45–50.
</p>
<p>Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
pages 109–117.
</p>
<p>Sascha Rothe and Hinrich Schütze. 2015. Autoex-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, Vol-
ume 1: Long Papers. pages 1793–1803.
</p>
<p>Tobias Schnabel, Igor Labutov, David Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing. pages 298–307.
</p>
<p>Roy Schwartz, Roi Reichart, and Ari Rappoport. 2015.
Symmetric pattern based word embeddings for im-
proved word similarity prediction. In Proceedings
of CoNLL 2015. pages 258–267.
</p>
<p>Cyrus Shaoul and Chris Westbury. 2010. The westbury
lab wikipedia corpus. Edmonton, AB: University of
Alberta .
</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing. pages 1631–1642.
</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems. pages 3104–3112.
</p>
<p>Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING 2014, the 25th In-
ternational Conference on Computational Linguis-
tics: Technical Papers. pages 151–160.
</p>
<p>Luke Vilnis and Andrew McCallum. 2015. Word rep-
resentations via gaussian embedding. International
Conference on Learning Representations (ICLR) .
</p>
<p>Zhaohui Wu and C. Lee Giles. 2015. Sense-aware se-
mantic analysis: A multi-prototype word represen-
tation model using wikipedia. In Proceedings of the
Twenty-Ninth AAAI Conference on Artificial Intelli-
gence. pages 2188–2194.
</p>
<p>Heng Zhang and Guoqiang Zhong. 2016. Improv-
ing short text classification by learning vector
representations of both words and hidden topics.
Knowledge-Based Systems 102:76–86.
</p>
<p>127</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 128–134,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Aligning Script Events with Narrative Texts
</p>
<p>Simon Ostermann† Michael Roth†‡ Stefan Thater† Manfred Pinkal†
† Saarland University ‡ University of Illinois at Urbana-Champaign
{simono|mroth|stth|pinkal}@coli.uni-saarland.de
</p>
<p>Abstract
</p>
<p>Script knowledge plays a central role in
text understanding and is relevant for a va-
riety of downstream tasks. In this paper, we
consider two recent datasets which provide
a rich and general representation of script
events in terms of paraphrase sets. We in-
troduce the task of mapping event mentions
in narrative texts to such script event types,
and present a model for this task that ex-
ploits rich linguistic representations as well
as information on temporal ordering. The
results of our experiments demonstrate that
this complex task is indeed feasible.
</p>
<p>1 Introduction
</p>
<p>Event structure is a prominent topic in NLP. While
semantic role labelers (Gildea and Jurafsky, 2002;
Palmer et al., 2010) are well-established tools for
the analysis of the internal structure of event de-
scriptions, modeling relations between events has
gained increasing attention in recent years. Re-
search on event coreference (Bejan and Harabagiu,
2010; Lee et al., 2012), temporal event ordering
in newswire texts (Ling and Weld, 2010), as well
as shared tasks on cross-document event ordering
(Minard et al., 2015, inter alia) have in common
that they model cross-document relations.
</p>
<p>The focus of this paper is on the task of analyzing
text-internal event structure. We share the view of a
long tradition in NLP (see e.g. Schank and Abelson
(1975); Chambers and Jurafsky (2009); Regneri
et al. (2010)) that script knowledge is of central im-
portance to this task, i.e. common-sense knowledge
about events and their typical order in everyday
activities (also referred to as scenarios, Barr and
Feigenbaum (1981)). Script knowledge guides ex-
pectation by predicting which type of event or dis-
course referent might be addressed next in a story
</p>
<p>(Modi et al., 2017), allows to infer missing events
from events explicitly mentioned (Chambers and
Jurafsky, 2009; Jans et al., 2012; Rudinger et al.,
2015), and to determine text-internal temporal or-
der (Modi and Titov, 2014; Frermann et al., 2014).
</p>
<p>We address the task of automatically mapping
narrative texts to scripts, which will leverage ex-
plicit script knowledge for the afore-mentioned as-
pects of text understanding, as well as for down-
stream tasks such as textual entailment, question
answering or paraphrase detection. We build on
the work of Regneri et al. (2010) and Wanzare et al.
(2016), who collect explicit script knowledge via
crowdsourcing, by asking people to describe ev-
eryday activities. These crowdsourced descriptions
form a basis for high-quality automatic extraction
of script structure without any human intervention
(Regneri et al., 2010; Wanzare et al., 2017). The
events of the resulting structure are defined as sets
of alternative realizations, which cover lexical vari-
ation and provide paraphrase information. To the
best of our knowledge, these advantages have not
been explicitly used elsewhere.
</p>
<p>Aligning script structures with texts is a complex
task. In a first attempt, we assume that three steps
are necessary to solve it, although in the long run,
an integrated approach will be preferable: First, the
script which is addressed by the event mention must
be identified. Second, it has to be decided whether
a verb denotes a script event at all. Finally, event
verbs need to be assigned a script-specific event
type label. This work focuses on the last two steps:
We use a corpus of narrative stories each of which
is centered around a specific script scenario, and
distinguish verbs related to the central script from
all other verb occurrences with a simple decision
tree classifier. We then train a sequence labeling
model only on crowdsourced script data and assign
event type labels to all script-related event verbs.
</p>
<p>Our results substantially outperform informed
</p>
<p>128</p>
<p />
</div>
<div class="page"><p />
<p>Yesterday was my sister's birthday. I decided to bake a cake.
</p>
<p>I looked up the recipe. In my kitchen, I mixed the ingredients.
</p>
<p>CHOOSE_
RECIPE
- find recipe
- look for recipe
</p>
<p>TAKE_INGR.
- take ing. from 
cupboard
- get eggs 
</p>
<p>BUY_INGR.
- buy flour
- go to store
</p>
<p>MIX_INGR.
- mix 
thoroughly
- use mixer
</p>
<p>BAKE
- put cake in 
oven
- bake batter
</p>
<p>Figure 1: An example of text-to-script mapping
with an excerpt of the BAKING A CAKE script and
a story snippet.
</p>
<p>baselines, in spite of the availability of only small
amounts of training data. In particular, we also
demonstrate the relevance of event ordering infor-
mation provided by script knowledge.
</p>
<p>Our code and all data and parameters that
are used are publicly available under https://
github.com/SimonOst.
</p>
<p>2 Task and Data
</p>
<p>As a basis for the task of text-to-script mapping,
we make use of two recently published datasets.
DeScript (Wanzare et al., 2016) is a collection of
crowdsourced linguistic descriptions of event pat-
terns for everyday activities, so called event se-
quence descriptions (ESDs). ESDs consist of short
telegram-style descriptions of single events (event
descriptions, ED). The textual order of EDs corre-
sponds to the temporal order of respective events,
i.e. temporal information is explicitly encoded. De-
Script contains 50 ESDs for each of 40 different
scenarios. Alongside the ESDs, it also provides
gold event paraphrase sets, i.e. clusters of all event
descriptions denoting the same event type, labeled
with the respective type.
</p>
<p>While DeScript is a source of structured script
knowledge, the InScript corpus (Modi et al., 2016)
provides us with the appropriate kind of narrative
texts. InScript is a collection of 910 stories cen-
tered around some specific scenario, for 10 of the
40 scenarios in DeScript, e.g. BAKING A CAKE,
RIDING A BUS, TAKING A SHOWER. All verbs
occurring in the texts are annotated with an event
type if they are relevant to the script instantiated by
the story; as non-script event otherwise.
</p>
<p>In the upper part of Fig. 1, you see the initial
fragment of a story about baking a cake; together
with a script excerpt in the lower part, depicted
by labeled event paraphrase sets. I looked up the
</p>
<p>recipe and I mixed the ingredients mention rele-
vant script events, and therefore should be labeled
with the indicated event types (CHOOSE RECIPE,
MIX INGREDIENTS). Fig. 1 also illustrates the po-
tential of text-to-script mapping: script knowledge
enables to predict that a baking event might be ad-
dressed next in the story. The verb was does not
denote an event at all, and decide is not part of the
BAKING A CAKE script, so they are assigned the la-
bel non-script event. Actually, InScript comes with
two additional categories of verbs (script-related
and script-evoking), which we subsume under non-
script event.
</p>
<p>The central task addressed in our paper, the au-
tomatic labeling of all script-relevant verbs in the
InScript text with a script-specific event type, uses
only DeScript data for training; event-type labels
of InScript are used for evaluation purposes only.
</p>
<p>3 Model
</p>
<p>Section 3.1 defines the central part of our system,
a sequence model for classifying script-relevant
verbs into scenario-specific event types. For full
automation of the text-to-script mapping, we de-
scribe in Section 3.2 a model for identifying script-
relevant verbs.
</p>
<p>3.1 Event Type Classification
</p>
<p>For identifying the correct event type given a script-
relevant verb, we leverage two types of information:
We require a representation for the meaning and
content of the event mention, which takes into ac-
count not only the verb, but also the persons and
objects involved in an event, i.e. the script partic-
ipants. In addition, we take event ordering infor-
mation into account, which helps to disambiguate
event mentions based on their local context. To
model both event types and sequences thereof, we
implement a linear-chain conditional random field
(CRF, Lafferty et al. (2001)). Our implementation
is based on the CRF++ toolkit1 and employs two
types of features:
</p>
<p>Sequential Feature. Our CRF model utilizes
event ordering information in the form of binary
indicator features that encode the co-occurrence of
two event type labels in sequence.
</p>
<p>Meaning Representation Features. Two fea-
ture types encode the meaning of a textual event
mention. One is a shallow form of representa-
tion derived from precomputed word embeddings
</p>
<p>1taku910.github.io/crfpp/
</p>
<p>129</p>
<p />
</div>
<div class="page"><p />
<p>(word2vec, Mikolov et al. (2013)). This feature
type captures distributional information of the verb
and its direct nominal dependents2, which we as-
sume to denote script participants, and is computed
by averaging over the respective word vector rep-
resentations.3 We use pretrained 300-dimensional
embeddings that are trained on the Google News
corpus.4As a more explicit but sparse form of con-
tent representation, we use as the other type of
feature the lemma of the verb, its indirect object
and its direct object.
</p>
<p>3.2 Identifying Script-Relevant Verbs
</p>
<p>We use a decision tree classifier for identifying
script-relevant verbs (J48 from the Weka toolkit,
Frank et al. (2016)) that takes into account four
classes: the three non-script event classes from In-
Script and one class for all event-verbs. At test
time, the three non-script event classes are merged
into one class. Due to the lack of non-script event
instances in DeScript, we train and test our model
on all verbs occurring in InScript. We use the fol-
lowing feature types:
</p>
<p>Syntactic Features. We employ syntactic fea-
tures for identifying verbs that only rarely denote
script events, independent of the scenario: a feature
for auxiliaries; for verbs that govern an adverbial
phrase (mostly if-clauses); a feature indicating the
number of direct and indirect objects; and a lex-
ical feature that checks if the verb belongs to a
predefined list of non-action verbs.
</p>
<p>Script Features. For finding verbs that match
the current script scenario, we employ two features:
a binary feature indicating whether the verb is used
in the ESDs for the given scenario; and a scenario-
specific tf–idf score that is computed by treating all
ESDs from a scenario as one document, summed
over the verb and its dependents. In Section 4.2, we
evaluate models with and without script features,
to test the impact of scenario-specific information.
</p>
<p>Frame Feature. We further employ frame-
semantic information because we expect script
events to typically evoke certain frames.We use
a state-of-the-art semantic role labeler (Roth, 2016;
Roth and Lapata, 2016) based on FrameNet (Rup-
</p>
<p>2For EDs, we use all mentioned head nouns.
3To emphasize the importance of the verb, we double its
</p>
<p>weight when averaging.
4Because our CRF model only supports nominal fea-
</p>
<p>tures, we discretize embeddings from code.google.com/
archive/p/word2vec/ by binning the component val-
ues into three intervals [−∞,−�], [−�, �], [�,∞]. The hyper-
parameter � is determined on a held-out development set.
</p>
<p>P R F1
Lemma 0.365 0.949 0.526
Our model 0.628 0.817 0.709
Our model (scen. indep.) 0.513 0.877 0.645
</p>
<p>Table 1: Identification of script-relevant verbs
within a scenario and independent of the scenario.
</p>
<p>penhofer et al., 2006) to predict frames for all verbs,
encoding the frame as a feature. We address spar-
sity of too specific frames by mapping all frames
to higher-level super frames using the framenet
querying package5.
</p>
<p>4 Evaluation
</p>
<p>4.1 Experimental Setup
</p>
<p>We evaluate our model for text-to-script mapping
based on the resources introduced in Section 2. We
process the InScript and DeScript data sets using
the Stanford Parser (Klein and Manning, 2003)6.
We further resolve pronouns in InScript using an-
notated coreference chains from the gold standard.
</p>
<p>We individually test the two components, i.e.
the identification of script-relevant verbs and event
classification. Experiments on the first sub-task
are described in Section 4.2. Sections 4.3 and 4.4
present results on the latter task and a combination
of both tasks, respectively.
</p>
<p>4.2 Identifying Script-Relevant Verbs
</p>
<p>In this evaluation, we test the ability of our model
to identify verbs in narrative texts that instanti-
ate script events. Our experiments make use of
a 10-fold cross-validation setting within all texts
of one scenario. To test the model in a scenario-
independent setting, we perform additional experi-
ments based on a cross-validation with the 10 sce-
narios as one fold each and exclude the script fea-
tures. That is, we repeatedly train our model on 9
scenarios and evaluate on the remaining scenario,
without using any information about the test sce-
nario.
</p>
<p>Models. We compare the model described in
Section 3.2 to a baseline (Lemma) that always as-
signs the event class if the verb lemma is mentioned
in DeScript. We report precision, recall and F1-
score on event verbs, averaged over all scenarios.
</p>
<p>5github.com/icsi-berkeley/framenet
6To improve performance on the simplistic sentences from
</p>
<p>DeScript, we follow Regneri (2013) and re-train the parser.
</p>
<p>130</p>
<p />
</div>
<div class="page"><p />
<p>Results. Table 1 gives an overview of the results
based on 10-fold cross-validation. Our scenario-
specific model is capable of identifying more than
81% of script-relevant verbs at a precision of about
63%. This is a notable improvement over the base-
line, which identifies 94.9% of the event verbs, but
at a precision of only 36.5%.
</p>
<p>The table also gives numbers for the scenario-
independent setting: Precision drops to around 51%
if only training data from other scenarios is avail-
able. One of the main difficulties here lies in clas-
sifying different non-script event verb classes in a
way that generalizes across scenarios. Modi et al.
(2016) also found that distinguishing specific types
of non-script events from script events can be diffi-
cult even for humans.
</p>
<p>4.3 Event Type Classification
In this section, we describe experiments on the
text-to-script mapping task based on the subset of
event instances from InScript that are annotated as
script-related. As training data, we use the ESDs
and the event type annotations from the DeScript
gold standard7. The evaluation task is to classify
individual event mentions in InScript based on their
verbal realization in the narrative text. We evaluate
against the gold-standard annotations from InScript.
Since event type annotations are used for evaluation
purposes only, this task comes close to a realistic
setup, in which script knowledge is available for
specific scenarios but no training data in the form
of event-type annotated narrative texts exists.
</p>
<p>Models. We evaluate our CRF model described
in Section 3.1 against two baselines that are based
on textual similarity. Both baselines compare the
event verb and its dependents in InScript to all EDs
in DeScript and assign the event type with the high-
est similarity. Lemma is a simple measure based on
word overlap, word2vec uses the same embedding
representation as the CRF model (before discretiza-
tion) but simply assigns the best matching event
type label based on cosine similarity. We report pre-
cision, recall and F1-scores, macro-averaged over
all script-event types and scenarios.
</p>
<p>Results. Results for all models are presented in
Table 2. Our CRF model achieves a F1-score of
0.545, a considerably higher performance in com-
parison to the baselines. As can be seen from ex-
cluding the sequential feature, ordering information
</p>
<p>7In DeScript, there are some rare cases of EDs that do not
describe a script event, but that are labeled as non-script event.
We exclude these from the training data.
</p>
<p>P R F1
Lemma 0.343 0.416 0.374
Word2vec 0.356 0.448 0.395
CRF model 0.608 0.496 0.545
CRF, no seq. 0.599 0.487 0.536
</p>
<p>Table 2: Event Type Classification performance,
with and without sequential features.
</p>
<p>P R F1
Ident. model+Lemma 0.253 0.451 0.323
Ident. model+Word2vec 0.255 0.477 0.331
Ident. model+CRF model 0.445 0.520 0.479
</p>
<p>Table 3: Full text-to-script mapping results.
</p>
<p>improves the result. The rather small difference is
due to the fact that ordering information can also
be misleading (cf. Section 5). We found, however,
that including the sequential feature accounts for
an improvement of up to 4% in F1 score, depending
on the scenario.
</p>
<p>4.4 Full Text-to-Script Mapping Task
</p>
<p>We now address the full text-to-script mapping task,
a combination of the identification of relevant verbs
and event type classification. This setup allows
us to assess whether the general task of a fully
automatic mapping of verbs in narrative texts to
script events is feasible.
</p>
<p>Models. We compare the same models as in
Section 4.3, but use them on top of our model for
identifying script-relevant verbs (cf. Section 4.2)
instead of using the gold standard for identification.
</p>
<p>Results. On the full text-to-script mapping
task, our combined identification and CRF model
achieves a precision and recall of 0.445 and 0.52,
resp. (cf. Table 3). This reflects an absolute im-
provement over the baselines of 0.148 and 0.156
in terms of F1-score. The results reflect the general
difficulty of this task but are promising overall. As
reported by Modi et al. (2016), even human anno-
tators only achieve an agreement of 0.64 in terms
of Fleiss’ Kappa (1971).
</p>
<p>5 Discussion
</p>
<p>In this section, we discuss cases in which our sys-
tem predicted the wrong event type and give exam-
ples for each case. We identified three major error
sources:
</p>
<p>131</p>
<p />
</div>
<div class="page"><p />
<p>Lexical Coverage. We found that although De-
Script is a small resource, training a model purely
on ESDs works reasonably well. Coverage prob-
lems can be seen in cases of events for which only
few EDs exist. An example is the CHOOSE TREE
event (the event of picking a tree at the shop) in the
PLANTING A TREE scenario. There are only 3 EDs
describing the event, each of which uses the event
verb “choose”. In contrast, we find that “choose”
is used in less than 10% of the event mentions in
InScript. Because of this mismatch, which can be
attributed to the small training data size, more fre-
quently used verbs for this event in InScript, such
as “pick” and “decide”, are labeled incorrectly.
</p>
<p>We observe that our meaning representation
might be insufficient for finding synonyms for
about 30% of observed verb tokens. This specif-
ically includes scenario-specific and uncommon
verbs, such as “squirt” in the context of the BAK-
ING A CAKE scenario (squirt the frosting onto the
cake). Problems may also arise from the fact that
about 23% of the verb types occur in multiple para-
phrase clusters of a scenario.
</p>
<p>Misleading Ordering Information. We found
that ordering information is in general beneficial
for text-to-script alignment. We however also iden-
tified cases for which it can be misleading, by com-
paring the output of our full model to the model
that does not use sequential features. As another
result of the small size of DeScript, there are plau-
sible event sequences that appear only rarely or
never in the training data. This error source is
involved in 60–70% of the observed misclassifi-
cations due to misleading ordering information.
An example is the WASH event in the GETTING A
HAIRCUT scenario: It never appears directly after
the MOVE IN SALON event (i.e. walking from the
counter to the chair) in DeScript, but its a plausible
sequence that is misclassified by our model.
</p>
<p>In almost 15% of the observed errors, an event
type is mentioned more than once, leading to mis-
classifications whenever ordering information is
used. One reason for this might be that events in
InScript are described in a more exhaustive or fine-
grained way. For example, the WASH event in the
TAKING A BATH scenario is often broken up into
three mentions: wetting the hair, applying sham-
poo, and washing it again. However, because there
is only one event type for the three mentions, this
sequence is never observed in DeScript.
</p>
<p>Events with an interchangeable natural order
</p>
<p>lead to errors in a number of cases: In the BAKING
A CAKE scenario, a few misclassifications happen
because the order in which e.g. ingredients are pre-
pared, the pan is greased and the oven is preheated
is very flexible, but the model overfits to what it
observed from the training.
</p>
<p>As last, there are also a few cases in which an
event is mentioned, even before it actually takes
place. In the case of the borrowing a book scenario,
there are cases in InScript that mention in the first
sentence that the purpose of the visit is to return a
book. In DeScript in contrast, the RETURN event
always takes place in the very end.
</p>
<p>Near Misses. For many verbs, it is also difficult
for humans to come up with one correct event la-
bel. By investigating confusion matrices for single
scenarios, we found that for at least 3–5% of script
event verbs in the test set, our model predicted
an “incorrect” label for such verbs, but that label
might still be plausible. In the BAKING A CAKE
scenario, for example, there is little to no differ-
ence between mentions of making the dough and
preparing ingredients. As a consequence, these two
events are often confused: Approximately 50% of
the instances labeled as PREPARE INGREDIENTS
are actually instances of MAKE DOUGH.
</p>
<p>6 Summary
</p>
<p>In this paper, we addressed the task of automati-
cally mapping event denoting expressions in nar-
rative texts to script events, based on an explicit
script representation that is learned from crowd-
sourced data rather than from text collections. Our
models outperform two similarity-based baselines
by leveraging rich event representations and or-
dering information. We showed that models of
script knowledge can be successfully trained on
crowdsourced data, even if the number of training
examples is small. This work thus builds a basis
for utilizing the advantages of crowdsourced script
representations for downstream tasks and future
work, e.g. paraphrase identification in discourse
context or event prediction on narrative texts.
</p>
<p>Acknowledgments
</p>
<p>We thank the anonymous reviewers for their help-
ful comments. This research was funded by the
German Research Foundation (DFG) as part of
SFB 1102 ‘Information Density and Linguistic En-
coding’. Work by MR in Illinois was supported by
a DFG Research Fellowship (RO 4848/1-1).
</p>
<p>132</p>
<p />
</div>
<div class="page"><p />
<p>References
Avron Barr and Edward A. Feigenbaum. 1981. The
</p>
<p>Handbook of Artificial Intelligence. Addison-
Wesley.
</p>
<p>Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Un-
supervised event coreference resolution with rich lin-
guistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 1412–1422.
</p>
<p>Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP .
</p>
<p>Joseph L Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin
76(5):378.
</p>
<p>Eibe Frank, Mark A. Hall, and Ian H. Witten. 2016.
The weka workbench. online appendix for ”data
mining: Practical machine learning tools and tech-
niques” .
</p>
<p>Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014.
A hierarchical bayesian model for unsupervised in-
duction of script knowledge. In EACL. volume 14,
pages 49–57.
</p>
<p>Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational linguistics
28(3):245–288.
</p>
<p>Bram Jans, Steven Bethard, Ivan Vulić, and
Marie Francine Moens. 2012. Skip n-grams
and ranking functions for predicting script events.
In Proceedings of the 13th Conference of the
European Chapter of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics, pages 336–344.
</p>
<p>Dan Klein and Christopher D. Manning. 2003. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In In Advances in Neural Informa-
tion Processing Systems 15 (NIPS. MIT Press, pages
3–10.
</p>
<p>John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning. ICML
’01, pages 282–289.
</p>
<p>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Associ-
ation for Computational Linguistics, pages 489–500.
</p>
<p>Xiao Ling and Daniel S Weld. 2010. Temporal infor-
mation extraction. In AAAI. volume 10, pages 1385–
1390.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR .
</p>
<p>Anne-Lyse Minard, Manuela Speranza, Eneko
Agirre, Itziar Aldabe, Marieke van Erp, Bernardo
Magnini, German Rigau, Ruben Urizar, and Fon-
dazione Bruno Kessler. 2015. Semeval-2015 task
4: Timeline: Cross-document event ordering. In
Proceedings of the 9th International Workshop
on Semantic Evaluation (SemEval 2015). pages
778–786.
</p>
<p>Ashutosh Modi, Tatjana Anikina, Simon Ostermann,
and Manfred Pinkal. 2016. Inscript: Narrative texts
annotated with script information. Proceedings of
the 10th International Conference on Language Re-
sources and Evaluation (LREC 16) .
</p>
<p>Ashutosh Modi and Ivan Titov. 2014. Inducing neu-
ral models of script knowledge. In Proceedings
of the Conference on Computational Natural Lan-
guage Learning (CoNLL). Baltimore, MD, USA.
</p>
<p>Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Say-
eed, and Manfred Pinkal. 2017. Modelling seman-
tic expectation: Using script knowledge for referent
prediction. Transactions of the Association for Com-
putational Linguistics 5:31–44.
</p>
<p>Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Semantic role labeling. Synthesis Lectures on Hu-
man Language Technologies 3(1):1–103.
</p>
<p>Michaela Regneri. 2013. Event Structures in Knowl-
edge, Pictures and Text. Ph.D. thesis, Universität
des Saarlandes.
</p>
<p>Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACL ’10, pages 979–988.
</p>
<p>Michael Roth. 2016. Improving frame semantic pars-
ing via dependency path embeddings. In Book of Ab-
stracts of the 9th International Conference on Con-
struction Grammar. Juiz de Fora, Brazil, pages 165–
167.
</p>
<p>Michael Roth and Mirella Lapata. 2016. Neural seman-
tic role labeling with dependency path embeddings.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Berlin, Germany, pages 1192–1202.
</p>
<p>Rachel Rudinger, Vera Demberg, Ashutosh Modi,
Benjamin Van Durme, and Manfred Pinkal. 2015.
Learning to predict script events from domain-
specific text. Lexical and Computational Semantics
(* SEM 2015) page 205.
</p>
<p>133</p>
<p />
</div>
<div class="page"><p />
<p>Josef Ruppenhofer, Michael Ellsworth, Miriam RL
Petruck, Christopher R Johnson, and Jan Scheffczyk.
2006. Framenet ii: Extended theory and practice.
</p>
<p>Roger C Schank and Robert P Abelson. 1975. Scripts,
plans, and knowledge. Yale University New Haven,
CT.
</p>
<p>Lilian D. A. Wanzare, Alessandra Zarcone, Stefan
Thater, and Manfred Pinkal. 2016. A crowdsourced
database of event sequence descriptions for the ac-
quisition of high-quality script knowledge. Proceed-
ings of the Tenth International Conference on Lan-
guage Resources and Evaluation (LREC’16) .
</p>
<p>Lilian D. A. Wanzare, Alessandra Zarcone, Stefan
Thater, and Manfred Pinkal. 2017. Inducing script
structure from crowdsourced event descriptions via
semi-supervised clustering. Proceedings of the 2nd
Workshop on Linking Models of Lexical, Sentential
and Discourse-level Semantics .
</p>
<p>134</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 135–148,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>The (Too Many) Problems of Analogical Reasoning with Word Vectors
</p>
<p>Anna Rogers
Dept. of Computer Science
</p>
<p>University of Massachusetts Lowell
</p>
<p>Lowell, MA, USA
</p>
<p>arogers@cs.uml.edu
</p>
<p>Aleksandr Drozd
Global Scient. Inf. and Comput. Center
</p>
<p>Tokyo Institute of Technology
</p>
<p>Tokyo, Japan
</p>
<p>alex@smg.is.titech.ac.jp
</p>
<p>Bofang Li
School of Information
</p>
<p>Renmin University of China
</p>
<p>Beijing, China
</p>
<p>libofang@ruc.edu.cn
</p>
<p>Abstract
</p>
<p>This paper explores the possibilities of
analogical reasoning with vector space
models. Given two pairs of words with
the same relation (e.g. man:woman ::
king:queen), it was proposed that the off-
set between one pair of the corresponding
word vectors can be used to identify the
unknown member of the other pair (
</p>
<p>−−→
king
</p>
<p>− −−→man + −−−−−→woman = ?−−−→queen). We ar-
gue against such “linguistic regularities”
as a model for linguistic relations in vector
space models and as a benchmark, and we
show that the vector offset (as well as two
other, better-performing methods) suffers
from dependence on vector similarity.
</p>
<p>1 Introduction
</p>
<p>This paper considers the phenomenon of “vector-
oriented reasoning” via linear vector offset in
vector space models (VSMs) (Mikolov et al.,
2013c,a). Given two pairs of words with the same
linguistic relation (woman:man :: king:queen), it
has been proposed that the offset between one pair
of word vectors can be used to identify the un-
known member of a different pair of words via
solving proportional analogy problems (
</p>
<p>−−→
king −−−→man + −−−−−→woman = ?−−−→queen), as shown in Fig. 1.
</p>
<p>We will refer to this method as 3CosAdd.
This approach attracted a lot of attention, both
</p>
<p>as the “poster child” of word embeddings, and
for its potential practical utility. Given the vi-
tal role that analogical reasoning plays in human
cognition for discovering new knowledge and un-
derstanding new concepts, automated analogical
reasoning could become a game-changer in many
fields, providing a universal mechanism for detect-
ing linguistic relations (Turney, 2008) and word
sense disambiguation (Federici et al., 1997). It is
</p>
<p>already used in many downstream NLP tasks, such
as splitting compounds (Daiber et al., 2015), se-
mantic search (Cohen et al., 2015), cross-language
relational search (Duc et al., 2012), to name a few.
</p>
<p>Figure 2: Left panel shows vector offsets for three word
pairs illustrating the gender relation. Right panel shows
a different projection, and the singular/plural relation for
two words. In high-dimensional space, multiple relations
can be embedded for a single word.
</p>
<p>provided. We have explored several related meth-
ods and found that the proposed method performs
well for both syntactic and semantic relations. We
note that this measure is qualitatively similar to rela-
tional similarity model of (Turney, 2012), which pre-
dicts similarity between members of the word pairs
(xb, xd), (xc, xd) and dis-similarity for (xa, xd).
</p>
<p>6 Experimental Results
</p>
<p>To evaluate the vector offset method, we used
vectors generated by the RNN toolkit of Mikolov
(2012). Vectors of dimensionality 80, 320, and 640
were generated, along with a composite of several
systems, with total dimensionality 1600. The sys-
tems were trained with 320M words of Broadcast
News data as described in (Mikolov et al., 2011a),
and had an 82k vocabulary. Table 2 shows results
for both RNNLM and LSA vectors on the syntactic
task. LSA was trained on the same data as the RNN.
We see that the RNN vectors capture significantly
more syntactic regularity than the LSA vectors, and
do remarkably well in an absolute sense, answering
more than one in three questions correctly. 2
</p>
<p>In Table 3 we compare the RNN vectors with
those based on the methods of Collobert and We-
ston (2008) and Mnih and Hinton (2009), as imple-
mented by (Turian et al., 2010) and available online
3 Since different words are present in these datasets,
we computed the intersection of the vocabularies of
the RNN vectors and the new vectors, and restricted
the test set and word vectors to those. This resulted
in a 36k word vocabulary, and a test set with 6632
</p>
<p>2Guessing gets a small fraction of a percent.
3http://metaoptimize.com/projects/wordreprs/
</p>
<p>Method Adjectives Nouns Verbs All
LSA-80 9.2 11.1 17.4 12.8
LSA-320 11.3 18.1 20.7 16.5
LSA-640 9.6 10.1 13.8 11.3
RNN-80 9.3 5.2 30.4 16.2
RNN-320 18.2 19.0 45.0 28.5
RNN-640 21.0 25.2 54.8 34.7
RNN-1600 23.9 29.2 62.2 39.6
</p>
<p>Table 2: Results for identifying syntactic regularities for
different word representations. Percent correct.
</p>
<p>Method Adjectives Nouns Verbs All
RNN-80 10.1 8.1 30.4 19.0
CW-50 1.1 2.4 8.1 4.5
CW-100 1.3 4.1 8.6 5.0
HLBL-50 4.4 5.4 23.1 13.0
HLBL-100 7.6 13.2 30.2 18.7
</p>
<p>Table 3: Comparison of RNN vectors with Turian’s Col-
lobert and Weston based vectors and the Hierarchical
Log-Bilinear model of Mnih and Hinton. Percent correct.
</p>
<p>questions. Turian’s Collobert and Weston based vec-
tors do poorly on this task, whereas the Hierarchical
Log-Bilinear Model vectors of (Mnih and Hinton,
2009) do essentially as well as the RNN vectors.
These representations were trained on 37M words
of data and this may indicate a greater robustness of
the HLBL method.
</p>
<p>We conducted similar experiments with the se-
mantic test set. For each target word pair in a rela-
tion category, the model measures its relational sim-
ilarity to each of the prototypical word pairs, and
then uses the average as the final score. The results
are evaluated using the two standard metrics defined
in the task, Spearman’s rank correlation coefficient
ρ and MaxDiff accuracy. In both cases, larger val-
ues are better. To compare to previous systems, we
report the average over all 69 relations in the test set.
</p>
<p>From Table 4, we see that as with the syntac-
tic regularity study, the RNN-based representations
perform best. In this case, however, Turian’s CW
vectors are comparable in performance to the HLBL
vectors. With the RNN vectors, the performance im-
proves as the number of dimensions increases. Sur-
prisingly, we found that even though the RNN vec-
</p>
<p>Figure 1: Linguistic relations modeled by linear
vector offset (Mikolov et al., 2013c)
</p>
<p>The idea that linguistic relations are mirrored in
neat geometrical relations (as shown in Fig. 1) is
also intuitively appealing, and 3CosAdd has be-
come a popular benchmark. Roughly, the current
VSMs score between 40% (Lai et al., 2016) and
75% (Pennington et al., 2014) on the Google test
set (Mikolov et al., 2013a). However, in fact per-
formance varies widely for different types of re-
lations (Levy and Goldberg, 2014; Köper et al.,
2015; Gladkova et al., 2016).
</p>
<p>One way to explain the current limitations is to
attribute them to the imperfections of the current
models and/or corpora with which they are built:
with this view, in a perfect VSM, any linguistic
relation should be recoverable via vector offset.
</p>
<p>The alternative to be explored in this paper is
that perhaps natural language semantics is more
complex than suggested by Fig. 1, and there may
be both theoretical and mathematical issues with
analogical reasoning with word vectors and its
3CosAdd implementation.
</p>
<p>We present a series of experiments with two
popular VSMs (GloVe and Word2Vec) to show
that the accuracy of 3CosAdd depends on the
proximity of the target vector to its source (i.e.
</p>
<p>135</p>
<p />
</div>
<div class="page"><p />
<p>−−−→queen should be quite similar to −−→king). Since
not all linguistic relations can be expected to result
in high word vector proximity, the method is lim-
ited to those that happen to be so in a given VSM.
Furthermore, its accuracy also varies because the
“linguistic regularities” are actually not so regu-
lar, and should not be expected to be so. We also
compare 3CosAdd to two alternative methods to
investigate whether better algorithms can improve
on these and other accounts.
</p>
<p>2 Background: “Relational Similarity”
vs “Word Analogies”
</p>
<p>The most fundamental term for what 3CosAdd is
supposed to capture is actually not analogy, but
rather relational similarity, i.e. the idea that pairs
of words may hold similar relations to those be-
tween other pairs of words. For example, the re-
lation between cat and feline is similar to the re-
lation between dog and canine. Notably, this is
similarity rather than identity: “instances of a sin-
gle relation may still have significant variability in
how characteristic they are of that class” (Jurgens
et al., 2012).
</p>
<p>Analogy as it is known in philosophy and logic
is something quite different. The “classical” ana-
logical reasoning follows roughly this template:
objects X and Y share properties a, b, and c; there-
fore, they may also share the property d. For ex-
ample, both Earth and Mars orbit the Sun, have at
least one moon, revolve on axis, and are subject to
gravity; therefore, if Earth supports life, so could
Mars (Bartha, 2016).
</p>
<p>The NLP move from relational similarity to
analogy follows the use of the term by P. Turney,
who distinguishes between attributional similarity
between two words and relational similarity be-
tween two pairs of words. On this interpretation,
two word pairs that have a high degree of rela-
tional similarity are analogous (Turney, 2006).
</p>
<p>In terms of practical NLP tasks, Turney et al.
(2003) introduced the task of solving SAT1 anal-
ogy problems by choosing from several provided
options. These problems were formulated as pro-
portional analogies, written in the form a : a′ ::
b : b′ (a is to a′ as b is to b′)
</p>
<p>It is this use of the term “analogy” that Mikolov
et al. (2013c) followed in proposing the 3CosAdd
method. They formulated the task as selecting a
single best fitting vector out of the whole vocabu-
</p>
<p>1Scholastic Aptitude Test.
</p>
<p>lary of the VSM. It became known as word anal-
ogy task, but in its core it is still basically esti-
mation of relational similarity, and could be for-
mulated as such: given a pair of words a and a′,
find how they are related and then find word b′,
such that it has a similar relation with the word b.
A crucial difference is that the graded, non-binary
nature of relational similarity is now not in focus:
the goal is to find a single correct answer.
</p>
<p>The dataset that came to be known as the
Google analogy test set (Mikolov et al., 2013a),
included 14 linguistic relations with 19544 ques-
tions in total. It has become one of the most
popular benchmarks for VSMs. This evaluation
paradigm assumes that:
</p>
<p>(1) Words in similar linguistic relations should in
principle be recoverable via relational similar-
ity to known word pairs.
</p>
<p>(2) 3CosAdd score reflects the extent to which a
given VSM encodes linguistic relations.
</p>
<p>(1) became dubious when it was shown that ac-
curacy of 3CosAdd varies widely between cate-
gories (Levy and Goldberg, 2014), and even the
best-performing GloVe model scores under 30%
on the more challenging Bigger Analogy Test Set
(BATS) (Gladkova et al., 2016). It appears that
not all relations can be identified in this way, with
lexical semantic relations such as synonymy and
antonymy being particularly difficult (Köper et al.,
2015; Vylomova et al., 2016). The assumption of
a single best-fitting candidate answer is also being
targeted (Newman-Griffis et al., 2017).
</p>
<p>(2) was refuted when Drozd et al. (2016)
demonstrated that some relations missed by
3CosAdd could be recovered with a supervised
method, and therefore the information was present
in the VSM – just not recoverable with 3CosAdd.
</p>
<p>Let us consider why both (1) and (2) failed.
</p>
<p>3 What Does 3CosAdd Really Do?
</p>
<p>3.1 Methodology
</p>
<p>We present a series of experiments performed with
BATS dataset. Although there are more results on
analogy task published with Google test than with
BATS, Google test only contains 15 types of lin-
guistic relations, and these happen to be the easier
ones (Gladkova et al., 2016).
</p>
<p>Table 1 lists examples of each BATS category:
there are 50 word pairs for each of 40 linguistic
</p>
<p>136</p>
<p />
</div>
<div class="page"><p />
<p>Inflectional Nouns regular plurals (student:students), plurals with orthographic changes (wife:wives)
morphology Adjectives comparative degree (strong:stronger), superlative degree (strong:strongest)
</p>
<p>Verbs infinitive: 3Ps.Sg (follow:follows), infinitive: participle (follow:following), infinitive:
past (follow:followed), participle: 3Ps.Sg (following:follows), participle: past (follow-
ing:followed), 3Ps.Sg : past (follows:followed)
</p>
<p>Derivational
morphology
</p>
<p>Stem
change
</p>
<p>verb+er (bake:baker), verb+able (edit:editable), verb+ation (continue:continuation),
verb+ment (argue:argument)
</p>
<p>No stem
change
</p>
<p>re+verb (create:recreate), noun+less (home:homeless), adj.+ness (mad:madness),
un+adj. (able:unable), adj.+ly (usual:usually), over+adj. (used:overused)
</p>
<p>Lexicographic Hypernyms animals (turtle:reptile), miscellaneous (peach:fruit)
semantics Hyponyms miscellaneous (color:white)
</p>
<p>Meronyms part-whole (car:engine), substance (sea:water), member (player:team),
Antonyms opposites (up:down), gradable (clean:dirty)
Synonyms exact (sofa:couch), intensity (cry:scream)
</p>
<p>Encyclopedic Animals the young (cat:kitten), sounds (dog:bark), shelter fox:den
semantics Geography capitals (Athens:Greece), languages (Peru:Spanish), UK city:county York:Yorkshire
</p>
<p>People occupation (Lincoln:president), nationalities (Lincoln:American)
Other thing:color (blood:red), male:female (actor:actress)
</p>
<p>Table 1: The Bigger Analogy Test Set: categories and examples
</p>
<p>relations (98,000 questions in total). BATS covers
most relations in the Google set, but it adds many
new and more difficult relations, balanced across
derivational and inflectional morphology, lexico-
graphic and encyclopedic semantics (10 relations
of each type). Thus BATS provides a less flatter-
ing, but more accurate estimate of the capacity for
analogical reasoning in the current VSMs.
</p>
<p>We use pre-trained GloVe vectors by Penning-
ton et al. (2014), released by the authors2 and
trained on Gigaword 5 + Wikipedia 2014 (300 di-
mensions, window size 10). We also experiment
with Word2Vec vectors (Mikolov et al., 2013b) re-
leased by the authors3, trained on a subcorpus of
Google news (also with 300 dimensions).
</p>
<p>The evaluation with 3CosAdd and LRCos meth-
ods was conducted with the Python script that ac-
companies BATS. We also added an implementa-
tion of 3CosMul, a multiplicative objective pro-
posed by Levy and Goldberg (2014), now avail-
able in the same script4. Since 3CosMul requires
normalization, we used normalized GloVe and
Word2Vec vectors in all experiments.
</p>
<p>Questions with words not in the model vocab-
ulary were excluded (0.01% BATS questions for
GloVe and 0.016% for Word2Vec).
</p>
<p>3.2 The “Honest” 3CosAdd
</p>
<p>Let us remember that 3CosAdd as initially formu-
lated by Mikolov et al. (2013c) excludes the three
</p>
<p>2https://nlp.stanford.edu/projects/
glove/
</p>
<p>3https://code.google.com/archive/p/
word2vec/
</p>
<p>4http://vsm.blackbird.pw/tools/
</p>
<p>source vectors a, a′ and b from the pool of possi-
ble answers. Linzen (2016) showed that if that is
not done, the accuracy drops dramatically, hitting
zero for 9 out of 15 Google test categories.
</p>
<p>Let us investigate what happens on BATS data,
split by 4 relation types. The rows of Fig. 2
represent all questions of a given category, with
darker color indicating higher percentage of pre-
dicted vectors being the closest to a, a′, b, b′, or
any other vector.
</p>
<p>a a' b b' other
</p>
<p>Encyclopedia
</p>
<p>Lexicography
</p>
<p>Inflections
</p>
<p>Derivation
</p>
<p>0.00
</p>
<p>0.15
</p>
<p>0.30
</p>
<p>0.45
</p>
<p>0.60
</p>
<p>0.75
</p>
<p>0.90
</p>
<p>Figure 2: The result of a − a′ + b calculation on
BATS: source vectors a, a′, and b are not excluded.
</p>
<p>Fig. 2 shows that if we do not exclude the
source vectors, b is the most likely to be predicted;
in derivational and encyclopedic categories a′ is
also possible in under 30% of cases. b′ is as un-
likely to be predicted as a, or any other vector.
</p>
<p>This experiment suggests that the addition of
the offset between a and a′ typically has a very
small effect on the b vector – not sufficient to in-
duce a shift to a different vector on its own. This
would in effect limit the search space of 3CosAdd
to the close neighborhood of the b vector.
</p>
<p>It explains another phenomenon pointed out by
Linzen (2016): for the plural noun category in the
</p>
<p>137</p>
<p />
</div>
<div class="page"><p />
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) similarity between vectors a and a′
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(b) similarity between vectors a′ and b′
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(c) similarity between vectors b and b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(d) similarity between vector b′ and pre-
dicted vector
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(e) similarity between vector b′ and a
</p>
<p>0 10 20 30 40 50 60 70 80 900.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(f) rank of b in the neighborhood of b′
</p>
<p>*X-axis labels indicate lower boundary of the corresponding similarity/rank bins.
The numerical values for all data can be found in the Appendix.
</p>
<p>Figure 3: Accuracy of 3CosAdd method on GloVe vs characteristics of the vector space.
</p>
<p>Google test set 70% accuracy was achieved by
simply taking the closest neighbor of the vector
b, while 3CosAdd improved the accuracy by only
10%. That would indeed be expected if most sin-
gular (a) and plural (a′) forms of the same noun
were so similar, that subtracting them would result
in a nearly-null vector which would not change
much when added to b.
</p>
<p>3.3 Distance to the Target Vector
Levy and Goldberg (2014, p.173) suggested that
3CosAdd method is “mathematically equivalent to
seeking a word (b′) which is similar to b and a′ but
is different from a.” We examined the similarity
between all source vector pairs, looking not only
at the actual, top-1 accuracy of the 3CosAdd (i.e.
the vector the closest to the hypothetical vector),
but also at whether the correct answer was found
in the top-3 and top-5 neighbors of the predicted
vector. For each similarity bin we also estimated
how many questions of the whole BATS dataset
there were. The results are presented in Fig. 3.
</p>
<p>Our data indicates that, indeed, for all combina-
tions of source vectors, the accuracy of 3CosAdd
decreases as their distance in vector space in-
creases. It is the most successful when all three
source vectors are relatively close to each other
and the target vector. This is in line with the above
</p>
<p>evidence from the “honest” 3CosAdd: if the offset
is typically small, for it to lead to the target vector,
that target vector should be close.
</p>
<p>Consider also the ranks of the b vectors in the
neighborhood of b′, shown in Fig. 3f. For nearly
40% of the successful questions b′ was within 10
neighbors of b – and over 40% of low-accuracy
questions were over 90 neighbors away.
</p>
<p>As predicted by Levy et al., b′ and a vectors do
not exhibit the same clear trend for higher accu-
racy with higher similarity that is observed in all
other cases (Fig. 3f). However, in experiments
with only 20 morphological categories we did ob-
serve the same trend for b′ and a as for the other
vector pairs (see Fig. 4). This is counter-intuitive,
and requires further examination.
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Figure 4: The similarity between b′ and a on
GloVe: morphological BATS categories only.
</p>
<p>138</p>
<p />
</div>
<div class="page"><p />
<p>The observed correlation between the accuracy
of 3CosAdd and the distance to the target vec-
tor could explain in particular the overall lower
performance on BATS derivational morphology
questions (only 0.08% top-1 accuracy) as opposed
to inflectional (0.59%) or encyclopedic seman-
tics (0.26%). −−→man and −−−−−→woman could be ex-
pected to be reasonably similar distributionally,
as they combine with many of the same verbs:
both men and women sit, sleep, drink etc. How-
ever, the same could not be said of words derived
with prefixes that change part of speech. Going
from
</p>
<p>−−−→
happy to
</p>
<p>−−−−−−−→
happiness, or from −−−−→govern to−−−−−−−−→
</p>
<p>government, is likely to have to take us further
in the vector space.
</p>
<p>To make sure that the above trend is not spe-
cific to GloVe, we repeated these experiments with
Word2Vec, which exhibited the same trends. All
data is presented in Appendix A.1.
</p>
<p>3.4 Uniqueness of a Relation
</p>
<p>Note that the dependence of 3CosAdd on similar-
ity is not entirely straightforward: Fig. 3b shows
that for the highest similarity (0.9 and more) there
is actually a drop in accuracy. The same trend was
observed with Word2Vec (Fig 10 in Appendix 1).
Theoretically, it could be attributed to there not be-
ing much data in the highest similarity range; but
BATS has 98,000 questions, and even 0.1% of that
is considerable.
</p>
<p>The culprit is the “dishonesty” of 3CosAdd: as
discussed above, it excludes the source vectors a,
a′, and b from the pool of possible answers. Not
only does this mask the real extent of the differ-
ence between a and a′, but it also creates a funda-
mental difficulty with categories where the source
vectors may be the correct answers.
</p>
<p>This is what explains the unexpected drops in
accuracy at the highest similarity between vec-
tors b′ and a′. Consider the question
</p>
<p>−−−→
blood:
</p>
<p>−→
red
</p>
<p>:: −−−→snow:−−−−→?white. The vector offset could theoret-
ically solve it, but if the question is −−−→snow:−−−→white
:: −−−→sugar:−−−−→?white, the correct answer would a pri-
ori be excluded. In BATS data, this factor af-
fects several semantic categories, including coun-
try:language, thing:color, animal:young, and ani-
mal:shelter.
</p>
<p>3.5 Density of Vector Neighborhoods
</p>
<p>If solving proportional analogies with word vec-
tors is like shooting, the farther away the target
</p>
<p>vector is, the more difficult it should be to hit.
Also, we can hypothesize that the more crowded
a particular region is, the more difficult it should
be to hit a particular target.
</p>
<p>However, density of vector neighborhoods is
not as straightforward to measure as vector sim-
ilarity. We could look at average similarity be-
tween, e.g., top-10 ranking neighbors, but that
could misrepresent the situation if some neighbors
were very close and some were very far.
</p>
<p>In this experiment we estimate density as the
similarity to the 5th neighbor. The higher it is, the
more highly similar neighbors a word vector has.
This approach is shown in Fig. 5.
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Figure 5: The similarity between b′ and its 5th
neighbor
</p>
<p>The results seem counter-intuitive: denser
neighborhoods actually yield higher accuracy (al-
though there are virtually no cases of very tight
neighborhoods). One explanation could be its re-
verse correlation with distance: if the neighbor-
hood of b′ is sparse, the closest word is likely to be
relatively far away. But that runs contrary to the
above findings that closer source vectors improve
the accuracy of 3CosAdd. Then we could expect
lower accuracy in sparser neighborhoods.
</p>
<p>In this respect, too, GloVe and Word2Vec be-
have similarly (Fig. 15).
</p>
<p>4 Comparison with Other Methods
</p>
<p>We repeat the above experiments on GloVe with
3CosMul, a multiplication-based alternative to
3CosAdd proposed by Levy and Goldberg (2014):
</p>
<p>argmaxb′∈V
cos(b′, b)cos(b′, a′)
</p>
<p>cos(b′, a) + ε
</p>
<p>(ε = 0.001 is used to prevent division by zero)
</p>
<p>As 3CosMul does not explicitly calculate the
predicted vector, we did not plot the similarity of
b′ to the predicted vector. But for other vector
pairs shown in Fig. 6, we can see that 3CosMul,
</p>
<p>139</p>
<p />
</div>
<div class="page"><p />
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) similarity between vectors a and a′
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(b) similarity between vectors a′ and b′
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(c) similarity between vectors b and b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(d) similarity between vector b′ and a
</p>
<p>0 10 20 30 40 50 60 70 80 900.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>(e) rank of b in the neighborhood of b′
</p>
<p>*X-axis labels indicate lower boundary of the corresponding similarity/rank bins.
The numerical values for all data can be found in the Appendix.
</p>
<p>Figure 6: Accuracy of 3CosMul method on GloVe model vs characteristics of the vector space.
</p>
<p>like 3CosAdd, has much higher chances of success
where target vectors are close to the source.
</p>
<p>We also consider LRCos, a method based on su-
pervised learning from a set of word pairs (Drozd
et al., 2016). LRCos reinterprets the analogy
task as follows: given a set of word pairs (e.g.
brother:sister, husband:wife, man:woman, etc.),
the available examples of the class of the target
b′ vector (sister, wife, woman, etc.) and randomly
selected negative examples are used to learn a rep-
resentation of the target class with a supervised
classifier. The question is this: what word is the
closest to
</p>
<p>−−→
king, but belongs to the “women” class?
</p>
<p>With LRCos it is only meaningful to look at the
similarity of b to b′ (Fig. 7). Once again, we see
the same trend: closer targets are easier to hit.
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Figure 7: Accuracy of LRCos method vs similar-
ity between vectors b and b′
</p>
<p>However, if we look at overall accuracy, there is
a big difference between the three methods.
</p>
<p>Fig. 8b shows that the accuracy of LRCos is
much higher than the top-1 3CosAdd or 3Cos-
Mul. Moreover, its “honest” version (Fig. 8a) per-
forms just as well as the “dishonest” one. These
results are consistent with the results reported by
Drozd et al. (2016). As for 3CosMul, Levy et al.
(2015) show that 3CosMul outperforms 3CosAdd
in PPMI, SGNS, GloVe and SVD models with the
Google dataset, sometimes yielding 10-25% im-
provement. Our BATS experiment confirms the
overall superiority of 3CosMul to 3CosAdd, al-
though the difference is less dramatic.
</p>
<p>Thus LRCos considerably outdoes its competi-
tors, although it does not manage to avoid the sim-
ilarity problem. We attribute this to the set-based,
supervised nature of LRCos that gives it an edge
on a different problem that affects both 3CosAdd
and 3CosMul: the assumption of “linguistic regu-
larities” from which we started.
</p>
<p>5 Discussion: What Should We Expect
from the Word Analogy Task?
</p>
<p>5.1 How Regular Are “Linguistic
Regularities”?
</p>
<p>There are unresolved questions about the underly-
ing assumption that the offset between vectors a′
</p>
<p>140</p>
<p />
</div>
<div class="page"><p />
<p>All categories
</p>
<p>Encyclopedia
</p>
<p>Lexicography
</p>
<p>Derivation
</p>
<p>Inflections
</p>
<p>Category
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>1.
0
</p>
<p>ac
cu
</p>
<p>ra
cy
</p>
<p>3CosAdd (top 1)
3CosAdd (top 3)
3CosAdd (top 5)
3CosAdd (top 10)
3CosAdd (top 15)
</p>
<p>3CosMul (top 1)
3CosMul (top 3)
3CosMul (top 5)
3CosMul (top 10)
3CosMul (top 15)
</p>
<p>LRCos (top 1)
LRCos (top 3)
LRCos (top 5)
LRCos (top 10)
LRCos (top 15)
</p>
<p>(a) 3CosAdd vs 3CosMul vs LRCos (“honest”
versions)
</p>
<p>All categories
</p>
<p>Encyclopedia
</p>
<p>Lexicography
</p>
<p>Derivation
</p>
<p>Inflections
</p>
<p>Category
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>1.
0
</p>
<p>ac
cu
</p>
<p>ra
cy
</p>
<p>3CosAdd (top 1)
3CosAdd (top 3)
3CosAdd (top 5)
3CosAdd (top 10)
3CosAdd (top 15)
</p>
<p>3CosMul (top 1)
3CosMul (top 3)
3CosMul (top 5)
3CosMul (top 10)
3CosMul (top 15)
</p>
<p>LRCos (top 1)
LRCos (top 3)
LRCos (top 5)
LRCos (top 10)
LRCos (top 15)
</p>
<p>(b) CosAdd vs 3CosMul vs LRCos
</p>
<p>Figure 8: LRCos performance on BATS
</p>
<p>and a provides access to certain features combin-
able with vector b to detect b′, and that such offset
should be more or less constant for all words in a
given linguistic relations.
</p>
<p>Table 2 shows that this does not happen in a re-
liable way (data: BATS category D06 “re+verb”).
</p>
<p>Table 2: 3CosAdd: effect of various a : a′ vector
pairs with the same b : b′ pair (−−−−→marry:−−−−−−→remarry)
No a a′ b predicted
</p>
<p>vector
Sim.
score
</p>
<p>correct
b′ score
</p>
<p>1 acquire reacquire marry fiancée 0.54 &lt;0.51
2 tell retell marry betrothed 0.51 0.49
3 engage reengage marry eloped 0.52 0.51
4 appear reappear marry marries 0.65 0.55
5 establish reestablish marry marries 0.58 0.52
6 invest reinvest marry marries 0.59 0.57
7 adjust readjust marry marrying 0.59 0.55
8 arrange rearrange marry marrying 0.52 0.43
9 discover rediscover marry marrying 0.54 0.49
10 apply reapply marry remarry 0.53 0.53
</p>
<p>Both correct and incorrect answers lie in about
the same similarity range, so we cannot attribute
the failures to the reliance of 3CosAdd on close
neighborhoods. The distance from −−−−→marry to
</p>
<p>−−−−−−→remarry is the same; thus it must be the case
that the offset between different a and a′ is not
the same, and leads to different answers – with a
frustratingly small margin of error.
</p>
<p>5.2 Can We Just Blame the Corpus?
Source corpora are noisy, and it is tempting to
blame almost anything on that. It could be lit-
eral text-processing noise (e.g. not quite cleaned
HTML data and ad texts) or, more broadly, any
kind of information in the VSM that is irrelevant
to the question at hand. This includes polysemy:
for a word-level VSM the difference between
</p>
<p>−−→
king
</p>
<p>and−−−→queen is not exactly the same as the difference
between −−→man and −−−−−→woman just for the existence
of the Queen band (although that factor should not
affect the “re-” prefix verbs in Table 2).
</p>
<p>In addition to irrelevant information, there is
also missing information. Corpora of written texts
are a priori not the same source of input as what
children get when they learn their language. Natu-
ral language semantics relies on much data that the
current VSMs do not have, including multimodal
data and frequencies of events too commonplace
to be mentioned in writing (Erk, 2016, p.18).
</p>
<p>This means that the distributional difference be-
tween
</p>
<p>−→
tell and
</p>
<p>−−−→
retell (or −−−−→marry and −−−−−−→remarry,
</p>
<p>or both pairs) does not necessarily reflect the full
range of the relevant difference, which could per-
haps have helped to bring the vector offset calcula-
tion closer to the desired outcome. On this view, in
the ideal world all word vectors with the “re-” fea-
ture would be nearly aligned. Some blame could
also be passed to the condensed vectors such as
SVD or neural word embeddings, which blend dis-
tributional features in a non-transparent way, po-
tentially obscuring the relevant ones.
</p>
<p>The current source corpora and VSMs could
certainly be improved. But both linguistics and
philosophy suggest that there are also issues with
the idea of linguistic relations being so regular.
</p>
<p>5.3 Semantics is Messy
In theory, according to the distributional hypoth-
esis, we would expect the relatively straightfor-
ward “repeated action” paradigm of verbs with
and without the prefix “re-” in Table 2 to surface
distributionally in the use of adverbs like “again”.
However, we have no reason to expect this to hap-
pen in quantitatively exactly the same way for all
the verbs, even in an “ideal” corpus. And variation
would lead to irregularities that we observe.
</p>
<p>141</p>
<p />
</div>
<div class="page"><p />
<p>In fact, such variation would make VSMs more
like human mental lexicon, not less. A well-
known problem in psychology is the asymmetry of
similarity judgments, upon which relational simi-
larity and analogical reasoning are based. Logi-
cally a is like b is equivalent to b is like a, but
humans do not necessarily agree with both state-
ments to the same degree (Tversky, 1977).
</p>
<p>Consider the “re-” prefix examples above. We
could expect 100% success by native English
speakers on a “complete the verb paradigm” task,
because they would be inevitably made aware of
the “add re-” rule during its completion. Even so,
processing time would vary due to such factors
as frequencies and prototypicality. The psycho-
logical evidence is piling for certain gradedness
in mental representation of morphological rules:
people can rate the same structure differently on
complexity (“settlement” is reported more affixed
that “government”), similarity judgments for se-
mantically transparent and non-transparent bases
are continuous, and there are graded priming ef-
fects for both orthographic, semantic and phono-
logical similarity between derived words and their
roots (Hay and Baayen, 2005).
</p>
<p>There are several connectionist proposals to
simulate asymmetry through biases, saliency
features, or structural alignment (Thomas and
Mareschal, 1997, p.758). The irregularities we
observe in the VSMs could perhaps even be wel-
comed as another way to model this phenomenon
- although it remains to be seen to what extent the
parallel we draw here is appropriate.
</p>
<p>As a side note, let us remember that equations
such as
</p>
<p>−−→
king − −−→man + −−−−−→woman = −−−→queen should
</p>
<p>only be interpreted distributionally, although it is
tempting to suppose that they reflect something
like semantic features. That would be mislead-
ing on several accounts. First of all, the 3CosAdd
math is commutative, which would be dubious for
semantic features5. Secondly, it would bring us
to the wall that componential analysis in linguistic
semantics has hit a long time ago: semantic fea-
tures defy definitions6, they only apply to a por-
tion of vocabulary, and they impose binary op-
positions that are psycholinguistically unrealistic
(Leech, 1981, pp.117-119).
</p>
<p>5((−−−−−−→remarry −−−−−→marry) +−−−→write) makes some sense, but
((
−−−→
write − −−−−→marry) + −−−−−−→remarry) does not.
</p>
<p>6Is the−−→man−−−−−−→woman result certainly “femaleness” – or
perhaps “maleness”, or some mysterious “malefemale gender
change” semantic feature?
</p>
<p>5.4 Analogy Is Not an Inference Rule
</p>
<p>Let us now come back to the fact that the “linguis-
tic regularities” are in fact relying on relational
similarity (Section 2), and relational similarity is
not something binary. That takes us straight to the
most fundamental difficulty with analogy as it is
known in philosophy and logic. Analogy is unde-
niably fundamental to human reasoning as an in-
strument for discovery and understanding the un-
known from the known – but it is not, and has
never been an inference rule.
</p>
<p>Consider the example where Mars is similar to
Earth in several ways, and therefore could be sup-
porting life. This analogy does not guarantee the
existence of Martians, and it could even be simi-
larly applied to even less suitable planets.
</p>
<p>Basically, the problem with analogy is that not
all similarities warrant all conclusions, and estab-
lishing valid analogies requires much case-by-case
consideration. For this and some other reasons,
analogy has long been rejected in generative lin-
guistics as a mechanism for language acquisition
through discovery, although now it is making a
comeback (Itkonen, 2005, p.67-75).
</p>
<p>This general difficulty with analogical reason-
ing – it does work in humans, but selectively, so
to say, – is inherited by the so-called proportional
analogies of the a : a′ :: b : b′ kind. A case in
point is their use in schools as verbal reasoning
tests. In 2005 analogies were removed from SAT,
its criticisms including ambiguity, guesswork and
puzzle-like nature (Pringle, 2003). It is also telling
that SAT analogy problems came with a set of po-
tential answers to choose from, because otherwise
students would supply a range of answers with
varying degrees of incorrectness.
</p>
<p>In case of the “re-” prefix above, once again, we
could expect 100% success rate by humans who
could see the “add re-” pattern; but semantic BATS
questions would yield more variation. Consider
the question “trout is to river as lion is to ”.
Some would say den, thinking of the river as the
trout’s “home”, but some could say savanna in the
broader habitat terms; cage or zoo or safari park
or even circus would all be valid to various de-
grees. BATS accepts several answer options, but it
is hardly feasible to list them all for all cases.
</p>
<p>Given the above, the question is: if analogi-
cal reasoning requires much case-by-case consid-
eration in humans, what should we expect from
VSMs with a single linear algebra operation?
</p>
<p>142</p>
<p />
</div>
<div class="page"><p />
<p>6 Implications for Evaluation of VSMs
</p>
<p>The analogy task continues to enjoy immense pop-
ularity in the NLP community as the standard eval-
uation task for VSMs. We have already mentioned
two problems with the task: the problem of the
Google test scores being flattering to the VSMs
(Gladkova et al., 2016), and also 3CosAdd disad-
vantaging them, because the required semantic in-
formation may be encoded in more complex ways
(Drozd et al., 2016).
</p>
<p>What the present work adds to the discussion is
the demonstration of how strongly the accuracy on
the analogy task depends on the target vector being
relatively close to the source in the vector space
model – not only for 3CosAdd, but also 3CosMul
and LRCos. This is in fact a fundamental problem
that is encountered in many other NLP tasks7.
</p>
<p>That problem brings about the following ques-
tion: what have we been evaluating with 3CosAdd
all this time?
</p>
<p>The answer seems to be this: analogy task
scores indicate to what extent the semantic space
of a given VSM was structured in a way that, for
each word category, favored the linguistic relation
that happened to be picked by the creators of the
particular test dataset. BATS makes this clearer,
because it is well balanced across different types
of relations. Most models score well on morpho-
logical inflections – because morphological forms
of the same word are highly distributionally sim-
ilar and are likely to be close. But we do not
see equal success for synonyms, suffixes, colors
and other categories – because it is hard to ex-
pect of any one model to “guess” which words
should have synonyms as closest neighbors and
which words should be close to their antonyms.
</p>
<p>As a matter of fact, for a general-purpose VSM
we would not want that: every word can partic-
ipate in hundreds of linguistic relations that we
may be interested in, but we cannot expect them
all to be close neighbors. We would want a VSM
whose vector neighborhoods simply reflect what-
ever distributional properties were observed in a
corpus. The challenge is to find reasoning meth-
ods that could reliably identify linguistic relations
from vectors at any distance.
</p>
<p>Given the irregularities discussed in section 5,
</p>
<p>7E.g. in taxonomy construction it was found helpful to
narrow the semantic space with domains or clusters, essen-
tially “zooming in” on certain relations (Fu et al., 2014; Es-
pinosa Anke et al., 2016).
</p>
<p>these methods would also have to rely on a more
linguistically and cognitively realistic model of
how meanings are reflected in distributional prop-
erties of words.
</p>
<p>LRCos made a step in the right direction, as it
does not rely on unique and neatly aligned word
pairs, but it can only work for relations between
coherent word classes. That excludes many lexi-
cographic relations like synonyms (car is to auto-
mobile as snake is to serpent), frame-semantic or
encyclopedic relations (white is to snow as red is
to rose).
</p>
<p>7 Conclusion
</p>
<p>While it would be highly desirable to have au-
tomated reasoning about linguistic relations with
VSMs as a powerful, all-purpose tool, it is so far
a remote goal. We investigated the potential of
the vector offset method in solving the so-called
proportional analogies, which rely on one pair of
words with a known linguistic relation to identify
the missing member of another pair of words.
</p>
<p>We have presented a series of experiments
showing that the success of the linear vector off-
set (as well as two better-performing methods) de-
pends on the structure of the VSM: the targets that
are further away in the vector space have worse
chances of being recovered. This is a crucial lim-
itation: no model could possibly hold all related
words close in the vector space, as there are many
thousands of linguistic relations, and many are
context-dependent.
</p>
<p>Furthermore, the offsets of different word vec-
tor pairs appear to not be so regular, even for rel-
atively straightforward linguistic relations. We ar-
gue that the observed irregularities should not just
be blamed on the corpus. There is a number of the-
oretical issues with the very approach to linguistic
relations as something neat and binary. We hope
to drive attention to the graded nature of relational
similarity that underlies analogical reasoning, and
the need for automated reasoning algorithms to be-
come more psychologically plausible in order to
become more successful.
</p>
<p>Acknowledgements
</p>
<p>This work was partially supported by JST CREST
Grant number JPMJCR1303, JSPS KAKENHI
Grant number JP17K12739, and performed under
the auspices of Real-world Big-Data Computation
Open Innovation Laboratory, Japan.
</p>
<p>143</p>
<p />
</div>
<div class="page"><p />
<p>References
Paul Bartha. 2016. Analogy and analogical reason-
</p>
<p>ing. In Edward N. Zalta, editor, The Stanford
Encyclopedia of Philosophy, Metaphysics Research
Lab, Stanford University. Winter 2016 edition.
https://plato.stanford.edu/archives/win2016/entries/
reasoning-analogy/.
</p>
<p>Trevor Cohen, Dominic Widdows, and Thomas Rind-
flesch. 2015. Expansion-by-analogy: a vec-
tor symbolic approach to semantic search. In
Quantum Interaction, Springer, pages 54–66.
https://doi.org/10.1007/978-3-319-15931-7 5.
</p>
<p>Joachim Daiber, Lautaro Quiroz, Roger Wechsler, and
Stella Frank. 2015. Splitting compounds by seman-
tic analogy. In Proceedings of the 1st Deep Machine
Translation Workshop. Charles University in Prague,
Praha, Czech Republic, 3-4 September 2015, pages
20–28. http://www.aclweb.org/anthology/W15-
5703.
</p>
<p>Aleksandr Drozd, Anna Gladkova, and Satoshi Mat-
suoka. 2016. Word embeddings, analogies, and
machine learning: beyond king - man + woman
= queen. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers. pages 3519–3530.
https://www.aclweb.org/anthology/C/C16/C16-
1332.pdf.
</p>
<p>Nguyen Tuan Duc, Danushka Bollegala, and Mitsuru
Ishizuka. 2012. Cross-language latent relational
search between Japanese and English languages us-
ing a Web corpus. ACM Transactions on Asian Lan-
guage Information Processing (TALIP) 11(3):11.
http://dl.acm.org/citation.cfm?id=2334805.
</p>
<p>Katrin Erk. 2016. What do you know about
an alligator when you know the company it
keeps. Semantics and Pragmatics 9(17):1–63.
https://doi.org/10.3765/sp.9.17.
</p>
<p>Luis Espinosa Anke, Jose Camacho-Collados, Clau-
dio Delli Bovi, and Horacio Saggion. 2016. Su-
pervised distributional hypernym discovery via do-
main adaptation. In Proceedings of the 2016
Conference on Empirical Methods in Natural
Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 424–435.
https://aclweb.org/anthology/D16-1041.
</p>
<p>Stefano Federici, Simonetta Montemagni, and Vito Pir-
relli. 1997. Inferring semantic similarity from dis-
tributional evidence: An analogy-based approach
to word sense disambiguation. In Proceedings
of the ACL/EACL Workshop on Automatic Infor-
mation Extraction and Building of Lexical Se-
mantic Resources for NLP Applications. pages
90–97. http://aclweb.org/anthology/W/W97/W97-
0813.pdf.
</p>
<p>Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che,
Haifeng Wang, and Ting Liu. 2014. Learn-
ing semantic hierarchies via word embeddings.
</p>
<p>In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Baltimore, Maryland, USA, pages 1199–1209.
http://202.118.253.69/ rjfu/publications/acl2014.pdf.
</p>
<p>Anna Gladkova, Aleksandr Drozd, and Satoshi Mat-
suoka. 2016. Analogy-based detection of mor-
phological and semantic relations with word em-
beddings: What works and what doesn’t. In
Proceedings of the NAACL-HLT SRW. ACL, San
Diego, California, June 12-17, 2016, pages 47–54.
https://doi.org/10.18653/v1/N16-2002.
</p>
<p>Jennifer B. Hay and R. Harald Baayen. 2005. Shift-
ing paradigms: Gradient structure in morphol-
ogy. Trends in cognitive sciences 9(7):342–348.
https://doi.org/10.1016/j.tics.2005.04.002.
</p>
<p>Esa Itkonen. 2005. Analogy as Structure and Pro-
cess: Approaches in Linguistic, Cognitive Psy-
chology, and Philosophy of Science. Num-
ber 14 in Human cognitive processing. John
Benjamins Pub. Co, Amsterdam ; Philadelphia.
https://doi.org/10.1075/hcp.14.
</p>
<p>David A. Jurgens, Peter D. Turney, Saif M. Mo-
hammad, and Keith J. Holyoak. 2012. Semeval-
2012 task 2: measuring degrees of relational sim-
ilarity. In Proceedings of the First Joint Con-
ference on Lexical and Computational Semantics
(*SEM). Association for Computational Linguistics,
Montréal, Canada, June 7-8, 2012, pages 356–364.
http://dl.acm.org/citation.cfm?id=2387693.
</p>
<p>Maximilian Köper, Christian Scheible, and
Sabine Schulte im Walde. 2015. Multilingual
reliability and “semantic” structure of continuous
word spaces. In Proceedings of the 11th Interna-
tional Conference on Computational Semantics.
Association for Computational Linguistics, pages
40–45. http://www.aclweb.org/anthology/W15-
01#page=56.
</p>
<p>Siwei Lai, Kang Liu, Liheng Xu, and Jun Zhao.
2016. How to generate a good word embed-
ding? IEEE Intelligent Systems 31(6):5–14.
https://doi.org/10.1109/MIS.2016.45.
</p>
<p>Geoffrey Leech. 1981. Semantics: The Study of Mean-
ing. Harmondsworth: Penguin Books.
</p>
<p>Omer Levy and Yoav Goldberg. 2014. Linguistic regu-
larities in sparse and explicit word representations.
In Proceedings of the Eighteenth Conference on
Computational Natural Language Learning. pages
171–180. https://doi.org/10.3115/v1/W14-1618.
</p>
<p>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the As-
sociation for Computational Linguistics 3:211–225.
http://www.aclweb.org/anthology/Q15-1016.
</p>
<p>144</p>
<p />
</div>
<div class="page"><p />
<p>Tal Linzen. 2016. Issues in evaluating semantic spaces
using word analogies. In Proceedings of the First
Workshop on Evaluating Vector Space Representa-
tions for NLP. Association for Computational Lin-
guistics. https://doi.org/10.18653/v1/W16-2503.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. Proceedings of Inter-
national Conference on Learning Representations
(ICLR) http://arxiv.org/abs/1301.3781.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S.
Corrado, and Jeff Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Advances in Neural Information Pro-
cessing Systems 26 (NIPS 2013). pages 3111–3119.
http://papers.nips.cc/paper/5021-di.
</p>
<p>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the
2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Associa-
tion for Computational Linguistics, pages 746–751.
http://aclweb.org/anthology/N13-1090.
</p>
<p>Denis Newman-Griffis, Albert M. Lai, and Eric Fosler-
Lussier. 2017. Insights into analogy completion
from the biomedical domain. arXiv:1706.02241
[cs] http://arxiv.org/abs/1706.02241.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP). volume 12, pages
1532–1543. https://doi.org/10.3115/v1/D14-1162.
</p>
<p>Paul Pringle. 2003. College board scores with
critics of SAT analogies. Los Angeles Times
http://articles.latimes.com/2003/jul/27/local/me-
sat27/2.
</p>
<p>Michael SC Thomas and Denis Mareschal. 1997. Con-
nectionism and psychological notions of similar-
ity. In The Proceedings of the 19th Annual Con-
ference of the Cognitive Science Society. Mah-
wah, NJ: Erlbaum, Stanford, USA, pages 757–762.
http://eprints.bbk.ac.uk/4611/.
</p>
<p>Peter Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining inde-
pendent modules to solve multiple-choice syn-
onym and analogy problems. In Proceed-
ings of the International Conference on Re-
cent Advances in Natural Language Process-
ing. pages 482–489. http://nparc.cisti-icist.nrc-
cnrc.gc.ca/npsi/ctrl?action=rtdoc&amp;an=8913366.
</p>
<p>Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics 32(3):379–416.
https://doi.org/10.1162/coli.2006.32.3.379.
</p>
<p>Peter D. Turney. 2008. A uniform approach
to analogies, synonyms, antonyms, and associa-
tions. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008). pages 905–912. http://nparc.cisti-icist.nrc-
cnrc.gc.ca/npsi/ctrl?action=rtdoc&amp;an=5764174.
</p>
<p>Amos Tversky. 1977. Features of similar-
ity. Psychological Review 84(4):327–352.
https://doi.org/10.1037/0033-295X.84.4.327.
</p>
<p>Ekaterina Vylomova, Laura Rimmel, Trevor Cohn,
and Timothy Baldwin. 2016. Take and took, gag-
gle and goose, book and read: evaluating the util-
ity of vector differences for lexical relation learn-
ing. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 1671–
1682. https://doi.org/10.18653/v1/P16-1158.
</p>
<p>145</p>
<p />
</div>
<div class="page"><p />
<p>A Supplementary Material
</p>
<p>A.1 3CosAdd on GloVe and Word2Vec
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 8.5 7.2 13.1 16.0
</p>
<p>0.1 - 0.2 10.9 12.0 21.0 25.7
0.2 - 0.3 13.1 12.4 22.7 28.0
0.3 - 0.4 14.0 16.9 29.2 35.4
0.4 - 0.5 15.9 21.8 34.6 41.3
0.5 - 0.6 14.0 31.8 46.7 53.3
0.6 - 0.7 10.1 51.4 65.7 70.4
0.7 - 0.8 10.3 54.1 73.6 78.2
0.8 - 0.9 3.1 56.2 76.7 81.9
0.9 - 1 0.1 61.4 77.3 77.3
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 2.3 3.9 6.9 9.2
</p>
<p>0.1 - 0.2 7.0 6.7 12.7 15.9
0.2 - 0.3 11.5 5.9 12.2 16.0
0.3 - 0.4 15.0 11.4 20.0 25.1
0.4 - 0.5 16.5 17.3 29.4 35.9
0.5 - 0.6 18.0 31.5 45.6 52.1
0.6 - 0.7 18.4 48.4 62.8 68.3
0.7 - 0.8 9.7 52.6 69.7 75.3
0.8 - 0.9 1.3 37.5 53.1 59.7
0.9 - 1 0.3 32.6 48.5 56.8
</p>
<p>Figure 9: Similarity between vectors a and a′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 27.6 10.4 19.3 24.5
</p>
<p>0.1 - 0.2 25.8 20.9 33.4 38.8
0.2 - 0.3 21.2 33.5 47.9 53.4
0.3 - 0.4 13.3 42.1 58.2 64.0
0.4 - 0.5 6.2 49.6 65.7 71.6
0.5 - 0.6 3.9 45.9 67.5 73.8
0.6 - 0.7 0.9 61.6 77.2 82.3
0.7 - 0.8 0.5 60.4 77.1 80.9
0.8 - 0.9 0.0 91.2 94.1 97.1
0.9 - 1 0.6 10.0 24.1 31.6
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 22.0 15.2 26.9 32.9
</p>
<p>0.1 - 0.2 29.1 21.4 33.6 39.7
0.2 - 0.3 22.0 31.5 43.8 49.1
0.3 - 0.4 13.4 40.1 50.5 54.7
0.4 - 0.5 8.0 33.7 43.9 48.5
0.5 - 0.6 3.4 29.4 41.3 47.6
0.6 - 0.7 1.1 35.8 53.4 59.6
0.7 - 0.8 0.3 59.6 71.1 74.8
0.8 - 0.9 0.4 64.5 76.6 79.6
0.9 - 1 0.4 12.5 22.3 25.5
</p>
<p>Figure 10: Similarity between vectors a′ and b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 8.5 1.3 3.1 4.0
</p>
<p>0.1 - 0.2 11.0 4.1 9.0 11.5
0.2 - 0.3 13.1 7.1 13.9 17.9
0.3 - 0.4 14.0 12.1 22.6 28.4
0.4 - 0.5 15.9 17.3 31.0 38.5
0.5 - 0.6 14.0 29.7 52.9 63.7
0.6 - 0.7 10.1 56.2 78.8 85.5
0.7 - 0.8 10.3 77.3 93.3 96.3
0.8 - 0.9 3.1 85.5 97.8 99.2
0.9 - 1 0.0 – – –
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 2.3 1.3 2.3 3.0
</p>
<p>0.1 - 0.2 7.0 2.7 4.3 5.3
0.2 - 0.3 11.6 1.7 4.0 5.5
0.3 - 0.4 15.0 1.9 5.5 8.4
0.4 - 0.5 16.5 6.3 15.6 23.2
0.5 - 0.6 18.0 27.2 47.7 58.1
0.6 - 0.7 18.4 57.8 79.2 85.7
0.7 - 0.8 9.7 78.1 91.8 95.4
0.8 - 0.9 1.3 83.5 93.6 96.2
0.9 - 1 0.2 90.5 100 100
</p>
<p>Figure 11: Similarity between vectors b and b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 6.6 0.4 1.2 1.7
</p>
<p>0.1 - 0.2 11.3 0.7 2.0 3.2
0.2 - 0.3 16.4 1.7 4.0 5.7
0.3 - 0.4 18.6 4.2 13.7 22.3
0.4 - 0.5 16.2 24.3 53.0 67.9
0.5 - 0.6 12.0 55.0 83.6 90.2
0.6 - 0.7 11.0 71.3 89.2 90.5
0.7 - 0.8 6.8 86.7 92.3 92.5
0.8 - 0.9 1.0 92.5 92.7 92.8
0.9 - 1 0.1 100 100 100
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 3.0 0.1 0.7 0.9
</p>
<p>0.1 - 0.2 8.3 0.4 1.1 1.7
0.2 - 0.3 15.7 0.4 1.3 2.2
0.3 - 0.4 20.7 1.9 5.6 9.6
0.4 - 0.5 18.7 12.7 35.9 52.5
0.5 - 0.6 14.8 50.4 83.2 91.1
0.6 - 0.7 12.6 81.4 92.1 93.0
0.7 - 0.8 5.5 86.3 87.6 87.7
0.8 - 0.9 0.7 90.2 90.2 90.2
0.9 - 1 0.0 100 100 100
</p>
<p>Figure 12: Similarity between vector b′ and predicted vector
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 52.0 21.5 34.0 39.6
</p>
<p>0.1 - 0.2 25.8 29.2 42.7 48.3
0.2 - 0.3 14.3 33.5 46.5 51.6
0.3 - 0.4 5.8 35.8 48.9 53.5
0.4 - 0.5 1.6 37.0 48.0 52.6
0.5 - 0.6 0.4 35.7 44.3 48.1
0.6 - 0.7 0.1 33.6 41.8 46.4
0.7 - 0.8 0.0 47.6 52.4 52.4
0.8 - 0.9 0.0 – – –
0.9 - 1 0.0 6.2 6.2 12.5
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 43.5 27.1 39.9 45.8
</p>
<p>0.1 - 0.2 32.4 27.6 39.1 44.3
0.2 - 0.3 14.2 25.7 35.8 40.7
0.3 - 0.4 6.0 16.7 25.8 30.7
0.4 - 0.5 2.8 13.6 22.2 26.9
0.5 - 0.6 0.8 17.2 23.5 26.7
0.6 - 0.7 0.2 33.5 36.0 37.8
0.7 - 0.8 0.0 50.0 53.8 53.8
0.8 - 0.9 0.0 66.7 66.7 66.7
0.9 - 1 0.0 13.3 13.3 13.3
</p>
<p>Figure 13: Similarity between vector b′ and a
</p>
<p>146</p>
<p />
</div>
<div class="page"><p />
<p>0 10 20 30 40 50 60 70 80 900.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
sh
</p>
<p>ar
e 
</p>
<p>/ a
cc
</p>
<p>ur
ac
</p>
<p>y
share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 10 36.5 53.3 73.0 79.8
</p>
<p>10 - 20 6.8 22.8 39.7 48.4
20 - 30 4.5 15.5 27.8 34.9
30 - 40 3.0 21.2 35.7 42.3
40 - 50 2.1 22.4 34.4 40.9
50 - 60 1.8 15.4 28.2 33.0
60 - 70 1.2 10.2 23.4 31.4
70 - 80 1.2 14.6 23.9 28.9
80 - 90 1.2 22.6 33.8 38.6
90 - 100 41.6 6.5 12.7 16.0
</p>
<p>0 10 20 30 40 50 60 70 80 900.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 10 38.0 58.3 77.5 84.3
10 - 20 8.1 22.1 41.3 51.4
20 - 30 3.7 9.0 20.9 31.0
30 - 40 2.3 9.3 17.8 25.4
40 - 50 1.7 15.3 32.4 41.4
50 - 60 0.9 12.8 25.1 34.1
60 - 70 0.8 10.2 21.7 33.6
70 - 80 1.1 11.6 23.8 29.8
80 - 90 0.3 6.5 17.6 21.6
</p>
<p>90 - 100 43.2 2.2 5.5 8.0
</p>
<p>Figure 14: The rank of b in the neighborhood of b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 0.0 – – –
</p>
<p>0.1 - 0.2 0.0 – – –
0.2 - 0.3 0.0 0.0 0.0 0.0
0.3 - 0.4 5.3 2.0 4.4 6.6
0.4 - 0.5 26.3 11.3 19.6 24.1
0.5 - 0.6 45.6 28.9 43.1 49.2
0.6 - 0.7 20.6 44.2 61.4 67.2
0.7 - 0.8 2.0 48.0 72.1 79.5
0.8 - 0.9 0.0 0.0 0.0 0.0
0.9 - 1 0.0 0.0 0.0 0.0
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 0.0 – – –
</p>
<p>0.1 - 0.2 0.0 – – –
0.2 - 0.3 0.0 – – –
0.3 - 0.4 0.0 – – –
0.4 - 0.5 0.0 – – –
0.5 - 0.6 0.0 – – –
0.6 - 0.7 0.1 0.0 0.0 0.0
0.7 - 0.8 54.1 23.5 33.8 38.9
0.8 - 0.9 45.7 28.7 41.9 47.6
0.9 - 1 0.2 91.1 99.3 100
</p>
<p>Figure 15: Similarity between b′ and its 5th neighbor
</p>
<p>A.2 3CosMul on GloVe
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 8.5 6.0 10.1 12.0
</p>
<p>0.1 - 0.2 10.9 11.7 18.8 22.1
0.2 - 0.3 13.1 11.5 20.0 23.8
0.3 - 0.4 14.0 16.8 26.9 31.9
0.4 - 0.5 15.9 21.9 32.8 38.2
0.5 - 0.6 14.0 33.0 45.5 51.1
0.6 - 0.7 10.1 53.7 65.2 69.4
0.7 - 0.8 10.3 57.7 73.4 77.4
0.8 - 0.9 3.1 60.5 77.4 81.8
0.9 - 1 0.1 61.4 77.3 77.3
</p>
<p>Figure 16: Similarity between vectors a and a′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 27.6 9.1 16.0 19.9
</p>
<p>0.1 - 0.2 25.8 21.8 31.9 36.3
0.2 - 0.3 21.2 35.8 47.6 51.9
0.3 - 0.4 13.3 44.7 58.4 63.1
0.4 - 0.5 6.2 50.7 64.6 69.6
0.5 - 0.6 3.9 46.0 62.7 68.9
0.6 - 0.7 0.9 59.1 71.8 77.3
0.7 - 0.8 0.5 48.7 66.1 69.9
0.8 - 0.9 0.0 88.2 91.2 94.1
0.9 - 1 0.6 10.6 21.3 27.8
</p>
<p>Figure 17: Similarity between vectors a′ and b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 8.5 1.4 2.6 3.5
</p>
<p>0.1 - 0.2 11.0 4.3 8.3 10.3
0.2 - 0.3 13.1 7.6 13.9 17.4
0.3 - 0.4 14.0 13.6 23.0 27.7
0.4 - 0.5 15.9 19.6 30.8 36.4
0.5 - 0.6 14.0 31.9 49.9 57.6
0.6 - 0.7 10.1 56.9 73.9 79.3
0.7 - 0.8 10.3 74.8 88.2 91.4
0.8 - 0.9 3.1 81.3 93.7 95.7
0.9 - 1 0.0 – – –
</p>
<p>Figure 18: Similarity between vectors b and b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 52.0 23.1 33.2 37.6
</p>
<p>0.1 - 0.2 25.8 29.2 40.3 44.5
0.2 - 0.3 14.3 32.9 43.7 48.0
0.3 - 0.4 5.8 34.9 46.2 50.4
0.4 - 0.5 1.6 36.2 46.3 49.7
0.5 - 0.6 0.4 34.8 42.4 46.9
0.6 - 0.7 0.1 30.9 41.8 45.5
0.7 - 0.8 0.0 47.6 52.4 52.4
0.8 - 0.9 0.0 – – –
0.9 - 1 0.0 0.0 6.2 12.5
</p>
<p>Figure 19: Similarity between vector b′ and a
</p>
<p>0 10 20 30 40 50 60 70 80 900.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 10 36.5 53.5 68.9 74.3
</p>
<p>10 - 20 6.8 25.0 38.5 45.1
20 - 30 4.5 17.7 28.1 33.5
30 - 40 3.0 22.2 34.8 40.9
40 - 50 2.1 24.0 34.7 40.7
50 - 60 1.8 16.5 27.6 31.8
60 - 70 1.2 10.2 22.7 27.8
70 - 80 1.2 16.9 25.9 31.2
80 - 90 1.2 23.9 34.6 38.9
90 - 100 41.7 7.2 12.6 15.3
</p>
<p>Figure 20: The rank of b in the neighborhood of b′
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 0.0 – – –
</p>
<p>0.1 - 0.2 0.0 – – –
0.2 - 0.3 0.0 – – –
0.3 - 0.4 0.0 – – –
0.4 - 0.5 0.0 – – –
0.5 - 0.6 0.0 – – –
0.6 - 0.7 5.3 1.7 3.9 5.7
0.7 - 0.8 71.9 23.3 33.1 37.4
0.8 - 0.9 22.7 44.8 59.4 64.5
0.9 - 1 0.1 0.0 0.0 2.1
</p>
<p>Figure 21: Similarity between b′ and its 5th neigh-
bor
</p>
<p>147</p>
<p />
</div>
<div class="page"><p />
<p>A.3 LRCos on GloVe and Word2Vec
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(a) GloVe
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0 - 0.1 8.6 1.8 4.1 5.3
</p>
<p>0.1 - 0.2 11.0 7.4 15.2 17.5
0.2 - 0.3 13.1 20.8 35.1 44.4
0.3 - 0.4 14.1 36.7 56.5 65.8
0.4 - 0.5 15.9 47.9 66.0 71.7
0.5 - 0.6 14.0 63.5 81.2 85.6
0.6 - 0.7 10.1 76.4 87.9 92.0
0.7 - 0.8 10.2 85.6 93.6 95.5
0.8 - 0.9 3.1 88.5 96.7 96.7
0.9 - 1 0.0 – – –
</p>
<p>0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1.0
</p>
<p>sh
ar
</p>
<p>e 
/ a
</p>
<p>cc
ur
</p>
<p>ac
y
</p>
<p>share of all questions
accuracy (top 1)
accuracy (top 3)
accuracy (top 5)
</p>
<p>(b) Word2Vec
</p>
<p>Similarity Share Accuracy (%)Bin top 1 top 3 top 5
0.0 - 0.1 2.6 2.1 2.1 4.2
0.1 - 0.2 6.9 5.4 7.7 7.7
0.2 - 0.3 11.2 9.0 15.7 22.9
0.3 - 0.4 15.1 19.1 41.7 48.8
0.4 - 0.5 16.5 43.2 62.0 70.1
0.5 - 0.6 18.2 67.4 77.4 82.4
0.6 - 0.7 18.5 87.3 92.5 95.4
0.7 - 0.8 9.6 91.1 93.9 95.0
0.8 - 0.9 1.2 69.6 95.7 100
0.9 - 1 0.2 33.3 100 100
</p>
<p>Figure 22: Similarity between vectors b and b′
</p>
<p>A.4 Comparison between 3CosAdd, 3CosMul and LRCos on GloVe
</p>
<p>All categories
</p>
<p>Encyclopedia
</p>
<p>Lexicography
</p>
<p>Derivation
</p>
<p>Inflections
</p>
<p>Category
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>1.
0
</p>
<p>ac
cu
</p>
<p>ra
cy
</p>
<p>3CosAdd (top 1)
3CosAdd (top 3)
3CosAdd (top 5)
3CosAdd (top 10)
3CosAdd (top 15)
</p>
<p>3CosMul (top 1)
3CosMul (top 3)
3CosMul (top 5)
3CosMul (top 10)
3CosMul (top 15)
</p>
<p>LRCos (top 1)
LRCos (top 3)
LRCos (top 5)
LRCos (top 10)
LRCos (top 15) Category Method
</p>
<p>Accuracy (%)
top 1 top 3 top 5 top 10 top 15
</p>
<p>All
3CosAdd 26.1 38.8 44.2 50.7 54.2
3CosMul 26.7 37.1 41.4 47.2 50.5
LRCos 44.3 57.6 62.6 67.9 71.4
</p>
<p>Encyclopedia
3CosAdd 26.3 36.6 41.2 46.9 50.1
3CosMul 26.2 35.4 39.2 44.5 47.5
LRCos 45.1 58.6 64.2 70.2 73.6
</p>
<p>Lexicography
3CosAdd 10.2 21.8 28.7 38.0 43.3
3CosMul 8.8 17.9 23.6 31.9 36.9
LRCos 25.2 43.1 50.2 58.0 65.3
</p>
<p>Derivation
3CosAdd 8.2 17.3 23.3 30.4 34.1
3CosMul 9.1 16.5 20.7 26.3 29.5
LRCos 29.0 41.5 46.6 51.8 54.8
</p>
<p>Inflections
3CosAdd 59.5 79.5 83.6 87.4 89.4
3CosMul 62.8 78.6 82.2 86.1 88.0
LRCos 77.9 87.2 89.4 91.6 91.8
</p>
<p>Figure 23: 3CosAdd vs 3CosMul vs LRCos
</p>
<p>All categories
</p>
<p>Encyclopedia
</p>
<p>Lexicography
</p>
<p>Derivation
</p>
<p>Inflections
</p>
<p>Category
</p>
<p>0.
0
</p>
<p>0.
2
</p>
<p>0.
4
</p>
<p>0.
6
</p>
<p>0.
8
</p>
<p>1.
0
</p>
<p>ac
cu
</p>
<p>ra
cy
</p>
<p>3CosAdd (top 1)
3CosAdd (top 3)
3CosAdd (top 5)
3CosAdd (top 10)
3CosAdd (top 15)
</p>
<p>3CosMul (top 1)
3CosMul (top 3)
3CosMul (top 5)
3CosMul (top 10)
3CosMul (top 15)
</p>
<p>LRCos (top 1)
LRCos (top 3)
LRCos (top 5)
LRCos (top 10)
LRCos (top 15) Category Method
</p>
<p>Accuracy (%)
top 1 top 3 top 5 top 10 top 15
</p>
<p>All
3CosAdd 6.0 34.0 43.0 51.7 56.0
3CosMul 15.0 35.5 41.8 48.8 52.5
LRCos 44.7 57.6 62.3 68.2 71.3
</p>
<p>Encyclopedia
3CosAdd 10.8 36.7 45.0 53.0 56.6
3CosMul 19.5 38.6 44.4 50.9 54.2
LRCos 44.5 59.2 64.6 70.2 73.6
</p>
<p>Lexicography
3CosAdd 2.1 16.7 26.6 38.4 44.9
3CosMul 2.7 15.9 23.2 33.2 38.8
LRCos 25.4 43.5 48.6 57.8 64.5
</p>
<p>Derivation
3CosAdd 0.3 10.4 18.6 28.5 32.8
3CosMul 3.8 13.0 18.6 25.2 28.8
LRCos 30.7 40.9 46.4 52.8 55.2
</p>
<p>Inflections
3CosAdd 10.5 72.2 81.6 86.9 89.4
3CosMul 34.1 74.5 81.0 85.9 88.0
LRCos 78.3 86.8 89.8 91.8 92.0
</p>
<p>Figure 24: 3CosAdd vs 3CosMul vs LRCos (“honest” version)
</p>
<p>148</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 149–154,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Semantic Frames and Visual Scenes:
Learning Semantic Role Inventories from Image and Video Descriptions
</p>
<p>Ekaterina Shutova
Computer Laboratory
</p>
<p>University of Cambridge, UK
es407@cam.ac.uk
</p>
<p>Andreas Wundsam
Big Switch Networks
</p>
<p>Santa Clara, CA
andi@wundsam.net
</p>
<p>Helen Yannakoudakis
ALTA Institute
</p>
<p>University of Cambridge, UK
hy260@cl.cam.ac.uk
</p>
<p>Abstract
</p>
<p>Frame-semantic parsing and semantic role
labelling, that aim to automatically assign
semantic roles to arguments of verbs in
a sentence, have recently become an ac-
tive strand of research in NLP. However,
to date these methods have relied on a pre-
defined inventory of semantic roles. In
this paper, we present a method to auto-
matically learn argument role inventories
for verbs from large corpora of text, im-
ages and videos. We evaluate the method
against manually constructed role inven-
tories in FrameNet and show that the vi-
sual model outperforms the language-only
model and operates with a high precision.
</p>
<p>1 Introduction
</p>
<p>The theory of frame semantics (Fillmore, 1976)
postulates that our interpretation of word mean-
ings is not limited to isolated concepts, but rather
instantiates complex knowledge structures about
events and their participants, known as semantic
frames. For instance, the COMMERCIAL TRANS-
ACTION frame includes elements such as a seller,
a buyer, goods and money which can be mapped to
higher-level semantic roles such as agent, patient,
instrument etc. The verbs linked to this frame are
buy, sell, pay, cost and charge, each evoking dif-
ferent aspects of the frame.
</p>
<p>This theory has been implemented in a lexical-
semantic resource called FrameNet (Fillmore
et al., 2003). Each semantic frame is encoded in
FrameNet as a list of lexical units that evoke this
frame (typically verbs) and the roles that their se-
mantic arguments may take given the scenario rep-
resented by the frame. FrameNet has inspired a di-
rection in NLP research known as semantic role la-
belling (Gildea and Jurafsky, 2002; Màrquez et al.,
</p>
<p>2008) and frame-semantic parsing (Das et al.,
2014), whose goal is to assign semantic roles to
arguments of the verbs in a sentence. However,
these works point out the coverage limitations of
the hand-constructed FrameNet database, suggest-
ing that a data-driven frame acquisition method
is needed to enable the integration of frame se-
mantics into real-world NLP applications. In this
paper, we propose such a method, experimenting
with semantic frame induction from linguistic and
visual data. Our system first performs clustering
of verb arguments to identify their possible seman-
tic roles and then computes the level of associa-
tion between a given argument role and the verb,
thus deriving the structure of the semantic frame
in which the verb participates.
</p>
<p>Frame semantics emphasizes the relation be-
tween our lexical semantic knowledge and our ex-
perience in the world, suggesting that semantic
frames are not merely a linguistic construct but
also a result of our sensory-motor and perceptual
experience. However, frame semantic approaches
in NLP typically rely on textual data. Our method,
in contrast, induces semantic frames from both a
text corpus and a corpus of tagged images and
videos. We evaluate the method against hand-
constructed frames in FrameNet. Our results show
that the visual model outperforms the language-
only model and achieves a high precision. This
frame induction method can be used to comple-
ment existing FrameNets or to construct a new re-
source of automatically mined semantic frames,
free from manual annotation bias.
</p>
<p>2 Experimental Data
</p>
<p>Textual data. We extracted linguistic features
for our model from the British National Corpus
(BNC) (Burnard, 2007). We parsed the corpus us-
ing the RASP parser (Briscoe et al., 2006) and
</p>
<p>149</p>
<p />
</div>
<div class="page"><p />
<p>extracted subject–verb and verb–object relations
from its dependency output. These relations were
then used as features for clustering to obtain argu-
ments classes, which we then used as proxies for
frame elements, i.e. argument roles.
</p>
<p>Image and video data. We used the Yahoo! Web-
scope Flickr-100M dataset (Shamma, 2014) to ex-
tract visual relations between verbs and their argu-
ments. Flickr-100M contains 99.3 million images
and 0.7 million videos with natural language tags
for scenes, objects and actions annotated by users.
We first stem the tags and remove words that are
absent in WordNet (e.g. named entities and mis-
spellings). We then identify their part of speech
based on their visual context using the method of
Shutova et al. (2015) and extract verb–noun co-
occurrences.
</p>
<p>3 Frame Induction Model
</p>
<p>3.1 Argument Clustering
</p>
<p>We use a clustering method to obtain semantic
classes of arguments of verbs, thus generalising
from individual arguments to their semantic types
which correspond to frame roles. We obtain ar-
gument classes by means of spectral clustering of
nouns with lexico-syntactic features, which has
been shown effective in previous lexical classifi-
cation tasks (Sun and Korhonen, 2009).
</p>
<p>Spectral clustering partitions the data relying
on a similarity matrix that records similarities be-
tween all pairs of data points. We use Jensen-
Shannon divergence to measure similarity be-
tween feature vectors for two nouns, wi and wj ,
defined as follows:
</p>
<p>dJS(wi, wj) =
1
2
dKL(wi||m) + 12dKL(wj ||m),
</p>
<p>(1)
where dKL is the Kullback-Leibler divergence,
and m is the average of wi and wj . We construct
the similarity matrix S computing similarities Sij
as Sij = exp(−dJS(wi, wj)). The matrix S then
encodes a similarity graph G (over our nouns),
where Sij are the adjacency weights. The cluster-
ing problem can then be defined as identifying the
optimal partition, or cut, of the graph into clusters,
such that the intra-cluster weights are high and the
inter-cluster weights are low. We use the multi-
way normalized cut (MNCut) algorithm of Meila
and Shi (2001) for this purpose. The algorithm
transforms S into a stochastic matrix P containing
transition probabilities between the vertices in the
</p>
<p>graph as P = D−1S, where the degree matrix D
is a diagonal matrix with Dii =
</p>
<p>∑N
j=1 Sij . It then
</p>
<p>computes the K leading eigenvectors of P , where
K is the desired number of clusters. The graph
is partitioned by finding approximately equal el-
ements in the eigenvectors using a simpler clus-
tering algorithm, such as k-means. Meila and Shi
(2001) have shown that the partition I derived in
this way minimizes the MNCut criterion:
</p>
<p>MNCut(I) =
K∑
</p>
<p>k=1
</p>
<p>(1− P (Ik → Ik|Ik)), (2)
</p>
<p>which is the sum of transition probabilities across
different clusters. Since k-means starts from a ran-
dom cluster assignment, we ran the algorithm mul-
tiple times and used the partition that minimizes
the cluster distortion, i.e. distances to its centroid.
</p>
<p>We clustered the 2,000 most frequent nouns in
the BNC, using their grammatical relations as fea-
tures. The features consisted of verb lemmas ap-
pearing in the subject, direct object and indirect
object relations with the given nouns in the RASP-
parsed BNC, indexed by relation type. The fea-
ture vectors were first constructed from the corpus
counts, and subsequently normalized by the sum
of the feature values.
</p>
<p>Our use of linguistic dependency features for
argument clustering is motivated by the results
of previous research (Sun and Korhonen, 2011;
Shutova et al., 2015), that has shown that such
features lead to clusters of nouns belonging to
the same semantic type, as opposed to topic or
scene as it is the case with linguistic window-
based features or image-derived features (Shutova
et al., 2015). Since the argument roles in seman-
tic frames correspond to semantic types (such as
location or instrument), the linguistic dependency
features are best suited to generalise the predicate-
argument structure in semantic frames. Exam-
ple clusters produced by our method are shown in
Fig. 1. The resulting clusters represent frame ele-
ments, i.e. argument roles, in our model.
</p>
<p>3.2 Predicate–Argument Association
We then use the verb–noun co-occurrence infor-
mation extracted from the visual data to quantify
the strength of association of a given verb with
each of the argument classes, thus identifying the
relevant argument roles for the verb. We adopted
an information theoretic measure originally pro-
posed by Resnik (1993) in his selectional pref-
erence model. Resnik first measures selectional
</p>
<p>150</p>
<p />
</div>
<div class="page"><p />
<p>official officer inspector journalist detective constable po-
lice policeman reporter
fire pipe torch candle lamp cigarette
potato apple slice food cake meat bread fruit
lifetime quarter period century succession stage generation
decade phase interval future
disorder infection illness disease virus cancer
profit surplus earnings income turnover revenue
</p>
<p>Figure 1: Clusters representing argument roles
</p>
<p>preference strength (SPS) of a verb in terms of
Kullback-Leibler divergence between the distribu-
tion of noun classes occurring as arguments of this
verb, p(c|v), and the prior distribution of the noun
classes, p(c):
</p>
<p>SPS(v) =
∑
</p>
<p>c
</p>
<p>p(c|v) log p(c|v)
p(c)
</p>
<p>. (3)
</p>
<p>SPS measures how strongly the predicate con-
strains its arguments. Selectional association of
the verb with a particular argument class is then
defined as a relative contribution of that argument
class to the overall SPS of the verb:
</p>
<p>Ass(v, c) =
1
</p>
<p>SPS(v)
p(c|v) log p(c|v)
</p>
<p>p(c)
. (4)
</p>
<p>We use this measure to quantify the strength of
verb–argument association based on the visual co-
occurrence information. We extract verb-noun co-
occurrences from Flickr-200M, map the nouns to
argument classes and quantify selectional associa-
tion of a given verb with each argument class, thus
acquiring its semantic frame structure. An exam-
ple argument distribution for the verb kill, and thus
the KILLING frame, is presented in Fig. 2. One
can see from the figure that the argument clusters
correspond to specific roles in FrameNet, e.g. the
killer and the victim, the motive, the weapon (in-
strument) and death (result).
</p>
<p>4 Evaluation against FrameNet
</p>
<p>Baseline. We evaluate the effectiveness of vi-
sual information for our task by comparing the
model based on vision and language (VIS) to a
baseline model using language alone (LING). In
the LING system, the predicate-argument associa-
tion scores are computed based on verb-argument
co-occurrence information extracted from verb-
subject, verb-direct object and verb-indirect object
relations in the BNC. In case of the indirect object
relations, the accompanying prepostions were dis-
carded and the noun counts were aggregated.
</p>
<p>0.180 defeat fall death tragedy loss collapse decline disas-
ter destruction fate
0.141 girl other woman child person people
0.128 suicide killing offence murder breach crime ...
0.113 handle weapon horn knife blade stick sword ankle
waist neck wrist
0.095 victim bull teenager prisoner hero gang enemy rider
offender youth killer thief driver defender hell
0.086 recession disappointment shock pain frustration em-
barrassment guilt sensation depression wound
0.030 sister daughter parent relative lover cousin friend
wife mother husband brother father
0.020 motive self origin meaning cause secret truth ...
0.018 official officer inspector journalist detective consta-
ble police policeman reporter
</p>
<p>Figure 2: System output for kill
</p>
<p>Evaluation setup. In order to investigate the
role of visual information for different types of
verbs, we selected 25 concrete verbs (e.g. cut,
throw, swim) and 25 abstract verbs (e.g. trust, pre-
pare, cheat), according to the MRC concreteness
database (Wilson, 1988). The verb was consid-
ered concrete if its concreteness score was &gt; 400
and abstract if it was &lt; 400. We extracted the 10
highest-ranked verb–argument class pairings pro-
duced by the system for each verb. Each pair-
ing was then evaluated against the argument roles
listed for this verb in FrameNet via manual com-
parison. This resulted in a dataset of 500 verb-
argument pairings for VIS and 500 for LING. The
pairing was considered correct if the argument
cluster corresponded to the semantic type of the
role listed in FrameNet and contained nouns listed
in the linguistic examples (if these were provided
in FrameNet). We have evaluated the system per-
formance in terms of precision at top 10 argument
classes and recall of the Core Frame Elements
(FEs) among the top 10 argument classes.
</p>
<p>Results The VIS model attained a performance of
P = 0.74 and R = 0.78, outperforming the LING
model with P = 0.72 and R = 0.76. When eval-
uated on the subsets of concrete and abstract verbs
separately, VIS attains a P = 0.76; R = 0.80
(concrete) and P = 0.72; R = 0.75 (abstract),
and LING attains P = 0.67; R = 0.75 (concrete)
and P = 0.78; R = 0.76 (abstract).
</p>
<p>5 Discussion and Data Analysis
</p>
<p>Our results show that the vision-based model out-
performs the language-only model on our dataset.
The difference in performance is particularly pro-
nounced for the concrete verbs. For the abstract
verbs in isolation, however, LING attains a higher
</p>
<p>151</p>
<p />
</div>
<div class="page"><p />
<p>precision and recall. This is not surprising, as the
visual information is better suited to capture the
properties of concrete concepts than the abstract
ones (Kiela et al., 2014). However, our results
indicate that integrating linguistic and visual in-
formation provides a better overall model than the
linguistic information alone.
</p>
<p>Our qualitative analysis of the data revealed a
number of interesting trends. Some of the errors
of both systems can be traced back to the clus-
tering step. Different argument roles according to
FrameNet are sometimes found in one cluster. For
instance, both the killer and the victim are in the
same cluster, as shown in Figure 2. However, it is
also the case that one FrameNet role can be split
into several clusters, e.g. the Victim role in the
killing frame is represented by two clusters of hu-
mans and animate beings more generally.
</p>
<p>The common error of the LING model concerns
frame mixing, i.e. both literal and metaphorical ar-
guments of the verb are present in the output. For
instance, eat has a disease cluster as one of its ar-
guments; however, disease is not part of the inges-
tion frame, but rather an instance of its metaphor-
ical transfer. A common trend in the LING out-
put is that it is dominated by the Agent and Theme
roles, with situational roles (e.g. Location) typi-
cally ranked lower or not appearing at all. In con-
trast, the output of VIS encompases a range of sit-
uational roles, such as Instrument, Location, Time
etc. The two models also sometimes differ in the
roles that they identify. For instance, for the verb
risk the VIS output is dominated by arguments of
type Asset and the LING output by the arguments
related to the Bad outcome role in FrameNet.
</p>
<p>6 Related Work
</p>
<p>6.1 Semantic Role Induction
</p>
<p>Approaches most similar in spirit to ours are those
concerned with unsupervised semantic role label-
ing. A number of methods represented seman-
tic roles as latent variables in a graphical model,
which related the verb, its semantic roles and
their syntactic realisations (Grenager and Man-
ning, 2006; Lang and Lapata, 2010; Garg and Hen-
derson, 2012). The induction process then re-
lied on inferring the state of the latent variable.
Other researchers adopted a similarity-based ar-
gument clustering framework to derive semantic
roles. The investigated methods include graph
partitioning algorithms (Lang and Lapata, 2014),
</p>
<p>Bayesian clustering based on Chinese Restaurant
Process (Titov and Klementiev, 2012) and integer
linear programming to incorporate semantic and
structural constraints during clustering (Woodsend
and Lapata, 2015). Titov and Khoddam (2015)
proposed a reconstruction-error minimization ap-
proach using a log-linear model to predict roles
given syntactic and lexical features and a proba-
bilistic tensor factorization model to identify ar-
gument fillers based on the role predictions and
the predicate. To the best of our knowledge, ours
is the first approach to this task exploiting visual
data, in the form of image and video descriptions.
</p>
<p>6.2 Multi-modal Methods in Semantics
</p>
<p>Visual data has been previously used to learn
meaning representations that project multiple
modalities into the same vector space. Semantic
models integrating linguistic and visual informa-
tion have been shown successful in tasks such as
modeling semantic similarity and relatedness (Sil-
berer and Lapata, 2014; Bruni et al., 2012), lexi-
cal entailment (Kiela et al., 2015a), composition-
ality (Roller and Schulte im Walde, 2013), bilin-
gual lexicon induction (Kiela et al., 2015b) and
metaphor identification (Shutova et al., 2016).
</p>
<p>Other applications of multimodal data include
language modeling (Kiros et al., 2014) and knowl-
edge mining from images (Chen et al., 2013; Div-
vala et al., 2014). Young et al. (2014) show that
large collections of image captions can be ex-
ploited for entailment tasks. Shutova et al. (2015)
used image and video descriptions to induce verb
selectional preferences enhanced with visual in-
formation.
</p>
<p>7 Conclusion
</p>
<p>We have presented a method for semantic frame
induction from text, images and videos and shown
that it operates with a high precision and recall.
Although our experiments relied on manually an-
notated tags for images and videos, recent research
shows that such tags can be generated automat-
ically (Bernardi et al., 2016). In the future, our
model can be applied to such automatically gener-
ated tags, reducing its dependence on manual an-
notation. While our current experiments focused
on nominal arguments of the verbs for semantic
role identification, in principle, our model can be
applied to other parts of speech, e.g. adverbs, to
better incorporate argument roles such as Manner.
</p>
<p>152</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgment
</p>
<p>We are grateful to the *SEM reviewers for their
feedback. Ekaterina Shutova’s research is sup-
ported by the Leverhulme Trust Early Career Fel-
lowship.
</p>
<p>References
R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem,
</p>
<p>N. Ikizler-Cinbis, F. Keller, A. Muscat, and B. Plank.
2016. Automatic description generation from im-
ages: A survey of models, datasets, and evaluation
measures. JAIR .
</p>
<p>Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL. pages 77–80.
</p>
<p>Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL. Korea.
</p>
<p>Lou Burnard. 2007. Reference Guide for the
British National Corpus (XML Edition).
http://www.natcorp.ox.ac.uk/XMLedition/URG/.
</p>
<p>Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta.
2013. NEIL: Extracting Visual Knowledge from
Web Data. In Proceedings of ICCV 2013.
</p>
<p>Dipanjan Das, Desai Chen, Andr F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguistics
40:1:9–56.
</p>
<p>S. Divvala, A. Farhadi, and C. Guestrin. 2014. Learn-
ing everything about anything: Webly-supervised
visual concept learning. In Proceedings of CVPR.
</p>
<p>Charles Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences: Conference on the Origin and Develop-
ment of Language and Speech 280(1):20–32.
</p>
<p>Charles Fillmore, Christopher Johnson, and Miriam
Petruck. 2003. Background to FrameNet. Interna-
tional Journal of Lexicography 16(3):235–250.
</p>
<p>Nikhil Garg and James Henderson. 2012. Unsuper-
vised semantic role induction with global role or-
dering. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Short Papers - Volume 2. pages 145–149.
</p>
<p>D. Gildea and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics 28(3).
</p>
<p>Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing. EMNLP
’06, pages 1–8.
</p>
<p>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of ACL.
</p>
<p>Douwe Kiela, Laura Rimell, Ivan Vulić, and Stephen
Clark. 2015a. Exploiting image generality for lex-
ical entailment detection. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers). Beijing, China.
</p>
<p>Douwe Kiela, Ivan Vulić, and Stephen Clark. 2015b.
Visual bilingual lexicon induction with transferred
convnet features. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing. Lisbon, Portugal.
</p>
<p>Ryan Kiros, Ruslan Salakhutdinov, and Richard S.
Zemel. 2014. Multimodal neural language mod-
els. In Proceedings of ICML 2014. pages 595–603.
http://jmlr.org/proceedings/papers/v32/kiros14.html.
</p>
<p>Joel Lang and Mirella Lapata. 2010. Unsupervised
induction of semantic roles. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics. HLT ’10, pages 939–
947.
</p>
<p>Joel Lang and Mirella Lapata. 2014. Similarity-
driven semantic role induction via graph partition-
ing. Computational Linguistics 40(3):633–669.
</p>
<p>Lluı́s Màrquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: An introduction to the special
issue. Computational Linguistics 34(2):145–159.
</p>
<p>Marina Meila and Jianbo Shi. 2001. A random walks
view of spectral segmentation. In Proceedings of
AISTATS.
</p>
<p>Philip Resnik. 1993. Selection and information: A
class-based approach to lexical relationships. Tech-
nical report, University of Pennsylvania.
</p>
<p>Stephen Roller and Sabine Schulte im Walde. 2013. A
Multimodal LDA Model integrating Textual, Cog-
nitive and Visual Modalities. In Proceedings of
EMNLP 2013. Seattle, WA, pages 1146–1157.
</p>
<p>David Shamma. 2014. One hundred million
Creative Commons Flickr images for research.
Http://labs.yahoo.com/news/yfcc100m/.
</p>
<p>Ekaterina Shutova, Douwe Kiela, and Jean Maillard.
2016. Black holes and white rabbits: Metaphor
identification with visual features. In NAACL HLT
2016. pages 160–170.
</p>
<p>Ekaterina Shutova, Niket Tandon, and Gerard de Melo.
2015. Perceptually grounded selectional prefer-
ences. In Proceedings of ACL 2015. Beijing, China.
</p>
<p>153</p>
<p />
</div>
<div class="page"><p />
<p>Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of ACL 2014. Baltimore,
Maryland.
</p>
<p>Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of EMNLP 2009.
</p>
<p>Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP. Edinburgh, UK, pages 1023–1033.
</p>
<p>Ivan Titov and Ehsan Khoddam. 2015. Unsupervised
induction of semantic roles within a reconstruction-
error minimization framework. In HLT-NAACL. The
Association for Computational Linguistics, pages 1–
10.
</p>
<p>Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics. EACL ’12, pages 12–22.
</p>
<p>M.D. Wilson. 1988. The MRC Psycholinguistic
Database: Machine Readable Dictionary, Version
2. Behavioural Research Methods, Instruments and
Computers 20(1):6–11.
</p>
<p>Kristian Woodsend and Mirella Lapata. 2015. Dis-
tributed representations for unsupervised semantic
role labeling. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015. pages 2482–2491.
</p>
<p>Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to vi-
sual denotations. Transactions of the Association of
Computational Linguistics pages 67–78.
</p>
<p>154</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 155–160,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Acquiring Predicate Paraphrases from News Tweets
</p>
<p>Vered Shwartz Gabriel Stanovsky
Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel
</p>
<p>{vered1986, gabriel.satanovsky}@gmail.com, dagan@cs.biu.ac.il
</p>
<p>Ido Dagan
</p>
<p>Abstract
</p>
<p>We present a simple method for ever-
growing extraction of predicate para-
phrases from news headlines in Twitter.
Analysis of the output of ten weeks of col-
lection shows that the accuracy of para-
phrases with different support levels is es-
timated between 60-86%. We also demon-
strate that our resource is to a large extent
complementary to existing resources, pro-
viding many novel paraphrases. Our re-
source is publicly available, continuously
expanding based on daily news.
</p>
<p>1 Introduction
</p>
<p>Recognizing that various textual descriptions
across multiple texts refer to the same event or
action can benefit NLP applications such as rec-
ognizing textual entailment (Dagan et al., 2013)
and question answering. For example, to answer
“when did the US Supreme Court approve same-
sex marriage?” given the text “In June 2015, the
Supreme Court ruled for same-sex marriage”, ap-
prove and ruled for should be identified as describ-
ing the same action.
</p>
<p>To that end, much effort has been devoted to
identifying predicate paraphrases, some of which
resulted in releasing resources of predicate entail-
ment or paraphrases. Two main approaches were
proposed for that matter; the first leverages the
similarity in argument distribution across a large
corpus between two predicates (e.g. [a]0 buy [a]1
/ [a]0 acquire [a]1) (Lin and Pantel, 2001; Berant
et al., 2010). The second approach exploits bilin-
gual parallel corpora, extracting as paraphrases
pairs of texts that were translated identically to for-
eign languages (Ganitkevitch et al., 2013).
</p>
<p>While these methods have produced exhaustive
resources which are broadly used by applications,
</p>
<p>[a]0 introduce [a]1 [a]0 welcome [a]1
[a]0 appoint [a]1 [a]0 to become [a]1
[a]0 die at [a]1 [a]0 pass away at [a]1
</p>
<p>[a]0 hit [a]1 [a]0 sink to [a]1
[a]0 be investigate [a]1 [a]0 be probe [a]1
</p>
<p>[a]0 eliminate [a]1 [a]0 slash [a]1
[a]0 announce [a]1 [a]0 unveil [a]1
[a]0 quit after [a]1 [a]0 resign after [a]1
</p>
<p>[a]0 announce as [a]1 [a]0 to become [a]1
[a]0 threaten [a]1 [a]0 warn [a]1
</p>
<p>[a]0 die at [a]1 [a]0 live until [a]1
[a]0 double down on [a]1 [a]0 stand by [a]1
</p>
<p>[a]0 kill [a]1 [a]0 shoot [a]1
[a]0 approve [a]1 [a]0 pass [a]1
</p>
<p>[a]0 would be cut under [a]1 [a]1 slash [a]0
seize [a]0 at [a]1 to grab [a]0 at [a]1
</p>
<p>Table 1: A sample from the top-ranked predicate
paraphrases.
</p>
<p>their accuracy is limited. Specifically, the first ap-
proach may extract antonyms, that also have sim-
ilar argument distribution (e.g. [a]0 raise to [a]1
/ [a]0 fall to [a]1) while the second may conflate
multiple senses of the foreign phrase.
</p>
<p>A third approach was proposed to harvest para-
phrases from multiple mentions of the same event
in news articles.1 This approach assumes that
various redundant reports make different lexical
choices to describe the same event. Although
there has been some work following this approach
(e.g. Shinyama et al., 2002; Shinyama and Sekine,
2006; Roth and Frank, 2012; Zhang and Weld,
2013), it was less exhaustively investigated and
did not result in creating paraphrase resources.
</p>
<p>In this paper we present a novel unsupervised
method for ever-growing extraction of lexically-
divergent predicate paraphrase pairs from news
tweets. We apply our methodology to create a
resource of predicate paraphrases, exemplified in
Table 1.
</p>
<p>Analysis of the resource obtained after ten
</p>
<p>1This corresponds to instances of event coreference
(Bagga and Baldwin, 1999).
</p>
<p>155</p>
<p />
</div>
<div class="page"><p />
<p>weeks of acquisition shows that the set of para-
phrases reaches accuracy of 60-86% at differ-
ent levels of support. Comparison to existing
resources shows that, even as our resource is
still smaller in orders of magnitude from exist-
ing resources, it complements them with non-
consecutive predicates (e.g. take [a]0 from [a]1)
and paraphrases which are highly context specific.
</p>
<p>The resource and the source code are avail-
able at http://github.com/vered1986/
Chirps.2 As of the end of May 2017, it con-
tains 456,221 predicate pairs in 1,239,463 differ-
ent contexts. Our resource is ever-growing and
is expected to contain around 2 million predicate
paraphrases within a year. Until it reaches a large
enough size, we will release a daily update, and at
a later stage, we plan to release a periodic update.
</p>
<p>2 Background
</p>
<p>2.1 Existing Paraphrase Resources
A prominent approach to acquire predicate para-
phrases is to compare the distribution of their argu-
ments across a corpus, as an extension to the dis-
tributional hypothesis (Harris, 1954). DIRT (Lin
and Pantel, 2001) is a resource of 10 million para-
phrases, in which the similarity between predicate
pairs is estimated by the geometric mean of the
similarities of their argument slots. Berant (2012)
constructed an entailment graph of distributionally
similar predicates by enforcing transitivity con-
straints and applying global optimization, releas-
ing 52 million directional entailment rules (e.g.
[a]0 shoot [a]1 → [a]0 kill [a]1).
</p>
<p>A second notable source for extracting para-
phrases is multiple translations of the same text
(Barzilay and McKeown, 2001). The Para-
phrase Database (PPDB) (Ganitkevitch et al.,
2013; Pavlick et al., 2015) is a huge collection of
paraphrases extracted from bilingual parallel cor-
pora. Paraphrases are scored heuristically, and the
database is available for download in six increas-
ingly large sizes according to scores (the smallest
size being the most accurate). In addition to lex-
ical paraphrases, PPDB also consists of 140 mil-
lion syntactic paraphrases, some of which include
predicates with non-terminals as arguments.
</p>
<p>2.2 Using Multiple Event Descriptions
Another line of work extracts paraphrases from re-
dundant comparable news articles (e.g. Shinyama
</p>
<p>2Chirp is a paraphrase of tweet.
</p>
<p>et al., 2002; Barzilay and Lee, 2003). The as-
sumption is that multiple news articles describing
the same event use various lexical choices, pro-
viding a good source for paraphrases. Heuristics
are applied to recognize that two news articles dis-
cuss the same event, such as lexical overlap and
same publish date (Shinyama and Sekine, 2006).
Given such a pair of articles, it is likely that predi-
cates connecting the same arguments will be para-
phrases, as in the following example:
</p>
<p>1. GOP lawmakers introduce new health care plan
2. GOP lawmakers unveil new health care plan
</p>
<p>Zhang and Weld (2013) and Zhang et al. (2015)
introduced methods that leverage parallel news
streams to cluster predicates by meaning, using
temporal constraints. Since this approach acquires
paraphrases from descriptions of the same event, it
is potentially more accurate than methods that ac-
quire paraphrases from the entire corpus or trans-
lation phrase table. However, there is currently no
paraphrase resource acquired in this approach.3
</p>
<p>Finally, Xu et al. (2014) developed a supervised
model to collect sentential paraphrases from Twit-
ter. They used Twitter’s trending topic service, and
considered two tweets from the same topic as para-
phrases if they shared a single anchor word.
</p>
<p>3 Resource Construction
</p>
<p>We present a methodology to automatically collect
binary verbal predicate paraphrases from Twitter.
We first obtain news related tweets (§3.1) from
which we extract propositions (§3.2). For a can-
didate pair of propositions, we assume that if both
arguments can be matched then the predicates are
likely paraphrases (§3.3). Finally, we rank the
predicate pairs according to the number of in-
stances in which they were aligned (§3.4).
</p>
<p>3.1 Obtaining News Headlines
</p>
<p>We use Twitter as a source of readily avail-
able news headlines. The 140 characters limit
makes tweets concise, informative and indepen-
dent of each other, obviating the need to resolve
document-level entity coreference. We query the
Twitter Search API4 via Twitter Search.5 We use
</p>
<p>3Zhang and Weld (2013) released a small collection of
10k predicate paraphrase clusters (with average cluster size
of 2.4) produced by the system.
</p>
<p>4
https://apps.twitter.com/
</p>
<p>5
https://github.com/ckoepp/TwitterSearch
</p>
<p>156</p>
<p />
</div>
<div class="page"><p />
<p>Turkey intercepts the plane which took off from Moscow
</p>
<p>subj
obj subj
</p>
<p>prep from
</p>
<p>Russia , furious about the plane , threatens to retaliate
</p>
<p>prop of
</p>
<p>prep about
</p>
<p>subj
</p>
<p>comp
</p>
<p>(1) [Turkey]0 intercepts [plane]1 (2) [plane]0 took off from [Moscow]1 [Russia]0 threatens to [retaliate]1
</p>
<p>Figure 1: PropS structures and the corresponding propositions extracted by our process. Left: multi-word
predicates and multiple extractions per tweet. Right: argument reduction.
</p>
<p>Manafort hid payments from Ukraine party with Moscow ties [a]0 hide [a]1 Paul Manafort payments
Manafort laundered the payments through Belize [a]0 launder [a]1 Manafort payments
Send immigration judges to cities to speed up deportations to send [a]0 to [a]1 immigration judges cities
Immigration judges headed to 12 cities to speed up deportations [a]0 headed to [a]1 immigration judges 12 cities
</p>
<p>Table 2: Examples of predicate paraphrase instances in our resource: each instance contains two tweets,
predicate types extracted from them, and the instantiations of arguments.
</p>
<p>Twitter’s news filter that retrieves tweets contain-
ing links to news websites, and limit the search to
English tweets.
</p>
<p>3.2 Proposition Extraction
</p>
<p>We extract propositions from news tweets using
PropS (Stanovsky et al., 2016), which simplifies
dependency trees by conveniently marking a wide
range of predicates (e.g, verbal, adjectival, non-
lexical) and positioning them as direct heads of
their corresponding arguments. Specifically, we
run PropS over dependency trees predicted by
spaCy6 and extract predicate types (as in Table 1)
composed of verbal predicates, datives, preposi-
tions, and auxiliaries.
</p>
<p>Finally, we employ a pre-trained argument re-
duction model to remove non-restrictive argument
modifications (Stanovsky and Dagan, 2016). This
is essential for our subsequent alignment step, as
it is likely that short and concise phrases will tend
to match more frequently in comparison to longer,
more specific arguments. Figure 1 exemplifies
some of the phenomena handled by this process,
along with the automatically predicted output.
</p>
<p>3.3 Generating Paraphrase Instances
</p>
<p>Following the assumption that different descrip-
tions of the same event are bound to be redun-
dant (as discussed in Section 2.2), we consider
two predicates as paraphrases if: (1) They appear
on the same day, and (2) Each of their arguments
aligns with a unique argument in the other predi-
cate, either by strict matching (short edit distance,
abbreviations, etc.) or by a looser matching (par-
</p>
<p>6
https://spacy.io
</p>
<p>tial token matching or WordNet synonyms).7 Ta-
ble 2 shows examples of predicate paraphrase in-
stances in the resource.
</p>
<p>3.4 Resource Release
The resource release consists of two files:
</p>
<p>1. Instances: the specific contexts in which the
predicates are paraphrases (as in Table 2). In
practice, to comply with Twitter policy, we
release predicate paraphrase pair types along
with their arguments and tweet IDs, and pro-
vide a script for downloading the full texts.
</p>
<p>2. Types: predicate paraphrase pair types (as in
Table 1). The types are ranked in a descend-
ing order according to a heuristic accuracy
score:
</p>
<p>s = count ·
(
</p>
<p>1 +
d
</p>
<p>N
</p>
<p>)
where count is the number of instances in
which the predicate types were aligned (Sec-
tion 3.3), d is the number of different days in
which they were aligned, and N is the num-
ber of days since the resource collection be-
gun.
</p>
<p>Taking into account the number of different
days in which predicates were aligned re-
duces the noise caused by two entities that
undergo two different actions on the same
day. For example, the following tweets from
the day of Chuck Berry’s death:
</p>
<p>1. Last year when Chuck Berry turned 90
2. Chuck Berry dies at 90
</p>
<p>7In practice, our publicly available code requires that at
least one pair of arguments will strictly match.
</p>
<p>157</p>
<p />
</div>
<div class="page"><p />
<p>10 20 50 ∞
</p>
<p>74
</p>
<p>60
</p>
<p>78
</p>
<p>86
</p>
<p>40.37
</p>
<p>15.38
</p>
<p>6.15
1.78
</p>
<p>Accuracy Types
</p>
<p>(a) Estimated accuracy (%) and number of types (×1K) of
predicate pairs with at least 5 instances in different score bins.
</p>
<p>1 2 3 4 5 6 7 8 9 10
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>50
</p>
<p>60
</p>
<p>70
</p>
<p>80
</p>
<p>90
</p>
<p>100
</p>
<p>Accuracy Instances Types
</p>
<p>(b) Estimated accuracy (%), number of instances (×10K)
and types (×10K) in the first 10 weeks.
</p>
<p>Figure 2: Resource statistics after ten weeks of collection.
</p>
<p>yield the incorrect type [a]0 turn [a]1 / [a]0
die at [a]1. While there may be several oc-
currences of that type on the same day, it is
not expected to re-occur in other news events
(in different days), yielding a low accuracy
score.
</p>
<p>4 Analysis of Resource Quality
</p>
<p>We estimate the quality of the resource obtained
after ten weeks of collection by annotating a sam-
ple of the extracted paraphrases.
</p>
<p>The annotation task was carried out in Ama-
zon Mechanical Turk.8 To ensure the quality of
workers, we applied a qualification test and re-
quired a 99% approval rate for at least 1,000 prior
tasks. We assigned each annotation to 3 workers
and used the majority vote to determine the cor-
rectness of paraphrases.
</p>
<p>We followed a similar approach to instance-
based evaluation (Szpektor et al., 2007), and let
workers judge the correctness of a predicate pair
(e.g. [a]0 purchase [a]1/[a]0 acquire [a]1) through
5 different instances (e.g. Intel purchased Mobil-
eye/Intel acquired Mobileye). We considered the
type as correct if at least one of its instance-pairs
were judged as correct. The idea that lies behind
this type of evaluation is that predicate pairs are
difficult to judge out-of-context.
</p>
<p>Differently from Szpektor et al. (2007), we used
the instances in which the paraphrases appeared
originally, as those are available in the resource.
</p>
<p>8
https://www.mturk.com/mturk
</p>
<p>4.1 Quality of Extractions and Ranking
</p>
<p>To evaluate the resource accuracy, and follow-
ing the instance-based evaluation scheme, we only
considered paraphrases that occurred in at least 5
instances (which currently constitute 10% of the
paraphrase types). We partition the types into four
increasingly large bins according to their scores
(the smallest bin being the most accurate), simi-
larly to PPDB (Ganitkevitch et al., 2013), and an-
notate a sample of 50 types from each bin. Fig-
ure 2(a) shows that the frequent types achieve up
to 86% accuracy.
</p>
<p>The accuracy expectedly increases with the
score, except for the lowest-score bin ((0, 10])
which is more accurate than the next one
((10, 20]). At the current stage of the resource
there is a long tail of paraphrases that appeared
few times. While many of them are incorrect,
there are also true paraphrases that are infrequent
and therefore have a low accuracy score. We ex-
pect that some of these paraphrases will occur
again in the future and their accuracy score will
be strengthened.
</p>
<p>4.2 Size and Accuracy Over Time
</p>
<p>To estimate future usefulness, Figure 2(b) plots the
resource size (in terms of types and instances) and
estimated accuracy through each week in the first
10 weeks of collection.
</p>
<p>The accuracy at a specific time was estimated
by annotating a sample of 50 predicate pair types
with accuracy score ≥ 20 in the resource obtained
</p>
<p>158</p>
<p />
</div>
<div class="page"><p />
<p>drag [a]0 from [a]1 [a]0 remove from [a]1
leak [a]0 to [a]1 to share [a]0 with [a]1
</p>
<p>oust [a]0 from [a]1 [a]0 be force out at [a]1
reveal [a]0 to [a]1 share [a]0 with [a]1
</p>
<p>[a]0 add [a]1 [a]0 beef up [a]1
[a]0 admit to [a]1 [a]0 will attend [a]1
</p>
<p>[a]0 announce as [a]1 [a]0 to become [a]1
[a]0 arrest in [a]1 [a]0 charge in [a]1
[a]0 attack [a]1 [a]0 clash with [a]1
</p>
<p>[a]0 be force out at [a]1 [a]0 have be fire from [a]1
[a]0 eliminate [a]1 [a]0 slash [a]1
</p>
<p>[a]0 face [a]1 [a]0 hit with [a]1
[a]0 mock [a]1 [a]0 troll [a]1
</p>
<p>[a]0 open up about [a]1 [a]0 reveal [a]1
[a]0 get [a]1 [a]0 sentence to [a]1
</p>
<p>Table 3: A sample of types from our resource that
are not found in Berant or in PPDB.
</p>
<p>at that time, which roughly correspond to the top
ranked 1.5% types.
</p>
<p>Figure 2(b) demonstrates that these types main-
tain a level of around 80% in accuracy. The re-
source growth rate (i.e. the number of new types)
is expected to change with time. We predict that
the resource will contain around 2 million types in
one year.9
</p>
<p>5 Comparison to Existing Resources
</p>
<p>The resources which are most similar to ours are
Berant (Berant, 2012), a resource of predicate
entailments, and PPDB (Pavlick et al., 2015), a re-
source of paraphrases, both described in Section 2.
</p>
<p>We expect our resource to be more accurate than
resources which are based on the distributional ap-
proach (Berant, 2012; Lin and Pantel, 2001). In
addition, in comparison to PPDB, we specialize on
binary verbal predicates, and apply an additional
phase of proposition extraction, handling various
phenomena such as non-consecutive particles and
minimality of arguments.
</p>
<p>Berant (2012) evaluated their resource against
a dataset of predicate entailments (Zeichner et al.,
2012), using a recall-precision curve to show the
performance obtained with a range of thresholds
on the resource score. This kind of evaluation is
less suitable for our resource; first, predicate en-
tailment is directional, causing paraphrases with
the wrong entailment direction to be labeled neg-
ative in the dataset. Second, since our resource is
still relatively small, it is unlikely to have sufficient
coverage of the dataset at that point. We therefore
</p>
<p>9For up-to-date resource statistics, see: https://github.
com/vered1986/Chirps/tree/master/resource.
</p>
<p>leave this evaluation to future work.
To demonstrate the added value of our resource,
</p>
<p>we show that even in its current size, it already
contains accurate predicate pairs which are absent
from the existing resources. Rather than compar-
ing against labeled data, we use types with score
≥ 50 from our resource (1,778 pairs), which were
assessed as accurate (Section 4.2).
</p>
<p>We checked whether these predicate pairs are
covered by Berant and PPDB. To eliminate di-
rectionality, we looked for types in both directions,
i.e. for a predicate pair (p1, p2) we searched for
both (p1, p2) and (p2, p1). Overall, we found that
67% of these types do not exist in Berant, 62%
in PPDB, and 49% in neither.
</p>
<p>Table 3 exemplifies some of the predicate pairs
that do not exist in both resources. Specifically,
our resource contains many non-consecutive pred-
icates (e.g. reveal [a]0 to [a]1 / share [a]0 with [a]1)
that by definition do not exist in Berant.
</p>
<p>Some pairs, such as [a]0 get [a]1 / [a]0 sentence
to [a]1, are context-specific, occurring when [a]0
is a person and [a]1 is the time they are about to
serve in prison. Given that get has a broad distri-
bution of argument instantiations, this paraphrase
and similar paraphrases are less likely to exist in
resources that rely on the distribution of arguments
in the entire corpus.
</p>
<p>6 Conclusion
</p>
<p>We presented a new unsupervised method to ac-
quire fairly accurate predicate paraphrases from
news tweets discussing the same event. We re-
lease a growing resource of predicate paraphrases.
Qualitative analysis shows that our resource adds
value over existing resources. In the future, when
the resource is comparable in size to the existing
resources, we plan to evaluate its intrinsic accu-
racy on annotated test sets, as well as its extrinsic
benefits in downstream NLP applications.
</p>
<p>Acknowledgments
</p>
<p>This work was partially supported by an Intel
ICRI-CI grant, the Israel Science Foundation grant
880/12, and the German Research Foundation
through the German-Israeli Project Cooperation
(DIP, grant DA 1600/1-1).
</p>
<p>159</p>
<p />
</div>
<div class="page"><p />
<p>References
Amit Bagga and Breck Baldwin. 1999. Cross-
</p>
<p>document event coreference: Annotations, experi-
ments, and observations. In Workshop on Corefer-
ence and its Applications.
</p>
<p>Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceed-
ings of the 2003 Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguistics.
http://aclweb.org/anthology/N03-1003.
</p>
<p>Regina Barzilay and R. Kathleen McKeown. 2001.
Extracting paraphrases from a parallel corpus.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics.
http://aclweb.org/anthology/P01-1008.
</p>
<p>Jonathan Berant. 2012. Global Learning of Textual En-
tailment Graphs. Ph.D. thesis, Tel Aviv University.
</p>
<p>Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, pages 1220–
1229. http://aclweb.org/anthology/P10-1124.
</p>
<p>Ido Dagan, Dan Roth, and Mark Sammons. 2013. Rec-
ognizing textual entailment .
</p>
<p>Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies. Association for
Computational Linguistics, pages 758–764.
http://aclweb.org/anthology/N13-1092.
</p>
<p>Zellig S Harris. 1954. Distributional structure. Word
10(2-3):146–162.
</p>
<p>Dekang Lin and Patrick Pantel. 2001. Dirt – Discovery
of inference rules from text. In Proceedings of the
seventh ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, pages
323–328.
</p>
<p>Ellie Pavlick, Pushpendre Rastogi, Juri Ganitke-
vitch, Benjamin Van Durme, and Chris Callison-
Burch. 2015. PPDB 2.0: Better paraphrase rank-
ing, fine-grained entailment relations, word em-
beddings, and style classification. In Proceed-
ings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 425–430.
https://doi.org/10.3115/v1/P15-2070.
</p>
<p>Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
</p>
<p>texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics – Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval 2012). Associa-
tion for Computational Linguistics, pages 218–227.
http://aclweb.org/anthology/S12-1030.
</p>
<p>Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference. http://aclweb.org/anthology/N06-1039.
</p>
<p>Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the second interna-
tional conference on Human Language Technology
Research. Morgan Kaufmann Publishers Inc., pages
313–318.
</p>
<p>Gabriel Stanovsky and Ido Dagan. 2016. Annotating
and predicting non-restrictive noun phrase modifica-
tions. In Proceedings of the 54rd Annual Meeting of
the Association for Computational Linguistics (ACL
2016).
</p>
<p>Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and
Yoav Goldberg. 2016. Getting more out of
syntax with props. CoRR abs/1603.01648.
http://arxiv.org/abs/1603.01648.
</p>
<p>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. As-
sociation for Computational Linguistics, pages 456–
463. http://aclweb.org/anthology/P07-1058.
</p>
<p>Wei Xu, Alan Ritter, Chris Callison-Burch, William B
Dolan, and Yangfeng Ji. 2014. Extracting lexi-
cally divergent paraphrases from twitter. Transac-
tions of the Association for Computational Linguis-
tics 2:435–448.
</p>
<p>Naomi Zeichner, Jonathan Berant, and Ido Da-
gan. 2012. Crowdsourcing inference-rule evalua-
tion. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Association
for Computational Linguistics, pages 156–160.
http://aclweb.org/anthology/P12-2031.
</p>
<p>Congle Zhang, Stephen Soderland, and Daniel S Weld.
2015. Exploiting parallel news streams for unsuper-
vised event extraction. Transactions of the Associa-
tion for Computational Linguistics 3:117–129.
</p>
<p>Congle Zhang and Daniel S Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing. Seattle, Washington, USA, pages 1776–
1786.
</p>
<p>160</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Evaluating Semantic Parsing against a Simple Web-based Question
Answering Model
</p>
<p>Alon Talmor
Tel-Aviv University
</p>
<p>alontalmor@mail.tau.ac.il
</p>
<p>Mor Geva
Tel-Aviv University
</p>
<p>morgeva@mail.tau.ac.il
</p>
<p>Jonathan Berant
Tel-Aviv University
</p>
<p>joberant@cs.tau.ac.il
</p>
<p>Abstract
</p>
<p>Semantic parsing shines at analyzing com-
plex natural language that involves com-
position and computation over multiple
pieces of evidence. However, datasets
for semantic parsing contain many factoid
questions that can be answered from a sin-
gle web document. In this paper, we pro-
pose to evaluate semantic parsing-based
question answering models by comparing
them to a question answering baseline that
queries the web and extracts the answer
only from web snippets, without access to
the target knowledge-base. We investigate
this approach on COMPLEXQUESTIONS,
a dataset designed to focus on composi-
tional language, and find that our model
obtains reasonable performance (∼35 F1
compared to 41 F1 of state-of-the-art). We
find in our analysis that our model per-
forms well on complex questions involv-
ing conjunctions, but struggles on ques-
tions that involve relation composition and
superlatives.
</p>
<p>1 Introduction
</p>
<p>Question answering (QA) has witnessed a surge
of interest in recent years (Hill et al., 2015; Yang
et al., 2015; Pasupat and Liang, 2015; Chen et al.,
2016; Joshi et al., 2017), as it is one of the promi-
nent tests for natural language understanding. QA
can be coarsely divided into semantic parsing-
based QA, where a question is translated into a
logical form that is executed against a knowledge-
base (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Liang et al., 2011; Kwiatkowski
et al., 2013; Reddy et al., 2014; Berant and
Liang, 2015), and unstructured QA, where a ques-
tion is answered directly from some relevant text
</p>
<p>(Voorhees and Tice, 2000; Hermann et al., 2015;
Hewlett et al., 2016; Kadlec et al., 2016; Seo et al.,
2016).
</p>
<p>In semantic parsing, background knowledge
has already been compiled into a knowledge-base
(KB), and thus the challenge is in interpreting the
question, which may contain compositional con-
structions (“What is the second-highest mountain
in Europe?”) or computations (“What is the dif-
ference in population between France and Ger-
many?”). In unstructured QA, the model needs
to also interpret the language of a document, and
thus most datasets focus on matching the question
against the document and extracting the answer
from some local context, such as a sentence or a
paragraph (Onishi et al., 2016; Rajpurkar et al.,
2016; Yang et al., 2015).
</p>
<p>Since semantic parsing models excel at han-
dling complex linguistic constructions and reason-
ing over multiple facts, a natural way to exam-
ine whether a benchmark indeed requires model-
ing these properties, is to train an unstructured QA
model, and check if it under-performs compared
to semantic parsing models. If questions can be
answered by examining local contexts only, then
the use of a knowledge-base is perhaps unneces-
sary. However, to the best of our knowledge, only
models that utilize the KB have been evaluated on
common semantic parsing benchmarks.
</p>
<p>The goal of this paper is to bridge this evalua-
tion gap. We develop a simple log-linear model,
in the spirit of traditional web-based QA systems
(Kwok et al., 2001; Brill et al., 2002), that answers
questions by querying the web and extracting the
answer from returned web snippets. Thus, our
evaluation scheme is suitable for semantic pars-
ing benchmarks in which the knowledge required
for answering questions is covered by the web (in
contrast with virtual assitants for which the knowl-
edge is specific to an application).
</p>
<p>161</p>
<p />
</div>
<div class="page"><p />
<p>We test this model on COMPLEXQUESTIONS
(Bao et al., 2016), a dataset designed to re-
quire more compositionality compared to earlier
datasets, such as WEBQUESTIONS (Berant et al.,
2013) and SIMPLEQUESTIONS (Bordes et al.,
2015). We find that a simple QA model, despite
having no access to the target KB, performs rea-
sonably well on this dataset (∼35 F1 compared to
the state-of-the-art of 41 F1). Moreover, for the
subset of questions for which the right answer can
be found in one of the web snippets, we outper-
form the semantic parser (51.9 F1 vs. 48.5 F1). We
analyze results for different types of composition-
ality and find that superlatives and relation com-
position constructions are challenging for a web-
based QA system, while conjunctions and events
with multiple arguments are easier.
</p>
<p>An important insight is that semantic parsers
must overcome the mismatch between natural lan-
guage and formal language. Consequently, lan-
guage that can be easily matched against the web
may become challenging to express in logical
form. For example, the word “wife” is an atomic
binary relation in natural language, but expressed
with a complex binary λx.λy.Spouse(x, y) ∧
Gender(x,Female) in knowledge-bases. Thus,
some of the complexity of understanding natural
language is removed when working with a natural
language representation.
</p>
<p>To conclude, we propose to evaluate the extent
to which semantic parsing-based QA benchmarks
require compositionality by comparing semantic
parsing models to a baseline that extracts the an-
swer from short web snippets. We obtain rea-
sonable performance on COMPLEXQUESTIONS,
and analyze the types of compositionality that are
challenging for a web-based QA model. To en-
sure reproducibility, we release our dataset, which
attaches to each example from COMPLEXQUES-
TIONS the top-100 retrieved web snippets.1
</p>
<p>2 Problem Setting and Dataset
</p>
<p>Given a training set of triples {q(i), R(i), a(i)}Ni=1,
where q(i) is a question, R(i) is a web result set,
and a(i) is the answer, our goal is to learn a model
that produces an answer a for a new question-
result set pair (q,R). A web result set R consists
of K(= 100) web snippets, where each snippet si
</p>
<p>1Data can be downloaded from https:
//worksheets.codalab.org/worksheets/
0x91d77db37e0a4bbbaeb37b8972f4784f/
</p>
<p>R:
</p>
<p>s1: Billy Batts (Character) - Biography - IMDb
Billy Batts (Character) on IMDb: Movies, TV,
Celebs, and more... ... Devino is portrayed by
Frank Vincent in the film Goodfellas. Page last up-
dated by !!!de leted!!!
s2: Frank Vincent - Wikipedia He appeared in
Scorsese’s 1990 film Goodfellas, where he played
Billy Batts, a made man in the Gambino crime fam-
ily. He also played a role in Scorsese’s...
...
s100: Voice-over in Goodfellas In the summer when
they played cards all night, nobody ever called the
cops. .... But we had a problem with Billy Batts.
This was a touchy thing. Tommy had killed a made
man. Billy was a part of the Bambino crew and un-
touchable. Before you...
</p>
<p>q: “who played the part of billy batts in goodfellas?”
a: “Frank Vincent”
</p>
<p>Figure 1: A training example containing a result set R, a
question q and an answer a. The result set R contains 100
web snippets si, each including a title (boldface) and text.
The answer is underlined.
</p>
<p>has a title and a text fragment. An example for a
training example is provided in Figure 1.
</p>
<p>Semantic parsing-based QA datasets contain
question-answer pairs alongside a background
KB. To convert such datasets to our setup, we run
the question q against Google’s search engine and
scrape the top-K web snippets. We use only the
web snippets and ignore any boxes or other infor-
mation returned (see Figure 1 and the full dataset
in the supplementary material).
</p>
<p>Compositionality We argue that if a dataset
truly requires a compositional model, then it
should be difficult to tackle with methods that
only match the question against short web snip-
pets. This is since it is unlikely to integrate all
necessary pieces of evidence from the snippets.
</p>
<p>We convert COMPLEXQUESTIONS into the
aforementioned format, and manually analyze the
types of compositionality that occur on 100 ran-
dom training examples. Table 1 provides an ex-
ample for each of the question types we found:
</p>
<p>SIMPLE: an application of a single binary re-
lation on a single entity.
FILTER: a question where the semantic type
of the answer is mentioned (“tv shows” in Ta-
ble 1).
N-ARY: A question about a single event that
involves more than one entity (“juni” and
“spy kids 4” in Table 1).
CONJUNCTION: A question whose answer is
the conjunction of more than one binary rela-
tion in the question.
</p>
<p>162</p>
<p />
</div>
<div class="page"><p />
<p>Type Example %
</p>
<p>SIMPLE “who has gone out with cornelis de graeff” 17%
FILTER “which tv shows has wayne rostad starred in” 18%
N-ARY “who played juni in spy kids 4?” 51%
CONJ. “what has queen latifah starred in that doug 10%
</p>
<p>mchenry directed”
COMPOS. “who was the grandson of king david’s father?” 7%
SUPERL. “who is the richest sports woman?” 9%
OTHER “what is the name george lopez on the show?” 8%
</p>
<p>Table 1: An example for each compositionality type and the
proportion of examples in 100 random examples. A question
can fall into multiple types, and thus the sum exceeds 100%.
</p>
<p>COMPOSITION A question that involves
composing more than one binary relation
over an entity (“grandson” and “father” in
Table 1).
SUPERLATIVE A question that requires sort-
ing or comparing entities based on a numeric
property.
OTHER Any other question.
</p>
<p>Table 1 illustrates that COMPLEXQUESTIONS
is dominated by N-ARY questions that involve an
event with multiple entities. In Section 4 we eval-
uate the performance of a simple QA model for
each compositionality type, and find that N-ARY
questions are handled well by our web-based QA
system.
</p>
<p>3 Model
</p>
<p>Our model comprises two parts. First, we extract
a set of answer candidates, A, from the web result
set. Then, we train a log-linear model that outputs
a distribution over the candidates inA, and is used
at test time to find the most probable answers.
</p>
<p>Candidate Extraction We extract all 1-grams,
2-grams, 3-grams and 4-grams (lowercased) that
appear inR, yielding roughly 5,000 candidates per
question. We then discard any candidate that fully
appears in the question itself, and define A to be
the top-K candidates based on their tf-idf score,
where term frequency is computed on all the snip-
pets inR, and inverse document frequency is com-
puted on a large external corpus.
</p>
<p>Candidate Ranking We define a log-linear
model over the candidates in A:
</p>
<p>pθ(a | q,R) = exp(φ(q,R, a)
&gt;θ)∑
</p>
<p>a′∈A exp(φ(q,R, a′)&gt;θ)
,
</p>
<p>where θ ∈ Rd are learned parameters, and
φ(·) ∈ Rd is a feature function. We train
</p>
<p>our model by maximizing the regularized condi-
tional log-likelihood objective
</p>
<p>∑N
i=1 log pθ(a
</p>
<p>(i) |
q(i), R(i)) + λ · ||θ||22. At test time, we return the
most probable answers based on pθ(a | q,R) (de-
tails in Section 4). While semantic parsers gener-
ally return a set, in COMPLEXQUESTIONS 87% of
the answers are a singleton set.
</p>
<p>Features A candidate span a often has multiple
mentions in the result setR. Therefore, our feature
function φ(·) computes the average of the features
extracted from each mention. The main informa-
tion sources used are the match between the candi-
date answer itself and the question (top of Table 2)
and the match between the context of a candidate
answer in a specific mention and the question (bot-
tom of Table 2), as well as the Google rank in
which the mention appeared.
</p>
<p>Lexicalized features are useful for our task, but
the number of training examples is too small to
train a fully lexicalized model. Therefore, we de-
fine lexicalized features over the 50 most common
non-stop words in COMPLEXQUESTIONS. Last,
our context features are defined in a 6-word win-
dow around the candidate answer mention, where
the feature value decays exponentially as the dis-
tance from the candidate answer mention grows.
Overall, we compute a total of 892 features over
the dataset.
</p>
<p>4 Experiments
</p>
<p>COMPLEXQUESTIONS contains 1,300 training
examples and 800 test examples. We performed
5 random 70/30 splits of the training set for de-
velopment. We computed POS tags and named
entities with Stanford CoreNLP (Manning et al.,
2014). We did not employ any co-reference reso-
lution tool in this work. If after candidate extrac-
tion, we do not find the gold answer in the top-
K(=140) candidates, we discard the example, re-
sulting in a training set of 856 examples.
</p>
<p>We compare our model, WEBQA, to STAGG
(Yih et al., 2015) and COMPQ (Bao et al.,
2016), which are to the best of our knowledge
the highest performing semantic parsing models
on both COMPLEXQUESTIONS and WEBQUES-
TIONS. For these systems, we only report test F1
numbers that are provided in the original papers,
as we do not have access to the code or predic-
tions. We evaluate models by computing average
F1, the official evaluation metric defined for COM-
PLEXQUESTIONS. This measure computes the F1
</p>
<p>163</p>
<p />
</div>
<div class="page"><p />
<p>Template Description
</p>
<p>SPAN LENGTH Indicator for the number of tokens in am
TF-IDF Binned and raw tf-idf scores of am for every
</p>
<p>span length
CAPITALIZED Whether am is capitalized
STOP WORD Fraction of words in am that are stop words
IN QUEST Fraction of words in am that are in q
IN QUEST+COMMON Conjunction of IN QUEST with common words
</p>
<p>in q
IN QUESTION DIST. Max./avg. cosine similarity between am
</p>
<p>words and q words
WH+NE Conjunction of wh-word in q and named entity
</p>
<p>tags (NE) of am
WH+POS Conjunction of wh-word in q and
</p>
<p>part-of-speech tags of am
NE+NE Conjunction of NE tags in q and NE tags in am
NE+COMMON Conjunction of NE tags in am and common
</p>
<p>words in q
MAX-NE Whether am is a NE with maximal span
</p>
<p>(not contained in another NE)
YEAR Binned indicator for year if am is a year
</p>
<p>CTXT MATCH Max./avg. over non stop words in q, for
whether a q word occurs around am, weighted
by distance from am
</p>
<p>CTXT SIMILARITY Max./avg. cosine similarity over non-stop
words in q, between q words and words around
am, weighted by distance
</p>
<p>IN TITLE Whether am is in the title part of the snippet
CTXT ENTITY Indicator for whether a common word appears
</p>
<p>between am and a named entity that appears
in q
</p>
<p>GOOGLE RANK Binned snippet rank of am in the result set R
</p>
<p>Table 2: Features templates used to extract features from each
answer candidate mention am. Cosine similarity is computed
with pre-trained GloVe embeddings (Pennington et al., 2014).
The definition of common words and weighting by distance is
in the body of the paper.
</p>
<p>between the set of answers returned by the system
and the set of gold answers, and averages across
questions. To allow WEBQA to return a set rather
than a single answer, we return the most proba-
ble answer a∗ as well as any answer a such that
(φ(q,R, a∗)&gt;θ − φ(q,R, a)&gt;θ) &lt; 0.5. We also
compute precision@1 and Mean Reciprocal Rank
(MRR) for WEBQA, since we have a ranking over
answers. To compute metrics we lowercase the
gold and predicted spans and perform exact string
match.
</p>
<p>Table 3 presents the results of our evaluation.
WEBQA obtained 32.6 F1 (33.5 p@1, 42.4 MRR)
compared to 40.9 F1 of COMPQ. Our candidate
extraction step finds the correct answer in the top-
K candidates in 65.9% of development examples
and 62.7% of test examples. Thus, our test F1
on examples for which candidate extraction suc-
ceeded (WEBQA-SUBSET) is 51.9 (53.4 p@1,
67.5 MRR).
</p>
<p>We were able to indirectly compare WEBQA-
SUBSET to COMPQ: Bao et al. (2016) graciously
provided us with the predictions of COMPQ when
it was trained on COMPLEXQUESTIONS, WE-
BQUESTIONS, and SIMPLEQUESTIONS. In this
</p>
<p>Dev Test
System F1 p@1 F1 p@1 MRR
</p>
<p>STAGG - - 37.7 - -
COMPQ - - 40.9 - -
</p>
<p>WEBQA 35.3 36.4 32.6 33.5 42.4
WEBQA-EXTRAPOL - - 34.4 - -
</p>
<p>COMPQ-SUBSET - - 48.5 - -
WEBQA-SUBSET 53.6 55.1 51.9 53.4 67.5
</p>
<p>Table 3: Results on development (average over random splits)
and test set. Middle: results on all examples. Bottom: results
on the subset where candidate extraction succeeded.
</p>
<p>setup, COMPQ obtained 42.2 F1 on the test set
(compared to 40.9 F1, when training on COM-
PLEXQUESTIONS only, as we do). Restricting the
predictions to the subset for which candidate ex-
traction succeeded, the F1 of COMPQ-SUBSET is
48.5, which is 3.4 F1 points lower than WEBQA-
SUBSET, which was trained on less data.
</p>
<p>Not using a KB, results in a considerable disad-
vantage for WEBQA. KB entities have normalized
descriptions, and the answers have been annotated
according to those descriptions. We, conversely,
find answers on the web and often predict a cor-
rect answer, but get penalized due to small string
differences. E.g., for “what is the longest river in
China?” we answer “yangtze river”, while the
gold answer is “yangtze”. To quantify this effect
we manually annotated all 258 examples in the
first random development set split, and determined
whether string matching failed, and we actually
returned the gold answer.2 This improved perfor-
mance from 53.6 F1 to 56.6 F1 (on examples that
passed candidate extraction). Further normaliz-
ing gold and predicted entities, such that “Hillary
Clinton” and “Hillary Rodham Clinton” are uni-
fied, improved F1 to 57.3 F1. Extrapolating this to
the test set would result in an F1 of 34.4 (WEBQA-
EXTRAPOL in Table 3) and 34.9, respectively.
</p>
<p>Last, to determine the contribution of each fea-
ture template, we performed ablation tests and we
present the five feature templates that resulted in
the largest drop to performance on the develop-
ment set in Table 4. Note that TF-IDF is by far the
most impactful feature, leading to a large drop of
12 points in performance. This shows the impor-
tance of using the redundancy of the web for our
QA system.
</p>
<p>Analysis To understand the success of WEBQA
on different compositionality types, we manu-
</p>
<p>2We also publicly release our annotations.
</p>
<p>164</p>
<p />
</div>
<div class="page"><p />
<p>Simple Filter N-ary Conj. Compos. Superl. Other
0%
</p>
<p>10%
</p>
<p>20%
</p>
<p>30%
</p>
<p>40%
</p>
<p>50%
</p>
<p>60% % Passed cand. extraction
%  Failed cand. extraction
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>50
</p>
<p>60
</p>
<p>70
</p>
<p>F1
 V
</p>
<p>al
ue
</p>
<p>Average dev F1
</p>
<p>Figure 2: Proportion of examples that passed or failed can-
didate extraction for each compositionality type, as well as
average F1 for each compositionality type. COMPOSITION
and SUPERLATIVE questions are difficult for WEBQA.
</p>
<p>Feature Template F1 ∆
</p>
<p>WEBQA 53.6
- MAX-NE 51.8 -1.8
- NE+COMMON 51.8 -1.8
- GOOGLE RANK 51.4 -2.2
- IN QUEST 50.1 -3.5
- TF-IDF 41.5 -12
</p>
<p>Table 4: Feature ablation results. The five features that lead
to largest drop in performance are displayed.
</p>
<p>ally annotated the compositionality type of 100
random examples that passed candidate extrac-
tion and 50 random examples that failed candi-
date extraction. Figure 2 presents the results of
this analysis, as well as the average F1 obtained
for each compositionality type on the 100 exam-
ples that passed candidate extraction (note that
a question can belong to multilpe compositional-
ity types). We observe that COMPOSITION and
SUPERLATIVE questions are challenging for WE-
BQA, while SIMPLE, FILTER, and N-ARY quesi-
tons are easier (recall that a large fraction of the
questions in COMPLEXQUESTIONS are N-ARY).
Interestingly, WEBQA performs well on CON-
JUNCTION questions (“what film victor garber
starred in that rob marshall directed”), possibly
because the correct answer can obtain signal from
multiple snippets.
</p>
<p>An advantage of finding answers to ques-
tions from web documents compared to seman-
tic parsing, is that we do not need to learn the
“language of the KB”. For example, the ques-
tion “who is the governor of California 2010”
can be matched directly to web snippets, while
in Freebase (Bollacker et al., 2008) the word
“governor” is expressed by a complex predicate
λx.∃z.GoverPos(x, z) ∧ PosTitle(z,Governor).
This could provide a partial explanation for the
reasonable performance of WEBQA.
</p>
<p>5 Related Work
</p>
<p>Our model WEBQA performs QA using web snip-
pets, similar to traditional QA systems like MUL-
DER (Kwok et al., 2001) and AskMSR (Brill et al.,
2002). However, it it enjoys the advances in com-
merical search engines of the last decade, and uses
a simple log-linear model, which has become stan-
dard in Natural Language Processing.
</p>
<p>Similar to this work, Yao et al. (2014) ana-
lyzed a semantic parsing benchmark with a simple
QA system. However, they employed a semantic
parser that is limited to applying a single binary
relation on a single entity, while we develop a QA
system that does not use the target KB at all.
</p>
<p>Last, in parallel to this work Chen et al. (2017)
evaluated an unstructured QA system against se-
mantic parsing benchmarks. However, their fo-
cus was on examining the contributions of multi-
task learning and distant supervision to training
rather than to compare to state-of-the-art seman-
tic parsers.
</p>
<p>6 Conclusion
</p>
<p>We propose in this paper to evaluate semantic
parsing-based QA systems by comparing them to
a web-based QA baseline. We evaluate such a QA
system on COMPLEXQUESTIONS and find that it
obtains reasonable performance. We analyze per-
formance and find that COMPOSITION and SU-
PERLATIVE questions are challenging for a web-
based QA system, while CONJUNCTION and N-
ARY questions can often be handled by our QA
model.
</p>
<p>Reproducibility Code, data, annotations, and
experiments for this paper are available on the
CodaLab platform at https://worksheets.
codalab.org/worksheets/
0x91d77db37e0a4bbbaeb37b8972f4784f/.
</p>
<p>Acknowledgments
</p>
<p>We thank Junwei Bao for providing us with the test
predictions of his system. We thank the anony-
mous reviewers for their constructive feedback.
This work was partially supported by the Israel
Science Foundation, grant 942/16.
</p>
<p>References
J. Bao, N. Duan, Z. Yan, M. Zhou, and T. Zhao. 2016.
</p>
<p>Constraint-based question answering with knowl-
</p>
<p>165</p>
<p />
</div>
<div class="page"><p />
<p>edge graph. In International Conference on Com-
putational Linguistics (COLING).
</p>
<p>J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Se-
mantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
</p>
<p>J. Berant and P. Liang. 2015. Imitation learning of
agenda-based semantic parsers. Transactions of the
Association for Computational Linguistics (TACL)
3:545–558.
</p>
<p>K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively created
graph database for structuring human knowledge. In
International Conference on Management of Data
(SIGMOD). pages 1247–1250.
</p>
<p>A. Bordes, N. Usunier, S. Chopra, and J. Weston. 2015.
Large-scale simple question answering with mem-
ory networks. arXiv preprint arXiv:1506.02075 .
</p>
<p>E. Brill, S. Dumais, and M. Banko. 2002. An analy-
sis of the AskMSR question-answering system. In
Association for Computational Linguistics (ACL).
pages 257–264.
</p>
<p>D. Chen, J. Bolton, and C. D. Manning. 2016. A thor-
ough examination of the CNN / Daily Mail reading
comprehension task. In Association for Computa-
tional Linguistics (ACL).
</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Association for Computa-
tional Linguistics (ACL).
</p>
<p>K. M. Hermann, T. Koisk, E. Grefenstette, L. Espe-
holt, W. Kay, M. Suleyman, and P. Blunsom. 2015.
Teaching machines to read and comprehend. In Ad-
vances in Neural Information Processing Systems
(NIPS).
</p>
<p>D. Hewlett, A. Lacoste, L. Jones, I. Polosukhin,
A. Fandrianto, J. Han, M. Kelcey, and D. Berthelot.
2016. Wikireading: A novel large-scale language
understanding task over Wikipedia. In Association
for Computational Linguistics (ACL).
</p>
<p>F. Hill, A. Bordes, S. Chopra, and J. Weston. 2015. The
goldilocks principle: Reading children’s books with
explicit memory representations. In International
Conference on Learning Representations (ICLR).
</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551 .
</p>
<p>R. Kadlec, M. Schmid, O. Bajgar, and J. Kleindienst.
2016. Text understanding with the attention sum
reader network. In Association for Computational
Linguistics (ACL).
</p>
<p>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
</p>
<p>C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scaling
question answering to the web. ACM Transactions
on Information Systems (TOIS) 19:242–262.
</p>
<p>P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
pages 590–599.
</p>
<p>C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.
Bethard, and D. McClosky. 2014. The stanford
coreNLP natural language processing toolkit. In
ACL system demonstrations.
</p>
<p>T. Onishi, H. Wang, M. Bansal, K. Gimpel, and
D. McAllester. 2016. Whodid what: A large-scale
person-centered cloze dataset. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
</p>
<p>P. Pasupat and P. Liang. 2015. Compositional semantic
parsing on semi-structured tables. In Association for
Computational Linguistics (ACL).
</p>
<p>J. Pennington, R. Socher, and C. D. Manning. 2014.
Glove: Global vectors for word representation. In
Empirical Methods in Natural Language Processing
(EMNLP).
</p>
<p>P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016.
Squad: 100,000+ questions for machine comprehen-
sion of text. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
</p>
<p>S. Reddy, M. Lapata, and M. Steedman. 2014. Large-
scale semantic parsing without question-answer
pairs. Transactions of the Association for Compu-
tational Linguistics (TACL) 2(10):377–392.
</p>
<p>M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi.
2016. Bidirectional attention flow for machine com-
prehension. arXiv .
</p>
<p>E. M. Voorhees and D. M. Tice. 2000. Building a
question answering test collection. In ACM Spe-
cial Interest Group on Information Retreival (SI-
GIR). pages 200–207.
</p>
<p>Y. Yang, W. Yih, and C. Meek. 2015. WikiQA: A chal-
lenge dataset for open-domain question answering.
In Empirical Methods in Natural Language Process-
ing (EMNLP). pages 2013–2018.
</p>
<p>X. Yao, J. Berant, and B. Van-Durme. 2014. Freebase
QA: Information extraction or semantic parsing. In
Workshop on Semantic parsing.
</p>
<p>W. Yih, M. Chang, X. He, and J. Gao. 2015. Semantic
parsing via staged query graph generation: Question
answering with knowledge base. In Association for
Computational Linguistics (ACL).
</p>
<p>166</p>
<p />
</div>
<div class="page"><p />
<p>M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic program-
ming. In Association for the Advancement of Arti-
ficial Intelligence (AAAI). pages 1050–1055.
</p>
<p>L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI). pages 658–
666.
</p>
<p>167</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 168–177,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Logical Metonymy in a Distributional Model of Sentence Comprehension
</p>
<p>Emmanuele Chersoni
Aix-Marseille University
</p>
<p>emmanuelechersoni@gmail.com
</p>
<p>Alessandro Lenci
University of Pisa
</p>
<p>alessandro.lenci@unipi.it
</p>
<p>Philippe Blache
Aix-Marseille University
</p>
<p>philippe.blache@univ-amu.fr
</p>
<p>Abstract
</p>
<p>In theoretical linguistics, logical
metonymy is defined as the combina-
tion of an event-subcategorizing verb
with an entity-denoting direct object
(e.g., The author began the book), so
that the interpretation of the VP requires
the retrieval of a covert event (e.g.,
writing). Psycholinguistic studies have
revealed extra processing costs for logical
metonymy, a phenomenon generally
explained with the introduction of new
semantic structure. In this paper, we
present a general distributional model for
sentence comprehension inspired by the
Memory, Unification and Control model
by Hagoort (2013, 2016). We show that
our distributional framework can account
for the extra processing costs of logical
metonymy and can identify the covert
event in a classification task.
</p>
<p>1 Logical Metonymy: Psycholinguistic
Evidence and Computational Modeling
</p>
<p>The interpretation of so-called logical metonymy
(e.g, The student begins the book) has received
an extensive attention in both psycholinguistic
and linguistic research. The phenomenon is ex-
tremely problematic for traditional theories of
compositionality (Asher, 2015) and is generally
explained as a type clash between an event-
selecting metonymic verb (e.g., begin) and an
entity-denoting nominal object (e.g., the book),
which triggers the recovery of a hidden event
(e.g., reading). Past research work brought ex-
tensive evidence that such metonymic construc-
tions also determine extra processing costs during
online sentence comprehension (McElree et al.,
</p>
<p>2001; Traxler et al., 2002), although such evidence
is not uncontroversial (Falkum, 2011). According
to Frisson and McElree (2008), event recovery is
triggered by the type clash, and the extra process-
ing load is due to ”the deployment of operations to
construct a semantic representation of the event”.
Thus, logical metonymy raises two major ques-
tions: i.) How is the hidden event recovered? ii.)
What is the relationship between such mechanism
and the increase in processing difficulty?
</p>
<p>One of the first accounts of the phenomenon
dates back to the works of Pustejovsky (1995) and
Jackendoff (1997), which assume that the covert
event is retrieved from complex lexical entries
consisting of rich knowledge structures (Puste-
jovsky’s qualia roles). For example, the repre-
sentation of a noun like book includes telic prop-
erties (the purpose of the entity, e.g. read) and
agentive properties (the mode of creation of the
entity, e.g. write). The predicate-argument type
mismatch triggers the retrieval of a covert event
from the object noun qualia roles, thereby pro-
ducing a semantic representation equivalent to be-
gin to write the paper (see also the discussion in
Traxler et al. (2002)).
</p>
<p>On the one hand, the lexicalist explanation is
very appealing, since it accounts for the existence
of default interpretations of logical metonymies
(e.g. begin the book is typically interpreted as
begin reading/writing the book). On the other
hand, Lascarides and Copestake (1998) and more
recently Zarcone et al. (2014) show that qualia
roles are simply not flexible enough to account for
the wide variety of interpretations that can be re-
trieved. These are in fact affected by the subject
choice, the general syntactic and discourse con-
text, and by our world knowledge. 1
</p>
<p>1Consider the classical example from Lascarides and
Copestake (1998): My goat eats anything. He really enjoys
</p>
<p>168</p>
<p />
</div>
<div class="page"><p />
<p>An alternative view on logical metonymy has
been proposed in the field of relevance-theoretic
pragmatics (Sperber and Wilson, 1986; Carston,
2002). According to studies such as de Almeida
(2004), de Almeida and Dwivedi (2008) and
Falkum (2011), the metonymy resolution process
is driven by post-lexical pragmatic inferences, re-
lying on both general world knowledge and dis-
course context. The ‘pragmatic hypothesis’ allows
for the necessary flexibility in the interpretation of
logical metonymies, since the range of the poten-
tial covert events is not constrained by the lexical
entry, but only by the hearer’s expectations of the
optimal relevance of the utterance. However, as
pointed out by Zarcone and Padó (2011), the prag-
matic account is not precise with respect to the
mechanism and to the type of knowledge involved
in the process of metonymy resolution. Moreover,
it tends to disregard the fact that there are default
interpretations that are activated in neutral, less in-
formative contexts.
</p>
<p>More recently, Zarcone and Padó (2011) and
Zarcone et al. (2014) brought experimental evi-
dence for the role of Generalized Event Knowl-
edge (GEK) (McRae and Matsuki, 2009) in the
interpretation of logical metonymies. The au-
thors refer to a long trend of psycholinguistic stud-
ies (McRae et al., 1998; Altmann, 1999; Kamide
et al., 2003; McRae et al., 2005; Hare et al., 2009;
Bicknell et al., 2010), which show that speakers
quickly make use of their rich event knowledge
during online sentence processing to build expec-
tations about the upcoming input.2 The experi-
ments on German by Zarcone et al. (2014) show
that the subjects combine the linguistic cues in the
input to activate typical events the sentences could
refer to. Given an agent-patient pair, if the covert
event is typical for that specific argument combi-
nation, it is read faster and it is more difficult to
inhibit in a probe recognition task. The authors
explained their results in the light of the words-as-
cues paradigm (Elman, 2009, 2014), which claims
that the words in the mental lexicon are cues to
event knowledge modulating language compre-
hension in an incremental fashion.
</p>
<p>Research in computational semantics has fo-
</p>
<p>your book (= eating). The event retrieval cannot be explained
in terms of qualia structures, as it is unlikely that the lexical
entry for book includes something related to eating-events.
</p>
<p>2It should be pointed out that, unlike relevance theory
which conceives world knowledge and linguistic knowledge
as separate modules, GEK includes both linguistic and ex-
tralinguistic information.
</p>
<p>cused on two different aspects of the phenomenon:
the first one is the retrieval of the covert event,
which has been approached by means of ei-
ther probabilistic methods (Lapata and Lascarides,
2003; Lapata et al., 2003; Shutova, 2009) or of
distributional similarity-based thematic fit estima-
tions (Zarcone et al., 2012), whereas the second
aspect concerns modeling the experimental data
about processing costs. Zarcone et al. (2013)
showed that a distributional model of verb-object
thematic fit can reproduce the reading times dif-
ferences in the experimental conditions found by
McElree et al. (2001) and Traxler et al. (2002).
Their merits notwithstanding, a limit of the for-
mer studies is that they did not try to build a single
model to account for both aspects involved in log-
ical metonymy.
</p>
<p>The goal of this paper is twofold. First of
all, we present a general distributional model
of sentence comprehension inspired by recent
proposals in neurocognitive sciences (Section 2).
Secondly, we introduce a semantic composition
weight that is used to model the reading times of
metonymic sentences reported in previous experi-
mental studies and to predict the covert event in a
binary classification task (Section 3).
</p>
<p>2 A Distributional Model of Sentence
Comprehension
</p>
<p>The model we present includes a Memory com-
ponent, containing distributional information ac-
tivated by lexical items, and a Unification com-
ponent, which combines the items in Memory
to form a coherent semantic representation of the
sentence.3 This architecture is directly inspired
by Memory, Unification and Control (MUC), pro-
posed by Peter Hagoort as a general model for the
neurobiology of language (Hagoort, 2013, 2016).
MUC incorporates three main functional compo-
nents: i.) Memory corresponds to linguistic knowl-
edge stored in long-term memory; ii.) Unifica-
tion refers to the assembly in working memory
of the constructions stored in Memory into larger
structures, with contributions from the context;
iii.) Control is responsible for relating language
to joint action and social interaction. Similarly to
</p>
<p>3A previous version of this model has already been intro-
duced in Chersoni et al. (2016a), the main difference being
the way the complexity score component based on Memory
was computed (see section 5 and 6 of the 2016 paper). More-
over, the model was applied to a different task (i.e., the com-
putation of context-sensitive argument typicality).
</p>
<p>169</p>
<p />
</div>
<div class="page"><p />
<p>MUC, we argue that the comprehension of a sen-
tence is an incremental process driven by the goal
of constructing a coherent semantic representation
of the event the speaker intends to communicate.
Our model rests on the following assumptions:
</p>
<p>• the Memory component contains information
about events and their typical participants,
which is derived from both first-hand ex-
perience and linguistic experience. Follow-
ing McRae and Matsuki (2009), we call this
information Generalized Event Knowledge
(GEK). In this paper we restrict ourselves
to the ‘linguistic’ subset of GEK (henceforth
GEKL), which we model with distributional
information extracted from corpora;
</p>
<p>• during sentence processing, lexical items ac-
tivate portions ofGEKL, and the Unification
component composes them into a coherent
representation of the event expressed by the
sentence;
</p>
<p>• the event representation is assigned a seman-
tic composition weight on the basis of i)
the availability and salience of information
stored inGEKL and activated by the linguis-
tic input; ii) the semantic coherence of the
unified event, depending in turn on the mu-
tual typicality of the event participants;
</p>
<p>• a sentence interpretation is the event with the
highest semantic composition weight, that is
the event that best satisfies the semantic con-
straints coming from lexical items and the
contextual information stored in GEKL.
</p>
<p>Sentence comprehension therefore results from a
“balance between storage and computation” (Bag-
gio and Hagoort, 2011; Baggio et al., 2012) that
simultaneously accounts for the unlimited possi-
bility to understand new sentences, which are con-
structed by means of Unification, and for the pro-
cessing advantage guaranteed by the retrieval from
Memory of “ready-to-use” information about typ-
ical events and situations.
</p>
<p>Crucially, we argue that logical metonymy in-
terpretation shares this same mechanism of on-line
sentence processing and that the covert event is
i.) an event retrieved from GEKL that is strongly
activated by the lexical items, ii.) and with a
high degree of mutual semantic congruence with
the other arguments in the sentence. Therefore,
there is no formal difference between simple and
</p>
<p>enriched forms of compositionality (Jackendoff,
1997), both being instances of the same general
model of sentence processing.
</p>
<p>2.1 The Memory Component: A
Distributional Model of GEKL
</p>
<p>In our framework, we assume that each
lexical item wi activates a set of events
〈e1, σ1〉, . . . , 〈en, σn〉 such that ei is an event
in GEKL, and σi is an activation score computed
as the conditional probability P (e|wi), which
quantifies the ‘strength’ with which the event is
activated by wi.
</p>
<p>We represent events in GEKL as feature struc-
tures specifying participants and roles, and we
extract this information from parsed sentences in
corpora: the attributes are syntactic dependencies,
which we use as a surface approximation of
semantic roles, and the values are distributional
vectors of dependent lexemes.4 For example,
from the sentence The student reads a book we
extract the following event representation:
</p>
<p>[EV ENT NSUBJ:
−−−−−→
student HEAD:
</p>
<p>−−→
read DOBJ:
</p>
<p>−−→
book]
</p>
<p>Events in GEKL can be cued by several lexical
items, with a strength depending on the salience
of the event given the item. For example, the event
above is cued by student, read and book. Besides
complete events, we assume GEKL to contain
schematic (i.e., underspecified) events too. For
instance, from the sentence The student reads a
book we also generate schematic events such as
[EV ENT NSUBJ:
</p>
<p>−−−−−→
student DOBJ:
</p>
<p>−−→
book], obtained by ab-
</p>
<p>stracting over one or more of the instantiated at-
tribute values. Such representation describes an
underspecified event schema involving a student
and a book, which can be instantiated by different
activities (e.g., reading, borrowing, etc.). Accord-
ing to this view, GEKL is not a flat list of events,
but a structured repository of prototypical knowl-
edge about event contingencies.
</p>
<p>It is worth remarking that the events in GEKL
are complex symbolic structures including distri-
butional representations of the event head and its
participants. Events in GEKL are therefore mod-
eled like a sort of semantic frames whose ele-
ments are distributional vectors.5
</p>
<p>4We represent dependencies according to
the Universal Dependencies annotation scheme:
http://universaldependencies.org/.
</p>
<p>5Unlike traditional semantic frames, our events are satu-
</p>
<p>170</p>
<p />
</div>
<div class="page"><p />
<p>2.2 The Unification Component: Building
Semantic Representations
</p>
<p>Language can be seen as a set of instructions that
the comprehender uses to create a representation
of the situation that is being described by the
speaker. In our framework, we make use of situ-
ation models (henceforth SMs),6 defined as data
structures that contain a representation of the event
currently being processed (Zwaan and Radvansky,
1998). Comprehension always occurs within the
context of an existing SM : during online sentence
processing, lexical items cue portions of GEKL
and the SM is dynamically updated by unify-
ing its current content with the new information.
In this perspective, the goal of sentence compre-
hension consists in recovering (reconstructing) the
event e that the sentence is most likely to describe
(Kuperberg, 2016). The event e is the event that
best satisfies all the constraints set by the lexical
items in the sentence and by the active SM .7
</p>
<p>Let w1, w2, . . . , wn be an input linguistic se-
quence (e.g., a sentence or a discourse) that is cur-
rently being processed. Let SMi be the seman-
tic representation built for the linguistic input until
w1, . . . , wi, and let ei be the event representation
in SMi. When we process wi+1:
</p>
<p>1. the GEKL associated with wi+1 in the lexi-
con, GEKL[wi+1], is activated;
</p>
<p>2. GEKL[wi+1] is integrated with SMi to pro-
duce SMi+1, containing the new event ei+1.
</p>
<p>We model semantic composition as an event
construction and update function F , whose aim
is to build a coherent SM by integrating theGEKL
cued by the linguistic elements that are composed:
</p>
<p>F (SMi, GEKL[wi+1]) = SMi+1 (1)
</p>
<p>The composition function is responsible for two
distinct processes:
</p>
<p>• F unifies two event feature structures into a
new event, provided that the attribute-value
features of the input events are compatible.
</p>
<p>rated structures, with participants specified for each role.
6SMs are akin to Discourse Representation Structures in
</p>
<p>DRT (Kamp, 2013).
7The idea also bears some similarities with the inferen-
</p>
<p>tial model of communication proposed by Relevance The-
ory, where the interpretation of a given utterance is the one
that maximizes the hearer’s expectations of relevance (Sper-
ber and Wilson, 1986).
</p>
<p>Here is an example of unification:
</p>
<p>[EV ENT NSUBJ:
−−−−−−→
mechanic DOBJ:
</p>
<p>−−−−→
engine] t
</p>
<p>[EV ENT NSUBJ:
−−−−−−→
mechanic HEAD:
</p>
<p>−−−→
check] = [EV ENT
</p>
<p>NSUBJ:
−−−−−−→
mechanic HEAD:
</p>
<p>−−−→
check DOBJ:
</p>
<p>−−−−→
engine]
</p>
<p>The event of a mechanic performing an ac-
tion on an engine and the event of a mechanic
checking something are unified into a new
event of a mechanic checking an engine;
</p>
<p>• F weights the unified event ek with a pair of
scores 〈θek , σek〉, weighting ek with respect
to its semantic coherence and its salience
given the lexical cues activating it.
</p>
<p>The score θek quantifies the degree of seman-
tic coherence of the unified event ek. We assume
that the semantic coherence (or internal unity) of
an event depends on the mutual typicality of its
components. Consider the following sentences:
</p>
<p>(1) a. The student writes a thesis.
b. The mechanic writes a sonnet.
</p>
<p>The event represented in (1-a) has a high degree
of semantic coherence because all its components
are mutually typical: student is a typical subject
for the verb write and thesis has a strong typical-
ity both as an object of write and as an object oc-
curring in student-related events. Conversely, the
components in the event expressed by (1-b) have
a low level of mutual typicality, thereby resulting
into an event with much lower semantic coher-
ence. Although the sentence is perfectly under-
standable, it sounds a little weird because it depicts
an unusual situation.
</p>
<p>We measure the mutual typicality of the com-
ponents by extending the notion of thematic fit,
which is normally used to measure the congruence
of a predicate with an argument (McRae et al.,
1998). In our case, instead, thematic fit is a general
measure of the semantic typicality or congruence
among event participants. Extending the approach
by Erk et al. (2010), thematic fit is measured with
vector cosine in the following way:
</p>
<p>θ(−→a |si,−→b ) (the thematic fit of−→a given−→
b and the role si) is the cosine between−→a and the prototype vector built out of
</p>
<p>the k top values −→c1 , . . . ,−→ck , such that
si:−→cz , for 1 ≤ z ≤ k, co-occurs with−→
b in the same event structures
</p>
<p>171</p>
<p />
</div>
<div class="page"><p />
<p>For instance, the thematic fit of student as a subject
of write is given by the cosine between the vector
of student and the centroid vector built out of the
k most salient subjects of write. Similarly, we as-
sess the typicality of thesis as an object related to
student (i.e., as an object of events involving stu-
dent as subject) by measuring the cosine between
the vector of thesis and the centoid vector built out
of the k most salient objects related to student. Fi-
nally, we measure in the same way the typicality
of thesis as an object of write.
</p>
<p>Formally, the global score θek of an event ek is
defined as:
</p>
<p>θek =
∏
</p>
<p>a,b,si∈e
θ(−→a |si,−→b ) (2)
</p>
<p>meaning that the degree of semantic coherence of
an event is given by the product of the partial the-
matic fit scores between all its components.8
</p>
<p>On the other hand, the σek score weights the
salience of the unified event ek by combining the
weights of ei and ej into a new weight assigned
to ek. In this work, we compute activation of an
event e simply by summing the activation scores
of the single lexical items cuing it (i.e., the condi-
tional probabilities of the event given each lexical
item in the input sentence):
</p>
<p>σi = P (e|i) = P (e, i)
P (i)
</p>
<p>(3)
</p>
<p>F (σi, σj) = σek = σi + σj (4)
</p>
<p>Thus, the score σek measures the degree to which
the unified event is activated by the linguistic ex-
pressions composing it. Consequently, events that
are cued by many constructions in the sentence
should incrementally increase their salience.
</p>
<p>To sum up, we weight unified events along two
dimensions: internal semantic coherence (θ), and
degree of activation by linguistic expressions (σ).
The latter is used to estimate the importance of
“ready-to-use” event structures stored in GEKL
and retrieved during sentence processing. On the
the other hand, the θ score allows us to weight
events not available in the Memory component. In
fact, the Unification component can construct new
event never observed before, thereby accounting
</p>
<p>8For the present study, we discarded the modifiers. How-
ever, θ scores could also be computed for measuring the co-
herence of modified arguments (e.g. the angry child smiled).
We thank one of our reviewers for pointing this out.
</p>
<p>for the ability to comprehend novel sentences rep-
resenting atypical and yet possible events. For in-
stance, the event expressed by (1-a) might be ex-
pected to be already stored in GEKL because of
its high typicality, thereby having a high σ score.
Suppose instead that the sentence (1-b) expresses a
brand new event, and that its components never co-
occurred together before. In this case, its weight
will only depend on the θ score, that is on how
similar are its participants to other events stored in
the event repository (e.g., how mechanic is similar
to the prototypical subjects of write). Therefore,
the joint effect of the σ and θ scores captures the
“balance between storage and computation” driv-
ing sentence processing (cf. above).
</p>
<p>Given an input sentence s, its interpretation
INT(s) is the event ek with the highest semantic
composition weight (SCW), defined as follows:
</p>
<p>INT(s) = argmax
e
</p>
<p>(SCW(e)) (5)
</p>
<p>SCW(e) = θe + σe (6)
</p>
<p>We model the semantic complexity (Semp-
Comp) of a sentence s as inversely related to the
SCW of the event representing its interpretation:
</p>
<p>SemComp(s) =
1
</p>
<p>SCW(INT(s))
(7)
</p>
<p>The less internally coherent is the event repre-
sented by the sentence and the less strong is its
activation by the lexical items, the more the uni-
fication is cognitively expensive and the sentence
semantically complex.
</p>
<p>3 Modeling Logical Metonymy
</p>
<p>We apply the distributional model of sentence
comprehension presented in the previous sec-
tion to account for psycholinguistic data about
metonymic sentences. In particular, we predict
that metonymic sentences will have higher Sem-
Comp scores than non-coercion sentences, be-
cause they do not comply with the semantic pref-
erences of the event-selecting verb. According to
Zarcone et al. (2013), it is exactly the low thematic
fit between verb and object that triggers comple-
ment coercion and that, at the same time, causes
the extra processing load.
</p>
<p>Additionally, we predict that the covert event in
metonymic sentence is i.) strongly activated by
the lexical items in the context, and is ii.) semanti-
cally coherent with respect to the participants that
</p>
<p>172</p>
<p />
</div>
<div class="page"><p />
<p>are overtly realized. In other words, the inferred
covert event is the event that maximizes the SCW
of the global event structure representing the inter-
pretation of the sentence.
</p>
<p>3.1 Datasets
</p>
<p>We used two datasets created for previous psy-
cholinguistic studies: the McElree dataset (McEl-
ree et al., 2001) and the Traxler dataset (Traxler
et al., 2002). Each dataset compared three differ-
ent experimental conditions, by contrasting con-
structions requiring a type-shift with constructions
requiring normal composition:
</p>
<p>(2) a. The author was starting the book.
b. The author was writing the book.
c. The author was reading the book.
</p>
<p>Sentence (2-a) corresponds to the metonymic con-
dition (MET), while sentences (2-b) and (2-c) cor-
respond to non-metonymic constructions, with the
difference that (2-b) represents a typical event
given the subject and the object (HIGH TYP),
whereas (2-c) expresses a plausible but less typ-
ical event (LOW TYP). The McElree dataset was
created for the self-paced reading study by McEl-
ree et al. (2001), and includes 99 sentences (33
triplets), while the Traxler dataset was used in the
eye-tracking experiment by Traxler et al. (2002)
and contains 108 sentences (36 triplets).9
</p>
<p>3.2 Extracting GEKL
In order to populate the repository of events
in GEKL, we followed the procedure proposed
by Chersoni et al. (2016b) to extract syntac-
tic joint contexts from a concatenation of four
different corpora: the Reuters Corpus Vol.1
(Lewis et al., 2004); the Ukwac and the Wack-
ypedia Corpus (Baroni et al., 2009) and the
British National Corpus (Leech, 2013). For
each sentence, we generated an event (as de-
scribed in Section 2.1) by extracting the verb
and its direct dependencies. In the present case,
the dependency relations of interest are subject
(SUBJ), direct (DOBJ) and indirect object (IOBJ),
infinitive and gerund complements (XCOMP),
and a generic prepositional complement rela-
tion (PREPCOMP), on which we mapped all
the complements introduced by a preposition.
We discarded the adjectival/adverbial modifiers
</p>
<p>9The sentences in the same triple have the same syntactic
complexity, as they differ only for the verb.
</p>
<p>and we just keep their heads. For instance,
from the joint context director-n-subj write-
v-head article-n-dobj we generated the event
[EV ENT NSUBJ:
</p>
<p>−−−−−→
student HEAD:
</p>
<p>−−→
read DOBJ:
</p>
<p>−−→
book]. For
</p>
<p>each joint context, we also generated schematic
events from its dependency subsets. We totally ex-
tracted 1,043,766 events that include at least one
of the words of the evaluation datasets.
</p>
<p>All the lexemes in the events are represented
as distributional vectors. We built a syntax-based
distributional semantic model by using as targets
the 20K most frequent nouns and verbs in our
concatenated corpus, plus any other word occur-
ring in the events in the GEKL. Words with
frequency below 100 were excluded. The total
number of targets is 20,560 (cf. Table 1 for the
dataset coverage). As vector dimensions, we used
the same target words, while the dependency rela-
tions are the same used to build the joint contexts
(SUBJ:author-n and DOBJ:book-n are examples
of dimensions for the target write-v). Syntactic
co-occurrences were weighted with Local Mutual
Information (Evert, 2004):
</p>
<p>LMI(t, r, f) = log
</p>
<p>(
Otrf
Etrf
</p>
<p>)
∗Otrf (8)
</p>
<p>with Otrf the co-occurrence frequency of the tar-
get t, the syntactic relation r and the filler f , and
Etrf their expected co-occurrence frequency.
</p>
<p>Dataset Coverage
McElree et al. (2001) 30/33
Traxler et al. (2002) 36/36
</p>
<p>Table 1: GEKL coverage for the evaluation triplets
</p>
<p>3.3 Modeling the Processing Cost of
Metonymic Sentences
</p>
<p>The sentences in the original datasets were repre-
sented as S(subject)-V(verb)-O(object) tuples. For
each sentence s, SemComp(s) was measured as in
equation (7), by computing θe and σe as follows:
</p>
<p>• θe is the product of the thematic fit of O given
V, θO,V , the thematic fit of S given V, θS,V ,
and the thematic fit of O given S, θO,S (see
Equation 2). θO,V is the cosine between the
vector of O and the centroid vector built out
of the k most salient direct objects of V (e.g.,
the cosine between the vector of book and the
centroid vector of the most salient objects of
write); θS,V is the cosine between the vector
of S and the centroid vector built out of the
</p>
<p>173</p>
<p />
</div>
<div class="page"><p />
<p>HIGH_TYP LOW_TYP MET
</p>
<p>2
4
</p>
<p>6
8
</p>
<p>10
12
</p>
<p>lo
g(
S
em
C
om
p)
</p>
<p>HIGH_TYP LOW_TYP MET
</p>
<p>2
4
</p>
<p>6
8
</p>
<p>10
12
</p>
<p>lo
g(
S
em
C
om
p)
</p>
<p>Figure 1: SemComp scores for McElree (left) and Traxler (right)
</p>
<p>k most salient subjects of V (e.g., the cosine
between the vector of author and the centroid
vector of the most salient objects of write);
finally, θO,S is the cosine between the vector
of O and the centroid vector built out of the k
most salient direct objects occurring in events
whose subject is S (e.g., the cosine between
the vector of book and the prototype vector of
the most salient objects of events whose sub-
ject is author). Following Baroni and Lenci
(2010), we used LMI scores to identify the
most salient fillers of each target-specific syn-
tactic slot and we fixed k = 20.
</p>
<p>• σe is the salience score of the triple s, and
it corresponds to the sum of the activation
scores of i.) the full event represented by the
triple and of ii.) the sub-events correspond-
ing to all the partial combinations of the verb
and its arguments. Each activation score is
the conditional probability of the event given
a lexical item in the test tuple.
</p>
<p>Given the verb-argument triple s, the set E
is the set of i events containing i.) the en-
tire event e; ii.) all the schematic events
e1, . . . , ei generated by abstracting over one
of the lexemes of the triples (e.g., for s =
{author − write − book}), E = {&lt;
author, write, book &gt;,&lt; author, write &gt;
,&lt; author, book &gt;,&lt; write, book &gt;}. σe
is computed with the following equation:
</p>
<p>σe =
∑
</p>
<p>ei∈E
σei (9)
</p>
<p>Figure 1 shows the boxplots of the log Sem-
Comp scores for three types of sentences (MET,
</p>
<p>HIGH TYP, and LOW TYP) in the datasets. The
Kruskal-Wallis rank sum test reveals a main effect
of the sentence types on the SemComp scores as-
signed by our GEKL-based distributional model
for the McElree dataset (χ2 = 17.18, p &lt; 0.001).
Post-hoc tests (cf. Table 2) show that SemComp
scores for the HIGH TYP conditions are signif-
icantly lower than those in the LOW TYP (p &lt;
0.05) and MET conditions (p &lt; 0.001). These re-
sults mirror exactly those of McElree et al. (2001)
for the reading times at the type-shifted noun (both
conditions engendered significantly longer read-
ing times than the preferred condition).
</p>
<p>p-values HIGH TYP LOW TYP
LOW TYP 0.04* -
MET 0.00046* 0.31
</p>
<p>Table 2: Results of the pairwise post-hoc comparisons for
the three conditions on the McElree dataset (Wilcoxon rank
sum test with Bonferroni correction).
</p>
<p>p-values HIGH TYP LOW TYP
LOW TYP 0.31 -
MET 9.7e-06* 0.01*
</p>
<p>Table 3: Results of the pairwise post-hoc comparisons for the
three conditions on the Traxler dataset (Wilcoxon rank sum
test with Bonferroni correction).
</p>
<p>A main effect of sentence types on the SemComp
score also also exists for the Traxler dataset (χ2 =
15.39, p &lt; 0.001). In their eye-tracking experi-
ment (Experiment 1), Traxler et al. (2002) found
no significant difference between HIGH TYP and
LOW TYP conditions, but they observed higher
values for second-pass and total time data in the
MET condition with respect to the other two. In-
terestingly, the distributional model produces sim-
</p>
<p>174</p>
<p />
</div>
<div class="page"><p />
<p>ilar results: post-hoc tests reveal no difference
between non-coerced conditions, but significantly
higher SemComp scores for metonymic sentences
with respect to both the HIGH TYP (p &lt; 0.001)
and the LOW TYP condition (p &lt; 0.05).
</p>
<p>3.4 Identifying the Covert Event
</p>
<p>We assume that the interpretation of a metonymic
sentence like The author starts the book is the fol-
lowing conjunction of events:
</p>
<p>(3) [EV ENT NSUBJ:
−−−−→
author HEAD:
</p>
<p>−−−→
start DOBJ:−→e ]
</p>
<p>[EV ENT NSUBJ:
−−−−→
author HEAD:−→e DOBJ:−−→book]
</p>
<p>where e is the covert event to be recovered (e.g.,
writing). We modeled covert event retrieval as
a binary classification task, as in Zarcone et al.
(2012), using the following procedure: i.) for
each metonymic sentence (e.g. The author starts
the book) in the McElree and Traxler datasets,
we selected as candidate covert events, Ecov, the
verbs in the non-coercion sentences, which we re-
fer to respectively as HIGH TYP EVENT (e.g.
write) and LOW TYP EVENT (e.g., read); ii.) for
each sentence SVmetO, we computed SCW(e) (cf.
equation 6) of the events composing its interpreta-
tion, that is [EV ENT S Vmet Ecov] and [EV ENT
S Ecov O];10 iii.) the model accuracy was com-
puted as the percentage of test items for which
SCW(Ecov = HIGH TYP EVENT) is higher than
SCW(Ecov = LOW TYP EVENT).
</p>
<p>Model McElree Traxler
Random 50% 50%
</p>
<p>σ 46.66% 30.55%
θ 73.3% 75%
</p>
<p>σ + θ 80% 77.77%
</p>
<p>Table 4: Accuracy of model components and random base-
line on the binary classification task for covert event retrieval.
</p>
<p>The results for the covert event identification
are shown in Table 4. We tested both the full
model (SCW = σ + θ) and its σ and θ components
separately, to check their contribution to the task.
Overall, it can be observed that the full model is
the best performing one, classifying correctly just
a few items more than the thematic fit-based, θ-
only model. Both models are significantly bet-
ter than the random baseline at p &lt; 0.05 on
the Traxler dataset, whereas only the full model
achieves a significant advantage over the baseline
</p>
<p>10Importantly, the covert events do not contribute to the σ
scores, since they are not present in the linguistic input.
</p>
<p>on McElree.11
</p>
<p>The performance of the σ component, which
makes use only of the information stored in
GEKL, is pretty weak, especially on the Traxler
dataset. This is the same problem affecting purely
probabilistic approaches, given also the fact that
many of the words of the evaluation datasets have
low frequencies in corpora. The θ component
therefore plays a crucial role in the covert event
prediction. In fact, θ works like a generalization
component, and it serves to compute and weight
new event representations when the information
stored in memory is not sufficient. The strong per-
formance of a thematic fit-based method is also
consistent with the results obtained by Zarcone
et al. (2012) on German data.
</p>
<p>Interestingly, a further study by Zarcone et al.
(2013) has proposed thematic fit estimation as the
mechanism which is responsible also for the trig-
gering of logical metonymy, hypothesizing that
the recovery of the implicit event could be a con-
sequence of the dispreference of the verb for the
entity-denoting argument. This means, in our per-
spective, that the low thematic fit between verb
and patient triggers a retrieval operation with the
aim of increasing the semantic coherence of the
event represented in the situation model. To
test this claim, we compared the θ scores of the
events containing the HIGH TYP covert event
(i.e., [EV ENT S Vmet Ecov] + [EV ENT S Ecov O])
and the corresponding MET event (i.e., [EV ENT
S Vmet O]), predicting that the former events are
more semantically coherent than the latter.12 This
hypothesis turned out to be correct: according to
the Wilcoxon rank sum test, both in the McElree
(W = 199, p &lt; 0.01) and in the Traxler dataset
(W = 157, p &lt; 0.01) the θ of the events contain-
ing the covert events are significantly higher.
</p>
<p>4 Conclusions
</p>
<p>In this paper, we have presented a distributional
model of sentence comprehension as an incremen-
tal process to build the semantic representation of
the event expressed by the sentence. Events are
represented with complex formal structures that
contain the distributional vectors of its compo-
nent. Sentence interpretation is carried out by
unifying stored distributional information about
</p>
<p>11p-values computed with the χ2 statistical test.
12Since the computation of the two θs requires a different
</p>
<p>number n of factors, the scores have been normalized by ele-
vating them to the power of 1/n.
</p>
<p>175</p>
<p />
</div>
<div class="page"><p />
<p>events, GEKL. The event representing a sentence
is the event with the highest semantic composi-
tion weight, SCW, which is in turn a function of
its internal semantic coherence and the activation
strength by the linguistic input. The semantic co-
herence of an event, measured by the θ score, de-
pends on its similarity to stored events. There-
fore, the unlimited ability of understanding new
sentences can be conceived as the ability to adapt
our general knowledge about events to novel situ-
ations: in brief, productivity is adaptation, and
adaptation is by similarity.
</p>
<p>The model has been successfully applied to the
case of logical metonymy, accounting for two as-
pects of this phenomenon that have always been
treated separately in the literature, namely pro-
cessing costs and covert event retrieval. Given
these encouraging results, we are planning to ap-
ply the model also to other semantic tasks in-
volving event knowledge, such as the detection of
anomalies (e.g. violations of selectional restric-
tions), the recovery of implicit arguments and of
bridging inferences.
</p>
<p>Acknowledgments
</p>
<p>This work has been carried out thanks to the sup-
port of the A*MIDEX grant (nANR-11-IDEX-
0001-02) funded by the French Government ’In-
vestissements d’Avenir’ program. We would like
to thank the three anonymous reviewers for their
many insightful comments and suggestions.
</p>
<p>References
Gerry T. M. Altmann. 1999. Thematic Role Assign-
</p>
<p>ment in Context. Journal of Memory and Language
41(1):124–145.
</p>
<p>Nicholas Asher. 2015. Types, Meanings and Coercions
in Lexical Semantics. Lingua 157:66–82.
</p>
<p>Giosuè Baggio and Peter Hagoort. 2011. The Balance
between Memory and Unification in Semantics: A
Dynamic Account of the N400. Language and Cog-
nitive Processes 26(9):1338–1367.
</p>
<p>Giosuè Baggio, Michiel van Lambalgen, and Peter Ha-
goort. 2012. The Processing Consequences of Com-
positionality. In Markus Werning, Wolfram Hinzen,
and Edouard Machery, editors, The Oxford Hand-
book of Compositionality, Oxford University Press,
Oxford, pages 1–23.
</p>
<p>Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
</p>
<p>Web-crawled Corpora. Language Resources and
Evaluation 43(3):209–226.
</p>
<p>Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional Memory: A General Framework for Corpus-
based Semantics. Comput. Linguist. 36(4):673–721.
</p>
<p>Klinton Bicknell, Jeffrey L. Elman, Mary Hare, Ken
McRae, and Marta Kutas. 2010. Effects of Event
knowledge in Processing Verbal Arguments. Jour-
nal of Memory and Language 63:489–505.
</p>
<p>Robyn Carston. 2002. Thoughts and Utterances.
Blackwell, Oxford.
</p>
<p>Emmanuele Chersoni, Philippe Blache, and Alessan-
dro Lenci. 2016a. Towards a Distributional Model
of Semantic Complexity. In COLING Workshop on
Computational Linguistics for Linguistic Complex-
ity.
</p>
<p>Emmanuele Chersoni, Alessandro Lenci, Enrico San-
tus, Philippe Blache, and Chu-Ren Huang. 2016b.
Representing Verbs with Rich Contexts: An Evalua-
tion on Verb Similarity. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing. pages 1967–1972.
</p>
<p>Roberto G. de Almeida. 2004. The Effect of Context
on the Processing of Type-shifting Verbs. Brain and
Language 90:249–261.
</p>
<p>Roberto G. de Almeida and Veena D. Dwivedi. 2008.
Coercion without Lexical Decomposition: Type-
shifting Effects Revisited. Canadian Journal of Lin-
guistics 53(2/3):301–326.
</p>
<p>Jeffrey L. Elman. 2009. On the Meaning of Words and
Dinosaur Bones: Lexical Knowledge without a Lex-
icon. Cognitive Science 33(4):547–582.
</p>
<p>Jeffrey L. Elman. 2014. Systematicity in the Lexicon:
On Having your Cake and Eating it too. In Paco
Calvo and John Symons, editors, The Architecture
of Cognition: Rethinking Fodor and Pylyshyn’s Sys-
tematicity Challenge, The MIT Press, Cambridge,
MA.
</p>
<p>Katrin Erk, Sebastian Padó, and Ulrike Padó. 2010. A
Flexible, Corpus-driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics 36(4):723–763.
</p>
<p>Stefan Evert. 2004. The Statistics of Word Co-
occurrences: Word Pairs and Collocations. Ph.D.
thesis.
</p>
<p>Ingrid L. Falkum. 2011. A Pragmatic Account of Log-
ical Metonymy’. In Proceedings of Metonymy 2011.
pages 11–17.
</p>
<p>Steven Frisson and Brian McElree. 2008. Complement
Coercion is not Modulated by Competition: Evi-
dence from Eye Movements. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition
34(1):1–11.
</p>
<p>176</p>
<p />
</div>
<div class="page"><p />
<p>Peter Hagoort. 2013. MUC (Memory, Unification,
Control) and Beyond. Frontiers in Psychology
4(JUL):1–13.
</p>
<p>Peter Hagoort. 2016. MUC (Memory, Unification,
Control): A Model on the Neurobiology of Lan-
guage beyond Single Word Processing. In Gregory
Hickok and Steve Small, editors, Neurobiology of
Language, Elsevier, Amsterdam, volume 28, pages
339–347.
</p>
<p>Mary Hare, Michael N. Jones, Caroline Thomson,
Sarah Kelly, and Ken McRae. 2009. Activating
Event Knowledge. Cognition 111:151–167.
</p>
<p>Ray Jackendoff. 1997. The Architecture of the Lan-
guage Faculty. The MIT Press, Cambridge, MA.
</p>
<p>Yuki Kamide, Gerry T. M. Altmann, and Sarah L. Hay-
wood. 2003. The Time-course of Prediction in In-
cremental Sentence Processing: Evidence from An-
ticipatory Eye Movements. Journal of Memory and
Language 49:133–156.
</p>
<p>Hans Kamp. 2013. Meaning and the Dynamics of In-
terpretation: Selected Papers by Hans Kamp. Brill,
Leiden-Boston.
</p>
<p>Gina R. Kuperberg. 2016. Separate Streams or Proba-
bilistic Inference? What the N400 can Tell us about
the Comprehension of Events. Language, Cognition
and Neuroscience 31(5):602–616.
</p>
<p>Mirella Lapata, Frank Keller, and Christoph Scheepers.
2003. Intra-sentential Context Effects on the Inter-
pretation of Logical Metonymy. Cognitive Science
27(4):649–668.
</p>
<p>Mirella Lapata and Alex Lascarides. 2003. A Proba-
bilistic Account of Logical Metonymy. Computa-
tional Linguistics 29(2):261–315.
</p>
<p>Alex Lascarides and Ann Copestake. 1998. Pragmatics
and Word Meaning. Journal of Linguistics 34:378–
414.
</p>
<p>Geoffrey Neil Leech. 2013. 100 Million Words of En-
glish: the British National Corpus (bnc)*.
</p>
<p>David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. Journal of Machine
Learning Research 5(Apr):361–397.
</p>
<p>Brian McElree, Matthew J. Traxler, Martin J. Pick-
ering, Rachel E. Seely, and Ray Jackendoff. 2001.
Reading Time Evidence for Enriched Composition.
Cognition 78:B17–B25.
</p>
<p>Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd R.
Ferretti. 2005. A Basis for Generating Expectan-
cies for Verbs from Nouns. Memory &amp; cognition
33(7):1174–1184.
</p>
<p>Ken McRae and Kazunaga Matsuki. 2009. People Use
their Knowledge of Common Events to Understand
Language, and Do So as Quickly as Possible. Lan-
guage and Linguistics Compass 3(6):1417–1429.
</p>
<p>Ken McRae, Michael J. Spivey-Knowlton, and
Michael K. Tanenhaus. 1998. Modeling the Influ-
ence of Thematic Fit (and Other Constraints) in On-
line Sentence Comprehension. Journal of Memory
and Language 38:283–312.
</p>
<p>James Pustejovsky. 1995. The Generative Lexicon.
The MIT Press, Cambridge, MA.
</p>
<p>Ekaterina Shutova. 2009. Sense-based Interpretation
of Logical Metonymy Using a Statistical Method. In
Proceedings of the ACL-IJCNLP 2009 Student Re-
search Workshop. pages 1–9.
</p>
<p>Dan Sperber and Deirdre Wilson. 1986. Relevance:
Communication and Cognition. Blackwell, Oxford.
</p>
<p>Matthew J. Traxler, Martin J. Pickering, and Brian
McElree. 2002. Coercion in Sentence Process-
ing: Evidence from Eye-movements and Self-
paced Reading. Journal of Memory and Language
47:530–547.
</p>
<p>Alessandra Zarcone, Alessandro Lenci, Sebastian
Padó, and Jason Utt. 2013. Fitting, not Clash-
ing! A Distributional Semantic Model of Logi-
cal Metonymy. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013). pages 404–410.
</p>
<p>Alessandra Zarcone and Sebastian Padó. 2011. Gener-
alized Event Knowledge in Logical Metonymy Res-
olution. In Proceedings of the 33rd annual meeting
of the Cognitive Science Society (CogSci 2011).
</p>
<p>Alessandra Zarcone, Sebastian Padó, and Alessandro
Lenci. 2014. Logical Metonymy Resolution in a
Words-as-Cues Framework: Evidence from Self-
paced Reading and Probe Recognition. Cognitive
Science 38(5):973–996.
</p>
<p>Alessandra Zarcone, Jason Utt, and Sebastian Padó.
2012. Modeling Covert Event Retrieval in Logi-
cal Metonymy: Probabilistic and Distributional Ac-
counts. In Proceedings of the 3rd Workshop on
Cognitive Modeling and Computational Linguistics
(CMCL 2012). pages 70–79.
</p>
<p>Rolf A. Zwaan and Gabriel A. Radvansky. 1998. Situa-
tion Models in Language Comprehension and Mem-
ory. Psychological bulletin 123(2):162–185.
</p>
<p>177</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 178–188,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Double Trouble:
The Problem of Construal in Semantic Annotation of Adpositions
</p>
<p>Jena D. Hwang and Archna Bhatia
IHMC
</p>
<p>{jhwang,abhatia}@ihmc.us
</p>
<p>Na-Rae Han
University of Pittsburgh
naraehan@pitt.edu
</p>
<p>Tim O’Gorman
University of Colorado Boulder
timothy.ogorman@colorado.edu
</p>
<p>Vivek Srikumar
University of Utah
</p>
<p>svivek@cs.utah.edu
</p>
<p>Nathan Schneider
Georgetown University
</p>
<p>nathan.schneider@georgetown.edu
</p>
<p>Abstract
</p>
<p>We consider the semantics of preposi-
tions, revisiting a broad-coverage annota-
tion scheme used for annotating all 4,250
preposition tokens in a 55,000 word corpus
of English. Attempts to apply the scheme to
adpositions and case markers in other lan-
guages, as well as some problematic cases
in English, have led us to reconsider the
assumption that an adposition’s lexical con-
tribution is equivalent to the role/relation
that it mediates. Our proposal is to embrace
the potential for construal in adposition
use, expressing such phenomena directly
at the token level to manage complexity
and avoid sense proliferation. We suggest
a framework to represent both the scene
role and the adposition’s lexical function so
they can be annotated at scale—supporting
automatic, statistical processing of domain-
general language—and discuss how this
representation would allow for a simpler
inventory of labels.
</p>
<p>1 Introduction
</p>
<p>Prepositions and postpositions (collectively adpo-
sitions) are widespread in the world’s languages as
grammatical markers expressing spatial, temporal,
thematic,1 and other kinds of semantic relations.
Unfortunately for semantic processing, a handful
of high-frequency types carry an immense payload
by way of extreme polysemy. Thus, disambigua-
tion of adpositional meaning is crucial to piecing
together the interpretation of a sentence (§2).
</p>
<p>A line of previous work (Srikumar and Roth,
2013a; Schneider et al., 2015, 2016, see §2) has
developed a scheme for broad-coverage annotation
</p>
<p>1In the sense of thematic roles (agent, patient, etc.).
</p>
<p>of adpositions with an eye toward building auto-
matic disambiguation systems. Their most recent
proposal consists of an inventory of 75 categorical
labels known as supersenses that characterize the
polysemy of English prepositions in a lexically-
neutral and coarse-grained fashion. They envision
disambiguation as assigning a single one of these
supersenses to each preposition token.
</p>
<p>While formalizing disambiguation via single-
label classification works well for prototypical
members of the categories, on closer examination,
we argue that it is overly simplistic for many us-
ages. This became particularly evident when we
tried to adapt the English-centric supersense labels
to other languages.
</p>
<p>Here we advance a more nuanced view that an
adposition can contribute a semantic perspective, or
construal, over and above the scenario relation that
its object participates in. We argue that it is essen-
tial to distinguish the contribution of the preposi-
tion itself, i.e., what the adposition codes for, from
the semantic role or relation that the adposition me-
diates and that a predicate or scene calls for; and as
a result, the label that would be most appropriate
is underdetermined for many tokens (§3). In our
view, the mismatch can be understood through the
lens of construal, and this should be made explicit
in corpora (§4).
</p>
<p>To that end, we sketch an annotation approach
that disentangles the two elements of the meaning
while retaining the advantages of a broad-coverage
(rather than lexicographic-sense-based) scheme.
Preliminary analysis suggests that this scheme will
work well not only for English, but also for the
other languages examined. §5 surveys some of the
phenomena that our new analysis addresses with
examples from multiple languages; §6 suggests that
this added flexibility at the token level removes the
need for a great deal of complexity in the super-
sense inventory itself: i.e., we can get away with
</p>
<p>178</p>
<p />
</div>
<div class="page"><p />
<p>StartState
</p>
<p>Configuration
</p>
<p>Circumstance
Temporal
</p>
<p>Place
</p>
<p>Whole Elements
Possessor
</p>
<p>Species
Instance
</p>
<p>Quantity
</p>
<p>Superset
</p>
<p>Causer
Agent
</p>
<p>CreatorCo-Agent
</p>
<p>Explanation Attribute
</p>
<p>Manner
</p>
<p>Reciprocation Purpose
Function
</p>
<p>Age Time Frequency
Duration
</p>
<p>RelativeTime
</p>
<p>EndTimeStartTime ClockTimeCxn
DeicticTime
</p>
<p>Path Locus
Value
</p>
<p>Comparison/Contrast
Scalar/Rank
</p>
<p>ValueComparison
</p>
<p>Approximator
Contour
</p>
<p>Direction
</p>
<p>Extent
Location Source State
</p>
<p>Goal
InitialLocation
</p>
<p>Material
Donor/Speaker
</p>
<p>Destination
Recipient
</p>
<p>EndState
</p>
<p>Via
Traversed
</p>
<p>1DTrajectory
2DArea 3DMedium
</p>
<p>Transit
</p>
<p>Instrument
</p>
<p>Patient
</p>
<p>Co-Patient
</p>
<p>Activity
</p>
<p>Means
</p>
<p>Course
</p>
<p>Accompanier
</p>
<p>Beneficiary
</p>
<p>Theme
Co-Theme Topic
</p>
<p>ProfessionalAspectUndergoer
</p>
<p>Co-Participant
</p>
<p>Affector
</p>
<p>Participant
</p>
<p>Experiencer Stimulus
</p>
<p>Figure 1: Preposition supersense hierarchy (from Schneider et al., 2016). Top-level categories are circled and subcategories
radiate outward.
</p>
<p>1/3 fewer semantic labels, reducing somewhat the
practical concern of sparse data. Here we also dis-
cuss challenges and tradeoffs inherent in the pro-
posed approach. We have begun testing the work-
ability of this proposal empirically by annotating
data in multiple languages, with disambiguation
experiments to follow.
</p>
<p>2 Approaches to Prepositional Polysemy
</p>
<p>The most frequent English prepositions are extraor-
dinarily polysemous. For example, the preposition
at expresses different information in each of the
following usages:
</p>
<p>(1) a. It is at 123 Main St. (LOCATION)
b. We met him at 7pm. (TIME)
c. Everyone pointed at him. (GOAL)
d. She laughed at my acting. (STIMULUS)
e. He held her at gunpoint. (INSTRUMENT)
</p>
<p>When confronted with a new instance of at, NLU
systems must determine whether it marks an entity
or scene’s location, time, goal, or something else.
</p>
<p>As lexical classes go, prepositions are something
of a red-headed stepchild in the linguistics litera-
ture. Most of the semantics literature on prepo-
sitions has revolved around how they categorize
space and time (e.g., Herskovits, 1986; Verkuyl and
Zwarts, 1992; Bowerman and Choi, 2001). How-
ever, there have been a couple of lines of work
addressing preposition semantics broadly. In cogni-
tive linguistics, studies have examined abstract as
well as concrete uses of English prepositions (e.g.,
Dirven, 1993; Lindstromberg, 2010). Notably, the
polysemy of over and other prepositions has been
explained in terms of sense networks encompass-
ing core senses and motivated extensions (Brug-
man, 1981; Lakoff, 1987; Dewell, 1994; Tyler and
Evans, 2001, 2003). The Preposition Project (TPP;
Litkowski and Hargraves, 2005) broke ground in
</p>
<p>stimulating computational work on fine-grained
word sense disambiguation of English prepositions
(Litkowski and Hargraves, 2005; Ye and Baldwin,
2007; Tratz and Hovy, 2009; Dahlmeier et al.,
2009). Typologists, meanwhile, have developed
semantic maps of functions, where the nearness
of two functions reflects their tendency to fall un-
der the same adposition or case marker in many
languages (Haspelmath, 2003; Wälchli, 2010).
</p>
<p>Preposition supersenses. Following Srikumar
and Roth (2013b), Schneider et al. (2015) deve-
loped coarse-grained semantic categories of prepo-
sitions as a broader-coverage alternative to fine-
grained senses, using categories similar to those ap-
pearing in semantic maps (LOCATION, RECIPIENT,
etc.) rather than lexicalized senses. Schneider et al.
(2015) refined their inventory of categories through
extensive deliberation involving the use of dictio-
naries, corpora, and pilot annotation experiments.
They call the categories supersenses to emphasize
their similarity to coarse-grained classifications of
nouns and verbs that go by that name (Ciaramita
and Altun, 2006; Schneider et al., 2012).
</p>
<p>The at examples in (1) are accompanied by
the appropriate supersenses from the supersense
scheme. Most supersenses resemble thematic roles
(cf. Fillmore (1968)); a few others are needed to
describe preposition-marked relations between en-
tities. There are multiple English prepositions per
supersense; e.g., “in the city” and “on the table”
would join “at 123 Main St.” in being labeled as
LOCATIONs. We understand the supersenses as
prototype-based categories, and in some cases use
heuristics like paraphrasability (“in order to” for
PURPOSE) and WH-question words (“Why?” for
PURPOSE and EXPLANATION) to help determine
which tokens are instances of the category.
</p>
<p>The 75 supersenses are organized in a taxonomy
based on that of VerbNet (Bonial et al., 2011), with
</p>
<p>179</p>
<p />
</div>
<div class="page"><p />
<p>PARTICIPANT, CIRCUMSTANCE, and CONFIGU-
RATION at the top level.2 The taxonomy uses multi-
ple inheritance to account for subcategories which
are considered to include properties of multiple
supercategories. The full hierarchy is in figure 1.
</p>
<p>The approach to preposition annotation is com-
prehensive, i.e., every token of every preposition
type is given a supersense label. The supersenses
were applied to annotate a 55,000 word corpus of
online reviews in English, covering all 4,250 prepo-
sition tokens (Schneider et al., 2016). For each
token, annotators chose a single label from the in-
ventory. This is not an easy task, but with documen-
tation of many examples in a lexical resource, Prep-
Wiki,3 trained university students were able to
achieve reasonable levels of inter-annotator agree-
ment. Every token was initially labeled by at least
two independent annotators, and differences were
adjudicated by experts.
</p>
<p>3 Problems with Preposition Supersenses
</p>
<p>While the above approach works reasonably well
for most English tokens, difficulties in directly ap-
plying the scheme to adpositions and case markers
in other languages, as well as some of the persistent
issues arising in English, have led us to conclude
that perhaps the supersense hierarchy as it stands
is too simplistic to provide a faithful account of the
prepositions’ semantic behavior. This has caused
us to examine fundamental assumptions made by
previous work and reevaluate what it means to se-
mantically label an adposition.
</p>
<p>3.1 Semantic Overlap in English
In the original English annotation (Schneider
et al., 2016), a few phenomena caused much hand-
wringing—not because there was no appropriate su-
persense, but because multiple supersenses seemed
to fit. For example, it was observed that TOPIC and
STIMULUS could compete for semantic territory.
(2) evinces related usages of about with different
governors:
</p>
<p>2These loosely correspond to event arguments, adjuncts,
and adnominal complements, respectively. However, super-
sense organization does not make any claims with regard to
coreness or the argument/adjunct distinction, as there are many
phenomena that do not conform to either of the prototypes
for argument and adjunct (for a review of the literature on the
argument/adjunct distinction, see Hwang, 2011). We are also
not convinced that a firm distinction between lexical and non-
lexical/functional adpositions (Rauh, 1993) can be established,
though the relevance of this distinction in the context of the
construal approach merits further investigation.
</p>
<p>3 http://tiny.cc/prepwiki
</p>
<p>(2) a. I read [a book about the strategy].
b. I read about the strategy.
c. I knew about the strategy.
d. I cared about the strategy.
</p>
<p>Usages (2a–2c) could reasonably be labeled as
TOPIC. This is because the about-PP indicates what
is communicated (2a, 2b) and known (2c). The
fourth example (2d), however, presents an over-
lap in its interpretation. On the one hand, tradi-
tional thematic role inventories include the category
STIMULUS for something that prompts a percep-
tual or emotional experience, as in (3).
</p>
<p>(3) I was afraid of the strategy.
</p>
<p>Surely, cared in (2d) describes an emotional
state, so about marks the STIMULUS. However,
much like examples (2a–2c), the semantics relating
to TOPIC is still very much present in the use of
about, drawing attention to the aspects of the car-
ing process involving thought or judgement. This
contrasts with the use of for in “I cared for my
grandmother,” where the prepositional choice calls
attention to the benefactive aspect of the caring act.
</p>
<p>If we are constrained to one label per argument,
where should the line be drawn between STIMU-
LUS and TOPIC in cases of overlap? In other words,
should the semantic representation emphasize the
semantic commonality between all of the examples
in (2), or between (2d) and (3)?
</p>
<p>Observing that annotators were inconsistent on
such tokens, Schneider et al. (2016) drew a bound-
ary between TOPIC and STIMULUS in an attempt
to force consistency, stating that “TOPIC should
be used if the governor is a predicate of commu-
nication or of ‘higher-level’ cognition—i.e., pri-
marily mental/intellectual rather than emotional/
perceptual/bodily in nature”.4 This criterion seems
artificial to us; at the very least, it splits hairs in a
way that would be difficult to explain to annotators.
</p>
<p>Below, we instead argue that the idea of con-
strual/conceptualization offers a more principled
answer; in our new analysis, the TOPIC suggested
by about and the STIMULUS suggested by cared
can coexist.
</p>
<p>3.2 Applying the Supersenses to Other
Languages
</p>
<p>One of the premises of using unlexicalized super-
senses was that the scheme would port well to other
</p>
<p>4 http://tiny.cc/prepwiki/index.php/Category:
SST-Topic
</p>
<p>180</p>
<p />
</div>
<div class="page"><p />
<p>languages (as the WordNet noun and verb super-
senses have: Picca et al., 2008; Schneider et al.,
2012, inter alia). To test this, we have begun apply-
ing the existing supersenses to three new languages,
namely, Hebrew, Hindi, and Korean. Pilot annota-
tion in these languages has echoed the fundamental
problem discussed in the previous section.
</p>
<p>Consider the Hindi examples below. In (4a), the
experiencer of an emotion is marked with a postpo-
sition kaa, the genitive case marker in Hindi.
</p>
<p>(4) a. [Hindi]: EXPERIENCER vs. POSSESSOR
bipaashaa kaa gussaa
Bipasha GEN anger
“Bipasha’s anger”
</p>
<p>b. [Hindi]: EXPERIENCER
bipaashaa bahut gussaa hui
Bipasha very angry became
“Bipasha got very angry.”
</p>
<p>The use of kaa strongly suggests possession (in
(4), possession of an abstract quality). However,
the semantics of the phrase also includes EXPE-
RIENCER—thus, it seems inappropriate to force a
choice between EXPERIENCER and POSSESSOR
for this token. (The same problem is seen in a simi-
lar phrase “the anger of Bipasha” in English.) There
are other ways to attribute anger to Bipasha—e.g.,
see (4b). Here Bipasha is not construed as a posses-
sor when the postposition kaa is not used.
</p>
<p>Our preliminary annotation of Hindi, Korean,
and Hebrew has suggested that instances of overlap
between multiple supersenses are fairly frequent.
</p>
<p>4 The Construal Analysis
</p>
<p>Why do “cared about the strategy” in (2d) and
“anger of Bipasha” (cf. (4a)) not lend themselves
to a single label? These seem to be symptoms of
the fact that no English preposition prototypically
marks EXPERIENCER or STIMULUS roles, though
from the perspective of the predicates, such roles
are thought to be important generalizations in char-
acterizing events of perception and emotion. In
essence, there is an apparent mismatch between
the roles that the verb care or the noun anger calls
for, and the functions that English prepositions pro-
totypically code for. While about prototypically
codes for TOPIC and of prototypically codes for
POSSESSOR, there is no preposition that “naturally”
codes for EXPERIENCER or STIMULUS in the same
way. Thus, if a predicate marks an EXPERIENCER
or STIMULUS with a preposition, the preposition
</p>
<p>will contribute something new to the conceptual-
ization of the scene being described. With “cared
about the strategy,” it is TOPIC-ness that the prepo-
sition brings to the table; with “anger of Bipasha,”
it is the conceptualization of anger as an attribute
that somebody possesses.
</p>
<p>Thus, we turn to theories in Cognitive Seman-
tics to define the phenomenon of construal as a
means of understanding the contributions that are
emerging from the adpositions with respect to the
expressed event or situation. Then, we propose a
method to handle the problem posed by construal
and to resolve the apparent semantic overlap which
is pervasive across languages.
</p>
<p>4.1 Defining Construal
The world is not neatly organized into bits of in-
formation that map directly to linguistic symbols.
Rather, linguistic meaning reflects the priorities
and categorizations of particular expressions in
a language (Langacker, 1998; Jackendoff, 2002;
Croft and Cruse, 2004, ch. 3). Much like pictures
of a scene from different viewpoints will result in
different renderings, a real-world situation being
described will “look” different depending on the
linguistic choices made by a speaker. This includes
within-language choices: e.g., the choice of “John
sold Mary a book” vs. “John sold a book to Mary”
vs. “Mary bought a book from John.” In the pro-
cess called construal (a.k.a. conceptualization), a
speaker “packages” ideas for linguistic expression
in a way that foregrounds certain elements of a
situation while backgrounding others.
</p>
<p>We propose to incorporate this notion of con-
strual in adposition supersense annotation. We use
the term scene to refer to events or situations in
which an adpositional phrase plays a role. (We do
not formalize the full scene, but assume its roles
can be characterized with supersense labels from
figure 1.) Contrast the use of the prepositions by
and of in (5):
</p>
<p>(5) a. The festival features works by Puccini.
b. I’m an expert on the works of Puccini.
</p>
<p>While both prepositional phrases indicate works
created by the operatic composer Puccini (i.e., CRE-
ATOR), the different choices of preposition reflect
different construals: by highlights the agency of
Puccini, whereas of construes Puccini as the source
of his composition. Thus, “works by Puccini” and
“works of Puccini” are paraphrases, but present sub-
tly different portrayals of the relationship between
</p>
<p>181</p>
<p />
</div>
<div class="page"><p />
<p>Puccini and his works. In other words, these para-
phrases are not identical in meaning because the
preposition carries with it different nuances of con-
strual. In this paper, we focus on differences in con-
strual manifested in different adposition choices,
and the possibility that an adposition construal com-
plements the construal of a scene and its roles (as
evoked by the governing head or predicate).
</p>
<p>For instances like “I read about the strategy” in
(2b) that were generally unproblematic for annota-
tion under the original preposition guidelines, the
semantics of the adposition and the semantic role
assigned by the predicate are congruent. However,
for examples like “cared about the strategy” in (2d)
and “anger of Bipasha” in (4a), we say that the
adposition construes the role as something other
than what the scene specifies. Competition between
different adposition construals accounts for many
of the alternations that are near-paraphrases, but
potentially involve slightly different nuances of
meaning (e.g., “talk to someone” vs. “talk with
someone”; “angry at someone” vs. “angry with
someone”).
</p>
<p>Thus, the notion of construal challenges Schnei-
der et al.’s (2015; 2016) original conception that
each supersense reflects the semantic role assigned
by its governing predicate (i.e. verbal or event nomi-
nal predicate), and that a single supersense label
can be assigned to each adposition token. Rather
than trying to ignore these construals to favor a
single-label approach, or possibly create new labels
to capture the meaning distinctions that construals
impose on semantic roles, we adopt an approach
that gives us the flexibility to deal with both the
semantics coming from the scene and the construal
evoked by the adpositional choice.
</p>
<p>4.2 Formulating a Construal Analysis
We address the issues of construal by decoupling
the semantics signaled by the adposition from the
role expected by the scene. Essentially, we bor-
row from Construction Grammar (Fillmore et al.,
1988; Kay and Fillmore, 1999; Goldberg, 2006) the
notion that semantic contributions can be made at
various levels of syntactic structure, beginning with
the semantics contributed by the lexical items.
</p>
<p>Under the original single-label analysis, the full
weight of semantic assignment rested on the predi-
cate’s semantic role, with the indirect assumption
that the predicate selects for adpostions relevant to
the assignment. Under the construal analysis, we
assign semantics at both scene and adposition lev-
</p>
<p>els of meaning: we capture what the scene calls
for, henceforth scene role and what the adposition
itself codes for, henceforth function. Both labels
are drawn from the supersense hierarchy (figure 1).
Allowing tokens to be annotated with both a role
and a function accounts for the non-congruent ad-
position construals, as in (6).
</p>
<p>(6) a. The festival features works by Puccini.
scene role: CREATOR; function: AGENT
</p>
<p>b. I’m an expert on the works of Puccini.
scene role: CREATOR; function: SOURCE
</p>
<p>We recognize that both of these sentences carry the
meaning represented by the supersense CREATOR
at the scene level, but also recognize the construal
that arises from the chosen preposition: by is as-
signed the function of AGENT and of is assigned
the function of SOURCE.5
</p>
<p>5 Applying the Construal Analysis
</p>
<p>In this section, we discuss some of the more
productive examples of non-congruent constru-
als in English as well as in Hindi, Korean,
and Hebrew. Hereafter, we will use the notation
ROLE;FUNCTION to indicate such construals.
Adopting the “realization” metaphor of articulating
an idea linguistically, this can be read as “ROLE is
realized with an adposition that marks FUNCTION.”
</p>
<p>5.1 Emotion and Perception Construals
Scenes of emotion and perception (Dirven, 1997;
Osmond, 1997; Radden, 1998) provide a com-
pelling case for the construal analysis. Consider
the sentences involving emotion in example (7):
</p>
<p>(7) a. I was scared by the bear.
STIMULUS;CAUSER
</p>
<p>b. I was scared about getting my ears pierced.
STIMULUS;TOPIC
</p>
<p>Comparing examples (7a) and (7b), we notice
that there are two different types of stimuli repre-
sented in otherwise semantically parallel sentences.
</p>
<p>5We also acknowledge that there is a level of construal
contributed by the verb. For example, Alex in Alex sent the
package to Pam can be AGENT or SOURCE depending whether
the interpretation is focused on the agency of the argument or
the spatial relation it has in reference to the action described by
the verb. These verb-triggered construals have been previously
explored, most notably by Jackendoff (1990). Perspective can
also be evident in the choice of syntactic constructions, e.g.,
active vs. passive voice (I made a mistake versus Mistakes
were made), which can be connected to sentiment (Greene
and Resnik, 2009). We specifically focus on the construal that
arises from the adposition in a given sentence.
</p>
<p>182</p>
<p />
</div>
<div class="page"><p />
<p>The preposition by gives the impression that the
stimulus is responsible for triggering an instinctive
fear reflex (i.e., CAUSER), while about portrays the
thing feared as the content or TOPIC of thought.6
</p>
<p>In some languages, the experiencer can be con-
ceptualized as a recipient of the emotion or feeling,
thus licensing dative marking.7 In the Hebrew ex-
ample (8a), the experiencer of bodily perception is
marked with the dative preposition l(e)- (Berman,
1982). Similarly, in Hindi, the dative postpostion
-ko marks an experiencer in (8b).
</p>
<p>(8) a. [Heb.]: EXPERIENCER;RECIPIENT
Koev l-i ha-rosh
Hurts DAT-me the-head
“My head hurts.”
</p>
<p>b. [Hindi]: EXPERIENCER;RECIPIENT
mujh-ko garmii lag rahii hai
I-DAT heat feel PROG PRES
“I’m feeling hot.”
</p>
<p>Contrast this with examples where scene role
and adposition function are congruent:
</p>
<p>(9) a. I ate dinner at 7:00. TIME;TIME
b. Let’s talk about our plan. TOPIC;TOPIC
</p>
<p>In (9a) and (9b), the preposition is prototypical
for the given scene role and its function directly
identifies the scene role. Because the semantics of
the role and function are congruent, these cases do
not exhibit the extra layer of construal seen in (7)
and (8).8 In essence, our analysis helps capture the
construals that characterize the less prototypical
scene role and function pairings.
</p>
<p>5.2 Professional Associate Construals
The online reviews corpus (Schneider et al., 2016)
shows that, at least in English, professional relation-
</p>
<p>6Interestingly, “scared about” seems to require an ex-
plicit or metonymic event/situation as the complement. Thus,
“scared about the bear” would be felicitous to describe appre-
hension about some mischief that the bear might get up to.
It would be less than felicitous to describe a hiker’s reaction
upon being surprised by a bear.
</p>
<p>7English displays this to a limited extent: “It feels/seems/
looks perfect to me.”
</p>
<p>8One might object that most or all adpositions impose
a spatial construal—and thus, (9a) should be annotated as
TIME;LOCATION. We do not discount the possibility that
such a metaphor can be cognitively active in speakers using
temporal adpositions; in fact, there is considerable evidence
that time-as-space metaphors are cross-linguistically perva-
sive and productive (Lakoff and Johnson, 1980; Núñez and
Sweetser, 2006; Casasanto and Boroditsky, 2008). However,
we do not see much practical benefit to annotating temporal
at or topical about as spatial.
</p>
<p>ships (especially employer–employee and business–
client ones) are fertile ground for alternating prepo-
sition construals. The following were among the
examples tagged as PROFESSIONALASPECT:
</p>
<p>(10) a. My dad worked for a record label.
PROFESSIONALASPECT;BENEFICIARY
</p>
<p>b. Dr. S— at CVTS is not a good doctor.
PROFESSIONALASPECT;LOCATION
</p>
<p>c. Nigel from Nidd Design has always
been great!
PROFESSIONALASPECT;SOURCE
</p>
<p>d. The owners and employees of this store ...
PROFESSIONALASPECT;POSSESSOR
</p>
<p>All of these construals are motivated in that they
highlight an aspect of prototypical professional re-
lationships: e.g., an employee’s work prototypically
takes place at the business location (hence “work
at”), though this is not a strict condition for using
“work at”—the meaning of at has been extended
from the prototype. Likewise, the pattern “person
{at, from, of} organization” has been conventional-
ized to signify employment or similar institutional-
belonging relationships.
</p>
<p>The construal analysis equips us with the abil-
ity to use the existing labels like BENEFICIARY
and LOCATION to deal with the overloading of the
PROFESSIONALASPECT label, instead of forcing
a difficult decision or creating several additional
categories. This analysis also accounts for simi-
lar construals presented by adpositions in other
languages. For example, the overlap of PROFES-
SIONALASPECT with SOURCE, as seen in English
example (10c), occurs in Hindi and Korean as well.
</p>
<p>5.3 Static vs. Dynamic Construals
Another source of difficulty in the original anno-
tation came from caused-motion verbs like put,
which takes a PP indicating part of a path. Some-
times the preposition lexically marks a source
or goal, e.g., into, onto, or out of (11a). Often,
however, the preposition is prototypically locative,
e.g., in or on (11b), though the object of the prepo-
sition is interpreted as a destination, equivalent to
the use of into or onto, respectively. This locative-
as-destination construal is highly productive, so
analyzing on as polysemous between LOCATION
and DESTINATION does not capture the regular-
ity. The PP is sometimes analyzed as a resultative
phrase (Goldberg, 2006). In our terms, we simply
say that the scene calls for a DESTINATION, but the
preposition codes for a LOCATION:
</p>
<p>183</p>
<p />
</div>
<div class="page"><p />
<p>(11) a. Cynthia put her things into a box.
DESTINATION;DESTINATION
</p>
<p>b. Cynthia put her things on her bed.
DESTINATION;LOCATION
</p>
<p>Thus, we avoid listing the preposition with multiple
lexical functions for this regular phenomenon.
</p>
<p>The opposite problem occurs with fictive motion
(Talmy, 1996): a path PP, and sometimes a motion
verb, construe a static scene as dynamic as seen
in “A road runs through my property.” Rather than
forcing annotators to side with the dynamic con-
strual effected by the language, versus the static
nature of the actual scene, we represent both: the
scene role is LOCATION (static) and the prepo-
sition function is PATH (dynamic) (i.e., LOCA-
TION;PATH).
</p>
<p>5.4 Metaphoric Scenes
Finally, our analysis gives us a way to handle
metaphoric scenes (Lakoff and Johnson, 1980). In
(12), the locative-as-destination construal (§5.3) is
layered with the states-are-locations metaphor. We
annotate the scene in terms of the governing pred-
icate’s target domain (domain which we seek to
describe), and the adposition function in terms of
the source domain (domain from which we draw
metaphorical expressions to conceptualize the tar-
get domain):
</p>
<p>(12) The election news put him in a very bad mood.
ENDSTATE;LOCATION
</p>
<p>Our construal analysis can capture both source and
target domains by assigning the source domain
meaning to the function of the preposition and the
target domain meaning to the scene role.
</p>
<p>6 Toward a Revised Hierarchy
</p>
<p>The annotation of both scene and function levels
of semantics allows us to trade more complexity at
the token level for less complexity in the label set.
As discussed in §4, separating the scene role and
function levels of annotation will more adequately
capture construal phenomena without forcing an
arbitrary choice between two labels or introducing
further complexity into the hierarchy.
</p>
<p>In fact, we intend to simplify the current super-
sense hierarchy, by collapsing some of the finer-
grained distinctions that can be accounted for with
the construal analysis instead. Candidates for re-
moval include the labels with multiple inheritance
such as CONTOUR (inheriting from PATH and
</p>
<p>Circumstance
</p>
<p>Temporal
</p>
<p>Time
</p>
<p>StartTime
</p>
<p>EndTime
</p>
<p>Frequency
</p>
<p>Duration
</p>
<p>Interval
</p>
<p>Locus
</p>
<p>Source
</p>
<p>Goal
</p>
<p>Path
</p>
<p>Direction
</p>
<p>Extent
</p>
<p>Means
</p>
<p>Manner
</p>
<p>Explanation
</p>
<p>Purpose
</p>
<p>Participant
</p>
<p>Causer
</p>
<p>Agent
</p>
<p>Co-Agent
</p>
<p>Theme
</p>
<p>Co-Theme
</p>
<p>Topic
</p>
<p>Stimulus
</p>
<p>Experiencer
</p>
<p>Originator
</p>
<p>Recipient
</p>
<p>Cost
</p>
<p>Beneficiary
</p>
<p>Instrument
</p>
<p>Configuration
</p>
<p>Identity
</p>
<p>Species
</p>
<p>Gestalt
</p>
<p>Possessor
</p>
<p>Whole
</p>
<p>Characteristic
</p>
<p>Possession
</p>
<p>Part/Portion
</p>
<p>Stuff
</p>
<p>Accompanier
</p>
<p>InsteadOf
</p>
<p>ComparisonRef
</p>
<p>RateUnit
</p>
<p>Quantity
</p>
<p>Approximator
</p>
<p>SocialRel
</p>
<p>OrgRole
</p>
<p>• Items in the CIRCUMSTANCE subhierarchy are prototypically expressed as
adjuncts of time, place, manner, purpose, etc. elaborating an event or en-
tity.
</p>
<p>• Items in the PARTICIPANT subhierarchy are prototypically entities func-
tioning as arguments to an event.
</p>
<p>• Items in the CONFIGURATION subhierarchy are prototypically entities or
properties in a static relationship to some entity.
</p>
<p>1.3 Limitations
</p>
<p>This inventory is only designed to capture semantic relations with a figure–ground
asymmetry. This excludes:
</p>
<p>• The semantics of coordination, where the two sides of the relation are on
equal footing, is not captured here. (Note that sometimes a morpheme
can have symmetric as well as asymmetric interpretations: e.g., Korean
-wa.)
</p>
<p>• Aspects of meaning that pertain to information structure, discourse, or
pragmatics.
</p>
<p>3
</p>
<p>Figure 2: Preliminary revised hierarchy of 50 adposition su-
persenses.
</p>
<p>MANNER; e.g., “The fly flew in zig-zags”) and
TRANSIT (inheriting from VIA and LOCATION;
e.g., “We traveled by bus”).
</p>
<p>A preliminary proposal for a new hierarchy ap-
pears in figure 2.9 It weighs in at only 50 categories,
a third fewer than the original 75. A significantly
smaller inventory will both ease the cognitive bur-
den on annotators and reduce the sparsity of labels
in the data, which should facilitate better statistical
generalizations with limited data.
</p>
<p>The added representational complexity of con-
struals seems justified to account for many of
the phenomena discussed above, especially as the
project grows to include more languages. But is the
complexity worth it on balance? We consider some
of the tradeoffs below.
</p>
<p>6.1 Challenges in Function Assignment
We encountered several examples in which func-
tion labels are difficult to identify. Consider the
following paraphrases:
</p>
<p>(13) a. [Korean]: LOCATION;LOCATION
Cheolsu-nun undongcang-eyse tallyessta.
Cheolsu-NOM schoolyard-at ran.
“Cheolsu ran in the schoolyard.”
</p>
<p>b. [Korean]: LOCATION;?
Cheolsu-nun undongcang-ul tallyessta.
Cheolsu-NOM schoolyard-ACC ran.
“Cheolsu ran in the schoolyard.”
</p>
<p>9Apart from changes enabled by the construal analy-
sis, a number of other simplifications and enhancements
are incorporated into the proposal, which space does
not allow us to enumerate here: for example, collaps-
ing LOCUS/LOCATION/STATE, SOURCE/INITIALLOCATION/
INITIALSATE, GOAL/DESTINATION/ENDSTATE, and TIME/
RELATIVETIME/CLOCKTIMECXN; and replacing PROFES-
SIONALASPECT with SOCIALREL and ORGROLE.
</p>
<p>184</p>
<p />
</div>
<div class="page"><p />
<p>In (13a), “schoolyard” is accompanied by a post-
position -eyse (comparable to English at), which
marks it as the location of running. This is the
unmarked choice. On the other hand, in sentence
(13b), the noun is paired with the accusative marker
-ul, the marked choice. The use of -ul evokes a spe-
cial construal: it indicates that the schoolyard is
more than just a backdrop of the running act and
that it is a location that Cheolsu mindfully chose
as the place of action. Additionally, marking the
location with the accusative marker, pragmatically,
brings focus to the noun (i.e., he ran in a schoolyard
as opposed to anywhere else). Such construals are
not limited to locations, but may also include other
scene roles such as GOAL and ACCOMPANIER,
in alternation with postpositions that can express
those functions. Since accusative case markers gen-
erally serve syntactic functions over semantic ones,
it may be difficult to identify a semantic function
the accusative marker carries.
</p>
<p>A similar phenomenon can be found in Hindi:
</p>
<p>(14) a. [Hindi]: bare NP as DESTINATION
maiN library jaa rahii thii
I library go PROG PST
“I was going to the library.”
</p>
<p>b. [Hindi]: DESTINATION;?
maiN library-ko jaa rahii thii
I library-ACC go PROG PST
“I was going to the LIBRARY.” [more
emphasis on the library]
</p>
<p>This suggests that, apart from spatiotemporal re-
lations and semantic roles, adpositions can mark
information structural properties for which we
would need a separate inventory of labels.
</p>
<p>In some idiomatic predicate–argument combina-
tions, the semantic motivation for the preposition
may not be clear (15).
</p>
<p>(15) a. Listen to the violin! STIMULUS;?
b. What’s he proudest of? STIMULUS;?
c. Unhappy with my meal! STIMULUS;?
d. I’m interested in politics. TOPIC;?
</p>
<p>While the scene role in (15a) and (15b) is clearly
STIMULUS, the function is less clear. Is the object
of attention construed (metaphorically) as a GOAL
in (15a), and the cause for pride as a SOURCE
in (15b)? Or are to and of semantically empty
argument-markers for these predicates (cf. the
“case prepositions” of Rauh, 1993)? We do not treat
either combination as an unanalyzable multiword
</p>
<p>expression because the ordinary meaning of the
predicate is very much present. (15c) and (15d) are
similarly fraught. But as we look at more data, we
will entertain the possibility that the function can
be null to indicate a marker which contributes no
lexical semantics.
</p>
<p>6.2 Challenges in Scene Role Assignment
There are complications which we are not yet pre-
pared to fully address. First, if the PP is not gov-
erned by a predicate which provides the roles—
such as a verb or eventive/relational noun—the
preposition may need to evoke a meaning more spe-
cific than our labels. E.g., for “children in pajamas”
and “woman in black,” in may be taken to evoke
the semantics of wearing clothing.10 The label set
we use for broad-coverage annotation is, of course,
vaguer, and would simply specify ATTRIBUTE for
the clothing sense of in. Copular constructions raise
similar issues. Consider “It is up to you to decide,”
meaning that deciding is the addressee’s responsi-
bility: this idiomatic sense of up to is closer to a
semantic predicate than to a semantic role or figure-
ground relation.
</p>
<p>6.3 Multi-Construal Analysis?
In rare instances, we are tempted to annotate a
chain of extensions from a prototypical function of
a preposition, which we term multiple construal.
For instance:
</p>
<p>(16) a. Bob’s boss yelled at him for his mistake.
RECIPIENT;BENEFICIARY;GOAL
</p>
<p>b. Jane was angry at him for his mistake.
STIMULUS;BENEFICIARY;GOAL
</p>
<p>c. I was involved in the project.
THEME;SUPERSET;LOCATION
</p>
<p>“Yelled at” in (16a) is a communicative action
whose addressee (RECIPIENT) is also a target of the
negative emotion (BENEFICIARY;GOAL: com-
pare the use of at in “shoot at the target”). (16b)
is similar, except “angry” focuses on the emotion
itself, which Bob is understood to have evoked in
his boss.
</p>
<p>With regard to (16c), the item “involved in” has
become fossilized, with in marking an underspec-
ified noncausal participant (hence, THEME as the
scene role). At the same time, one can understand
</p>
<p>10Indeed, this is the position adopted by version 1.7
of FrameNet, where in is listed as a lexical unit of
the WEARING frame (https://framenet2.icsi.berkeley.
edu/fnReports/data/frame/Wearing.xml).
</p>
<p>185</p>
<p />
</div>
<div class="page"><p />
<p>the in here as motivated by the member-of-set sense
(cf. “I am in the group”), which would be labeled
SUPERSET;LOCATION because it conceptualizes
membership in terms of containment. A similar
logic would apply to “people in the company”: PRO-
FESSIONALASPECT;SUPERSET;LOCATION. Ef-
fectively, the multiple construal analysis claims
that multiple steps of extending a preposition’s pro-
totypical meaning remain conceptually available
when understanding an instance of its use. That
said, we are not convinced that this logic could be
applied reliably by annotators, and thus may sim-
plify the usages in (16) to just the first and second
or the first and third labels.
</p>
<p>6.4 The Annotation Process
</p>
<p>Annotators are generally capable of interpreting
meaning in a given context. However, it might be
difficult to train annotators to develop intuitions
about adposition functions, which reflect prototyp-
ical meanings contributed by the lexical item that
may not be literally applicable. These distinctions
may be too subtle to annotate reliably. As we are
approaching this project with the goal of produc-
ing annotated datasets for training and evaluating
natural language understanding systems, it is an
important concern.
</p>
<p>We are currently planning pilot annotation stud-
ies to ascertain (i) the prevalence of the role
vs. function mismatches, and (ii) annotator agree-
ment on such instances. Enshrining role–function
pairs in the lexicon may facilitate inter-annotator
consistency: our experience thus far is that anno-
tators benefit greatly from examples illustrating
the possible supersenses that can be assigned to a
preposition.
</p>
<p>If initial pilots are successful, we would then
need to decide whether to annotate the role and
function together or in separate stages. Because
the function reflects one of the adposition’s proto-
typical senses, it may often be deterministic given
the adposition and scene role, in which case we
could focus annotators’ efforts on the scene roles.
Existing annotations for lexical resources such as
PropBank (Palmer et al., 2005), VerbNet (Palmer
et al., 2017; Kipper et al., 2008), and FrameNet
(Fillmore and Baker, 2009) might go a long way
toward disambiguating the scene role, limiting the
effort required from annotators.
</p>
<p>6.5 Linguistic Utility of Annotated Data
</p>
<p>Assuming the above theoretical and practical con-
cerns are surmountable, annotated corpora would
facilitate empirical studies of the nature and limits
of adposition/case construal within and across lan-
guages. For example: Is it the case that some of the
supersense labels can only serve as scene roles, or
only as functions? (A hypothesis is that PARTICI-
PANT subtypes tend to be limited to scene roles, but
this needs to be examined empirically.) Which role–
function pairs are attested in particular languages,
and are any universal? Thus far we have seen that
certain scene roles, such as EXPERIENCER, STIM-
ULUS, and PROFESSIONALASPECT, invite many
different adposition construals—is this universally
true? As adpositions are notoriously difficult for
second language learners, would it help to explain
which construals do and do not transfer from the
first language to the second language?
</p>
<p>7 Conclusion
</p>
<p>We have considered the semantics of adpositions
and case markers in English and a few other lan-
guages with the goal of revising a broad-coverage
annotation scheme used in previous work. We
pointed out situations where a single supersense did
not fully characterize the interaction between the
adposition and the scene elaborated by the PP. In
an attempt to tease apart the semantics contributed
specifically by the adposition from the semantics
coming from elsewhere, we proposed a construal
analysis. Though many details remain to be worked
out, we are optimistic that our analysis will ulti-
mately improve broad-coverage annotations as well
as constructional analyses of adposition behavior.
</p>
<p>Acknowledgments
</p>
<p>We thank the rest of our CARMLS team—Martha
Palmer, Ken Litkowski, Omri Abend, Katie Conger,
and Meredith Green—for participating in weekly
discussions of adposition semantics; Michael
Ellsworth for an insightful perspective on construal,
Paul Portner for a helpful clarification regarding
approaches to conceptualization in the literature,
and anonymous reviewers for their thoughtful com-
ments. We also thank the participants in the AAAI
Spring Symposium on Construction Grammar in
NLU, held earlier this year at Stanford, where an
early version of this work was presented.
</p>
<p>186</p>
<p />
</div>
<div class="page"><p />
<p>References
Ruth A. Berman. 1982. Dative marking of the affectee role:
</p>
<p>data from Modern Hebrew. Hebrew Annual Review 6:35–
59.
</p>
<p>Claire Bonial, William Corvey, Martha Palmer, Volha V.
Petukhova, and Harry Bunt. 2011. A hierarchical unifi-
cation of LIRICS and VerbNet semantic roles. In Fifth
IEEE International Conference on Semantic Computing.
Palo Alto, CA, USA, pages 483–489.
</p>
<p>Melissa Bowerman and Soonja Choi. 2001. Shaping mean-
ings for language: universal and language-specific in the
acquisition of spatial semantic categories. In Melissa Bow-
erman and Stephen Levinson, editors, Language Acquisi-
tion and Conceptual Development, Cambridge University
Press, Cambridge, UK, pages 475–511.
</p>
<p>Claudia Brugman. 1981. The story of ‘over’: polysemy, seman-
tics and the structure of the lexicon. MA thesis, University
of California, Berkeley, Berkeley, CA. Published New
York: Garland, 1981.
</p>
<p>Daniel Casasanto and Lera Boroditsky. 2008. Time in
the mind: using space to think about time. Cognition
106(2):579–593.
</p>
<p>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extraction
with a supersense sequence tagger. In Proc. of EMNLP.
Sydney, Australia, pages 594–602.
</p>
<p>William Croft and D. Alan Cruse. 2004. Cognitive linguistics.
Cambridge University Press, Cambridge, UK.
</p>
<p>Daniel Dahlmeier, Hwee Tou Ng, and Tanja Schultz. 2009.
Joint learning of preposition senses and semantic roles of
prepositional phrases. In Proc. of EMNLP. Suntec, Singa-
pore, pages 450–458.
</p>
<p>Robert B. Dewell. 1994. ‘Over’ again: Image-schema trans-
formations in semantic analysis. Cognitive Linguistics
5(4):351–380.
</p>
<p>René Dirven. 1993. Dividing up physical and mental space
into conceptual categories by means of English preposi-
tions. In Cornelia Zelinsky-Wibbelt, editor, The seman-
tics of prepositions: From mental processing to natural
language processing, Mouton de Gruyter, Berlin, pages
73–97.
</p>
<p>René Dirven. 1997. Emotions as cause and the cause of
emotions. In Susanne Niemeier and René Dirven, editors,
The Language of Emotions: Conceptualization, expression,
and theoretical foundation, John Benjamins, Amsterdam,
pages 55–86.
</p>
<p>Charles J. Fillmore. 1968. The case for case. In Emmon
Bach and Robert Thomas Harms, editors, Universals in
Linguistic Theory, Holt, Rinehart, and Winston, New York,
pages 1–88.
</p>
<p>Charles J. Fillmore and Collin Baker. 2009. A frames ap-
proach to semantic analysis. In Bernd Heine and Heiko
Narrog, editors, The Oxford Handbook of Linguistic Analy-
sis, Oxford University Press, Oxford, UK, pages 791–816.
</p>
<p>Charles J. Fillmore, Paul Kay, and Mary Catherine O’Connor.
1988. Regularity and idiomaticity in grammatical construc-
tions: the case of let alone. Language 64(3):501–538.
</p>
<p>Adele E. Goldberg. 2006. Constructions at work: the nature
of generalization in language. Oxford University Press.
</p>
<p>Stephan Greene and Philip Resnik. 2009. More than words:
syntactic packaging and implicit sentiment. In Proc. of
NAACL-HLT . Boulder, Colorado, pages 503–511.
</p>
<p>Martin Haspelmath. 2003. The geometry of grammatical
meaning: semantic maps and cross-linguistic comparison.
In Michael Tomasello, editor, The New Psychology of Lan-
guage: Cognitive and Function Approaches to Language
Structure, Lawrence Erlbaum Associates, Mahwah, NJ,
volume 2, pages 211–242.
</p>
<p>Annette Herskovits. 1986. Language and spatial cognition:
an interdisciplinary study of the prepositions in English.
Cambridge University Press, Cambridge, UK.
</p>
<p>Jena D. Hwang. 2011. Making verb argument adjunct distinc-
tions in English. Synthesis paper, University of Colorado,
Boulder, Colorado.
</p>
<p>Ray Jackendoff. 1990. Semantic structures. MIT press.
</p>
<p>Ray Jackendoff. 2002. Foundations of Language: Brain,
Meaning, Grammar, Evolution. Oxford.
</p>
<p>Paul Kay and Charles J. Fillmore. 1999. Grammatical con-
structions and linguistic generalizations: the What’s X do-
ing Y? construction. Language 75(1):1–33.
</p>
<p>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha
Palmer. 2008. A large-scale classification of English verbs.
Language Resources and Evaluation 42(1):21–40.
</p>
<p>George Lakoff. 1987. Women, fire, and dangerous things:
what categories reveal about the mind. University of
Chicago Press, Chicago.
</p>
<p>George Lakoff and Mark Johnson. 1980. Metaphors We Live
By. University of Chicago Press, Chicago.
</p>
<p>Ronald W. Langacker. 1998. Conceptualization, symboliza-
tion, and grammar. In Michael Tomasello, editor, The
New Psychology of Language: Cognitive and Functional
Approaches to Language Structure, Lawrence Erlbaum As-
sociates, Mahwah, NJ, pages 1–39.
</p>
<p>Seth Lindstromberg. 2010. English Prepositions Explained.
John Benjamins, Amsterdam, revised edition.
</p>
<p>Ken Litkowski and Orin Hargraves. 2005. The Preposition
Project. In Proc. of the Second ACL-SIGSEM Workshop on
the Linguistic Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and Applications.
Colchester, Essex, UK, pages 171–179.
</p>
<p>Rafael E. Núñez and Eve Sweetser. 2006. With the future
behind them: convergent evidence from Aymara language
and gesture in the crosslinguistic comparison of spatial
construals of time. Cognitive Science 30(3):401–450.
</p>
<p>Meredith Osmond. 1997. The prepositions we use in the
construal of emotions: why do we say fed up with but
sick and tired of? In Susanne Niemeier and René Dirven,
editors, The Language of Emotions: Conceptualization,
expression, and theoretical foundation, John Benjamins,
Amsterdam, pages 111–133.
</p>
<p>187</p>
<p />
</div>
<div class="page"><p />
<p>Martha Palmer, Claire Bonial, and Jena D. Hwang. 2017.
VerbNet: Capturing English verb behavior, meaning and
usage. In Susan E. F. Chipman, editor, The Oxford Hand-
book of Cognitive Science, Oxford University Press, pages
315–336.
</p>
<p>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: an annotated corpus of semantic roles.
Computational Linguistics 31(1):71–106.
</p>
<p>Davide Picca, Alfio Massimiliano Gliozzo, and Massimiliano
Ciaramita. 2008. Supersense Tagger for Italian. In Nico-
letta Calzolari, Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias,
editors, Proc. of LREC. Marrakech, Morocco, pages 2386–
2390.
</p>
<p>Günter Radden. 1998. The conceptualisation of emotional
causality by means of prepositional phrases. In Ange-
liki Athanasiadou, Elżbieta Tabakowska, René Dirven,
Ronald W. Langacker, and John R. Taylor, editors, Speak-
ing of emotions: conceptualisation and expression, Mouton
de Gruyter, pages 273–294.
</p>
<p>Gisa Rauh. 1993. On the grammar of lexical and non-lexical
prepositions in English. In Cornelia Zelinsky-Wibbelt, edi-
tor, The Semantics of Prepositions: From Mental Process-
ing to Natural Language Processing, Mouton de Gruyter,
New York, pages 99–150.
</p>
<p>Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Meredith
Green, Abhijit Suresh, Kathryn Conger, Tim O’Gorman,
and Martha Palmer. 2016. A corpus of preposition super-
senses. In Proc. of LAW X – the 10th Linguistic Annotation
Workshop. Berlin, Germany, pages 99–109.
</p>
<p>Nathan Schneider, Behrang Mohit, Kemal Oflazer, and
Noah A. Smith. 2012. Coarse lexical semantic annota-
tion with supersenses: an Arabic case study. In Proc. of
ACL. Jeju Island, Korea, pages 253–258.
</p>
<p>Nathan Schneider, Vivek Srikumar, Jena D. Hwang, and
Martha Palmer. 2015. A hierarchy with, of, and for preposi-
tion supersenses. In Proc. of The 9th Linguistic Annotation
Workshop. Denver, Colorado, USA, pages 112–123.
</p>
<p>Vivek Srikumar and Dan Roth. 2013a. An inventory of
preposition relations. Technical Report arXiv:1305.5785.
http://arxiv.org/abs/1305.5785.
</p>
<p>Vivek Srikumar and Dan Roth. 2013b. Modeling semantic
relations expressed by prepositions. Transactions of the
Association for Computational Linguistics 1:231–242.
</p>
<p>Leonard Talmy. 1996. Fictive motion in language and “cep-
tion”. In Paul Bloom, Mary A. Peterson, Nadel Lynn, and
Merrill F. Garrett, editors, Language and Space, MIT Press,
Cambridge, MA, pages 211–276.
</p>
<p>Stephen Tratz and Dirk Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features. In Proc.
of NAACL-HLT Student Research Workshop and Doctoral
Consortium. Boulder, Colorado, pages 96–100.
</p>
<p>Andrea Tyler and Vyvyan Evans. 2001. Reconsidering prepo-
sitional polysemy networks: the case of ‘over’. Language
77(4):724–765.
</p>
<p>Andrea Tyler and Vyvyan Evans. 2003. The Semantics of
English Prepositions: Spatial Scenes, Embodied Meaning
and Cognition. Cambridge University Press, Cambridge,
UK.
</p>
<p>Henk Verkuyl and Joost Zwarts. 1992. Time and space in
conceptual and logical semantics: the notion of Path. Lin-
guistics 30(3):483–512.
</p>
<p>Bernhard Wälchli. 2010. Similarity semantics and building
probabilistic semantic maps from parallel texts. Linguistic
Discovery 8(1):331–371.
</p>
<p>Patrick Ye and Timothy Baldwin. 2007. MELB-YB: Preposi-
tion sense disambiguation using rich semantic features. In
Proc. of SemEval. Prague, Czech Republic, pages 241–244.
</p>
<p>188</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 189–198,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Issues of Mass and Count: Dealing with ‘Dual-Life’ Nouns
</p>
<p>Tibor Kiss
Ruhr-Universität Bochum
</p>
<p>44801 Bochum, Germany
</p>
<p>tibor@linguistics.rub.de
</p>
<p>Francis Jeffry Pelletier
University of Alberta
</p>
<p>Edmonton, Canada
</p>
<p>francisp@ualberta.ca
</p>
<p>Halima Husić
Ruhr-Universität Bochum
</p>
<p>44801 Bochum, Germany
</p>
<p>husic@linguistics.rub.de
</p>
<p>Johanna Poppek
Ruhr-Universität Bochum
</p>
<p>44801 Bochum, Germany
</p>
<p>poppek@linguistics.rub.de
</p>
<p>Abstract
The topics of +MASS and +COUNT have been
studied for many decades in philosophy
(e.g., (Quine, 1960; Pelletier, 1975)), lin-
guistics (e.g., (McCawley, 1975; Allan,
1980; Krifka, 1991)) and psychology (e.g.,
(Middleton et al., 2004; Barner et al.,
2009). More recently, interest from within
computational linguistics has studied the
issues involved (e.g., (Pustejovsky, 1991;
Bond, 2005; Schmidtke and Kuperman,
2016)), to name just a few. As is pointed
out in these works, there are many difficult
conceptual issues involved in the study of
this contrast. In this article we study one
of these issues – the “Dual-Life” of be-
ing simultaneously +MASS and +COUNT – by
means of an unusual combination of hu-
man annotation, online lexical resources,
and online corpora.
</p>
<p>1 Background
</p>
<p>The standard story of +MASS and +COUNT usually
starts with some examples of nouns of both sorts.
</p>
<p>• +COUNT: car, dog, idea, university, belief, . . .
• +MASS: water, garbage, advice, oil, admira-
</p>
<p>tion, knowledge. . .
</p>
<p>These examples are usually accompanied by some
syntactic tests for +MASS and +COUNT:
</p>
<p>• Count terms can be pluralized, occur with in-
definite determiner, allow numeral modifiers,
occur with the quantifiers each, every, . . . .
Mass terms can’t do any of these.
</p>
<p>• Always occurring in the singular, mass terms
occur with “measure” terms (e.g., much),
with the quantifiers most, all and the un-
stressed some, and bare in (e.g.) subject po-
sition with singular verb agreement. Singular
count terms can’t do any of these.
</p>
<p>These lead to such comparisons as
</p>
<p>(1) a. a car, four ideas, each university
b. *a water, *four garbage, *each admiration
c. all garbage, most water, advice is helpful
d. *all dog, *most university, *car is fast
</p>
<p>It can be seen from this brief (and partial) de-
scription of the +MASS/+COUNT distinction that the
presumption is that the distinction applies to lexi-
cal nouns and that it is exhaustive and exclusive –
every such noun is either +COUNT or +MASS, and no
noun is both.
</p>
<p>Our account will deny all these presumptions,
and in doing so will open what we find to be a
much more plausible account of various myste-
rious phenomena surrounding +MASS and +COUNT
(although in this study we just examine the “dual-
life” case).
</p>
<p>2 Some Background Methodological
Issues
</p>
<p>A fundamental feature of (and, we think, a prob-
lem with) with the “usual story” is that it presumes
that the locus or home of +MASS and +COUNT is the
lexical noun. This is a feature of modern analyses
as well as the older ones despite the fact that even
in the oldest of the works we find remarks cau-
tioning against this, such as (Quine, 1960)’s am-
biguous Mary put a little chicken into the salad
and cautionary remarks by others that such sen-
tences as We had crocodile for supper last night!
are completely normal. And to the claim that this
“changes the sense” of chicken from +COUNT to
+MASS, (Pelletier, 1975, p.456) remarked “Such
a claim makes clear that either (1) surface struc-
ture is not what the criteria are talking about or
(2) we need to distinguish not between mass and
count nouns but between mass and count senses of
nouns.” This sort of remark can be found through-
out the literature on +MASS/+COUNT; however, dic-
</p>
<p>189</p>
<p />
</div>
<div class="page"><p />
<p>tionaries in fact do not usually make this distinc-
tion. The only relevant sense of crocodile in Word-
Net (Miller, 1995; Miller and Fellbaum, 2007) and
Webster’s New Collegiate Dictionary is “large vo-
racious aquatic reptile having a long snout with
massive jaws and sharp teeth and a body covered
with bony plates; of sluggish tropical waters”. In
fact, with only a few exceptions, the relationship
between an animal and its flesh used as food (when
it is the same noun for each) does not generate new
senses in dictionaries (nor WordNet). Somehow,
both of the alleged meanings are contained in the
same sense.
</p>
<p>One might postulate (and many theorists have,
e.g., (Bunt, 1985; Payne and Huddleston, 2002))
that there are some background “rules” that can
apply to a basic meaning of a noun and which will
generate the related sense that describes the oppo-
site value of the +MASS/+COUNT dimension. Sug-
gestions include rules for grinding (armadillo all
over the road), portioning (Order me a beer), sort-
ing (eight beers on tap), evaluating (too much car
for the average driver) and others. However, it
has seemed clear to most theorists that this sort of
strategy wouldn’t be able to account for all the var-
ied ways that +MASS and +COUNT senses of a given
noun might be related. Furthermore, for many so-
called abstract nouns, it is not even clear what it is
that makes a meaning be +MASS or +COUNT.
</p>
<p>Another problem is that researchers looking
into the issues involved in +MASS/+COUNT tend to
use their own intuitions, based on a very limited
number of data points (that is, a very limited group
of words and their meanings). The use of large-
scale resources is relatively rare (although see
(Baldwin and Bond, 2003; Grimm, 2014; Katz and
Zamparelli, 2012; Kulkarni et al., 2013)). And the
use of dictionary resources is also rare, meaning
that researchers rely on their own “intuitions into
meaning” when it comes to issues of +COUNT and
+MASS. Although many researchers have pointed
to these sorts of limitations, none have actually
investigated the actual senses of nouns as they
appear in large resources (e.g., dictionaries), nor
have they investigated what it means for a specific
sense to be “dual-life” – i.e., to be both +MASS and
+COUNT.
</p>
<p>For these sorts of reasons, we have decided to
investigate the possibility that the locus of +MASS
and +COUNT should be a given sense of a noun.
But a consequence of this will be that some senses
</p>
<p>are both +COUNT and +MASS, as for instance, our
examples beer and crocodile. Even if we wish
to retain the “semantic conversion/coercion” rules
mentioned above and thus excuse these from be-
ing both +MASS and +COUNT, there are many others
(as we will show below) that do not lend them-
selves to such coercions. So one of our goals is
to display many individual senses of nouns (we
call these “noun-senses”) that are both +MASS and
+COUNT;. The usual name for a theory that allows
something to be both +MASS and +COUNT is “dual-
life” – “dual-life nouns” if the locus is the noun,
but “dual-life noun-senses” for our viewpoint.
</p>
<p>3 What We Did
</p>
<p>We used the American National Corpus (ANC:
(Ide and Suderman, 2004; Ide, 2008)), parsed
with the Stanford NLP Group (http://nlp.
stanford.edu) parser (Chen and Manning,
2014). We then intersected the ANC’s set of
nouns with those in WordNet to form a reposi-
tory of nouns for which there were definitions (in
WordNet).1 We employed four graduate linguis-
tics students to (independently) evaluate the ex-
tent to which each of the WordNet senses thus
chosen could be used in certain contexts (we call
these tests the ‘syntactic tests’) and whether, given
the answers to some of the syntactic tests, cer-
tain implications follow from their use (we call
these the ‘semantic tests’). There are six of these
tests in all, four syntactic and two semantic, cho-
sen for their relevance to various of the issues that
are salient in the studies of +MASS and +COUNT
terms. Table 1 gives the bare-bones outline of
the annotators’ tasks, which ask whether the an-
notators can construct sentences obeying the syn-
tactic patterns specified, while maintaining the
NOUN’s meaning to be the one under investiga-
tion. Table 2 shows some noun-senses and how
they fare with the tests. We discovered that there
was very significant inter-annotator agreement in
these answers. We have approximately 13,000
annotated and agreed-upon noun-senses. Inter-
annotator agreement, as measured by Krippen-
dorff’s α, = 0.755, which (Artstein and Poesio,
2008, pp. 576, 591) define as highly reliable.
</p>
<p>1We employed the ANC as a natural corpus because we
also wished to investigate the actual senses in use. Our an-
notators characterized the MASS/COUNT feature of the various
senses, but we wished then to see which of these senses were
actually used in what contexts. That aspect of our research is
not reported in the present paper.
</p>
<p>190</p>
<p />
</div>
<div class="page"><p />
<p>Syn 1: Can the noun-sense pair in its singular form appear together with more?
Sem 1: If Syn 1 = yes, is the comparison based on number of entities, or another mode of
</p>
<p>measurement?
Syn 2: Can the noun-sense pair in its plural form appear together with more?
Sem 2: If Syn 2 = yes, is the sentence equivalent to a sentence with an explicit classifier?
Syn 3: Can the noun-sense pair in its singular form and combined with an indefinite determiner
</p>
<p>be the syntactic subject of a definition or characterization?
Syn 4: Can the noun-sense pair in its singular form but without a determiner be the syntactic
</p>
<p>subject of a definition or characterization?
</p>
<p>Table 1: Four syntactic tests and two semantic tests annotators answered for each noun-sense
</p>
<p>Noun WordNet description Syn 1 Sem 1 Syn 2 Sem 2 Syn 3 Syn 4
car#1 a motor vehicle with four no na yes ¬ equiv. yes no
</p>
<p>wheels
fruitcake#1 a whimsically eccentric person no na yes ¬ equiv. yes no
fruitcake#2 a rich cake containing dried yes ¬ num. yes ¬ equiv. yes yes
</p>
<p>fruit and nuts [. . . ]
lingerie#1 women’s underwear and yes num. na na no yes
</p>
<p>nightclothes
whiskey#1 a liquor made from fermented yes ¬ num. yes equiv. no yes
</p>
<p>mash of grain
</p>
<p>Table 2: Examples of Test Outcomes.
</p>
<p>Further details on the texts, the various senses
and the annotation process can be found in (Kiss
et al., 2014). Some other aspects of the general
research effort are in (Kiss et al., 2016).
</p>
<p>Each noun-sense thus gets some unique pat-
tern of answers, which we can represent as an or-
dered six-tuple of answers (we use the ordering
given in Table 1). We extracted and processed the
annotators’ responses using R (https:\cran.
r-project.org), allowing us not only to pro-
cess the resulting answers given by our annotators,
but also to aid in the inner-annotator agreement
evaluation. A side effect of using R numerical
names to the groups of senses that have the same
six-tuple of answers. In this study we focus on one
of these groups (“Classes”); R gave it the name
“726”, which we have kept (even though “dual-
life” might have been a more informative choice
of name).
</p>
<p>There are three possible answers for each of
the six Tests: yes, no, and not applicable (but
we sometimes use ‘num’ and ‘eq’, together with
negations), hence 729 possible classes. But the
questions are not independent of one another, and
in fact there are only 80 independently possible
classes. Our annotators found there to be 18 ac-
</p>
<p>tual classes to be populated with noun-senses, out
of these 80 possible classes.
</p>
<p>4 “Dual Life” senses
</p>
<p>Remembering now that we are describing senses
of nouns, as identified by WordNet, what sort of
noun-senses manifest this duality of being both
+MASS and +COUNT? Or put more accurately, since
we test the senses by determining answers to ques-
tions that have that sense used in a full noun
phrase, what sort of noun-senses manifest the pos-
sibility of occurring in both +MASS NPs and also in
+COUNT NPs?
</p>
<p>Our main group of such senses is called
Class 726 by R. This group has the profile
&lt;yes,¬num,yes,¬equiv,yes,yes&gt;. That is,
</p>
<p>(2) a. This noun-sense can be used in the sin-
gular with more. &lt;e.g., John has more X
than Mary&gt;
</p>
<p>b. This amount of X is not based on in-
stances of X.
</p>
<p>c. It can be used in the plural &lt;e.g., John
has more Xs than Mary&gt;
</p>
<p>d. The Xs are not equivalent to any clas-
sifier + X &lt;e.g., not equivalent to more
</p>
<p>191</p>
<p />
</div>
<div class="page"><p />
<p>cups (or kinds) of X&gt;
e. It can be used with an indefinite singu-
</p>
<p>lar determiner definition &lt;e.g., An X is a
(some definition)&gt;
</p>
<p>f. It can be used in the singular without a
determiner definition &lt;e.g., X is (some
definition)&gt;
</p>
<p>Class 726 contains 162 different senses of
nouns. There are two different broad categories
that we can distinguish within this group of dual-
life noun-senses. The two basic types are:
</p>
<p>1. Senses of Nominally-Oriented nouns
</p>
<p>2. Senses of Verbally-Oriented nouns
</p>
<p>There are 57 senses of Type 1 and 96 senses of
Type 2. There are thus 8 other senses. 3 of which
seem to fit into both categories equally, and 5 that
do not seem to be of either Type.
</p>
<p>4.1 Nominally-Oriented Noun Senses
</p>
<p>We start with the Type 1 senses. Unlike the nouns
that give rise to the Type 2 senses, the nouns be-
hind these senses are not formed from other parts
of speech: they are either simple nouns on their
own or else nouns compounded from nouns and
possibly other (non-verb) parts of speech.
</p>
<p>Some of the Nominally-Oriented Noun senses
are in groups that have been discussed in the liter-
ature before. One Nominally-Oriented noun type
that has long received play in the mass-count lit-
erature is that associated with food.2 For ani-
mal noun-senses other than pig, cow, e.g., alliga-
tor, the animal-designating “meaning” is +COUNT
while the flesh-designating “meaning” is +MASS. A
special version of this occurs when the particular
amount or type of the meat is typically cooked or
served as a unit, then such a unit gets called by a
special +COUNT term and that special term acquires
a +COUNT “meaning” for the meat that comprises it:
We had a steak for dinner/We had steak for dinner;
Mary cooked a ham for Easter/Mary served ham
for supper; George bought a large roast/George
</p>
<p>2As remarked above, dictionaries and other lexical
sources (in particular, WordNet) take these differing ways to
interpret a word like alligator to be parts of one and the same
sense. Yet in writing about them, we wish to be able to dis-
cuss the differences that are internal to a sense. So, we have
decided to use the term “meaning” (with double quote marks)
when we wish to discuss the different interpretations that can
be given to a single sense (or alternatively put, the different
ways that a single sense can be used).
</p>
<p>had leftover roast for a week. We give one exam-
ple of a noun-sense that is in this group, and then
list the nouns for the other 8 senses of the members
of this subgroup.
</p>
<p>(3) a. fruitcake#2: a rich cake containing dried
fruit and nuts and citrus peel [. . . ]
</p>
<p>b. cake#3, casserole#1, ham#1, marsh-
mallow#1, melon#2, pizza#1, salad#1,
steak#1
</p>
<p>(Rothstein, 2010) brought attention to a class
of dual-life nouns that we may call the “fence-
nouns”. She was motivated by considerations like
this:
</p>
<p>(4) a. Hans Müller’s ranch has more fence than
Alexis Sánchez’s granja.
</p>
<p>b. A fence can be cut in half and part of it
moved, and then there are two fences.
</p>
<p>As can be seen from (4-b), Rothstein’s dual-life
evidence is semantic in nature, as well as the syn-
tactic (4-a), although even in (4-b) we see the syn-
tactic point that ‘fence’ can be used with an indefi-
nite determiner. Rothstein makes a similar case for
other nouns, such as wire. And though she does
not remark on the fact that she is testing nouns,
as opposed to noun senses, one could plausibly ar-
gue that she in fact has kept the sense of ‘fence’
(and ‘wire’) constant in these examples, and that is
borne out by our annotators, who were just look-
ing at individual senses. Class 726 is the home of
a number of such noun senses.
</p>
<p>(5) a. cable#2: a conductor for transmitting
electrical or optical signals or electric
power
</p>
<p>b. cable#3, cord#1, ribbon#4, rope#1,
thread#1, wire#1
</p>
<p>Another identifiable subgroup of Class 726 is
what we call the -sides dual life group. This group
does not seem to have been identified in the ear-
lier literature. Our annotators have determined that
some sentence such as (6-a) is grammatical (and
not due to counting the number of distinct seasides
in the two locations), and as well, it is obvious that
sentences like (6-b) are grammatical. And yet this
is the same sense of seaside.
</p>
<p>(6) a. California has more seaside than Oregon.
b. We spent our vacation on a seaside in
</p>
<p>southern England.
</p>
<p>192</p>
<p />
</div>
<div class="page"><p />
<p>Our data included only 4 such senses:
</p>
<p>(7) a. seaside#1: the shore of a sea or ocean
regarded as a resort
</p>
<p>b. hillside#1, riverside#1, roadside#1
</p>
<p>A final small group of the nominally-oriented
dual-life senses we call the -land senses:
</p>
<p>(8) a. marshland#1: low-lying wet land with
grassy vegetation; usually is a transition
zone between land and water
</p>
<p>b. forest#2, marsh#1, rainforest#1,
swamp#1, wetland#1
</p>
<p>There are many other Nominally-Oriented dual-
life senses in Class 726 which do not manifest any
of the preceding four types of meaning. The most
common of the remaining ones are what we call
“Kind-Instance” (or “Type-Token”) in nature, usu-
ally where the kind-“meaning” is mass while the
instance-“meaning” is count. There are 31 such
senses, a few of which are:
</p>
<p>(9) a. drought#1: A shortage of rain-
fall.“Farmers most affected by the
drought hope that there may yet be
sufficient rain early in the growing
season”3
</p>
<p>b. mockery#3: humorous or satirical
mimicry.
</p>
<p>c. brunch#1: combination breakfast and
lunch; usually served in late morning
</p>
<p>d. anticoagulant#1: medicine that prevents
or retards the clotting of blood
</p>
<p>In (9-a) we have a general term, ‘drought’,
which has many instances such as exemplified
in A drought has bedevilled California since
1999. So this seems a straightforward example
of our Nominally-Oriented Kind-Instance dual-
life senses. Things are maybe a little less obvious
with (9-b), but it seems plausible to claim that it
is designating a kind or sort of linguistic activity,
and each particular case of a humorous or satiri-
cal mimicry is a mockery. Here we are not saying
that the particular cases are the result of the activ-
ity, but rather that they exemplify the kind, mock-
ery. With (9-c) one might wonder why it was not
classified as an example of the Food subtype. In
our opinion this is because brunch is not the “con-
</p>
<p>3The only other sense of drought in WordNet concerns
any prolonged shortage: “When England defeated Pakistan it
ended a ten-year drought”.
</p>
<p>tainer” that has the mass-stuff as its makeup, in
the way that a chicken is a container for chicken(-
meat). Instead, brunch is the name for a kind of
activity, and each one of its manifestations is a
brunch.
</p>
<p>As we noted above, most senses here make
the general “meaning” be mass, and the more
individual-denoting “meaning” be count. But
there are exceptions to this. Anticoagulant#1, for
example, seems to be a mass sense for the stuff
that is put into one’s body to retard blood clotting,
that is, this “meaning” picks out the physical man-
ifestation of the kind term (akin to the individual-
denoting “meaning” of the more usual terms). But
this makes this manifestation “meaning” be mass,
and the count sense seems to be a “sorting” mean-
ing, that is, the kind “meaning” is count. Thus, ‘an
anticoagulant’ names a kind or sort of stuff, rather
like ‘a beer’ can name a type/kind/sort of beer. So
‘anticoagulant’ without an indefinite article names
(something like) the stuff that is, in some particu-
lar case, doing the work of anti-coagulating.
</p>
<p>4.2 Verbally-Oriented Noun Senses
</p>
<p>There is a long-standing tradition claiming that
“event nouns”4 are ambiguous between a “mean-
ing” that describes an activity, action, event, or
process, on the one hand, and a “meaning” that de-
scribes the result of that activity, action, event, or
process, on the other hand. Some clear examples
of this in the literature are:
</p>
<p>(10) a. collection: the activity of gathering to-
gether a group of items vs. the group
that is thus gathered.
</p>
<p>b. invention: the process of generating
some new type of thing vs. the actual
kind of thing that has been generated.
</p>
<p>Although some linguists would say that the fact
that one “meaning” is predictable from the other
“meaning”, and so they shouldn’t both be entered
in the lexicon (e.g., (Payne and Huddleston, 2002,
p. 337)), sometimes these two “meanings” are dis-
tinguished as separate senses in WordNet (and in
dictionaries more generally), but sometimes not.
For example, in WordNet we find
</p>
<p>4We use ‘event’ to name this group, even though there
are nouns that don’t seem to be derived from event verbs nor
do they intuitively designate the occurrence of some event.
For example, some are actions, activities, achievements, pro-
cesses, etc..
</p>
<p>193</p>
<p />
</div>
<div class="page"><p />
<p>(11) a. collection#1: several things grouped to-
gether or considered as a whole
</p>
<p>b. collection#2: a publication containing a
variety of works
</p>
<p>c. collection#3: a request for a sum of
money
</p>
<p>d. collection#4: the act of gathering some-
thing together
</p>
<p>Here we see the two “meanings” separated as dif-
ferent senses (senses #1 vs. #4). On the other hand,
sometimes the two “meanings” are merged into
the same sense. WordNet gives
</p>
<p>(12) a. burglary#1: entering a building unlaw-
fully with intent to commit a felony or
to steal valuable property5
</p>
<p>b. emission#1: the act of emitting; caus-
ing to flow forth6
</p>
<p>c. amplification#1: addition of extra ma-
terial or illustration or clarifying detail7
</p>
<p>Here we see that burglary#1 describes both the
activity of burgling and also the result of the ac-
tivity (a burglary). emission#1 describes both the
event of causing something to flow, and also the
result of that event (an emission). Similarly, am-
plification#1 describes both the activity of adding
extra material and the result of doing so. The for-
mer “meanings” are mass(-like) while the latter
are count(-like), as the examples in (13)–(15) sug-
gest.
</p>
<p>(13) a. Burglary is not a difficult activity to
carry out.
</p>
<p>b. The Müllers’ house suffered a burglary
last night.
</p>
<p>(14) a. Methane emission in coal mines is a se-
rious health issue.
</p>
<p>b. The cause of the miners’ deaths was de-
termined to be an emission of methane.
</p>
<p>(15) a. The Opposition demanded amplifica-
tion of the Prime Minister’s remarks.
</p>
<p>b. The Ministers for Foreign Affairs and of
</p>
<p>5This is the only sense identified in WordNet.
6Other senses of emission focus on the kind of material
</p>
<p>that is released by an emission; one sense even picks out a
specific subtype of that sort of emission, namely that of water
from a pipe.
</p>
<p>7The other two senses of ‘amplification’ indicate rather
different features: amplification#2: ‘the amount of increase
in signal power or voltage or current expressed as the ratio
of output to input’; amplification#3: ‘(electronics) the act of
increasing voltage or power or current’.
</p>
<p>Defense each provided an amplification
of the Prime Minister’s remarks.
</p>
<p>There is another relation that is, in a way,
“between” these the categories of Nominally-
Oriented Kind-Instance and Verbally-Oriented
Event-Result. It happens when an event-noun (or
state- or process-noun) designates a general term
for a kind or type that has instances or tokens
called by the same name. Note that these are in-
stances or tokens of the kind, and not the result of
the event. In such cases, the event/state/process
“meaning” is usually or naturally seen as mass,
while the instance “meaning” seems usually or
naturally to be count. For example,
</p>
<p>(16) a. fantasy#1: imagination unrestricted by
reality; “a schoolgirl fantasy”
</p>
<p>b. litigation#1: a legal proceeding in a
court; a judicial contest to determine
and enforce legal rights
</p>
<p>c. silence#1: the state of being silent (as
when no one is speaking); “there was
a shocked silence”; “he gestured for si-
lence”
</p>
<p>These cases seem to suggest that we have a general
term (‘fantasy’, ‘litigation’, ‘silence’) denoting a
type or kind, which in turn has many instances.
The instances are thus not effects of these kinds.
For example, “a schoolgirl fantasy”, “a legal pro-
ceeding”, “a shocked silence” are all instances
of their respective kinds, but not effects of them.
Nonetheless, it seems clear that these all display
the fact that the ultimate source of these noun-
senses is a verb: fantasize, litigate, silence, whose
WordNet senses are: “to portray in the mind”, “to
engage in legal proceedings”, “to cause to be quiet
or not talk”. So such senses seem best classified
as Verbally-Oriented Kind-Instance senses, and as
we said, should be seen as forming a sort of mid-
dle ground between the Nominally-Oriented Kind-
Instance senses and the Verbally-Oriented Event-
Act senses. And so we call these senses Verbally-
Oriented Kind-Instance dual-life.
</p>
<p>But in many cases it is difficult to determine
whether we have a case of the Verbally-Oriented
Act–Result relation or of the Verbally-Oriented
Kind–Instance relationship. For instance, with
each of the senses identified in (17), it seems that
there is no good reason to choose between viewing
the relationship as an Event-Result or as a Kind-
</p>
<p>194</p>
<p />
</div>
<div class="page"><p />
<p>Instance:
</p>
<p>(17) a. eccentricity#1: strange and unconven-
tional behavior
</p>
<p>b. idealization#2: (psychiatry) a defence
mechanism that splits something you
are ambivalent about into two represen-
tations – one good and one bad
</p>
<p>c. imperfection#1: the state or an in-
stance of being imperfect
</p>
<p>Is eccentricity#1, for instance, the name for a pro-
cess, activity, or force, etc., that brings about an
eccentricity as a result? Or is it instead the name of
a kind or type of force (etc.) which has instances
that are called eccentricities? What about ideal-
ization#2? Or imperfection#1? In the latter case
it is explicitly defined as “either a state or an in-
stance” and so even the definition explicitly leaves
room for either interpretation. There seems to be
no good reason to view any of these (and others) in
one way or the other. About all that can be said is
that these are Verbally-Oriented, but we can’t fur-
ther determine which subtype they manifest. Or
maybe better put: they in fact do manifest both
types equally.
</p>
<p>Examples such as these make one want to go
back to the earlier examples of Event-Result and
Kind-Instance and reanalyze them also, making it
become easier to see them too as perhaps exem-
plifying both ways in which a single sense can be
simultaneously mass and count. In fact, we are
tempted to say that there is some sort of “con-
tinuum” or continuity between the two ways – in
the same way that one can order wavelengths of
light so as to display a continuum between blue
and green.
</p>
<p>The largest subgroup in the 726 Dual-Life Class
is the Verbally-Oriented Event-Result senses. (Al-
though keep in mind that many of these also man-
ifest at least a degree of Verbally-Oriented Kind-
Instance “meaning”.) There are 70 such senses in
this Class; 55 of them are senses of -tion nominal-
izations from activity verbs, 2 are -ment nominal-
ization, 3 are -ing nominalizations, and there are
15 others. A few examples of each of these types
senses are in (18-a)–(18-d) .
</p>
<p>(18) a. acclimation#1: adaptation to a new
climate (a new temperature or altitude
or environment); deception#1: the act
of deceiving; insertion#2: the act of
</p>
<p>putting one thing inside another; elim-
ination#4: the act of removing an un-
known mathematical quantity by com-
bining equations;
</p>
<p>b. embellishment#1: elaboration of an in-
terpretation by the use of decorative
(sometimes fictitious) detail; infringe-
ment#1: an act that disregards an agree-
ment or a right.
</p>
<p>c. borrowing#1: the appropriation (of
ideas or words etc) from another source;
ending#2: the act of ending something.
</p>
<p>d. analysis#2: the abstract separation of
a whole into its constituent parts in or-
der to study the parts and their relations;
burglary#1: entering a building unlaw-
fully with intent to commit a felony or
to steal valuable property; dispersal#1:
the act of dispersing or diffusing some-
thing; influx#1; revival#1; war#1
</p>
<p>A group that is somewhat smaller than the just-
mentioned Verbally-Oriented Event-Result senses
is that of Verbally-Oriented Kind-Instance senses,
with 18 members. Unlike the Verbally-Oriented
Event-Result senses where one “meaning” is a
name for the kind of activity and the other “mean-
ing” is a name for a result of that activity, here
we have a “meaning” as a name for the kind
of activity and the other “meaning” is a name
for tokens or instances of that activity, rather
than a result of that activity. But unlike the
Nominally-Oriented Kind-Instance noun-senses,
these Verbally-Oriented senses clearly rely on a
sense of a verb and not derived from a noun-sense.
10 of these noun-senses are -tion nominalizations,
one formation is from each of -ing, -ship, -ment,
while five are otherwise derived. A handful of the -
tion-formations are in (19-a), the -ing, -ship, -ment
formations are in (19-b), while the others are in
(19-c).
</p>
<p>(19) a. elaboration#3: a discussion that pro-
vides additional information; intona-
tion#1: rise and fall of the voice pitch;
recrimination#1: mutual accusations;
</p>
<p>b. looting#1: plundering during riots or
in wartime; displacement#4: (chem-
istry) a reaction in which an elemen-
tary substance displaces and sets free a
constituent element from a compound;
friendship#1: the state of being friends
</p>
<p>195</p>
<p />
</div>
<div class="page"><p />
<p>(or friendly)
c. curvature#1: (medicine) a curving
</p>
<p>or bending; often abnormal; fan-
tasy#1: imagination unrestricted by re-
ality; genocide#1: systematic killing of
a racial or cultural group; silence#1: the
state of being silent (as when no one is
speaking); tribute#1: something given
or done as an expression of esteem
</p>
<p>Here that there is always a verb-oriented situation,
where the activity it describes gives rise to a mass
general name for that activity, and the results are
described by a count “meaning” of the same name.
</p>
<p>4.3 Borderline Cases
</p>
<p>Our rationales for the distinction between Kind-
Instance noun-senses and Event-Result noun-
senses (when they are both Verbally-Oriented) is
this:
</p>
<p>(20) If the event’s happening suggests a
cause for the result, then it is a case of
Event-Result “meaning”.
</p>
<p>(21) When the event seems not to play any
role in the formation, causation, occur-
rence, or existence of the object in ques-
tion, then it is a Kind-Instance “mean-
ing”.
</p>
<p>We think that these two explanations can merge
into one another. E.g., in (22) it seems that the
event is causing an instance to occur, and so we
classify it in the Event-Result group, even though
one can also see that the so-called result maybe is
just an instance of the kind indicated by the event.
</p>
<p>(22) a. insertion#2: the act of putting one thing
into another
</p>
<p>b. encryption#1: the activity of convert-
ing data or information into code
</p>
<p>c. re-creation#1: the act of creating
again.
</p>
<p>d. amelioration#1: the act of relieving ills
and changing for the better
</p>
<p>Although the examples in (22) are most naturally
seen as cases where an event causes some result,
they could also be seen the other way. So, it
seems natural to say that putting one thing into an-
other causes there to be some result – an insertion.
The activity of converting data to code (encryp-
tion) brings about an encryption of the data. But
</p>
<p>on the other hand, one might say that the abstract
kind (or type), insertion, has various specific phys-
ical manifestations – various instances or tokens of
that type. As we said, we think the former is more
natural here and in the other members of (22), but
it also seems that the latter understanding is cer-
tainly possible.
</p>
<p>But when there was no salient particular causa-
tion involved, and it was merely a matter of some
abstract kind (or concept) which is then said to be
instantiated in a particular situation, we labelled it
as Verbally-Oriented Kind-Instance, as in (23):
</p>
<p>(23) a. elaboration#3: a discussion that pro-
vides additional information
</p>
<p>b. fantasy#1: imagination unrestricted by
reality; “a schoolgirl fantasy”
</p>
<p>c. retraction#1: a disavowal or taking
back of a previous assertion
</p>
<p>d. genocide#1: systematic killing of a
racial or cultural group
</p>
<p>Here it seems more natural to think that the noun
is describing some (abstract) kind or type, and the
count interpretation is a manifestation of that type.
In this way, ‘elaboration’ seems to us to describe
a type of speech act, and its manifestations or in-
stantiations will be this or that elaboration. (Of
course, one might also say that an act of elabora-
tion is an action which will result in some specific
elaboration, which makes the Event-Result read-
ing become more prominent.)
</p>
<p>However, there seem to be various noun
senses in Class 726 that are Verbally-Oriented,
but for which we find it impossible to decide
whether they are more clearly Event-Result or
more clearly Kind-Instance. Probably the best
thing to say about them is that they are both
Verbally-Oriented Event-Result and Verbally-
Oriented Kind-Instance to the same extent. Here
are two representatives:
</p>
<p>(24) a. defection#1: withdrawing support or
help despite allegiance or responsibility
</p>
<p>b. eccentricity#1: strange and unconven-
tional behavior
</p>
<p>Finally, we see a very few noun-senses that seem
to be equally Verbally- and Nominally-Oriented:
</p>
<p>(25) a. curve#1: the trace of a point whose di-
rection of motion changes
</p>
<p>b. poop#1: obscene terms for feces
</p>
<p>196</p>
<p />
</div>
<div class="page"><p />
<p>c. regret#1: sadness associated with some
wrong done or some disappointment
</p>
<p>5 Concluding Remarks
</p>
<p>We have offered some theoretical considerations
for favouring an analysis of (dictionary-defined)
senses of nouns, rather than the nouns themselves,
as the locus for explaining why a NP is +MASS or
+COUNT. We have also offered empirical evidence
in the form of a large repository of carefully an-
notated noun-senses. These annotated senses can
be analyzed to determine which individual ones of
them can be used only in +COUNT NPs, or only in
+MASS NPs, or in both +COUNT and +MASS NPs, or
are not usable in either +MASS or +COUNT NPs. This
paper in particular discussed a class of senses of
the third of these varieties: “Dual-life” senses –
those individual meanings that can be used in both
+MASS and +COUNT NPs.
</p>
<p>We view the current undertaking as a necessary
step in providing a complete semantic analysis of
+MASS and +COUNT NPs. Such an account requires
both the underlying meanings of the component
nouns, and also the semantic effect of the syntac-
tic method of forming the NP (that is, the giving
the meaning of the NP) from the noun’s meaning.
However, without a detailed account of the wide
range of senses of the component nouns, it will be
impossible to give the desired group of semantic
rules. And without that, there would be no hope
for a compositional account of these phenomena.
</p>
<p>We encourage other researchers to investigate
the resources available with the Bochum En-
glish Countability Lexicon (BECL). The BECL
2.1 database is publicly available at http://
count-and-mass.org.
</p>
<p>Acknowledgments
</p>
<p>We gratefully acknowledge the Alexander von
Humboldt Foundation for an Anneliese-Maier
prize and grant to Pelletier, and the Deutsche
Forschungsgemeinschaft (KI-759/5) grant to Kiss,
for their support of the work reported here and our
other reports.
</p>
<p>References
Allan, K. (1980). Nouns and countability. Lan-
</p>
<p>guage 56, 541–567.
</p>
<p>Artstein, R. and M. Poesio (2008). Inter-coder agree-
ment for computational linguistics. Computational
Linguistics 34, 555–596. http://aclweb.
org/anthology/J08-4004.
</p>
<p>Baldwin, T. and F. Bond (2003). Learning the count-
ability of English nouns from corpus data. In Proc.
of the 41st Annual Meeting of the Association for
Computational Linguistics. http://aclweb.
org/anthology/PS03-1059.
</p>
<p>Barner, D., S. Inagaki, and P. Li (2009). Language,
thought, and real nouns. Cognition 11, 329–344.
</p>
<p>Bond, F. (2005). Translating the Untranslatable: A
Solution to the Problem of Generating English De-
terminers. Stanford: CSLI Press.
</p>
<p>Bunt, H. (1985). Mass Terms and Model Theoretic Se-
mantics. Cambridge: Cambridge UP.
</p>
<p>Chen, D. and C. Manning (2014). A fast and accurate
dependency parser using neural networks. In Pro-
ceedings of EMNLP 2014. doi: 10.3115/v1/D14-
1082.
</p>
<p>Grimm, S. (2014). Individuating the Abstract. In
U. Etxeberria, A. Fălăuş, A. Irurtzun, and B. Lefer-
man (Eds.), Proceedings of Sinn und Bedeutung 18,
Bayonne and Vitoria-Gasteiz, pp. 182–200.
</p>
<p>Ide, N. (2008). The Amercian National Corpus: Then,
now, and tomorrow. In M. Haugh, K. Burridge,
J. Mulder, and P. Peters (Eds.), Selected Proceed-
ings of the 2008 HCSNet Workshop on Designing the
Australian National Corpus: Mustering Languages,
Summerville, MA. Cascadilla Proceedings Project.
</p>
<p>Ide, N. and K. Suderman (2004). The Amercian Na-
tional Corpus first release. In Proceedings of the
Fourth Language Resources and Evaluation Con-
ference (LREC), Lisbon, pp. 1681–1684. http:
//aclweb.org/anthology/L04-1313.
</p>
<p>Katz, G. and R. Zamparelli (2012). Quantifying
count/mass elasticity. In J. Choi (Ed.), Proceedings
of the 29th West Coast Conference on Formal Lin-
guistics, Somerville, MA, pp. 371–379. Cascadilla
Proceedings Project.
</p>
<p>Kiss, T., F. J. Pelletier, H. Husić, and J. Poppek (2016).
A sense-based lexicon of count and mass expres-
sions: The Bochum English countability lexicon.
In Proceedings of LREC 2016, Portoroz, Slovenia.
http://aclweb.org/anthology/L16.
</p>
<p>Kiss, T., F. J. Pelletier, and T. Stadtfeld (2014). Build-
ing a reference lexicon for countability in English.
In Proceedings of the Ninth LREC 2014, Reyk-
javik. http://aclweb.org/anthology/
L14-1312.
</p>
<p>197</p>
<p />
</div>
<div class="page"><p />
<p>Krifka, M. (1991). Massennomina. In A. von Ste-
chow and D. Wunderlich (Eds.), Semantics: An In-
ternational Handbook of Contemporary Research,
pp. 399–417. Berlin: Mouton de Gruyter.
</p>
<p>Kulkarni, R., S. Rothstein, and A. Treves (2013). A
statistical investigation into the cross-linguistic dis-
tribution of mass and count nouns: Morphosyntactic
and semantic perspectives. Biolinguistics 7, 132–
168.
</p>
<p>McCawley, J. (1975). Lexicography and the count-
mass distinction. In Berkeley Linguistic Society, Vol.
1, pp. 314–321. Reprinted in J. McCawley (ed.) Ad-
verbs, Vowels, and Other Objects of Wonder, Univ.
Chicago Press, Chicago, 1979, pages 165–173.
</p>
<p>Middleton, E., E. Wisniewski, K. Trindel, and M. Imai
(2004). Separating the chaff from the oats: Ev-
idence for a conceptual distinction between count
noun and mass noun aggregates. Journal of Mem-
ory and Language 50, 371–394.
</p>
<p>Miller, G. (1995). WordNet: A lexical database for
English. Communications of the ACM 38, 39–41.
</p>
<p>Miller, G. and C. Fellbaum (2007). WordNet then and
now. Language Resources and Evaluation 41, 209–
214. doi: 10.1007/s10579-007-9044-6.
</p>
<p>Payne, J. and R. Huddleston (2002). Nouns and noun
phrases. In R. Huddleston and G. K. Pullum (Eds.),
The Cambridge Grammar of the English Language,
pp. 323–523. Cambridge, UK: Cambridge UP.
</p>
<p>Pelletier, F. J. (1975). Non-singular reference: Some
preliminaries. Philosophia 5, 451–465. Reprinted
in (Pelletier, 1979, pp. 1-14).
</p>
<p>Pelletier, F. J. (Ed.) (1979). Mass Terms: Some Philo-
sophical Problems. Dordrecht: Kluwer Academic
Pub.
</p>
<p>Pustejovsky, J. (1991). The generative lexicon. Com-
putational Linguistics 17, 409–441. http://
aclweb.org/anthology/J91-4003.
</p>
<p>Quine, W. (1960). Word and Object. Cambridge, MA:
MIT Press.
</p>
<p>Rothstein, S. (2010). Counting and the mass-count dis-
tinction. Journal of Semantics 27, 343–397.
</p>
<p>Schmidtke, D. and V. Kuperman (2016). Mass counts
in world Englishes: A corpus linguistic study of
noun countability in non-native varieties of english.
Corpus Linguistics and Linguistic Theory 12. doi:
10.1515/clit-2015-0047.
</p>
<p>198</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 199–208,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Parsing Graphs with Regular Graph Grammars
</p>
<p>Sorcha Gilroy
University of Edinburgh
</p>
<p>s.gilroy@sms.ed.ac.uk
</p>
<p>Adam Lopez
University of Edinburgh
</p>
<p>alopez@inf.ed.ac.uk
</p>
<p>Sebastian Maneth
Universität Bremen
</p>
<p>smaneth@uni-bremen.de
</p>
<p>Abstract
</p>
<p>Recently, several datasets have become
available which represent natural language
phenomena as graphs. Hyperedge Re-
placement Languages (HRL) have been
the focus of much attention as a for-
malism to represent the graphs in these
datasets. Chiang et al. (2013) prove that
HRL graphs can be parsed in polynomial
time with respect to the size of the in-
put graph. We believe that HRL are more
expressive than is necessary to represent
semantic graphs and we propose the use
of Regular Graph Languages (RGL; Cour-
celle 1991), which is a subfamily of HRL,
as a possible alternative. We provide a top-
down parsing algorithm for RGL that runs
in time linear in the size of the input graph.
</p>
<p>1 Introduction
</p>
<p>NLP systems for machine translation, summariza-
tion, paraphrasing, and other tasks often fail to
preserve the compositional semantics of sentences
and documents because they model language as
bags of words, or at best syntactic trees. To pre-
serve semantics, they must model semantics. In
pursuit of this goal, several datasets have been pro-
duced which pair natural language with composi-
tional semantic representations in the form of di-
rected acyclic graphs (DAGs), including the Ab-
stract Meaning Representation Bank (AMR; Ba-
narescu et al. 2013), the Prague Czech-English
Dependency Treebank (Hajič et al., 2012), Deep-
bank (Flickinger et al., 2012), and the Univer-
sal Conceptual Cognitive Annotation (Abend and
Rappoport, 2013). To make use of this data, we
require models of graphs.
</p>
<p>Consider how we might use compositional
semantic representations in machine translation
</p>
<p>Anna fehlt
</p>
<p>ihrem Kater
</p>
<p>Anna’s cat
</p>
<p>misses her
</p>
<p>miss arg0
</p>
<p>arg1 cat
</p>
<p>poss
</p>
<p>Anna
</p>
<p>Figure 1: Semantic machine translation using AMR (Jones
et al., 2012). The edge labels identify ‘cat’ as the object of
the verb ‘miss’, ‘Anna’ as the subject of ‘miss’ and ‘Anna’
as the possessor of ‘cat’. Edges whose head nodes are not
attached to any other edge are interpreted as node labels.
</p>
<p>(Figure 1), a two-step process in which seman-
tic analysis is followed by generation. Jones
et al. (2012) observe that this decomposition can
be modeled with a pair of synchronous gram-
mars, each defining a relation between strings
and graphs. Necessarily, one projection of this
synchronous grammar produces strings, while the
other produces graphs, i.e., is a graph grammar.
A consequence of this representation is that the
complete translation process can be realized by
parsing: to analyze a sentence, we parse the in-
put string with the string-generating projection of
the synchronous grammar, and read off the syn-
chronous graph from the resulting parse. To gen-
erate a sentence, we parse the graph, and read off
the synchronous string from the resulting parse. In
this paper, we focus on the latter problem: using
graph grammars to parse input graphs. We call this
graph recognition to avoid confusion with other
parsing problems.
</p>
<p>Recent work in NLP has focused primarily
on hyperedge replacement grammar (HRG;
Drewes et al. 1997), a context-free graph grammar
formalism that has been studied in an NLP context
by several researchers (Chiang et al., 2013; Peng
et al., 2015; Bauer and Rambow, 2016). In partic-
ular, Chiang et al. (2013) propose that HRG could
be used to represent semantic graphs, and pre-
cisely characterize the complexity of a CKY-style
</p>
<p>199</p>
<p />
</div>
<div class="page"><p />
<p>algorithm for graph recognition from Lautemann
(1990) to be polynomial in the size of the input
graph. HRGs are very expressive—they can gen-
erate graphs that simulate non-context-free string
languages (Engelfriet and Heyker, 1991; Bauer
and Rambow, 2016). This means they are likely
more expressive than we need to represent the lin-
guistic phenomena that appear in existing seman-
tic datasets. In this paper, we propose the use of
Regular Graph Grammars (RGG; Courcelle 1991)
a subfamily of HRG that, like its regular counter-
parts among string and tree languages, is less ex-
pressive than context-free grammars but may ad-
mit more practical algorithms. By analogy to Chi-
ang’s CKY-style algorithm for HRG. We develop
an Earley-style recognition algorithm for RGLs
that is linear in the size of the input graph.
</p>
<p>2 Regular Graph Languages
</p>
<p>We use the following notation. If n is an integer,
[n] denotes the set {1, . . . , n}. Let Γ be an alpha-
bet, i.e., a finite set. Then s ∈ Γ∗ denotes that s
is a sequence of arbitrary length, each element of
which is in Γ. We denote by |s| the length of s. A
ranked alphabet is an alphabet Γ paired with an
arity mapping (i.e., a total function) rank: Γ→ N.
Definition 1. A hypergraph (or simply graph)
over a ranked alphabet Γ is a tuple G =
(VG, EG, attG, labG, extG) where VG is a finite set
of nodes; EG is a finite set of edges (distinct from
VG); attG : EG → V ∗G maps each edge to a se-
quence of nodes; labG : EG → Γ maps each edge
to a label such that |attG(e)| = rank(labG(e));
and extG is an ordered subset of VG called the ex-
ternal nodes of G.
</p>
<p>We assume that the elements of extG are pair-
wise distinct, and the elements of attG(e) for each
edge e are also pairwise distinct. An edge e is
attached to its nodes by tentacles, each labeled
by an integer indicating the node’s position in
attG(e) = (v1, . . . , vk). The tentacle from e to
vi will have label i, so the tentacle labels lie in the
set [k] where k = rank(e). To express that a node
v is attached to the ith tentacle of an edge e, we
say vert(e, i) = v. Likewise, the nodes in extG
are labeled by their position in extG. We refer to
the ith external node of G by extG(i) and in fig-
ures this will be labeled (i). The rank of an edge
e is k if att(e) = (v1, . . . , vk) (or equivalently,
rank(lab(e)) = k). The rank of a hypergraph G,
denoted by rank(G) is the size of extG.
</p>
<p>Example 1. Hypergraph G in Figure 2 has four
nodes (shown as black dots) and three hyperedges
labeled a, b, and X (shown boxed). The brack-
eted numbers (1) and (2) denote its external nodes
and the numbers between edges and the nodes are
tentacle labels. Call the top node v1 and, proceed-
ing clockwise, call the other nodes v2, v3, and v4.
Call its edges e1, e2 and e3. Its definition would
state attG(e1) = (v1, v2), attG(e2) = (v2, v3),
attG(e3) = (v1, v4, v3), labG(e1) = a, labG(e2) =
b, labG(e3) = X , and extG = (v4, v2).
</p>
<p>Definition 2. Let G be a hypergraph containing
an edge e with attG(e) = (v1, . . . , vk) and let H
be a hypergraph of rank k with node and edge sets
disjoint from those of G. The replacement of e by
H is the graph G′ = G[e/H]. Its node set VG′ is
V ∪ VH where V = VG − {v1, . . . , vk}. Its edge
set is EG′ = (EG−{e})∪EH . We define attG′ =
att ∪ attH where for every e′ ∈ (EG−{e}), att(e)
is obtained from attG(e′) by replacing vi by the ith
external node of H . Let labG′ = lab∪ labH where
lab is the restriction of labG to edges in EG−{e}.
Finally, let extG′ = extG.
</p>
<p>Example 2. A replacement is shown in Figure 2.
</p>
<p>2.1 Hyperedge Replacement Grammars
Definition 3. A hyperedge replacement grammar
G = (NG , TG , PG , SG) consists of ranked (dis-
joint) alphabets NG and TG of nonterminal and
terminal symbols, respectively, a finite set PG of
productions, and a start symbol SG ∈ NG . Every
production in PG is of the form X → G where
G is a hypergraph over NG ∪ TG and rank(G) =
rank(X).
</p>
<p>For each production p : X → G, we use L(p)
to refer to X (the left-hand side of p) and R(p)
to refer to G (the right-hand side of p). An edge
is a terminal edge if its label is terminal and a
nonterminal edge if its label is nonterminal. A
graph is a terminal graph if all of its edges are
terminal. The terminal subgraph of a graph is
the subgraph consisting of all terminal edges and
their incident nodes.
</p>
<p>Given a HRG G, we say that graph G im-
mediately derives graph G′, denoted G → G′,
iff there is an edge e ∈ EG and a nonterminal
X ∈ NG such that labG(e) = X and G′ =
G[e/H], where X → H is in PG . We extend
the idea of immediate derivation to its transitive
closure G →∗ G′, and say here that G derives
G′. For every X ∈ NG we also use X to de-
</p>
<p>200</p>
<p />
</div>
<div class="page"><p />
<p>X
a
</p>
<p>b
</p>
<p>(1) (2)
</p>
<p>G
</p>
<p>(2)
</p>
<p>c
</p>
<p>(1)
</p>
<p>a
</p>
<p>(3)
</p>
<p>d
</p>
<p>H
</p>
<p>(1)
</p>
<p>c
d
a
</p>
<p>(2)
</p>
<p>ba
</p>
<p>G[e/H]
</p>
<p>2
</p>
<p>1
1
</p>
<p>2
</p>
<p>1
</p>
<p>2
3
</p>
<p>2
</p>
<p>1
1
</p>
<p>2
1
</p>
<p>2
</p>
<p>2
</p>
<p>1
1
</p>
<p>2
1
</p>
<p>2
</p>
<p>1
</p>
<p>2
</p>
<p>1
</p>
<p>2
</p>
<p>Figure 2: The replacement of the X-labeled edge e in G by
the graph H .
</p>
<p>Sp :
X
</p>
<p>(1)
</p>
<p>1 go
1
</p>
<p>2
I
</p>
<p>arg0
Y Zs : (1)
</p>
<p>(2)
</p>
<p>1
</p>
<p>2
</p>
<p>1
</p>
<p>arg0
</p>
<p>arg1
</p>
<p>Xq : W
</p>
<p>Y
</p>
<p>(2)
</p>
<p>(1)
1
</p>
<p>2
</p>
<p>1
</p>
<p>1
</p>
<p>2
</p>
<p>arg1
</p>
<p>arg0 Wt :
</p>
<p>(1)
1
</p>
<p>want
</p>
<p>Yr : Z
</p>
<p>X
</p>
<p>(2)
</p>
<p>(1)
1
</p>
<p>2
</p>
<p>1
1
</p>
<p>2
</p>
<p>arg1
</p>
<p>arg0 Zu :
</p>
<p>(1)
1
</p>
<p>need
</p>
<p>Table 1: Productions of a HRG. The labels p, q, r, s, t, and u
label the productions so that we can refer to them in the text.
Note that Y can rewrite in two ways, either via production r
or s.
</p>
<p>note the graph consisting of a single edge e with
lab(e) = X and nodes (v1, . . . , vrank(X)) such
that attG(e) = (v1, . . . , vrank(X)), and we define
the language LX(G) as {G | X →∗ G ∧ G is
terminal}. The language of G is L(G) = LSG (G).
We call the family of languages that can be pro-
duced by any HRG the hyperedge replacement
languages (HRL).
</p>
<p>We assume that terminal edges are always of
rank 2, and depict them as directed edges where
the direction is determined by the tentacle labels:
the tentacle labeled 1 attaches to the source of the
edge and the tentacle labeled 2 attaches to the tar-
get of the edge.
</p>
<p>Example 3. Table 1 shows a HRG deriving AMR
graphs for sentences of the form ‘I need to want
to need to want to ... to want to go’. Figure 3 is
a graph derived by the grammar. The grammar is
somewhat unnatural, a point we will return to (§4).
</p>
<p>We can use HRGs to generate chain graphs
</p>
<p>(1)
</p>
<p>arg1
</p>
<p>arg1
</p>
<p>arg1
</p>
<p>arg1
</p>
<p>need
</p>
<p>want
</p>
<p>need
</p>
<p>want
</p>
<p>go
</p>
<p>I
</p>
<p>arg0
</p>
<p>arg0
</p>
<p>arg0
</p>
<p>arg0 arg0
</p>
<p>Figure 3: Graph derived by grammar in Table 1.
</p>
<p>S (1) S (2) (1) (2)1 2
a
</p>
<p>1 2
b a b
</p>
<p>Figure 4: A HRG producing the string language anbn.
</p>
<p>(strings) by restricting the form of the pro-
ductions in the grammars. Figure 4 shows a
HRG that produces the context-free string lan-
guage anbn. HRGs can simulate the class of
mildly context-sensitive languages that is charac-
terized, e.g., by linear context-free rewriting sys-
tems (LCFRS;Vijay-Shanker et al. 1987), where
the fan-out of the LCFRS will influence the max-
imum rank of nonterminal required in the HRG,
see (Engelfriet and Heyker, 1991).
</p>
<p>2.2 Regular Graph Grammars
A regular graph grammar (RGG; Courcelle 1991)
is a restricted form of HRG. To explain the restric-
tions, we first require some definitions.
</p>
<p>Definition 4. Given a graph G, a path in G from
a node v to a node v′ is a sequence
</p>
<p>(v0, i1, e1, j1, v1)(v1, i2, e2, j2, v2)
. . . (vk−1, ik, ek, jk, vk) (1)
</p>
<p>such that v0 = v, vk = v′, and for each r ∈ [k],
vert(er, ir) = vr−1 and vert(er, jr) = vr. The
length of this path is k.
</p>
<p>A path is terminal if every edge in the path has
a terminal label. A path is internal if each vi is
internal for 1 ≤ i ≤ k−1. Note that the endpoints
v0 and vk of an internal path can be external.
</p>
<p>Definition 5. A HRG G is a Regular Graph
Grammar (or simply RGG) if each nonterminal
in NG has rank at least one and for each p ∈ PG
the following hold:
</p>
<p>201</p>
<p />
</div>
<div class="page"><p />
<p>(C1) R(p) has at least one edge. Either it is a
single terminal edge, all nodes of which are exter-
nal, or each of its edges has at least one internal
node.
</p>
<p>(C2) Every pair of nodes in R(p) is connected
by a terminal and internal path.
</p>
<p>Example 4. The grammar in Table 1 is an RGG.
Although HRGs can produce context-free lan-
guages (and beyond) as shown in Figure 4, the
only string languages RGGs can produce are the
regular string languages. See Figure 5 for an
example of a string generating RGG. Similarly,
RGGs can produce regular tree languages, but not
context-free tree languages. Figure 6 shows a tree
generating RGG that generates binary trees the in-
ternal nodes of which are represented by a-labeled
edges, and the leaves of which are represented by
b-labeled edges. Note that these two results of reg-
ularity of the string- and tree-languages generated
by RGG follow from the fact that graph languages
produced by RGG are MSO-definable (Courcelle,
1991), and the well-known facts that the regular
string and graph languages are MSO-definable.
</p>
<p>X
(1)
</p>
<p>a
Y
</p>
<p>(1)
</p>
<p>b1 1
</p>
<p>Figure 5: A RGG for a regular string language.
</p>
<p>X
</p>
<p>(1)
</p>
<p>Y Z
</p>
<p>(1)1
a
</p>
<p>1 2
</p>
<p>1 1
</p>
<p>b
</p>
<p>Figure 6: A RGG for a regular tree language.
</p>
<p>We call the family of languages generated by
RGGs the regular graph languages (RGLs).
</p>
<p>3 RGL Recognition
</p>
<p>To recognize RGG, we exploit the property that
every nonterminal including the start symbol has
rank at least one (Definition 5), and we assume
that the corresponding external node is identified
in the input graph. This mild assumption may
be reasonable for applications like AMR parsing,
where grammars could be designed so that the ex-
ternal node is always the unique root. Later we
relax this assumption.
</p>
<p>The availability of an identifiable external node
suggests a top-down algorithm, and we take in-
</p>
<p>spiration from a top-down recognition algorithm
for the predictive top-down parsable grammars,
another subclass of HRG (Drewes et al., 2015).
These grammars, the graph equivalent of LL(1)
string grammars, are incomparable to RGG, but
the algorithms are related in their use of top-down
prediction and in that they both fix an order of the
edges in the right-hand side of each production.
</p>
<p>3.1 Top-Down Recognition for RGLs
</p>
<p>Just as the algorithm of Chiang et al. (2013) gen-
eralizes CKY to HRG, our algorithm generalizes
Earley’s algorithm (Earley, 1970). Both algo-
rithms operate by recognizing incrementally larger
subgraphs of the input graph, using a succinct rep-
resentation for subgraphs that depends on an arbi-
trarily chosen marker node m of the input graph.
</p>
<p>Definition 6. (Chiang et al. 2013; Definition 6)
Let I be a subgraph of a graph G. A boundary
node of I is a node which is either an endpoint
of an edge in G\I or an external node of G. A
boundary edge of I is an edge in I which has
a boundary node as an endpoint. The boundary
representation of I is the tuple b(I) = 〈bn(I),
be(I),m ∈ I〉 where
</p>
<p>1. bn(I) is the set of boundary nodes of I
2. be(I) is the set of boundary edges of I
3. (m ∈ I) is a flag indicating whether the
</p>
<p>marker node is in I .
</p>
<p>Chiang et al. (2013) prove each subgraph has
a unique boundary representation, and give algo-
rithms that use only boundary representations to
compute the union of two subgraphs, requiring
time linear in the number of boundary nodes; and
to check disjointness of subgraphs, requiring time
linear in the number of boundary edges.
</p>
<p>For each production p of the grammar, we im-
pose a fixed order on the edges of R(p), as in
Drewes et al. (2015). We discuss this order in de-
tail in §3.2. As in Earley’s algorithm, we use dot-
ted rules to represent partial recognition of pro-
ductions: X → ē1 . . . ēi−1 · ēi . . . ēn means that
we have identified the edges ē1 to ēi−1 and that
we must next recognize edge ēi. We write ē and
v̄ for edges and nodes in productions and e and v
for edges and nodes in a derived graph. When the
identity of the sequence is immaterial we abbrevi-
ate it as α, for example writing X → ·α.
</p>
<p>We present our recognizer as a deductive proof
system (Shieber et al., 1995). The items of the
</p>
<p>202</p>
<p />
</div>
<div class="page"><p />
<p>Name Rule Conditions
</p>
<p>PREDICT
[b(I), p : X → ē1 . . .· ēi . . . ēn, φp][q : Y → α]
</p>
<p>[φp(ēi), q : Y → ·α, φ0q[extR(q) = φp(ēi)]] lab(ēi) = Y
</p>
<p>SCAN
[b(I), X → ē1 . . .· ēi . . . ēn, φp][e = edglab(ēi)(v1, . . . , vm)]
</p>
<p>[b(I ∪ {e}), X → ē1 . . .· ēi+1 . . . ēn, φp[att(ēi) = (v1, . . . , vm)]] φp(ēi)(j) ∈ VG ⇒φp(ēi)(j) = vert(e, j)
</p>
<p>COMPLETE
[b(I), p : X → ē1 . . .· ēi . . . ēn, φp][b(J), q : Y → α·, φq]
</p>
<p>[b(I ∪ J), X → ē1 . . .· ēi+1 . . . ēn, φp[att(ēi) = φp(extR(q))]]
φp(ēi)(j) ∈ VG ⇒
</p>
<p>φp(ēi)(j) =
φq(extR(q))(j),
</p>
<p>lab(ēi) = Y,
EI ∩ EJ = ∅
</p>
<p>Table 2: The inference rules for the top-down recognizer.
</p>
<p>recognizer are of the form
</p>
<p>[b(I), p : X → ē1 . . .· ēi . . . ēn, φp]
where I is a subgraph that has been recognized
as matching ē1, . . . , ēi−1; p : X → ē1, . . . , ēn is a
production in the grammar with the edges in order;
and φp : ER(p) → V ∗G maps the endpoints of edges
in R(p) to nodes in G.
</p>
<p>For each production p, we number the nodes
in some arbitrary but fixed order. Using this, we
construct the function φ0p : ER(p) → V ∗R(p) such
that for ē ∈ ER(p) if att(ē) = (v̄1, v̄2) then
φ0p(ē) = (v̄1, v̄2). As we match edges in the graph
with edges in p, we assign the nodes v̄ to nodes in
the graph. For example, if we have an edge ē in
a production p such that att(ē) = (v̄1, v̄2) and we
find an edge e which matches ē, then we update
φp to record this fact, written φp[att(ē) = att(e)].
We also use φp to record assignments of external
nodes. If we assign the ith external node to v, we
write φp[extp(i) = v]. We write φ0p to represent a
mapping with no grounded nodes.
</p>
<p>Since our algorithm makes top-down predic-
tions based on known external nodes, our bound-
ary representation must cover the case where a
subgraph is empty except for these nodes. If
at some point we know that our subgraph has
external nodes φ(ē), then we use the shorthand
φ(ē) rather than the full boundary representation
〈φ(ē), ∅,m ∈ φ(ē)〉.
</p>
<p>To keep notation uniform, we use dummy non-
terminal S∗ 6∈ NG that derives SG via the produc-
tion p0. For graph G, our system includes the ax-
iom:
</p>
<p>[extG, p0 : S∗ →·SG , φ0p0 [extR(p0) = extG]].
</p>
<p>Our goal is to prove:
</p>
<p>[b(G), pS : S∗ → SG·, φpS ]
where φpS has a single edge ē in its domain which
has label SG in R(pS) and φpS (ē) = extG.
</p>
<p>As in Earley’s algorithm, we have three infer-
ence rules: PREDICT, SCAN and COMPLETE (Ta-
ble 2). PREDICT is applied when the edge after the
dot is nonterminal, assigning any external nodes
that have been identified. SCAN is applied when
the edge after the dot is terminal. Using φp, we
may already know where some of the endpoints of
the edge should be, so it requires the endpoints of
the scanned edge to match. COMPLETE requires
that each of the nodes of ēi in R(p) have been
identified, these nodes match up with the corre-
sponding external nodes of the subgraph J , and
that the subgraphs I and J are edge-disjoint.
</p>
<p>We provide a high-level proof that the recog-
nizer is sound and complete.
</p>
<p>Proposition 1. Let G be a HRG and G a graph.
Then the goal [b(G), pS : S∗ → SG ·, φpS ] can
be proved from the axiom [extG, pS : S∗ →·SG , φpS [extR(pS) = extG]] if and only if G ∈
L(G).
Proof. We prove that for each X ∈ NG ,
[b(G), pX : X∗ → X ·, φpX ] can be proved from
[extG, pX : X∗ → ·X,φpX [extR(pX) = extG]] if
and only if G ∈ LX(G) where the dummy non-
terminal X∗ was added to the set of nonterminals
and pX : X∗ → X was added to the set of produc-
tions. We prove this by induction on the number
of edges in G.
</p>
<p>We assume that each production in the grammar
contains at least one terminal edge. If the HRG is
not in this form, it can be converted into this form
</p>
<p>203</p>
<p />
</div>
<div class="page"><p />
<p>and in the case of RGGs they are already in this
form by definition.
</p>
<p>Base Case: Let G consist of a single edge.
If: Assume G ∈ LX(G). Since G consists
</p>
<p>of one edge, there must be a production q :
X → G. Apply PREDICT to the axiom and
pX : X∗ → X to obtain the item [φpX (X), q :
X → ·G,φ0q [extG = φpX (X)]]. Apply SCAN to
the single terminal edge that makes up G to ob-
tain [b(G), q : X → G·, φq] and finally apply
COMPLETE to this and the axiom reach the goal
[b(G), pX : X∗ → X,φpX ].
</p>
<p>Only if: Assume the goal can be reached from
the axiom and G = e. Then the item [b(e), q :
X → e, φq] must have been reached at some point
for some q ∈ PG . Therefore q : X → e is a
production and so e = G ∈ LX(G).
</p>
<p>Assumption: Assume that the proposition
holds when G has fewer than k edges.
</p>
<p>Inductive Step: Assume G has k edges.
If: Assume G ∈ LX(G), then there is a pro-
</p>
<p>duction q : X → H where H has nonterminals
Y1, . . . , Yn and there are graphs H1, . . . ,Hn such
that G = H[Y1/H1] . . . [Yn/Hn]. Each graph Hi
for i ∈ [n] has fewer than k edges and so we ap-
ply the inductive hypothesis to show that we can
prove the items [b(Hi), ri : Yi → Ji, φri ] for each
i ∈ [n]. By applying COMPLETE to each such item
and applying SCAN to each terminal edge ofH we
reach the goal [b(G), pX : X∗ → X ·, φpX ].
</p>
<p>Only If: Assume the goal can be proved from
the axiom. Then we must have at some point
reached an item of the form [b(G), q : X →
H,φq] and that H has nonterminals Y1, . . . , Yn.
This means that there are graphs H1, . . . ,Hn such
that [b(Hi), pYi : Y
</p>
<p>∗
i → Yi, φpYi ] for each i ∈ [n]
</p>
<p>and G = H[Y1/H1] . . . [Yn/Hn]. Since each Hi
has fewer than k edges, we apply the inductive hy-
pothesis to get that Hi ∈ LYi(G) for each i ∈ [n]
and therefore G ∈ LX(G).
Example 5. Using the RGG in Table 1, we show
how to recognize the graph in Figure 7, which can
be derived by applying production s followed by
production u, where the external nodes of Y are
(v3, v2). Assume the ordering of the edges in pro-
duction s is arg1, arg0, Z; the top node is v̄1; the
bottom node is v̄2; and the node on the right is v̄3;
and that the marker node is not in this subgraph—
we elide reference to it for simplicity. Let v̄4 be
the top node of R(u) and v̄5 be the bottom node
of R(u). The external nodes of Y are determined
</p>
<p>top-down, so the recognize of this subgraph is trig-
gered by this item:
</p>
<p>[{v3, v2}, Y →· arg1 arg0Z,
φ0s[extR(s) = (v3, v2)]] (2)
</p>
<p>where φs(arg1) = (v̄1, v3), φs(arg0) = (v̄1, v2),
and φs(Z) = (v̄1).
</p>
<p>Table 3 shows how we can prove the item
</p>
<p>[〈{v3, v2}, {e3, e2}〉, Y → arg1arg0Z·, φ]
The boundary representation
〈{v3, v2}, {e3, e2}〉 in this item represents
the whole subgraph shown in Figure 7.
</p>
<p>v1
</p>
<p>v4
</p>
<p>v2
</p>
<p>v3 . . .
</p>
<p>. . .
</p>
<p>need (e1)
</p>
<p>arg0 (e2)
</p>
<p>arg1 (e3)
</p>
<p>Figure 7: Top left subgraph of Figure 3. To refer to nodes and
edges in the text, they are labeled v1, v2, v3, e1, e2, and e3.
</p>
<p>3.2 Normal Ordering
Our algorithm requires a fixed ordering of the
edges in the right-hand sides of each production.
We will constrain this ordering to exploit the struc-
ture of RGG productions, allowing us to bound
recognition complexity. If s = ē1 . . . ēn is an or-
der, define si:j = ēi . . . ēj .
</p>
<p>Definition 7. Let s = ē1, . . . , ēn be an edge order
of a right-hand side of a production. Then s is
normal if it has the following properties:
</p>
<p>1. ē1 is connected to an external node,
2. s1:j is a connected graph for all j ∈ [n]
3. if ēi is nonterminal, each endpoint of ēi must
</p>
<p>be incident with some terminal edge ēj for which
j &lt; i.
</p>
<p>Example 6. The ordering of the edges of produc-
tion s in Example 5 is normal.
</p>
<p>Arbitrary HRGs do not necessarily admit a nor-
mal ordering. For example, the graph in Figure 8
cannot satisfy Properties 2 and 3 simultaneously.
However, RGGs do admit a normal ordering.
</p>
<p>(1) X (2)a 1 2 b
</p>
<p>Figure 8: This graph cannot be normally ordered.
</p>
<p>204</p>
<p />
</div>
<div class="page"><p />
<p>Current Item Reason
</p>
<p>1. [{v3, v2}, Y → · arg1arg0Z, φ0s[extR(s) = (v3, v2)]] Equation 2
2. [〈{v3, v2, v1}, {e3}〉, Y → arg1· arg0Z, φs[att(arg1) = (v1, v3)]] SCAN: 1. and e3 = edgarg1(v1, v3)
3. [〈{v3, v2, v1}, {e3, e2}〉, Y → arg1arg0·Z, φs[att(arg0) = (v1, v2)]] SCAN: 2. and e2 = edgarg0(v1, v2)]
4. [(v1), Z → · need, φ0u[extR(u) = (v1)]] PREDICT: 3. and Z → need
5. [〈{v1, v4}, {e1}〉, Z → need·, φu[att(need) = (v1, v4)]] SCAN: 4. and e1 = edgneed(v1, v4)
6. [〈{v3, v2}, {e3, e2}〉, Y → arg1arg0Z ·, φs[att(Z) = (v1)]] COMPLETE: 3. and 5.
</p>
<p>Table 3: The steps of recognizing that the subgraph shown in Figure 7 is derived from productions r2 and u in the grammar in
Table 1.
</p>
<p>Proposition 2. If G is an RGG, for every p ∈ PG ,
there is a normal ordering of the edges in R(p).
</p>
<p>Proof. If R(p) contains a single node then it must
be an external node and it must have a terminal
edge attached to it since R(p) must contain at
least one terminal edge. If R(p) contains multi-
ple nodes then by C2 there must be terminal inter-
nal paths between all of them, so there must be a
terminal edge attached to the external node, which
we use to satisfy Property 1. To produce a normal
ordering, we next select terminal edges once one
of their endpoints is connected to an ordered edge,
and nonterminal edges once all endpoints are con-
nected to ordered edges, possible by C2. There-
fore, Properties 2 and 3 are satisfied.
</p>
<p>A normal ordering tightly constrains the recog-
nition of edges. Property 3 ensures that when
we apply PREDICT, the external nodes of the pre-
dicted edge are all bound to specific nodes in the
graph. Properties 1 and 2 ensure that when we
apply SCAN, at least one endpoint of the edge is
bound (fixed).
</p>
<p>3.3 Recognition Complexity
</p>
<p>Assume a normally-ordered RGG. Let the maxi-
mum number of edges in the right-hand side of any
production be m; the maximum number of nodes
in any right-hand side of a production k; the maxi-
mum degree of any node in the input graph d; and
the number of nodes in the input graph n.
</p>
<p>As previously mentioned, Drewes et al. (2015)
also propose a HRG recognizer which can recog-
nize a subclass of HRG (incomparable to RGG)
called the predictive top-down parsable grammars.
Their recognizer in this case runs in O(n2) time.
A well-known bottom-up recognizing algorithm
for HRG was first proposed by Lautemann (1990).
</p>
<p>In this paper, the recognizer is shown to be polyno-
mial in the size of the input graph. Later, Chiang
et al. (2013) formulate the same algorithm more
precisely and show that the recognizing complex-
ity is O((3d × n)k+1) where k in their case is the
treewidth of the grammar.
Remark 1. The maximum number of nodes in any
right-hand side of a production (k) is also the max-
imum number of boundary nodes for any subgraph
in the recognizer.
</p>
<p>COMPLETE combines subgraphs I and J only
when the entire subgraph derived from Y has been
recognized. Boundary nodes of J are also bound-
ary nodes of I because they are nodes in the ter-
minal subgraph of R(p) where Y connects. The
boundary nodes of I ∪ J are also bounded by k
since form a subset of the boundary nodes of I .
Remark 2. Given a boundary node, there are at
most (dm)k−1 ways of identifying the remaining
boundary nodes of a subgraph that is isomorphic
to the terminal subgraph of the right-hand side of
a production.
</p>
<p>The terminal subgraph of each production is
connected by C2, with a maximum path length of
m. For each edge in the path, there are at most d
subsequent edges. Hence for the k − 1 remaining
boundary nodes there are (dm)k−1 ways of choos-
ing them.
</p>
<p>We count instantiations of COMPLETE for an
upper bound on complexity (McAllester, 2002),
using similar logic to (Chiang et al., 2013). The
number of boundary nodes of I, J and I ∪ J is
at most k. Therefore, if we choose an arbitrary
node to be some boundary node of I ∪J , there are
at most (dm)k−1 ways of choosing its remaining
boundary nodes. For each of these nodes, there
are at most (3d)k states of their attached boundary
edges: in I , in J , or in neither. The total number
</p>
<p>205</p>
<p />
</div>
<div class="page"><p />
<p>of instantiations is O(n(dm)k−1(3d)k), linear in
the number of input nodes and exponential in the
degree of the input graph. Note that in the case of
the AMR dataset (Banarescu et al. 2013), the max-
imum node degree is 17 and the average is 2.12.
</p>
<p>We observe that RGGs could be relaxed to pro-
duce graphs with no external nodes by adding a
dummy nonterminal S′ with rank 0 and a single
production S′ → S. To adapt the recognition al-
gorithm, we would first need to guess where the
graph starts. This would add a factor of n to the
complexity as the graph could start at any node.
</p>
<p>4 Discussion and Conclusions
</p>
<p>We have presented RGG as a formalism that
could be useful for semantic representations and
we have provided a top-down recognition algo-
rithm for them. The constraints of RGG en-
able more efficient recognition than general HRG,
and this tradeoff is reasonable since HRG is very
expressive—when generating strings, it can ex-
press non-context-free languages (Engelfriet and
Heyker, 1991; Bauer and Rambow, 2016), far
more power than needed to express semantic
graphs. On the other hand, RGG is so constrained
that it may not be expressive enough: it would be
more natural to derive the graph in Figure 4 from
outermost to innermost predicate; but constraint
C2 makes it difficult to express this, and the gram-
mar in Table 1 does not. Perhaps we need less
expressivity than HRG but more than RGG.
</p>
<p>HRL MSOL Graphs
</p>
<p>CFTL
</p>
<p>CFL∗
</p>
<p>RTL Trees
</p>
<p>RL Strings
</p>
<p>RGL DAGAL
</p>
<p>Figure 9: A Hasse diagram of various string, tree and graph
language families. An arrow from family A to family B indi-
cates that family A is a subfamily of family B.
</p>
<p>A possible alternative would be to consider Re-
stricted DAG Grammars (RDG; Björklund et al.
2016). Parsing for a fixed such grammar can be
achieved in quadratic time with respect to the in-
put graph. It is known that for a fixed HRG gen-
erating k-connected hypergraphs consisting of hy-
peredges of rank k only, parsing can be carried out
in cubic time (k-HRG; (Drewes, 1993)).
</p>
<p>More general than RDGs a is the class of graph
languages recognized by DAG automata (DA-
GAL; Blum and Drewes 2016), for which the de-
terministic variant provides polynomial time pars-
ing. Note that RGGs can generate graph languages
of unbounded node degree. With respect to ex-
pressive power, RDGs and k-HRGs are incompa-
rable to RGGs. Figure 9 shows the relationships
between the context-free and regular languages for
strings, trees and graphs. Monadic-second order
logic (MSOL; Courcelle and Engelfriet 2011) is
a form of logic which when restricted to strings
gives us exactly the regular string languages and
when restricted to trees gives us exactly the regu-
lar tree languages. RGLs lie in the intersection of
HRG and MSOL on graphs but they do not make
up this entire intersection. Courcelle (1991) de-
fined (non-constructively) this intersection to be
the strongly context-free languages (SCFL). We
believe that there may be other formalisms that are
subfamilies of SCFL which may be useful for se-
mantic representations. All inclusions shown in
Figure 9 are strict. For instance, RGL cannot pro-
duce “star graphs” (one node that has edges to n
other nodes), while DAGAL and HRL can pro-
duce such graphs. It is well-known that HRL and
MSOL are incomparable. There is a language in
RGL that is not in DAGAL, for instance, “ladders”
(two string graphs of n nodes each, with an edge
between the ith node of each string).
</p>
<p>Another alternative formalism to RGG that is
defined as a restriction of HRG are Tree-like
Grammars (TLG; Matheja et al. 2015). They de-
fine a subclass of SCFL, i.e., they are MSO de-
finable. TLGs have been considered for program
verification, where closure under intersection of
the formalism is essential. Note that RGGs are
also closed under intersection. While TLG and
RDG are both incomparable to RGG, they share
important characteristics, including the fact that
the terminal subgraph of every production is con-
nected. This means that our top-down recogni-
tion algorithm is applicable to both. In the future
we would like to investigate larger, less restrictive
(and more linguistically expressive) subfamilies of
SCFL. We plan to implement and evaluate our al-
gorithm experimentally.
</p>
<p>Acknowledgments
</p>
<p>This work was supported in part by the EPSRC
Centre for Doctoral Training in Data Science,
</p>
<p>206</p>
<p />
</div>
<div class="page"><p />
<p>funded by the UK Engineering and Physical Sci-
ences Research Council (grant EP/L016427/1) and
the University of Edinburgh; and in part by a
Google faculty research award (to AL). We thank
Clara Vania, Sameer Bansal, Ida Szubert, Fed-
erico Fancellu, Antonis Anastasopoulos, Marco
Damonte, and the anonymous reviews for helpful
discussion of this work and comments on previous
drafts of the paper.
</p>
<p>References
Omri Abend and Ari Rappoport. 2013. Univer-
</p>
<p>sal conceptual cognitive annotation (ucca). In
ACL (1). The Association for Computational
Linguistics, pages 228–238. http://dblp.uni-
trier.de/db/conf/acl/acl2013-1.html#AbendR13.
</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representa-
tion for sembanking. In Proceedings of the
7th Linguistic Annotation Workshop and Interoper-
ability with Discourse. Association for Computa-
tional Linguistics, Sofia, Bulgaria, pages 178–186.
http://www.aclweb.org/anthology/W13-2322.
</p>
<p>Daniel Bauer and Owen Rambow. 2016. Hy-
peredge replacement and nonprojective de-
pendency structures. In Proceedings of the
12th International Workshop on Tree Ad-
joining Grammars and Related Formalisms
(TAG+12), June 29 - July 1, 2016, Heinrich Heine
University, Düsseldorf, Germany. pages 103–
111. http://aclweb.org/anthology/W/W16/W16-
3311.pdf.
</p>
<p>Henrik Björklund, Frank Drewes, and Petter Eric-
son. 2016. Between a Rock and a Hard Place
– Uniform Parsing for Hyperedge Replacement
DAG Grammars, Springer International Publishing,
Cham, pages 521–532. https://doi.org/10.1007/978-
3-319-30000-9 40.
</p>
<p>Johannes Blum and Frank Drewes. 2016. Properties
of regular DAG languages. In Language and Au-
tomata Theory and Applications - 10th International
Conference, LATA 2016, Prague, Czech Republic,
March 14-18, 2016, Proceedings. pages 427–438.
https://doi.org/10.1007/978-3-319-30000-9 33.
</p>
<p>David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyper-
edge replacement grammars. In Proceedings
of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1:
Long Papers). Association for Computational
Linguistics, Sofia, Bulgaria, pages 924–932.
http://www.aclweb.org/anthology/P13-1091.
</p>
<p>Bruno Courcelle. 1991. The monadic second-order
logic of graphs V: on closing the gap between
definability and recognizability. Theor. Comput.
Sci. 80(2):153–202. https://doi.org/10.1016/0304-
3975(91)90387-H.
</p>
<p>Bruno Courcelle and Joost Engelfriet. 2011. Graph
Structure and Monadic Second-Order Logic, a Lan-
guage Theoretic Approach. Cambridge University
Press.
</p>
<p>Frank Drewes. 1993. Np-completeness of k-
connected hyperedge-replacement languages
of order k. Inf. Process. Lett. 45(2):89–94.
https://doi.org/10.1016/0020-0190(93)90221-T.
</p>
<p>Frank Drewes, Berthold Hoffmann, and Mark Mi-
nas. 2015. Predictive Top-Down Parsing for
Hyperedge Replacement Grammars, Springer
International Publishing, Cham, pages 19–34.
https://doi.org/10.1007/978-3-319-21145-9 2.
</p>
<p>Frank Drewes, Hans-Jörg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph grammars.
In Grzegorz Rozenberg, editor, Handbook of Graph
Grammars and Computing by Graph Transforma-
tion, World Scientific, pages 95–162.
</p>
<p>Jay Earley. 1970. An efficient context-free
parsing algorithm. ACM, New York,
NY, USA, volume 13, pages 94–102.
https://doi.org/10.1145/362007.362035.
</p>
<p>Joost Engelfriet and Linda Heyker. 1991. The string
generating power of context-free hypergraph gram-
mars. Journal of Computer and System Sciences
43(2):328–360.
</p>
<p>Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012.
Deepbank : a dynamically annotated treebank of the
Wall Street Journal. In Proceedings of the Eleventh
International Workshop on Treebanks and Linguistic
Theories (TLT11). Lisbon, pages 85–96. HU.
</p>
<p>Jan Hajič, Eva Hajičová, Jarmila Panevov, Petr
Sgall, Ondřej Bojar, Silvie Cinková, Eva Fučı́ková,
Marie Mikulová, Petr Pajas, Jan Popelka, Jiřı́ Se-
mecký, Jana Šindlerová, Jan Štěpánek, Josef Toman,
Zdeňka Urešová, and Zdeněk Žabokrtský. 2012.
Announcing prague czech-english dependency tree-
bank 2.0. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur
Doan, Bente Maegaard, Joseph Mariani, Asun-
cion Moreno, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC’12). European Language Resources Associ-
ation (ELRA), Istanbul, Turkey.
</p>
<p>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Mor-
tiz Hermann, and Kevin Knight. 2012. Semantics-
based machine translation with hyperedge replace-
ment grammars. In Proceedings of COLING.
</p>
<p>207</p>
<p />
</div>
<div class="page"><p />
<p>Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge re-
placement. Acta Informatica 27(5):399–421.
https://doi.org/10.1007/BF00289017.
</p>
<p>Christoph Matheja, Christina Jansen, and Thomas
Noll. 2015. Tree-Like Grammars and Separa-
tion Logic, Springer International Publishing, Cham,
pages 90–108. https://doi.org/10.1007/978-3-319-
26529-2 6.
</p>
<p>David McAllester. 2002. On the complexity anal-
ysis of static analyses. J. ACM 49(4):512–537.
https://doi.org/10.1145/581771.581774.
</p>
<p>Xiaochang Peng, Linfeng Song, and Daniel Gildea.
2015. A synchronous hyperedge replacement gram-
mar based approach for AMR parsing. In Pro-
ceedings of the 19th Conference on Computa-
tional Natural Language Learning, CoNLL 2015,
Beijing, China, July 30-31, 2015. pages 32–41.
http://aclweb.org/anthology/K/K15/K15-1004.pdf.
</p>
<p>Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming
24(1-2).
</p>
<p>K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descrip-
tions produced by various grammatical formalisms.
In Proceedings of the 25th Annual Meeting on
Association for Computational Linguistics. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, ACL ’87, pages 104–111.
https://doi.org/10.3115/981175.981190.
</p>
<p>208</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 209–219,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Embedded Semantic Lexicon Induction with Joint Global and Local
Optimization
</p>
<p>Sujay Kumar Jauhar
Language Technologies Institute
</p>
<p>Carnegie Mellon University
Pittsburgh, PA, USA
</p>
<p>sjauhar@cs.cmu.edu
</p>
<p>Eduard Hovy
Language Technologies Institute
</p>
<p>Carnegie Mellon University
Pittsburgh, PA, USA
hovy@cs.cmu.edu
</p>
<p>Abstract
</p>
<p>Creating annotated frame lexicons such
as PropBank and FrameNet is expen-
sive and labor intensive. We present
a method to induce an embedded frame
lexicon in an minimally supervised fash-
ion using nothing more than unlabeled
predicate-argument word pairs. We hy-
pothesize that aggregating such pair se-
lectional preferences across training leads
us to a global understanding that captures
predicate-argument frame structure. Our
approach revolves around a novel inte-
gration between a predictive embedding
model and an Indian Buffet Process pos-
terior regularizer. We show, through our
experimental evaluation, that we outper-
form baselines on two tasks and can learn
an embedded frame lexicon that is able to
capture some interesting generalities in re-
lation to hand-crafted semantic frames.
</p>
<p>1 Introduction
</p>
<p>Semantic lexicons such as PropBank (Palmer
et al., 2005) and FrameNet (Baker et al., 1998)
contain information about predicate-argument
frame structure. These frames capture knowledge
about the affinity of predicates for certain types
of arguments, their number and their semantic na-
ture, regardless of syntactic realization.
</p>
<p>For example, PropBank specifies frames in the
following manner:
</p>
<p>• eat→ [agent]0, [patient]1
• give→ [agent]0, [theme]1, [recipient]2
</p>
<p>These frames provide semantic information such
as the fact that “eat” is transitive, while “give” is
</p>
<p>ditransitive, or that the beneficiary of one action is
a “patient”, while the other is a “recipient”.
</p>
<p>This structural knowledge is crucial for a num-
ber of NLP applications. Information about
frames has been successfully used to drive and
improve diverse tasks such as information extrac-
tion (Surdeanu et al., 2003), semantic parsing (Das
et al., 2010) and question answering (Shen and La-
pata, 2007), among others.
</p>
<p>However, building these frame lexicons is very
expensive and time consuming. Thus, it remains
difficult to port applications from resource-rich
languages or domains to data impoverished ones.
The NLP community has tackled this issue along
two different lines of unsupervised work.
</p>
<p>At the local token level, researchers have at-
tempted to model frame structure by the selec-
tional preference of predicates for certain argu-
ments (Resnik, 1997; Séaghdha, 2010). For exam-
ple, on this problem a good model might assign a
high probability to the word “pasta” occurring as
an argument of the word “eat”.
</p>
<p>Contrastingly, at the global type level, work has
focussed on inducing frames by clustering pred-
icates and arguments in a joint framework (Lang
and Lapata, 2011a; Titov and Klementiev, 2012b).
In this case, one is interested in associating pred-
icates such as “eat”, “consume”, “devour”, with
a joint clustering of arguments such as “pasta”,
“chicken”, “burger”.
</p>
<p>While these methods have been useful for sev-
eral problems, they also have shortcomings. Se-
lectional preference modelling only captures local
predicate-argument affinities, but does not aggre-
gate these associations to arrive at a structural un-
derstanding of frames.
</p>
<p>Meanwhile, frame induction performs cluster-
ing at a global level. But most approaches tend
to be algorithmic methods (or some extension
thereof) that focus on semantic role labelling.
</p>
<p>209</p>
<p />
</div>
<div class="page"><p />
<p>Their lack of portable features or model parame-
ters unfortunately means they cannot be used to
solve other applications or problems that require
lexicon-level information – such as information
extraction or machine translation. Another limi-
tation is that they always depend on high-level lin-
guistic annotation, such as syntactic dependencies,
which may not exist in resource-poor settings.
</p>
<p>Thus, in this paper we propose to combine the
two approaches to induce a frame semantic lexi-
con in a minimally supervised fashion with noth-
ing more than unlabeled predicate-argument word
pairs. Additionally, we will learn an embedded
lexicon that jointly produces embeddings for pred-
icates, arguments and an automatically induced
collection of latent slots. The embeddings provide
flexibility for usage in downstream applications,
where predicate-argument affinities can be com-
puted at will.
</p>
<p>To jointly capture the local and global streams
of knowledge we propose a novel integration be-
tween a predictive embedding model and the pos-
terior of an Indian Buffet Process. The embed-
ding model maximizes the predictive accuracy of
predicate-argument selectional preference at the
local token level, while the posterior of the In-
dian Buffet process induces an optimal set of la-
tent slots at the global type level that capture the
regularities in the learned predicate embeddings.
</p>
<p>We evaluate our approach and show that our
models are able to outperform baselines on both
the local and global level of frame knowledge. At
the local level we score higher than a standard
predictive embedding model on selectional pref-
erence, while at the global level we outperform a
syntactic baseline on lexicon overlap with Prop-
Bank. Finally, our analysis on the induced latent
slots yields insight into some interesting general-
ities that we are able to capture from unlabeled
predicate-argument pairs.
</p>
<p>2 Related Work
</p>
<p>The work in this paper relates to research on
identifying predicate-argument structure in both
local and global contexts. These related areas
of research correspond to the NLP community’s
work respectively on selectional preference mod-
elling and semantic frame induction (which is also
known variously as unsupervised semantic role la-
belling or role induction).
</p>
<p>Selectional preference modelling seeks to cap-
</p>
<p>ture the semantic preference of predicates for cer-
tain arguments in local contexts. These prefer-
ences are useful for many tasks, including unsu-
pervised semantic role labelling (Gildea and Ju-
rafsky, 2002) among others.
</p>
<p>Previous work has sought to acquire these pref-
erences using various means, including ontologi-
cal resources such as WordNet (Resnik, 1997; Cia-
ramita and Johnson, 2000), latent variable mod-
els (Rooth et al., 1999; Séaghdha, 2010; Ritter
et al., 2010) and distributional similarity metrics
(Erk, 2007). Most closely related to our contribu-
tion is the work by Van de Cruys (2014) who use
a predictive neural network to capture predicate-
argument associations.
</p>
<p>To the best of our knowledge, our research is
the first to attempt using selectional preference as
a basis for directly inducing semantic frames.
</p>
<p>At the global level, frame induction subsumes
selectional preference by attempting to group ar-
guments of predicates into coherent and cohesive
clusters. While work in this area has included
diverse approaches, such as leveraging example-
based representations (Kawahara et al., 2014) and
cross-lingual resources (Fung and Chen, 2004;
Titov and Klementiev, 2012b), most attempts have
focussed on two broad categories. These are latent
variable driven models (Grenager and Manning,
2006; Cheung et al., 2013) and similarity driven
clustering models (Lang and Lapata, 2011a,b),
</p>
<p>Our work includes elements of both major cat-
egories, since we use latent slots to represent ar-
guments, but an Indian Buffet process induces
these latent slots in the first place. The work of
Titov and Klementiev (2012a) and Woodsend and
Lapata (2015) are particularly relevant to our re-
search. The former use another non-parametric
Bayesian model (a Chinese Restaurant process)
in their work, while the latter embed predicate-
argument structures before performing clustering.
</p>
<p>Crucially, however all these previous efforts in-
duce frames that are not easily portable to applica-
tions other than semantic role labelling (for which
they are devised). Moreover, they rely on syn-
tactic cues to featurize and help cluster argument
instances. To the best of our knowledge, ours
is the first attempt to go from unlabeled bag-of-
arguments to induced frame embeddings without
any reliance on annotated data.
</p>
<p>210</p>
<p />
</div>
<div class="page"><p />
<p>3 Joint Local and Global Frame Lexicon
Induction
</p>
<p>In this section we present our approach to in-
duce a frame lexicon with latent slots. Following
prior work on frame induction (Lang and Lapata,
2011a; Titov and Klementiev, 2012a), the proce-
dural pipeline can be split into two distinct phases:
argument identification and argument clustering.
</p>
<p>As with previous work, we focus on the lat-
ter stage, and assume that we have unlabeled
predicate-argument structure pairs – given to us
from gold standard annotation or through heuris-
tic means (Lang and Lapata, 2014).
</p>
<p>We begin with preliminary notation. Given a
vocabulary of predicate types P = {p1, ..., pn}
and contextual argument types A = {a1, ..., am}.
Let C = {(p1, a1), ..., (pN , aN )} be a corpus of
predicate-argument word token pairs1. Given this
corpus, we will attempt to learn an optimal set of
model parameters θ that maximizes a regularized
likelihood over the corpus.
</p>
<p>The model parameters include V = {vi | ∀pi ∈
P} an n × d embedding matrix for the predicates
and U = {ui | ∀ai ∈ A} anm×d embedding ma-
trix for the arguments. Additionally, assuming K
latent frame slots we define Z = {zik} an n × k
binary matrix that represents the presence or ab-
sence of the slot k for the predicate i, and a latent
K × d weight matrix S = {sk | 1 ≤ k ≤ K} that
associates a weight vector to each latent slot.
</p>
<p>The generalized form of the objective we opti-
mize is given by:
</p>
<p>θ̂ = arg max
θ
</p>
<p>∑
(pi,ai)∈C
</p>
<p>log
</p>
<p>(∑
k
</p>
<p>Pr(ai|pi, zik, sk)
)
</p>
<p>+ log prθ(Z|V ) (1)
This objective has two parts: a likelihood term,
and a posterior regularizer. The former will be
responsible for modelling the predictive accuracy
of selectional-preference at a local level, while the
latter will capture global consistencies for an opti-
mal set of latent slots.
</p>
<p>We detail the parametrization of each of these
components separately in what follows.
</p>
<p>1In this work, we assume argument chunks are broken
down into individual words, – to increase training data size
– but the model remains agnostic to this decision.
</p>
<p>Figure 1: The generative story depicting the re-
alization of an argument from a predicate. Ar-
gument words are generated from latent argument
slots. Observed variables are shaded in grey, while
latent variables are in white.
</p>
<p>3.1 Local Predicate-Argument Likelihood
The likelihood term of our model is based on
the popular Skip-gram model from Mikolov et al.
(2013) but suitably extended to incorporate the
latent frame slots and their associated weights.
Specifically, we define the probability for a single
predicate-argument pair (pi, ai) as:
</p>
<p>Pr(ai|pi) =
∑
k
</p>
<p>Pr(ai|pi, zik, sk) =
∑
k
</p>
<p>zik
exp((vi � sk) · ui)∑
ai′
</p>
<p>exp((vi � sk) · ui′) (2)
</p>
<p>where � represents the element-wise multiplica-
tion operator. Intuitively, in the likelihood term
we weight a general predicate embedding to a slot-
specific representations, which then predicts a spe-
cific argument. This is graphically represented in
Figure 1.
</p>
<p>3.2 Global Latent Slot Regularization
The posterior regularization term in equation 1
seeks to balance the likelihood term by yielding
an optimal set of latent slots, given the embedding
matrix of predicates.
</p>
<p>We choose the posterior of an Indian Buffet pro-
cess (IBP) (Griffiths and Ghahramani, 2005) in
this step to induce an optimal latent binary ma-
trix Z. The IBP itself places a prior on equiva-
lence classes of infinite dimensional sparse binary
matrices, and is the infinite limit (K → ∞) of a
beta-Bernoulli model.
</p>
<p>πk ∼ Beta(α/K, 1)
zik ∼ Bernoulli(πk)
</p>
<p>(3)
</p>
<p>Given a suitable likelihood function and some
</p>
<p>211</p>
<p />
</div>
<div class="page"><p />
<p>data, inference in an IBP computes a posterior that
yields an optimal finite binary matrix with respect
to regularities in the data.
</p>
<p>Setting the data, in our case, to be the embed-
ding matrix of predicates V , this gives us precisely
what we are seeking. It allows us to find regulari-
ties in the embeddings, while factorizing them ac-
cording to these consistencies. The model also au-
tomatically optimizes the number of and relation-
ship between latent slots, rather than setting these
a priori.
</p>
<p>Other desiderata are encoded as well, including
the fact that the the matrix Z remains sparse, while
the frequency of slots follows a power-law distri-
bution proportional to Poisson(α). In practise, this
captures the power-law distribution of relational
slots in real-world semantic lexicons such as Prop-
Bank (Palmer et al., 2005). All of these properties
stem directly from the choice of prior, and are a
natural consequence of using an IBP.
</p>
<p>In this paper, we use a linear-Gaussian model as
the likelihood function. This is a popular model
that has been applied to several problems, and for
which different approximate inference strategies
have been developed (Doshi-Velez et al., 2009;
Doshi-Velez and Ghahramani, 2009). According
to his model, the predicate embeddings are dis-
tributed as:
</p>
<p>vi ∼ Gaussian(ziW,σ2V I) (4)
where W is a K × d matrix of weights and σV is
a hyperparameter.
</p>
<p>For a detailed derivation of the posterior of
an IBP prior with a linear-Gaussian likelihood,
we point the reader to Griffiths and Ghahramani
(2011), who provide a meticulous summary.
</p>
<p>3.3 Optimization
</p>
<p>Since our objective in equation 1 contains two dis-
tinct components, we can optimize using alter-
nating maximization. Although guaranteed con-
vergence for this technique only exist for con-
vex functions, it has proven successful even for
non-convex problems (Jain et al., 2013; Netrapalli
et al., 2013).
</p>
<p>We thus alternate between keeping Z fixed and
optimizing the parameters V,U, S in the likeli-
hood component of section 3.1, and keeping V
fixed and optimizing the parameters Z in the pos-
terior regularization component of section 3.2.
</p>
<p>In practise, the likelihood component is opti-
mized using negative sampling with EM for the
latent slots. In particular we use hard EM, to se-
lect a single slot before taking gradient steps with
respect to the model parameters. This was shown
to work well for Skip-gram style models with la-
tent variables by Jauhar et al. (2015).
</p>
<p>In the E-Step we find the best latent slot for a
particular predicate-argument pair:
</p>
<p>k̂ = arg max
k
</p>
<p>Pr(ai|pi, zik, sk) (5)
</p>
<p>We follow this by making stochastic gradient
updates to the model parameters U, V, S in the M-
Step using the negative sampling objective:
</p>
<p>log zik̂σ
(
(vi � sk̂) · ui
</p>
<p>)
+∑
</p>
<p>l
</p>
<p>Eai′∼Prn(a)
[
log zik̂σ
</p>
<p>(
(vi � sk̂) · ui′
</p>
<p>)]
(6)
</p>
<p>where σ(·) is the sigmoid function, Prn(a) is a
unigram noise distribution over argument types
and l is the negative sampling parameter.
</p>
<p>As for optimizing the posterior regularization
component, an approximate inference technique
such as Gibbs sampling must be used. In Gibbs
sampling we iteratively sample individual zik
terms from the posterior:
</p>
<p>Pr(zik|X,Z−ik) ∝ Pr(X|Z) ·Pr(zik|Z−ik)
(7)
</p>
<p>where Z−ik is the Markov blanket of zik in Z. The
prior and likelihood terms are respectively those of
equations 3 and 4. Doshi-Velez and Ghahramani
(2009) present an accelerated version of Gibbs
sampling for this model, that computes the like-
lihood and prior terms efficiently. We use this ap-
proach in our work since it has the benefits of mix-
ing like a collapsed sampler, while maintaining the
running time of an uncollapsed sampler.
</p>
<p>In conclusion, the optimization steps iteratively
refine the parameters V,U, S to be better predic-
tors of the corpus, while Z is updated to best fac-
torize the regularities in the predicate embeddings
V , thereby capturing better relational slots.
</p>
<p>3.4 Relational Variant
</p>
<p>In addition to the standard model introduced
above, we also experiment with an extension
</p>
<p>212</p>
<p />
</div>
<div class="page"><p />
<p>where the input corpus consists of predicate-
argument-relation triples instead of just predicate-
argument pairs. These relations are observed rela-
tions, and should not be confused with the latent
slots of the model.
</p>
<p>To accommodate this change we modify the ar-
gument embedding matrix U to be of dimensions
m× d2 and introduce a new q× d2 embedding matrix
R = {ri | 1 ≤ i ≤ q} for the q observed relation
types.
</p>
<p>Then, wherever the original model calls for
an argument vector ui (which had dimensional-
ity d) we instead replace it with a concatenated
argument-relation vector [ui; rj ] (which now also
has dimensionality d). During training, we must
make gradient updates to R in addition to all the
other model parameters as usual.
</p>
<p>While this relation indicator can be used to cap-
ture arbitrary relational information, in this paper
we set it to a combination of the directionality of
the argument with respect to the predicate (L or
R), and the preposition immediately preceding the
argument phrase (or None if there isn’t one). Thus,
for example, we have relational indicators such as
“L-on”, “R-before”, “L-because”, “R-None”, etc.
We obtain a total of 146 such relations.
</p>
<p>Note, that in keeping with the goals of this
work, these relation indicators still require no an-
notation (prepositions are closed-class words than
can be enumerated).
</p>
<p>4 Experiments and Evaluation
</p>
<p>In what follows, we detail experimental results
on two quantitative evaluation tasks: at the lo-
cal and global levels of predicate-argument struc-
ture. In particular we evaluate on pseudo disam-
biguation of selectional preference, and seman-
tic frame lexicon overlap. We also qualitatively
inspect the learned latent relations against hand-
annotated roles. We first specify the implementa-
tional details.
</p>
<p>4.1 Implementational Details
</p>
<p>We begin by pre-training standard skip-gram vec-
tors (Mikolov et al., 2013) on the NY-Times sec-
tion of the Gigaword corpus, which consists of ap-
proximately 1.67 billion word tokens. These vec-
tors are used as initialization for the embedding
matrices V and U , before our iterative optimiza-
tion. While this step is not strictly required, we
found that it leads to generally better results than
</p>
<p>random initialization given the relatively small
size of our predicate-argument training corpus.
</p>
<p>For training our models, we use a combina-
tion of the training data released for the CoNLL
2008 shared task (Surdeanu et al., 2008) and the
extended PropBank release which covers annota-
tions of the Ontonotes (Hovy et al., 2006) and En-
glish Web Treebank (Bies et al., 2012) corpora.
We reserve the test portion of the CoNLL 2008
shared task data for one of our evaluations.
</p>
<p>In this work, we only focus on verbal predicates.
Our training data gives us a vocabulary of 4449
predicates, after pruning verbs that occur fewer
than 5 times.
</p>
<p>Then, from the training data we extract all
predicate-argument pairs using gold standard ar-
gument annotations, for the sake of simplicity.
Note that previous unsupervised frame induction
work also uses gold argument mentions (Lang
and Lapata, 2011a; Titov and Klementiev, 2012b).
Our method, however, does not depend on this,
or any other annotation, and we could as easily
use the output from an automated system such as
Abend et al. (2009) instead.
</p>
<p>In this manner, we obtain a total of approx-
imately 3.35 million predicate-argument word
pairs on which to train.
</p>
<p>Using this data we train a total of 4 distinct
models: a base model and a relational variant (see
Section 3.4), both of which are trained with two
different IBP hyperparameters of α = 0.35 and
α = 0.7. The hyperparameter controls the avidity
of the model for latent slots (a higher α implies a
greater number of induced slots).
</p>
<p>This results in the learned number of slots rang-
ing from 17 to 30, with the conservative model av-
eraging about 4 latent slots per word, while the
permissive model averaging about 6 latent slots
per word.
</p>
<p>Since our objective is non-convex we record the
training likelihood at each power iteration (includ-
ing an optimization over both the predictive and
IBP components of our objective), and save the
model with the highest training likelihood.
</p>
<p>We set our embedding size to d = 100 and, after
training, obtain latent slot factors ranging in num-
ber from 15 to 30.
</p>
<p>213</p>
<p />
</div>
<div class="page"><p />
<p>Model α Variant k slots % Acc
Skip-gram - - - 0.77
</p>
<p>pa2IBPVec
0.35
</p>
<p>Standard 17 0.81
Relational 15 0.84
</p>
<p>0.7
Standard 27 0.81
</p>
<p>Relational 30 0.81
</p>
<p>Table 1: Results on pseudo disambiguation of se-
lectional preference. Numbers are in % accuracy
of distinguishing true arguments from false ones.
Our models all outperform the skip-gram baseline.
</p>
<p>4.2 Pseudo Disambiguation of Selection
Preference
</p>
<p>The pseudo disambiguation task aims to evaluate
our models’ ability to capture predicate-argument
knowledge at the local level. In this task, systems
are presented with a set of triples: a predicate, a
true argument and a fake argument. The systems
are evaluated on the percentage of true arguments
they are able to select.
</p>
<p>For example, given a triple:
</p>
<p>resign, post, liquidation
</p>
<p>a successful model should rate the pair “resign-
post” higher than “resign-liquidation”.
</p>
<p>This task has often been used in the selectional
preference modelling literature as a benchmark
task (Rooth et al., 1999; Van de Cruys, 2014) .
</p>
<p>To obtain the triples for this task we use the test
set of the CoNLL 2008 shared task data. In par-
ticular, for every verbal predicate mention in the
data we select a random nominal word from each
of its arguments phrase chunks to obtain a true
predicate-argument word pair. Then, to introduce
distractors, we sample a random nominal from a
unigram noise distribution. In this way we obtain
9859 pseudo disambiguation triples as our test set.
</p>
<p>We use our models to score a word pair by tak-
ing the probability of the pair under our model,
using the best latent slot:
</p>
<p>max
k
</p>
<p>zikσ ((vi � sk) · ui) (8)
</p>
<p>where vi and ui are predicate and argument em-
beddings respectively, zik is the binary indicator
of the k’th slot for the i’th predicate, and sk is the
slot specific weight vector. The argument in the
higher scoring pair is selected as the correct one.
</p>
<p>In the relational variant, instead of the single ar-
gument vector ui we also take a max over the re-
lation indicators – since the exact indicator is not
observed at test time.
</p>
<p>We compare our models against a standard skip-
gram model (Mikolov et al., 2013) trained on the
same data. Word pairs in this model are scored us-
ing the dot product between their associated skip-
gram vectors.
</p>
<p>This is a fair comparison since our models as
well as the skip-gram model have access to the
same data – namely predicates and their neigh-
boring argument words. They are trained on their
ability to discriminate true argument words from
randomly sampled noise. The evaluation then,
is whether the additionally learned slot structure
helps in differentiating true arguments from noise.
The results of this evaluation are presented in Ta-
ble 1.
</p>
<p>The results show that all our models outperform
the skip-gram baseline. This demonstrates that
the added structural information gained from la-
tent slots in fact help our models to better capture
predicate-argument affinities in local contexts.
</p>
<p>The impact of latent slots or additional relation
information does not seem to impact basic perfor-
mance, however. This could be because of the
trade-off that occurs when a more complex model
is learned from the same amount of limited data.
</p>
<p>4.3 Frame Lexicon Overlap
</p>
<p>Next, we evaluate our models at their ability to
capture global predicate-argument structure. Pre-
vious work on frame induction has focussed on
evaluating instance-based argument overlap with
gold standard annotations in the context of se-
mantic role labelling (SRL). Unfortunately, be-
cause our models operate on individual predicate-
argument words rather than argument spans a fair
comparison becomes problematic.
</p>
<p>But unlike previous work, which clusters argu-
ment instances, our approach produces a model as
a result of training. We can thus directly evaluate
this model’s latent slot factors against a gold stan-
dard frame lexicon. Our evaluation framework is,
in many ways based on the metrics used in unsu-
pervised SRL, except applied at the “type” lexicon
level rather than the corpus-based “token” cluster
level.
</p>
<p>In particular, given a gold frame lexicon Ω with
K∗ real argument slots (i.e. the total number of
</p>
<p>214</p>
<p />
</div>
<div class="page"><p />
<p>Model α Variant
Coarse Fine
</p>
<p>PU CO F1 PU CO F1
Syntax - - 0.71 0.87 0.78 0.70 0.91 0.79
</p>
<p>pa2IBPVec
0.35
</p>
<p>Standard 0.76 0.89 0.82 0.76 0.97 0.85
Relational 0.73 0.90 0.81 0.73 0.97 0.83
</p>
<p>0.7
Standard 0.79 0.91 0.85 0.79 0.98 0.87
</p>
<p>Relational 0.80 0.92 0.85 0.80 0.98 0.88
</p>
<p>Table 2: Results on the lexicon overlap task. Our models outperform the syntactic baseline on all the
metrics.
</p>
<p>possible humanly assigned arguments in the lexi-
con), we evaluate our models’ latent slot matrix Z
in terms of its overlap with the gold lexicon.
</p>
<p>We define purity as the average proportion of
overlap between predicted latent slots and their
maximally similar gold lexicon slots:
</p>
<p>PU =
1
K
</p>
<p>∑
k
</p>
<p>max
k′
</p>
<p>1
n
</p>
<p>∑
i
</p>
<p>δ(ωik′ , zik) (9)
</p>
<p>where δ(·) is an indicator function. Given that the
ω’s and z’s we compare are binary values, this in-
dicator function is effectively an “XNOR” gate.
</p>
<p>Similarly we define collocation as the average
proportion of overlap between gold standard slots
and their maximally similar predicted latent slots:
</p>
<p>CO =
1
K∗
</p>
<p>∑
k′
</p>
<p>max
k
</p>
<p>1
n
</p>
<p>∑
i
</p>
<p>δ(ωik′ , zik) (10)
</p>
<p>Given, the purity and collocation metrics we
can define the F1 score as the harmonic mean of
the two:
</p>
<p>F1 =
2 · CO · PU
CO + PU
</p>
<p>(11)
</p>
<p>In our experiments we use the frame files pro-
vided with the PropBank corpus (Palmer et al.,
2005) as gold standard. We derive two variants
from the frame files.
</p>
<p>The first is a coarse-grained lexicon. In this
case, we extract only the functional arguments of
verbs in our vocabulary as gold standard slots.
These functions correspond to broad semantic ar-
gument types such as “prototypical agent”, “proto-
typical patient”, “instrument”, “benefactive”, etc.
A total of 16 gold slots are produced in this man-
ner, and are mapped to indices. For every verb
the corresponding binary ω vector marks the exis-
tence or not of the different functional arguments
according to the gold frame files.
</p>
<p>The second variant is a fine-grained lexicon.
Here, in addition to functional arguments we also
consider the numerical argument with which it is
associated, such as “ARG0”, “ARG1” etc. Note
that a single functional argument may appear with
more than one numerical slot with different verbs
over the entire lexicon. The fine-grained lexicon
yields 72 gold slots.
</p>
<p>We compare our models against a baseline in-
spired from the syntactic baseline often used for
evaluating unsupervised SRL models. For unsu-
pervised SRL, syntax has proven to be a difficult
to outperform baseline (Lang and Lapata, 2014).
</p>
<p>This baseline is constructed by taking the 21
most frequent syntactic labels in the training data
and associating them each with a slot. All other
syntactic labels are associated with a 22nd generic
slot. Given these slots, we associate a verbal pred-
icate with a specific slot if it takes on the corre-
sponding syntactic argument in the training data.
The results on the lexicon overlap task are pre-
sented in Table 2.
</p>
<p>They show that our models consistently outper-
form the syntactic baseline on all metrics in both
the coarse-grained and fine-grained settings. We
conclude that our models are better able to capture
predicate-argument structure at a global level.
</p>
<p>Inspecting and comparing the results of our dif-
ferent models seems to indicate that we perform
better when our IBP posterior allows for a greater
number of latent slots. This happens when the hy-
perparameter α = 0.7.
</p>
<p>Additionally our models consistently perform
better on the fine-grained lexicon than on the
coarse-grained one. The former itself does not
necessarily represent an easier benchmark, since
there is hardly any difference in the F1 score of
the syntactic baseline on the two lexicons.
</p>
<p>Overall it would seem that allowing for a greater
number of latent slots does help capture global
</p>
<p>215</p>
<p />
</div>
<div class="page"><p />
<p>Predicate
Latent Slot
</p>
<p>1 2 3 5 6 8 10 12
provide A0 A1 A2 A2
</p>
<p>enter A0 A1 AM-ADV
praise A0 A1 A2
travel A0 A0 AM-PNC AM-TMP
</p>
<p>distract A0 A1 A2
overcome AM-TMP A0 A0
</p>
<p>Table 3: Examples for several predicates with mappings of latent slots to the majority class of the closest
argument vector in the shared embedded space.
</p>
<p>predicate-argument structure better. This makes
sense, if we consider the fact that we are ef-
fectively trying to factorize a dense representa-
tion (the predicate embeddings) with IBP infer-
ence. Thus allowing for a greater number of latent
factors permits the discovery of greater structural
consistency within these embeddings.
</p>
<p>This finding does have some problematic impli-
cations, however. Increasing the IBP hyperparam-
eter α arbitrarily represents a computational bot-
tleneck since inference scales quadratically with
the number of latent slots K. There is also the
problem of splitting argument slots too finely,
which may result in optimizing purity at the ex-
pense of collocation. A solution to this trade-off
between performance and inference time remains
for future work.
</p>
<p>4.4 Qualitative Analysis of Latent Slots
</p>
<p>To better understand the nature of the latent slots
induced by our model we conduct an additional
qualitative analysis. The goal of this analysis is
to inspect the kinds of generalities about semantic
roles that our model is able to capture from com-
pletely unannotated data.
</p>
<p>Table 3 lists some examples of predicates and
their associated latent slots. The latent slots are
sorted according to their frequency (i.e. column
sum in the binary slot matrix Z). We map each la-
tent slot to the majority semantic role type – from
training data – of the closest argument word to the
predicate vector in the shared embedding space.
</p>
<p>The model for which we perform this qualita-
tive analysis is the standard variant with the IBP
hyperparameter set to α = 0.35; this model has
17 latent slots. Note that slots that do not feature
for any of the verbs are omitted for visual com-
pactness.
</p>
<p>There are several interesting trends to notice
</p>
<p>here. Firstly, the basic argument structure of pred-
icates is often correctly identified, when matched
against gold PropBank frame files. For example,
the core roles of “enter” identify it as a transitive
verb, while “praise”, “provide” and “distract” are
correctly shown as ditransitive verbs. Obviously
the structure isn’t always perfectly identified, as
with the verb “travel” where we are missing both
an “ARG1” and an “ARG2”.
</p>
<p>In certain cases a single argument type spans
multiple slots – as with “A2” for “provide” and
“A0” for “travel”. This is not surprising, since
there is no binding factor on the model to produce
one-to-one mappings with hand-crafted semantic
roles. Generally speaking, the slots represent dis-
tributions over hand-crafted roles rather than strict
mappings. In fact, to expect a one-to-one mapping
is unreasonable considering we use no annotations
whatsoever.
</p>
<p>Nevertheless, there is still some consistency in
the mappings. The core arguments of verbs – such
as “ARG0” and “ARG1” are typically mapped to
the most frequent latent slots. This can be ex-
plained by the fact that the more frequent argu-
ments tend to be the ones that are core to a predi-
cate’s frame. This is quite a surprising outcome of
the model, considering that it is given no annota-
tion about argument types. Of course, we do not
always get this right as can be seen with the case
of “overcome”, where a non-core argument occurs
in the most frequent slot.
</p>
<p>Since this is a data driven approach, we identify
non-core roles as well, if they occur with predi-
cates often enough in the data. For example we
have the general purpose “AM-ADV” argument
of “enter”, and the “ARG-PNC” and “ARG-TMP”
(purpose and time arguments) of the verb “travel”.
In future work we hope to explore methods that
might be able to automatically distinguish core
</p>
<p>216</p>
<p />
</div>
<div class="page"><p />
<p>slots from non-core ones.
In conclusion, our model show promise in that
</p>
<p>it is able to capture some interesting generalities
with respect to predicates and their hand-crafted
roles, without the need for any annotated data.
</p>
<p>5 Conclusion and Future Work
</p>
<p>We have presented a first attempt at learning
an embedded frame lexicon from data, using no
annotated information. Our approach revolves
around jointly capturing local predicate-argument
affinities with global slot-level consistencies. We
model this approach with a novel integration be-
tween a predictive embedding model and the pos-
terior of an Indian Buffet Process.
</p>
<p>We experiment with our model on two quantita-
tive tasks, each designed to evaluate performance
on capturing local and global predicate-argument
structure respectively. On both tasks we demon-
strate that our models are able to outperform base-
lines, thus indicating our ability to jointly model
the local and global level information of predicate-
argument structure.
</p>
<p>Additionally, we qualitatively inspect our in-
duced latent slots and show that we are able to
capture some interesting generalities with regards
to hand-crafted semantic role labels.
</p>
<p>There are several avenues of future work we are
exploring. Rather than depend on gold argument
mentions in training, we hope to fully automate
the pipeline to leverage much larger amounts of
data. With this greater data size, we also will likely
no longer need to break down argument spans into
individual words. Instead, we plan to models these
spans as chunks using an LSTM.
</p>
<p>With this additional modeling power we hope to
evaluate on downstream applications such as se-
mantic role labelling, and semantic parsing.
</p>
<p>In a separate line of work we hope to be able
to parallelize the Indian Buffet Process inference,
which remains a bottleneck of our current effort.
Speeding up this process will allow us to explore
more complex (and potentially better) models.
</p>
<p>Acknowledgments
</p>
<p>The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
</p>
<p>References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
</p>
<p>Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1. Association for Computational Linguistics, pages
28–36.
</p>
<p>Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 1. Association for Computational Linguis-
tics, pages 86–90.
</p>
<p>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English web treebank. Linguistic Data Con-
sortium, Philadelphia, PA .
</p>
<p>Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction.
arXiv preprint arXiv:1302.4813 .
</p>
<p>Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away ambiguity: Learning verb selectional
preference with bayesian networks. In Proceedings
of the 18th conference on Computational linguistics-
Volume 1. Association for Computational Linguis-
tics, pages 187–193.
</p>
<p>Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Human language technologies: The
2010 annual conference of the North American
chapter of the association for computational lin-
guistics. Association for Computational Linguistics,
pages 948–956.
</p>
<p>Finale Doshi-Velez and Zoubin Ghahramani. 2009.
Accelerated sampling for the indian buffet process.
In Proceedings of the 26th annual international con-
ference on machine learning. ACM, pages 273–280.
</p>
<p>Finale Doshi-Velez, Kurt T Miller, Jurgen Van Gael,
Yee Whye Teh, and Gatsby Unit. 2009. Variational
inference for the indian buffet process. In Proceed-
ings of the Intl. Conf. on Artificial Intelligence and
Statistics. volume 12, pages 137–144.
</p>
<p>Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Annual Meeting - Asso-
ciation For Computational Linguistics. volume 45,
page 216.
</p>
<p>Pascale Fung and Benfeng Chen. 2004. Biframenet:
bilingual frame semantics resource construction by
cross-lingual induction. In Proceedings of the 20th
international conference on Computational Lin-
guistics. Association for Computational Linguistics,
page 931.
</p>
<p>217</p>
<p />
</div>
<div class="page"><p />
<p>Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics 28(3):245–288.
</p>
<p>Trond Grenager and Christopher D Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 1–8.
</p>
<p>Thomas L Griffiths and Zoubin Ghahramani. 2005. In-
finite latent feature models and the indian buffet pro-
cess. In NIPS. volume 18, pages 475–482.
</p>
<p>Thomas L Griffiths and Zoubin Ghahramani. 2011.
The indian buffet process: An introduction and
review. Journal of Machine Learning Research
12(Apr):1185–1224.
</p>
<p>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the human lan-
guage technology conference of the NAACL, Com-
panion Volume: Short Papers. Association for Com-
putational Linguistics, pages 57–60.
</p>
<p>Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.
2013. Low-rank matrix completion using alternat-
ing minimization. In Proceedings of the forty-fifth
annual ACM symposium on Theory of computing.
ACM, pages 665–674.
</p>
<p>Sujay Kumar Jauhar, Chris Dyer, and Eduard H Hovy.
2015. Ontologically grounded multi-sense represen-
tation learning for semantic vector space models. In
HLT-NAACL. pages 683–693.
</p>
<p>Daisuke Kawahara, Daniel Peterson, Octavian
Popescu, Martha Palmer, and Fondazione Bruno
Kessler. 2014. Inducing example-based semantic
frames from a massive amount of verb uses. In
EACL. pages 58–67.
</p>
<p>Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1. Association for Com-
putational Linguistics, pages 1117–1126.
</p>
<p>Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
Proceedings of the conference on empirical meth-
ods in natural language processing. Association for
Computational Linguistics, pages 1320–1331.
</p>
<p>Joel Lang and Mirella Lapata. 2014. Similarity-
driven semantic role induction via graph partition-
ing. Computational Linguistics 40(3):633–669.
</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
</p>
<p>Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.
2013. Phase retrieval using alternating minimiza-
tion. In Advances in Neural Information Processing
Systems. pages 2796–2804.
</p>
<p>Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational linguistics 31(1):71–
106.
</p>
<p>Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How. Washington, DC, pages 52–
57.
</p>
<p>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet allocation method for selectional prefer-
ences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, pages 424–
434.
</p>
<p>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics. Association for Computational
Linguistics, pages 104–111.
</p>
<p>Diarmuid O Séaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, pages 435–444.
</p>
<p>Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP-
CoNLL. pages 12–21.
</p>
<p>Mihai Surdeanu, Sanda Harabagiu, John Williams,
and Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting on Association
for Computational Linguistics-Volume 1. Associa-
tion for Computational Linguistics, pages 8–15.
</p>
<p>Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the Twelfth
Conference on Computational Natural Language
Learning. Association for Computational Linguis-
tics, pages 159–177.
</p>
<p>Ivan Titov and Alexandre Klementiev. 2012a. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, pages 12–22.
</p>
<p>Ivan Titov and Alexandre Klementiev. 2012b.
Crosslingual induction of semantic roles. In
Proceedings of the 50th Annual Meeting of the
</p>
<p>218</p>
<p />
</div>
<div class="page"><p />
<p>Association for Computational Linguistics: Long
Papers-Volume 1. Association for Computational
Linguistics, pages 647–656.
</p>
<p>Tim Van de Cruys. 2014. A neural network approach
to selectional preference acquisition. In EMNLP.
pages 26–35.
</p>
<p>Kristian Woodsend and Mirella Lapata. 2015. Dis-
tributed representations for unsupervised semantic
role labeling. In EMNLP. Citeseer, pages 2482–
2491.
</p>
<p>219</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 220–229,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Generating Pattern-Based Entailment Graphs for Relation Extraction
</p>
<p>Kathrin Eichler, Feiyu Xu, Hans Uszkoreit, Sebastian Krause
German Research Center for Artificial Intelligence (DFKI)
</p>
<p>Berlin, Germany
firstname.lastname@dfki.de
</p>
<p>Abstract
</p>
<p>Relation extraction is the task of recogniz-
ing and extracting relations between enti-
ties or concepts in texts. A common ap-
proach is to exploit existing knowledge
to learn linguistic patterns expressing the
target relation and use these patterns for
extracting new relation mentions. De-
riving relation patterns automatically usu-
ally results in large numbers of candi-
dates, which need to be filtered to de-
rive a subset of patterns that reliably ex-
tract correct relation mentions. We ad-
dress the pattern selection task by exploit-
ing the knowledge represented by entail-
ment graphs, which capture semantic rela-
tionships holding among the learned pat-
tern candidates. This is motivated by the
fact that a pattern may not express the
target relation explicitly, but still be use-
ful for extracting instances for which the
relation holds, because its meaning en-
tails the meaning of the target relation.
We evaluate the usage of both automati-
cally generated and gold-standard entail-
ment graphs in a relation extraction sce-
nario and present favorable experimental
results, exhibiting the benefits of structur-
ing and selecting patterns based on entail-
ment graphs.
</p>
<p>1 Introduction
</p>
<p>The task of relation extraction (RE) is to recog-
nize and extract relations among entities or con-
cepts mentioned in texts. One common approach
to RE is to learn and exploit extraction patterns
(e.g., based on syntactic dependency trees), which
express the targeted semantic relations. In order
to circumvent the manual creation of patterns, nu-
</p>
<p>merous approaches have been investigated to de-
rive patterns automatically. Automatic methods
generally induce large numbers of unique candi-
date patterns, which only potentially express the
target relation and need to be filtered in order to
derive a subset of high-quality patterns for the re-
lation extraction task. The task of filtering or se-
lecting patterns can be tackled in various ways,
e.g., based on frequency information, or by apply-
ing syntactic or semantic criteria.
</p>
<p>For many RE applications, such as knowledge
base population, patterns are not only relevant if
they express the target relation explicitly, but also
if they extract facts from which the target relation
can be inferred. For example, all patterns below
can be utilized for extracting pairs of people who
are or were involved in a marriage relation:
</p>
<p>P1: PERSON1 &lt;marry&gt; PERSON21
</p>
<p>P2: PERSON1 &lt;be widow of&gt; PERSON2
</p>
<p>P3: PERSON1 &lt;divorce from&gt; PERSON2
</p>
<p>However, only pattern P1 expresses the target
relation explicitly. Patterns P2 and P3 are seman-
tically different from P1, but express a fact that
entails the marriage relation2. As being aware of
these semantic relationships holding among pat-
terns can be of help in the pattern selection pro-
cess, we propose to capture and exploit these re-
lationships using pattern-based entailment graphs
and show how technology from the area of recog-
nizing textual entailment can be adapted to auto-
matically generate these graphs. Finally, we apply
the generated knowledge for relation extraction.
</p>
<p>1PERSONx refers to a slot filler for a person recognized in
the input from which the pattern was extracted, &lt;text&gt; refers
to a normalized form of the text part of the extracted pattern.
</p>
<p>2We ignore tense aspects for the time being, which is
also the approach taken in the RTE challenges (Dagan et al.,
2005).
</p>
<p>220</p>
<p />
</div>
<div class="page"><p />
<p>2 Related Work
</p>
<p>The task of estimating the quality of automatically
learned extraction patterns has been dealt with
in various ways, for example based on integrity
constraints (Agichtein, 2006), frequency heuris-
tics (Krause et al., 2012) or lexical semantic crite-
ria (Moro et al., 2013). Another line of research
in RE groups similar patterns, e.g., by merging
patterns based on syntactic criteria (Banko et al.,
2007; Shinyama and Sekine, 2006; Thomas et al.,
2011; Angeli et al., 2015), by clustering patterns
that are semantically related (Kok and Domingos,
2008; Yates and Etzioni, 2009; Yao et al., 2011),
or by identifying patterns associated to a given
seed relation (Bauer et al., 2014). Such approaches
help gain generalization; however, their ability to
express semantic relationships is limited, as they
cannot capture the asymmetric nature of these re-
lationships. For example, clustering can help us
identify pattern P4 below as being semantically re-
lated to patterns P1 to P3 in section 1.
</p>
<p>P4: PERSON1 &lt;love&gt; PERSON2
</p>
<p>However, it falls short of expressing that two en-
tities linked by patterns P1 to P3 are mentions of
the marriage relation, whereas this is not necessar-
ily true of entities linked by pattern P4. Similarly,
clustering can identify patterns P1 and P3 as se-
mantically related. However, it cannot express that
the relation expressed by pattern P3 entails the re-
lation expressed by pattern P1, but not vice versa.
These asymmetric relationships have been consid-
ered by Riedel et al. (2013), who learns latent fea-
ture vectors for patterns based on matrix factoriza-
tion, and have also been studied extensively in the
context of recognizing textual entailment (RTE).
RTE is the task of determining, for two textual ex-
pressions T (text) and H (hypothesis), whether the
meaning of H can be inferred from the meaning
of T (Dagan and Glickman, 2004). In RE, RTE
systems have been applied to validate a given re-
lation instance (Wang and Neumann, 2008) and
to extract instances entailing a given target rela-
tion (Romano et al., 2006; Bar-Haim et al., 2007;
Roth et al., 2009).
</p>
<p>As illustrated above, RE can clearly benefit
from considering semantic relationships holding
among extraction patterns. However, previous
work in RE has either focussed on grouping re-
lated patterns without considering non-symmetric
relations, or, on computing entailment decisions
</p>
<p>for individual T/H pairs. We propose to exploit en-
tailment relationships holding among RE patterns
by structuring the candidate set in an entailment
graph. Entailment graphs are hierarchical struc-
tures representing entailment relations among tex-
tual expressions and have previously been gener-
ated for various types of expressions (Berant et al.,
2010, 2012; Mehdad et al., 2013; Levy et al., 2014;
Kotlerman et al., 2015). Entailment graphs can
be constructed by determining entailment relation-
ships between pairs of expressions or, as proposed
by Kolesnyk et al. (2016), by generating entailed
sentences from source sentences. Our work of
building entailment graphs based on RE patterns
is related to the work by Nakashole et al. (2012),
who create a taxonomy of binary relation patterns.
For their syntactic patterns, they compute partial
orders of generalization and subsumption based on
the set of mentions extracted by each pattern. In
contrast to their work, we construct pattern-based
entailment graphs using RTE technology. This is
motivated by the fact that entailment is semantic
and not mention-based, i.e., one pattern can entail
another pattern even if they extract disjoint sets of
mentions in a given text corpus.
</p>
<p>3 Entailment Graph Generation
</p>
<p>3.1 Pattern-Based Entailment Graphs
</p>
<p>A pattern-based entailment graph refers to a di-
rected graph, in which each node represents a
unique RE pattern, and each edge (→) denotes an
entailment relationship. Bidirectional edges (↔)
denote that the patterns represented by the two
nodes are considered semantically equivalent. A
sample subgraph for the marriage relation is given
in Figure 13, which shows all entailment relations
with respect to the pattern [PERSON1 &lt;marry&gt;
PERSON2]. Automatic entailment graph gener-
ation is usually performed in two steps: First,
entailment decisions for individual T/H pairs are
computed (using an RTE engine); second, an opti-
mization strategy is applied to derive a consistent,
transitive graph (Berant et al., 2010).
</p>
<p>3.2 RTE Engine
</p>
<p>For recognizing entailment relations between in-
dividual T/H pairs of patterns, we make use
</p>
<p>3For reasons of simplicity, the figure shows the text repre-
sentation of the patterns, which are in fact represented as de-
pendency structures. Since entailment is transitive, all edges
are omitted that can be recovered in the transitive closure.
</p>
<p>221</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Subgraph showing entailment relations for the pattern [PERSON1 &lt;marry&gt; PERSON2]
</p>
<p>of an RTE engine based on multi-level align-
ments. This RTE engine, referred to as Multi-
Align, is available through the RTE platform EX-
CITEMENT (Magnini et al., 2014) and achieved
state-of-the-art performance on several RTE cor-
pora (Noh et al., 2015). We opted for this RTE
system because it makes use of external knowl-
edge resources and, unlike more recent systems
based on neural networks (Rocktäschel et al.,
2015; Bowman et al., 2015), is able to cope with
the restricted amount of training data available
for the task. MultiAlign uses shallow parsing
for linguistic preprocessing and logistic regres-
sion for entailment classification. Features for
the classifier are generated on the basis of multi-
level alignments using four aligners: a lemma
aligner (aligning identical lemmas found in T and
H), an aligner based on the paraphrase tables
provided by the METEOR MT evaluation pack-
age (Denkowski and Lavie, 2014), and two lexi-
cal aligners based on Wordnet (Fellbaum, 1998)4
</p>
<p>and VerbOcean (Chklovski and Pantel, 2004). As
output, it produces a binary decision (entailment,
non-entailment) along with a computed confi-
dence score.
</p>
<p>As the RTE engine was originally designed for
sentences, rather than patterns, we converted each
pattern into its textual representation. The vari-
ables expressing type and semantic role of the enti-
ties linked by the pattern were excluded in this rep-
</p>
<p>4Relations considered by the WordNet aligner: synonym,
derivationally related, (instance) hypernym, member / part
holonym, entailment, and substance meronym.
</p>
<p>resentation, as the resulting variable alignments
would skew the RTE engine’s entailment deci-
sion. For our experiments, we used the original
MultiAlign implementation as well as an adapted
version, in which we made some changes to the
WordNet aligner. In particular, unlike in the orig-
inal implementation, which only considered the
first sense of each WordNet entry, we extended
this to cover all senses. This allowed us to re-
trieve additional relevant alignments such as wed
↔ marry. In addition, rather than retrieving rules
for all words in T, we only consider rules for full
verbs, nouns, and adjectives. This way, we par-
ticularly filter out rules for auxiliary verbs, which
tend to produce irrelevant alignments, especially
when considering all senses. A sample set of deci-
sions produced by our RTE engine among candi-
date patterns for the marriage relation is depicted
in Figure 2.
</p>
<p>3.3 Graph Optimization
</p>
<p>Automatically derived entailment decisions may
contradict each other. For example, as illus-
trated in Figure 2, our RTE engine correctly
decides that [PERSON1 &lt;spouse of&gt; PERSON2]
→ [PERSON1 &lt;’s marriage to&gt; PERSON2] and
that [PERSON1 &lt;’s marriage to&gt; PERSON2] →
[PERSON1 &lt;marry&gt; PERSON2]. However, it
misses the entailment relation between [PERSON1
&lt;spouse of&gt; PERSON2] and [PERSON1 &lt;marry&gt;
PERSON2], because the relationship between
spouse and marry is not covered by the seman-
tic resources underlying the system. This leads
</p>
<p>222</p>
<p />
</div>
<div class="page"><p />
<p>Figure 2: Sample set of RTE decisions (YES: entailment, NO: no entailment) with associated confidence
</p>
<p>Figure 3: Sample outputs using greedy (left) and global (right) graph optimizer
</p>
<p>to a set of decisions that is invalid given the tran-
sitivity of the entailment relation. For deriving a
consistent graph, we applied two different strate-
gies: First, a simple greedy strategy that assumes
each computed positive entailment relation with a
confidence exceeding a pre-defined threshold to be
valid, and adds missing entailment edges to ensure
transitive closure. Second, the global graph opti-
mization algorithm by Berant et al. (2012), which
searches for the best graph under a global transitiv-
ity constraint, approaching the optimization prob-
lem by Integer Linear Programming. The selec-
tion of the optimization strategy is crucial, as il-
lustrated in Figure 3, which shows two sample out-
puts from each of the two strategies for the deci-
sions in Figure 2.
</p>
<p>4 Applying Pattern-Based Entailment
Graphs for Relation Extraction
</p>
<p>In order to exploit entailment graphs for relation
extraction, we propose the following approach,
which is depicted in Figure 4:
</p>
<p>1. Create a set of candidate extraction patterns P
(applying any method of choice).
</p>
<p>2. Generate an entailment graph EG expressing
entailment relations among the patterns in P .
</p>
<p>3. Choose a base pattern5, expressing the target re-
lation explicitly and select all patterns entailing
the base pattern according to EG.
</p>
<p>4. Apply the selected patterns to extract relation
mentions.
</p>
<p>Given the sample graph in Figure 1 and the
base pattern H: [PERSON1 &lt;marry&gt; PERSON2],
our method would select all patterns entailing H ,
either directly or via transitivity, e.g., [PERSON1
&lt;be ex-husband of&gt; PERSON2], including pat-
terns considered semantically equivalent, such as
[PERSON1 &lt;wed&gt; PERSON2]. It would neither se-
lect [PERSON1 &lt;love&gt; PERSON2], as it has no en-
tailment relation to H , nor [PERSON1 &lt;be in rela-
tionship with&gt; PERSON2], as it is entailed by, but
not equivalent to H .
</p>
<p>5Note that the selection of a base pattern can be done man-
ually, but can also be automated. For example, for the rela-
tions at hand, the most frequent pattern candidate learned for
a particular relation turned out to be an appropriate choice.
</p>
<p>223</p>
<p />
</div>
<div class="page"><p />
<p>Figure 4: Procedure for relation extraction using pattern selection based on entailment graphs
</p>
<p>5 Experiments
</p>
<p>For evaluating our method on the relation extrac-
tion task, we conducted experiments on two freely
available datasets: TEG-REP (Eichler et al., 2016)
and FB15k-237 (Toutanova et al., 2015). On the
TEG-REG dataset, we carried out a detailed evalu-
ation of several pattern filtering strategies with re-
spect to two semantic relations. On the FB15k-237
corpus, we evaluate the scalability of our method
to other semantic relations.
</p>
<p>5.1 TEG-REP
The TEG-REP corpus contains automatically de-
rived relation extraction patterns as well as gold-
standard entailment graphs created from these pat-
terns for three relations typically considered in RE
tasks: marriage, acquisition, and award honor.
The patterns underlying this corpus are a subset
of the patterns used by Moro et al. (2013) and
were acquired automatically using the pattern dis-
covery system by Krause et al. (2012). The sys-
tem derives candidate patterns from dependency-
parsed sentences extracted using distant super-
vision based on relation instances from the fact
knowledge base Freebase (Bollacker et al., 2008).
The TEG-REP corpus is the only available cor-
pus of pattern-based entailment graphs and par-
ticularly suitable for our evaluation because it al-
lows for a comparison of patterns selected based
on both manually and automatically created entail-
ment graphs. For our experiments on this corpus,
we divided the full set of patterns in the corpus
(around 500 per relation) into two equally-sized
</p>
<p>portions, one for creating training data for the RTE
engine, and one for evaluating pattern selection
methods. For creating an evaluation dataset, we
applied all patterns in the evaluation split to 14.5
million ClueWeb sentences (Lemur Project, 2009)
linked to Freebase entities (Gabrilovich et al.,
2013), and manually annotated around 3000 of the
extracted mentions.6 A mention was annotated
as being correct if we found evidence for the tar-
get relation between the entities in the mention to
hold. Evidence was drawn either from the source
sentence itself, or, in cases were the source sen-
tence did not express the relation explicitly, from
external resources such as Freebase or Wikipedia.
</p>
<p>Our experiments on the TEG-REP dataset are
based on the relations marriage and acquisition7.
For our experiments, we split the evaluation set
into a development set for optimizing the graph
building parameters and a test set for the final
evaluation. In our experiments, we tested several
strategies for selecting patterns and measured per-
formance over the annotated relation mentions in
the evaluation dataset. For evaluating the graph-
based methods, we selected all patterns entailing
the base patterns [PERSON1 &lt;marry&gt; PERSON2]
(for marriage) and [ORGANIZATION1 &lt;acquire&gt;
</p>
<p>6The annotation was done by three annotators. About
10% of the mentions were annotated by two annotators in
parallel, who achieved a very high interannotator agreement
(Cohens Kappa &gt; 0.9). The remaining mentions were anno-
tated by a single person.
</p>
<p>7We did not evaluate the award honor relation because
the vast majority (&gt; 98%) of mentions extracted using these
patterns were correct, which would not have allowed for a
meaningful evaluation.
</p>
<p>224</p>
<p />
</div>
<div class="page"><p />
<p>ORGANIZATION2] (for acquisition). In order to
investigate the benefits of the graph structure, we
compared the results to those achieved when com-
puting entailment relations at a pair-wise level,
i.e., using the base pattern of the relation as H
and all other candidate patterns for the relation as
T. We also applied the approach by Moro et al.
(2013), who identify relation-relevant word senses
based on lexical semantic subgraphs derived from
BabelNet and filter out patterns not containing any
relevant word sense. Based on a parameter k, they
consider a word sense to be relevant, if it is at most
k-step distant to the core word sense for the target
relation.
</p>
<p>5.2 FB15k-237
</p>
<p>As training the RTE models requires appropriate
training data, which may not be available, we ran
additional experiments to investigate if the mod-
els trained on T/H pairs for one relation are gen-
eral enough to be used for computing entailment
relations among pattern candidates for other se-
mantic relations. To this end, we used the FB15k-
237 corpus (Toutanova et al., 2015), which con-
tains knowledge-base relation triples and textual
mentions of Freebase entity pairs. For our ex-
periments on this corpus, we generated candidate
patterns by extracting the first 1000 tuples match-
ing a particular relation from the pattern files in
the corpus, and then extracting all patterns link-
ing any of the tuples in the textual triples used by
Toutanova and Chen (2015). This way, our candi-
date pattern set contains both patterns expressing
the target relation as well as patterns expressing
other relations. For creating the entailment graph,
we converted all patterns into a textual represen-
tation, removed patterns with no lexical item, and,
from the remaining patterns, built an entailment
graph applying the RTE engine described in 3.2
with the model trained on the marriage relation
and the best parameter setting derived based on
the TEG-REP corpus. For evaluating the result,
we selected 10 relations, defined a base pattern for
each of them, and checked, for each pattern in the
graph, whether it entailed the base pattern accord-
ing to the graph structure and whether the entail-
ment decision was correct based on the semantics
expressed by the pattern.
</p>
<p>As in this setting, we evaluated the entailment
relations expressed by the pattern graph rather
than the usage of the patterns for relation extrac-
</p>
<p>tion, the results are not directly comparable to the
figures obtained on the TEG-REP corpus, but still
allow for an assessment of the quality of the se-
lected patterns.
</p>
<p>6 Results and Discussion
</p>
<p>6.1 TEG-REP
Table 1 shows results on the TEG-REP corpus and
contains, for each of the following pattern selec-
tion methods, the computed precision, recall, and
F1 scores:
</p>
<p>• All patterns All patterns from the test split
(baseline).
</p>
<p>• Lexical semantic filter (Moro et al., 2013) Pat-
terns selected using the lexical semantic filter.
</p>
<p>• Pair-wise entailment (MultiAlignAdapted) Pat-
terns selected based on pair-wise entailment de-
cisions using the model of MultiAlignAdapted.
</p>
<p>• Entailment Graph (MultiAlignOriginal / Mul-
tiAlignAdapted) Patterns selected based on en-
tailment graphs generated with the original /
adapted MultiAlign implementation.
</p>
<p>• Entailment Graph (TEG-REP gold standard)
Patterns selected based on gold-standard entail-
ment graphs from the TEG-REP corpus.
</p>
<p>For the lexical semantic filter method, we ex-
perimented with different levels of k and noted
down the value achieving the best F1 score. The
results in the table were produced setting k to 1 for
the marriage relation and k to 5 for the acquisi-
tion relation. For the RTE-based methods, we ex-
perimented with the two different graph optimiza-
tion strategies and, for each of them, with different
confidence threshold values, and optimized these
parameters based on the development split. The
figures in the table show the results achieved on
the test split using the parameter setting optimized
on the development set: the greedy optimization
strategy with thresholds of 0.71 (MultiAlignOrig-
inal) and 0.77 (MultiAlignAdapted) for the mar-
riage relation and thresholds of 0.74 (MultiAlig-
nOriginal) and 0.75 (MultiAlignAdapted) for the
acquisition relation. On our data, the greedy edge
selection strategy produced better results than the
global graph optimizer for both relations. This was
because the global strategy, even with low con-
fidence thresholds, was more restrictive and re-
moved too many edges from the graph, thus yield-
ing lower recall figures.
</p>
<p>225</p>
<p />
</div>
<div class="page"><p />
<p>Configuration Precision Recall F1
All patterns 0.15 1.00 0.27
Lexical semantic filter (Moro et al., 2013) 0.61 0.73 0.67
Pair-wise entailment (MultiAlignAdapted) 0.97 0.56 0.71
Entailment Graph (MultiAlignOriginal) 0.96 0.59 0.73
Entailment Graph (MultiAlignAdapted) 0.96 0.68 0.80
Entailment Graph (TEG-REP gold standard) 0.96 0.69 0.80
</p>
<p>Table 1: Results for marriage relation (TEG-REP corpus)
</p>
<p>Configuration Precision Recall F1
All patterns 0.30 1.00 0.46
Lexical semantic filter (Moro et al., 2013) 0.30 0.97 0.46
Pair-wise entailment (MultiAlignAdapted) 0.82 0.49 0.62
Entailment Graph (MultiAlignOriginal) 0.81 0.53 0.64
Entailment Graph (MultiAlignAdapted) 0.59 0.93 0.73
Entailment Graph (TEG-REP gold standard) 0.82 0.49 0.62
</p>
<p>Table 2: Results for acquisition relation (TEG-REP corpus)
</p>
<p>For both relations, the best overall results were
achieved using our proposed method based on en-
tailment graphs generated automatically applying
the adapted RTE engine. The results show that
entailment-based pattern selection is in fact more
powerful than the lexical semantic filter. It selects
patterns yielding a much higher precision because
it is able to successfully filter out non-entailing
patterns, such as [PERSON1 &lt;be in relationship
with&gt; PERSON2] for the marriage relation, which
are wrongly selected using the lexical semantic fil-
ter. For the marriage relation, the results not only
show that our RTE engine adaptations yielded a
much higher recall (with almost no loss in preci-
sion) than the original implementation (thanks to
an increased number of relevant alignments), but
also that pattern selection can in fact benefit from
the graph structure: Entailment graphs created us-
ing MultiAlignAdapted achieved much better per-
formance than a selection based on pair-wise en-
tailment computation using the same RTE model.
This was due to a higher recall achieved because
the graph structure allowed the algorithm to iden-
tify entailment relations that involved the combi-
nation of several inference steps and were missed
when applying RTE in a pair-wise manner. An ex-
ample is the relationship between wife and marry,
as shown below:
</p>
<p>wife
hyper-
nym−−−→ spouse
</p>
<p>member
holonym−−−−→ marriage
</p>
<p>deri-
vation−−−→ marry
</p>
<p>For the acquisition relation, we noticed that
the lexical semantic filter performed quite poorly
on our corpus. The relation requires a large k-
value, i.e., k &gt;= 5, since there are many ways
in which an acquisition can be described. A
company can for instance devour, take-over or
purchase another company. Each increase of k
allows many additional content words, thus in-
creasing the danger of inappropriate ones. An
example is [ORGANIZATION1 &lt;trademark of&gt;
ORGANIZATION2]. Where it is plausible that in
the training set, an acquired company may persist
as a brand of its new owner, trademark does not
express a take-over. Although the semantic filter
by Moro et al. (2013) can provide useful hints and
can be applied without manually annotating train-
ing data, it is not powerful enough to discriminate
content words as to whether they provide strong
evidence for an acquisition or not.
</p>
<p>Also patterns selected based on the entailment
graph gold-standard performed surprisingly
low on the acquisition relation. Here, recall
was affected negatively because some of the non-
entailing patterns that were filtered out were in fact
able to extract correct instances with good preci-
sion. In particular, patterns expressing a planned
acquisition, such as [ORGANIZATION1 &lt;plan to
purchase&gt; ORGANIZATION2], [ORGANIZATION1
&lt;be to acquire&gt; ORGANIZATION2], or
[ORGANIZATION1 &lt;announce intention to
acquire&gt; ORGANIZATION2] extracted many cor-
</p>
<p>226</p>
<p />
</div>
<div class="page"><p />
<p>Relation Base pattern Precision Recall F1
award-award_honor-ceremony win at 0.58 (0.31) 0.73 (1.00) 0.65 (0.47)
base-locations-continents-countries_within country in 0.67 (0.57) 0.84 (1.00) 0.75 (0.59)
education-education-major_field_of_study degree in 1.00 (0.47) 0.49 (1.00) 0.65 (0.64)
film-...-film_regional_debut_venue premiere at 0.73 (0.13) 0.89 (1.00) 0.80 (0.23)
film-performance-film star in 0.97 (0.57) 0.53 (1.00) 0.69 (0.72)
organization-place_founded founded in 0.83 (0.16) 0.83 (1.00) 0.83 (0.27)
people-marriage-location_of_ceremony marry in 0.80 (0.07) 1.00 (1.00) 0.89 (0.12)
people-marriage-spouse marry 1.00 (0.15) 0.58 (1.00) 0.73 (0.26)
people-person-place_of_birth born in 1.00 (0.41) 0.84 (1.00) 0.91 (0.58)
people-place_of_burial buried at 1.00 (0.33) 0.71 (1.00) 0.83 (0.50)
</p>
<p>Table 3: Entailment graph based pattern selection (vs. baseline) for FB15k-237 relations
</p>
<p>rect mentions, as the acquisition in fact happened
at a later stage. Nevertheless, filtering out these
cases is correct from a semantic point of view,
even if many of the reported plans or attempts
concerning acquisitions later become a reality.
</p>
<p>Precision on the acquisition gold-standard
was also lower than for the marriage re-
lation, due to patterns annotated as entail-
ing in the TEG-REP corpus, which extracted
comparably many incorrect instances. One
such pattern is [ORGANIZATION1 &lt;takeover of&gt;
ORGANIZATION2], which yields low precision
values because it often occurs in sentences ex-
pressing irrealis moods, such as the proposed Mi-
crosoft takeover of Yahoo or is a Pfizer takeover of
BMS realistic?, and because of its generality often
extracts non-company entities, e.g., Republican
takeover of Congress. Detecting the embedding
of correct patterns in irrealis contexts is a largely
unsolved problem and calls for the development
of general methods for recognizing nonfactual
modalities along the lines of the NegEx algorithm
for detecting negations in medical texts (Chapman
et al., 2001) and its later extensions.
</p>
<p>6.2 FB15k-237
</p>
<p>Our experiments on the FB15k-237 corpus are
presented in Table 3, showing the performance
of our pattern selection method based on entail-
ment graphs (with the adapted MultiAlign imple-
mentation) compared to a simple baseline (all pat-
terns). The results show that, even using an RTE
model trained on a completely different semantic
relation, our method achieves decent performance
on selecting meaningful patterns for a wide range
of relations. The figures in the table were pro-
duced with the simple graph optimization strat-
egy, but the global graph optimizer performed very
similar on this dataset, achieving the same results
for eight out of the ten relations. It performed
</p>
<p>worse on the award-award_honor-ceremony rela-
tion (F1: 0.48), and better for the base-locations-
continents-countries_within relation (F1: 0.91).
Nevertheless, when dealing with larger numbers
of patterns, the global graph optimizer should be
the method of choice, as it is less prone to seman-
tic drift.
</p>
<p>7 Conclusions and Future Work
</p>
<p>We presented an approach for structuring relation
extraction patterns using entailment graphs and
evaluated the usefulness of these graphs for pattern
selection. For generating entailment graphs auto-
matically, we employed and adapted an alignment-
based entailment classifier, which makes use of
external knowledge resources, and experimented
with different graph optimization strategies. Our
classifier was trained on a manageable amount of
annotated patterns for a single semantic relation,
resulting in a generic model that was shown to pro-
duce valid entailment decisions for a wide range of
other semantic relations. Our experimental results
suggest that meaningful pattern-based entailment
graphs can be constructed automatically and that
the derived knowledge is in fact valuable for se-
lecting useful relation extraction patterns. In par-
ticular, entailment graph based filtering can help
achieve higher precision than methods which do
not take into account the asymmetric nature of se-
mantic relations.
</p>
<p>Acknowledgments
</p>
<p>This research was supported by the German Fed-
eral Ministry of Education and Research (BMBF)
through the projects ALL-SIDES (01IW14002)
and BBDC (01IS14013E) and by the German Fed-
eral Ministry of Economics and Energy (BMWi)
through the project SDW (01MD15010A).
</p>
<p>227</p>
<p />
</div>
<div class="page"><p />
<p>References
Eugene Agichtein. 2006. Confidence estimation meth-
</p>
<p>ods for partially supervised information extraction.
In Proceedings of the Sixth SIAM International Con-
ference on Data Mining. Bethesda, MD, USA, pages
539–543.
</p>
<p>Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D. Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing. Association for Computational
Linguistics, Beijing, China, pages 344–354.
</p>
<p>Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open Information Extraction from the Web. In Pro-
ceedings of the 20th International Joint Conference
on Artifical Intelligence. pages 2670–2676.
</p>
<p>Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic Inference at the Lexical-
Syntactic Level. In Proceedings of the Twenty-
Second AAAI Conference on Artificial Intelligence.
AAAI Press, Vancouver, British Columbia, Canada,
pages 871–876.
</p>
<p>Sandro Bauer, Stephen Clark, Laura Rimell, and Thore
Graepel. 2014. Learning a theory of marriage (and
other relations) from a web corpus. In Advances in
Information Retrieval, Springer International Pub-
lishing, pages 591–597.
</p>
<p>Jonathan Berant, Ido Dagan, Meni Adler, and Jacob
Goldberger. 2012. Efficient tree-based approxima-
tion for entailment graph learning. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics. Jeju Island, Korea,
pages 117–125.
</p>
<p>Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. Uppsala,
Sweden, pages 1220–1229.
</p>
<p>Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the ACM SIG-
MOD international conference on Management of
data. pages 1247–1250.
</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference
pages 632–642.
</p>
<p>Wendy W. Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001.
A simple algorithm for identifying negated findings
and diseases in discharge summaries. Journal of
Biomedical Informatics 34(5):301 – 310.
</p>
<p>Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Seman-
tic Verb Relations. In Proceedings of EMNLP.
Barcelona, Spain, pages 33–40.
</p>
<p>Ido Dagan and Oren Glickman. 2004. Probabilistic
Textual Entailment: Generic Applied Modeling of
Language Variability. In Learning Methods for Text
Understanding and Mining.
</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entail-
ment Challenge. In Proceedings of the PASCAL
Challenges Workshop on Recognising Textual En-
tailment.
</p>
<p>Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
Ninth Workshop on Statistical Machine Translation.
</p>
<p>Kathrin Eichler, Feiyu Xu, Hans Uszkoreit, Leonhard
Hennig, and Sebastian Krause. 2016. TEG-REP: A
corpus of textual entailment graphs based on relation
extraction patterns. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC).
</p>
<p>Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA,
USA.
</p>
<p>Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. FACC1: Freebase annota-
tion of ClueWeb corpora, Version 1 (Release date
2013-06-26, Format version 1, Correction level 0).
</p>
<p>Stanley Kok and Pedro Domingos. 2008. Extracting
semantic networks from text via relational cluster-
ing. In Proceedings of the European Conference
on Machine Learning and Knowledge Discovery in
Databases. pages 624–639.
</p>
<p>Vladyslav Kolesnyk, Tim Rocktäschel, and Sebastian
Riedel. 2016. Generating natural language inference
chains. CoRR abs/1606.01404.
</p>
<p>Lili Kotlerman, Ido Dagan, Bernardo Magnini, and
Luisa Bentivogli. 2015. Textual entailment graphs.
Natural Language Engineering 21:699–724.
</p>
<p>Sebastian Krause, Hong Li, Hans Uszkoreit, and
Feiyu Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proceedings of the 11th International Se-
mantic Web Conference. Springer.
</p>
<p>The Lemur Project. 2009. Clueweb09. http://
www.lemurproject.org.
</p>
<p>Omer Levy, Ido Dagan, and Jacob Goldberger. 2014.
Focused Entailment Graphs for Open IE Proposi-
tions. In Proceedings of the Eighteenth Conference
on Computational Natural Language Learning. Ann
Arbor, Michigan, pages 87–97.
</p>
<p>228</p>
<p />
</div>
<div class="page"><p />
<p>Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin
Eichler, Günter Neumann, Tae-Gil Noh, Sebastian
Padó, Asher Stern, and Omer Levy. 2014. The Ex-
citement Open Platform for Textual Inferences. In
Proceedings of the ACL 2014 System Demonstra-
tions. ACL.
</p>
<p>Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng,
and Shafiq R. Joty. 2013. Towards topic labeling
with phrase entailment and aggregation. In Proceed-
ings of the Human Language Technologies: Confer-
ence of the North American Chapter of the Associa-
tion of Computational Linguistics. Atlanta, Georgia,
USA, pages 179–189.
</p>
<p>Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu,
Roberto Navigli, and Hans Uszkoreit. 2013. Seman-
tic rule filtering for web-scale relation extraction. In
Proceedings of the 12th International Semantic Web
Conference. pages 347–362.
</p>
<p>Ndapandula Nakashole, Gerhard Weikum, and
Fabian M. Suchanek. 2012. PATTY: A taxonomy
of relational patterns with semantic types. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning. Jeju
Island, Korea, pages 1135–1145.
</p>
<p>Tae-Gil Noh, Sebastian Padó, Vered Shwartz, Ido Da-
gan, Vivi Nastase, Kathrin Eichler, Lili Kotlerman,
and Meni Adler. 2015. Multi-level alignments as an
extensible representation basis for textual entailment
algorithms. In Proceedings of *SEM 2015. ACL.
</p>
<p>Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation Extraction
with Matrix Factorization and Universal Schemas.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
74–84.
</p>
<p>Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomás Kociský, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
CoRR abs/1509.06664.
</p>
<p>Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investigat-
ing a generic paraphrase-based approach for relation
extraction. In Proceedings of the 11st Conference of
the European Chapter of the Association for Com-
putational Linguistics. Trento, Italy.
</p>
<p>Dan Roth, Mark Sammons, and V. G. Vinod Vydis-
waran. 2009. A framework for entailed relation
recognition. In Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics and the 4th International Joint Conference
on Natural Language Processing of the Asian Fed-
eration of Natural Language Processing. The Asso-
ciation for Computer Linguistics, Singapore, pages
57–60.
</p>
<p>Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL. New
York City, USA, pages 304–311.
</p>
<p>Philippe Thomas, Stefan Pietschmann, Illés Solt,
Domonkos Tikk, and Ulf Leser. 2011. Not all links
are equal: Exploiting Dependency Types for the Ex-
traction of Protein-Protein Interactions from Text.
In Proceedings of the BioNLP Workshop. pages 1–
9.
</p>
<p>Kristina Toutanova and Danqi Chen. 2015. Observed
versus latent features for knowledge base and text in-
ference. In Proceedings of the Workshop on Contin-
uous Vector Space Models and Their Composition-
ality.
</p>
<p>Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases.
</p>
<p>Rui Wang and Günter Neumann. 2008. Relation vali-
dation via textual entailment. In Proceedings of the
1st International and KI-08 Workshop on Ontology-
based Information Extraction Systems. volume 400,
pages 26–37.
</p>
<p>Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing. Stroudsburg, PA, USA, pages
1456–1466.
</p>
<p>Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research 34:255–296.
</p>
<p>229</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Classifying Semantic Clause Types: Modeling Context and Genre
Characteristics with Recurrent Neural Networks and Attention
</p>
<p>Maria Becker♦♠, Michael Staniek♦♠, Vivi Nastase♦♠, Alexis Palmer♣, Anette Frank♦♠
</p>
<p>♦ Leibniz ScienceCampus “Empirical Linguistics and Computational Language Modeling”
♠Heidelberg University, Department of Computational Linguistics
</p>
<p>♣University of North Texas, Department of Linguistics
{mbecker,staniek,nastase,frank}@cl.uni-heidelberg.de
</p>
<p>alexis.palmer@unt.edu
</p>
<p>Abstract
</p>
<p>Detecting aspectual properties of clauses
in the form of situation entity types has
been shown to depend on a combination
of syntactic-semantic and contextual fea-
tures. We explore this task in a deep-
learning framework, where tuned word
representations capture lexical, syntactic
and semantic features. We introduce an
attention mechanism that pinpoints rele-
vant context not only for the current in-
stance, but also for the larger context.
Apart from implicitly capturing task rel-
evant features, the advantage of our neu-
ral model is that it avoids the need to re-
produce linguistic features for other lan-
guages and is thus more easily transfer-
able. We present experiments for English
and German that achieve competitive per-
formance. We present a novel take on
modeling and exploiting genre informa-
tion and showcase the adaptation of our
system from one language to another.
</p>
<p>1 Introduction
</p>
<p>Semantic clause types, called Situation Entity (SE)
types (Smith, 2003; Palmer et al., 2007) are lin-
guistic characterizations of aspectual properties
shown to be useful for argumentation structure
analysis (Becker et al., 2016b), genre characteriza-
tion (Palmer and Friedrich, 2014), and detection of
generic and generalizing sentences (Friedrich and
Pinkal, 2015). Recent work on automatic identi-
fication of SE types relies on feature-based classi-
fiers for English that have been successfully ap-
plied to various textual genres (Friedrich et al.,
2016), and also show that a sequence labeling ap-
proach that models contextual clause labels yields
improved classification performance.
</p>
<p>Deep learning provides a powerful framework
in which linguistic and semantic regularities can
be implicitly captured through word embeddings
(Mikolov et al., 2013b). Patterns in larger text
fragments can be encoded and exploited by re-
current (RNNs) or convolutional neural networks
(CNNs) which have been successfully used for
various sentence-based classification tasks, e.g.
sentiment (Kim, 2014) or relation classification
(Vu et al., 2016; Tai et al., 2015).
</p>
<p>We frame the task of classifying clauses with re-
spect to their aspectual properties – i.e., situation
entity types – in a recurrent neural network archi-
tecture. We adopt a Gated Recurrent Unit (GRU)-
based RNN architecture that is well suited to mod-
eling long sequences (Yin et al., 2017). This ini-
tial model is enhanced with an attention mecha-
nism shown to be beneficial for sentence classifi-
cation (Wang et al., 2016) and sequence modeling
(Dong and Lapata, 2016). We explore the useful-
ness of attention in two settings: (i) the individ-
ual classification task and (ii) in a setting approx-
imating sequential labeling in which the attention
vector provides features that describe the clauses
preceding the current instance. Compared to the
strong baseline provided by the feature based sys-
tem of Friedrich et al. (2016), we achieve compet-
itive performance and find that attention as well
as context representation using predicted or gold-
standard labels of the previous N clauses, and text
genre information improve our model.
</p>
<p>A strong motivation for developing NN-based
systems is that they can be transferred with low
cost to other languages without major feature en-
gineering or use of hand-crafted linguistic knowl-
edge resources. Given the highly-engineered fea-
ture sets used for SE classification so far (Friedrich
et al., 2016), porting such classifiers to other lan-
guages is a non-trivial issue. We test the portabil-
ity of our system by applying it to German.
</p>
<p>230</p>
<p />
</div>
<div class="page"><p />
<p>We present a novel take on modeling and ex-
ploiting genre information and test it on the En-
glish multi-genre corpus of Friedrich et al. (2016).
</p>
<p>Our aims and contributions are: (i) We study
the performance of GRU-based models enhanced
with attention for modeling local and non-local
characteristics of semantic clause types. (ii) We
compare the effectiveness of the learned attention
weights as features for a sequence labeling system
to the explicitly defined syntactic-semantic fea-
tures in (Friedrich et al., 2016). (iii) We define
extensions of our models that integrate external
knowledge about genre and show that this can be
used to improve classification performance across
genres. (iv) We test the portability of our models
to other languages by applying them to a smaller,
manually annotated German dataset. The perfor-
mance is comparable to English.
</p>
<p>2 Semantic Clause Types
</p>
<p>Semantic clause types can be distinguished by
the function they have within a text or discourse.
We use the inventory of semantic clause types,
also known as situation entity (SE) types, devel-
oped by Smith (2003) and extended in later work
(Palmer et al., 2007; Friedrich and Palmer, 2014).
SE types describe abstract semantic types of sit-
uations evoked in discourse through clauses. As
such, they capture the manner of presentation of
content, along with the information content itself.
The seven SE types we use are described below.
</p>
<p>1. STATE (S): Armin has brown eyes.
2. EVENT (EV): Bonnie ate three tacos.
3. REPORT (R) provides attribution:
</p>
<p>The agency said costs had increased.
4. GENERIC SENTENCE (GEN) predicates over
</p>
<p>classes or kinds:
Birds can fly. – Scientists make arguments.
</p>
<p>5. GENERALIZING SENTENCE (GS) describes
regularly occurring events:
Fei travels to India every year.
</p>
<p>6. QUESTION (Q): Why do you torment me so?
7. IMPERATIVE (IMP): Listen to this.
An eighth class OTHER is assigned to clauses
</p>
<p>without an SE label, e.g. bylines or email headers.
Features that distinguish SE types are a combi-
</p>
<p>nation of linguistic features of the clause and its
main verb, and the nature of the main referent of
the clause.1 There is a correlation between the
</p>
<p>1The main referent of a clause is roughly the per-
</p>
<p>distribution of SE types in text passages and dis-
course modes, e.g., narrative, informative, or argu-
mentative (Palmer and Friedrich, 2014; Mavridou
et al., 2015; Becker et al., 2016a).
</p>
<p>3 Related Work
</p>
<p>Feature-based classification of situation entity
types. The first robust system for SE type clas-
sification (Friedrich et al., 2016) combines task-
specific syntactic and semantic features with dis-
tributional word features, as captured by Brown
clusters (Brown et al., 1992). This system seg-
ments each text into a sequence of clauses and
then predicts the best sequence of SE labels for the
text using a linear chain conditional random field
(CRF) with label bigram features.2
</p>
<p>Although SE types are relevant across lan-
guages, their linguistic realization varies across
languages. Accordingly, some of Friedrich
et al. (2016)’s syntactic and semantic features are
language-specific and are extracted using English-
specific resources such as WordNet and Loaiciga
et al. (2014)’s rules for extracting tense and voice
information from POS tag sequences.
</p>
<p>Friedrich et al. (2016)’s system is trained and
evaluated on data sets from MASC and Wikipedia
(Section 5), reaching accuracies of 76.4% (F1
71.2) with 10-fold cross-validation, and 74.7% (F1
69.3) on a held-out test set. To evaluate the con-
tribution of sequence information, Friedrich et al.
(2016) compare the CRF model to a Maximum
Entropy baseline, with the result that the sequen-
tial model significantly outperforms the model
which classifies clauses in isolation, particularly
for the less-frequent SE types of GENERIC SEN-
TENCE and GENERALIZING SENTENCE.
</p>
<p>When trained and tested within a single genre
(of the 13 genres represented in the data sets),
Friedrich et al. (2016)’s system performance
ranges from 26.6 F1 (for government documents)
to 66.2 F1 (for jokes). Training on all genres lev-
els out this performance difference, with a range
of F1 scores from 58.1-69.8.
</p>
<p>Neural approaches to sentence classification,
sequence and context modeling. Inspired by
research in vision, sentence classification tasks
have initially been modeled using Convolutional
Neural Networks (Kim, 2014; Kalchbrenner et al.,
</p>
<p>son/thing/situation the clause is about, often realized as its
grammatical subject.
</p>
<p>2Code and data: https://github.com/annefried/sitent
</p>
<p>231</p>
<p />
</div>
<div class="page"><p />
<p>2014). RNN variations – with Gated Recur-
rent Units (GRU) (Cho et al., 2014) or Long
Short-Term Memory units (LSTM) (Hochreiter
and Schmidhuber, 1997) – have since achieved
state of the art performance in both sequence mod-
eling and classification tasks. Recent work ap-
plies bi-LSTM models in sequence modeling (PoS
tagging, Plank et al. (2016), NER Lample et al.
(2016)) and structure prediction tasks (Semantic
Role Labeling, Zhou and Xu (2015) or seman-
tic parsing into logical forms Dong and Lapata
(2016)). Tree-based LSTM models have been
shown to often perform better than purely sequen-
tial bi-LSTMs (Tai et al., 2015; Miwa and Bansal,
2016), but depend on parsed input.
</p>
<p>Attention. Attention has been established as an
effective mechanism that allows models to focus
on specific words in the larger context. A model
with attention learns what input tokens or token se-
quences to attend to and thus does not need to cap-
ture the complete input information in its hidden
state. Attention has been used successfully e.g. in
aspect-based sentiment classification (Wang et al.,
2016), for modeling relations between words or
phrases in encoder-decoder models for translation
(Bahdanau et al., 2015), or bi-clausal classifica-
tion tasks such as textual entailment (Rocktäschel
et al., 2016). We use attention to larger context
windows and previous labeling decisions to cap-
ture sequential information relevant for our classi-
fication task. We investigate the learned weights to
gain information about what the models learn, and
we start to explore how they can be used to provide
features for a sequential labeling approach.
</p>
<p>4 Models
</p>
<p>We aim for a system that can fine-tune input word
embeddings to the task, and can process clauses as
sequences of words from which to encode larger
patterns that help our particular clause classifica-
tion task. GRU RNNs are used because they can
successfully process long sequences and capture
long-term dependencies. Attention can encode
which parts of the input contain relevant informa-
tion. These modeling choices are described and
justified in detail below. The performance of the
models is reported in Section 6.
</p>
<p>4.1 Basic Model: Gated Recurrent Unit
Recurrent Neural Networks (RNNs) are modifica-
tions of feed-forward neural networks with recur-
</p>
<p>rent connections, which allow them to find pat-
terns in – and thus model – sequences. Sim-
ple RNNs cannot capture long-term dependencies
(Bengio et al., 1994) because the gradients tend to
vanish or grow out of control with long sequences.
Gated Recurrent Unit (GRU) RNNs, proposed by
Cho et al. (2014), address this shortcoming. GRUs
have fewer parameters and thus need less data to
generalize (Zhou et al., 2016) than LSTM RNNs,
and also outperform the LSTM in many cases (Yin
et al., 2017), which makes them a good choice for
our relatively small dataset.3 The relevant equa-
tions for a GRU are below. xt is the input at time
t, rt is a reset gate which determines how to com-
bine the new input with the previous memory, and
the update gate zt defines how much of the previ-
ous memory to keep. ht is the hidden state (mem-
ory) at time t, and h̃t is the candidate activation at
time t. W∗ and U∗ are weights that are learned.
� denotes the element-wise multiplication of two
vectors.
</p>
<p>rt = σ(Wrxt + Urht−1)
h̃t = tanh(Wxt + U(rt � ht−1))
zt = σ(Wzxt + Uzht−1)
</p>
<p>ht = (1− zt)� ht−1 + zt � h̃t (1)
The last hidden vector ht will be taken as the
</p>
<p>representation of the input clause. After compress-
ing it into a vector whose length is equal to the
number of class labels (=8) using a fully connected
layer with sigmoid function, we apply softmax.
</p>
<p>4.2 Attention Model
We extend our GRU model with a neural attention
mechanism to capture the most relevant words
in the input clauses for classifying SE types.
Specifically, we adapt the implementation of
attention used in Rocktäschel et al. (2016) for our
clause classification task as follows:
</p>
<p>M = tanh(WhH +Wvht ⊗ eL)
α = softmax(wTM)
r = HαT
</p>
<p>where H is a matrix consisting of the hidden
vectors [h1, ..., ht] produced by the GRU, ht is the
last output vector of the GRU, and eL is a vec-
tor of 1s where L denotes the L words of the in-
put clause. ⊗ denotes the outer product of the
</p>
<p>3Comparison of GRUs, bi-GRUs, LSTMs and bi-LSTMs
on our dataset for our classification task showed that GRUs
outperform the latter three, confirming this assumption.
</p>
<p>232</p>
<p />
</div>
<div class="page"><p />
<p>two vectors. α is a vector consisting of attention
weights and r is a weighted representation of the
input clause. Wh,Wv, and w are parameters to be
learned during training.
</p>
<p>The final clause representation is obtained from
a combination of the attention-weighted represen-
tation r of the clause and the last output vector v.
</p>
<p>h∗ = tanh(Wpr +Wxht) (2)
</p>
<p>where Wp and Wx are trained projection matrices.
We convert h∗ to a real-valued vector with length 8
(the number of target classes) and apply softmax
to transform it to a probability distribution.
</p>
<p>4.3 Modeling Context and Genre
</p>
<p>Text types differ in their situation entity type dis-
tributions: Palmer et al. (2007) find that GENERIC
SENTENCES and GENERALIZING SENTENCES
play a predominant role for texts associated with
the argument or commentary mode (such as es-
says), and EVENTS and STATES for texts associ-
ated with the report mode (such as news texts).
(Becker et al., 2016a) find that argumentative
texts are characterized by a high proportion of
GENERIC and GENERALIZING SENTENCES and
very few EVENTS, while reports and talks con-
tain a high proportion of STATES, and fiction is
characterized by a high number of EVENTS. N-
gram analyses show that sequences of SE types
differ among different genres: e.g. while ST-ST
is the most frequent bigram within journal arti-
cles, the most frequent bigram in Wikipedia ar-
ticles is GEN-GEN. The most frequent trigram
in Jokes is EV-EV-EV, followed by ST-ST-ST,
whereas in government documents the most fre-
quent trigrams are ST-ST-ST and EV-ST-ST.
These results show that n-grams cluster in texts
(cf. (Friedrich and Pinkal, 2015)), and they differ
among genres. This supports the choice of incor-
porating (sequential) context information for clas-
sification of SE types. Fig. 1 illustrates both the
context and the genre information our models con-
sider for classifying SE types, while Fig. 2 illus-
trates our model’s architecture.
</p>
<p>4.3.1 Context Modeling: Clauses and Labels
We develop two models that not only consider the
local sentence for SE classification in model train-
ing, but also the previous clauses’ token sequences
or the labels of previous clauses. When attending
to tokens of previous clauses we add one GRU
model with attention mechanism for each previous
</p>
<p>clause (N denotes the number of previous clauses)
and concatenate their final outputs with the final
output of the GRU with attention for the current
clause (cf. Fig. 2).
</p>
<p>h∗con1 =&lt; tanh(Wpr1 +Wxv1); ...;
tanh(WprN +WxvN ) &gt;
</p>
<p>We then transform the concatenated vector into
a dense vector equal to the number of class labels
and apply softmax.
</p>
<p>For attending to labels of the previous clauses,
we first transform the gold labels used during
training into embeddings and apply attention as
described in section 4.2 to these representations.
We then concatenate the last output of the current
clause with the embeddings for the labels of the
previous clauses (here N denotes the number of
previous labels):
</p>
<p>h∗con2 =&lt; tanh(Wpr+Wxv); yt−1; ...; yt−N &gt;
</p>
<p>where yt−i is the embedding representation for the
previous t-i label. At test time we use the predicted
probability distribution vector as the labels of the
previous clauses.
</p>
<p>4.3.2 Feature Modeling: Textual Genres
The English corpus we use consists of texts from
13 genres; the German corpus covers 7 genres
(Section 5).
</p>
<p>Information about genre is encoded as dense
embeddings g of size 10 initialized randomly,
and we apply attention mechanism to these rep-
resentations. Adding genre information pro-
duces three new versions of the model: (i)
genre+basic model: &lt; g;ht &gt; (ht from eq.1), (ii)
genre+attention model &lt; g;h∗ &gt; (h∗ from eq.2),
(iii) genre+context in form of previous labels (cf.
Fig.2). Results for all three combinations are re-
ported in Section 6.
</p>
<p>4.4 Word embeddings
</p>
<p>Word embeddings have been shown to capture
syntactic and semantic regularities (Mikolov et al.,
2013b) and to benefit from fine tuning for spe-
cific tasks. The features used by Friedrich et al.
(2016) cover a variety of linguistic features – such
as tense, voice, number, POS, semantic clusters
– some of which we expect to be encoded in
pre-trained embeddings, while others will emerge
through model training. We start with pre-trained
embeddings for both English and German, be-
cause this leads to better results than random ini-
</p>
<p>233</p>
<p />
</div>
<div class="page"><p />
<p>Figure 1: Context and genre information modeled in our system, example from Wikipedia
</p>
<p>Figure 2: Model Architecture, illustrated with an example (cf. Fig. 1).
</p>
<p>tialization. For German, we use 100-dimensional
word2vec embeddings trained on a large German
corpus of 116 million sentences (Reimers et al.,
2014).4 For English, we use 300-dimensional
word2vec embeddings (Mikolov et al., 2013a)
trained on part of the Google News dataset (about
100 billion words). The pre-trained embeddings
are tuned during training.
</p>
<p>4.5 Parameters and Tuning
</p>
<p>Hyperparameter settings were determined through
exhaustive random search using optunity (Bergstra
and Bengio, 2012) on the development set, and
we use the best setting for evaluating on the test
set. We tune batch size, number of layers, GRU
cell size, and regularization parameter (L2). For
learning rate optimization we use AdaGrad (Duchi
et al., 2011) and tune the initial learning rate. For
the basic model (without attention), the best result
on the development set is achieved for GRU with
batch size 100, 2 layers, cell size 350, learning rate
0.05, and L2 regularization parameter (0.01). For
the model using attention mechanism the parame-
ters are identical except for L2 (0.0001).
</p>
<p>4https://public.ukp.informatik.tu-darmstadt.de
/reimers/2014 german embeddings
</p>
<p>Data set # Clauses/SEs # Tokens
</p>
<p>English: MASC 30,333 357,078
English: Wiki 10,607 148,040
German: all 18,194 236,522
</p>
<p>Table 1: Data sets with SE-labeled clauses
</p>
<p>5 Data
</p>
<p>We use the English dataset described in Friedrich
and Palmer (2014).5 The texts, drawn from
Wikipedia and MASC (Ide et al., 2010), range
across 13 genres, e.g. news texts, government
documents, essays, fiction, jokes, emails. For
German, we combine two data sets described in
Mavridou et al. (2015) and Becker et al. (2016a)
and additional data annotated by ourselves.6 The
German texts cover 7 genres: argumentative es-
says (Peldszus and Stede, 2015), Wikipedia, fic-
tion, commentary, news texts, TED talks, and eco-
nomic reports. Statistics appear in Table 1.
</p>
<p>The distribution of SE types varies with the
genre. For the selected English Wiki texts, 50%
of the SE types are GENERIC SENTENCE clauses,
</p>
<p>5Available at: https://github.com/annefried/sitent
6The data is available at http://www.cl.uni-heidelberg.de/
</p>
<p>english/research/downloads/resource pages/GER SET/GER
SET data.shtml. This dataset only contains the German data
</p>
<p>that has been annotated within the Leibniz Science campus.
</p>
<p>234</p>
<p />
</div>
<div class="page"><p />
<p>Acc F1
Palmer07, Brown dataset 53.1 -
Fried16, set A (CRF, test) 69.8 63.9
Fried16, set B (CRF, test) 71.4 65.5
Fried16, set A+B (CRF, test) 74.7 69.3
Fried16, set A+B (CRF, CV) 76.4 71.2
Fried16, set A+B (MaxEnt+CRF, 77.9 73.9
CV, seq-oracle)
</p>
<p>Table 2: Reported results of baseline models for
English (accuracy and macro-average F1 score).
CV=10-fold cross validation, test=eval. on test set.
</p>
<p>with STATES second at 24.3%.7 For the 12 MASC
genres, STATE is the most frequent type (49.8%),
with EVENTS second at 24.3%. GENERIC SEN-
TENCES make up only 7.3% of the SE types in
the MASC texts. In the German data, the distribu-
tions of SE types also differ according to genre: in
argumentative texts, for example, GENERIC SEN-
TENCES make up 48% of the SE types, followed
by STATES with a frequency of 32%, while in most
other genres the most frequent class is STATE.
</p>
<p>The texts of the English dataset are split
into clauses using SPADE (Soricut and Marcu,
2003). For segmenting the German dataset into
clauses we use DiscourseSegmenter’s rule-based
segmenter (edseg, Sidarenka et al. (2015)), which
employs German-specific rules. Because Dis-
courseSegmenter occasionally oversplit segments,
we did a small amount of post-processing.
</p>
<p>6 Experiments and Evaluation
</p>
<p>For the English dataset, we use the same test-
train split as Friedrich et al. (2016).8 The German
dataset was split into training and testing with a
balanced distribution of genres (as is the case for
the English dataset). Both datasets have a 80-20
split between training and testing (20% of training
is used for development).
</p>
<p>We report results in terms of accuracy and
macro-average F1 score on the held-out test set.
</p>
<p>Baseline systems. The feature-based system of
Palmer07 (Palmer et al., 2007) (Palmer07 in Table
2) simulates context through predicted labels from
previous clauses. Friedrich et al. (2016) (Fried16
in Table 2) report results for their CRF-based SE
</p>
<p>7The Wiki texts were selected by Friedrich et al. (2015)
precisely in order to target GENERIC SENTENCE clauses.
</p>
<p>8The cross validation splits of the data used by Friedrich
et al. (2016) are not available.
</p>
<p>Acc F1
Basic GRU 66.55 46.04
Basic GRU + genre 65.82 46.32
GRU + attention 68.99 68.87
GRU + attention + genre 71.12 67.95
GRU + att + clause (1) 69.06 59.39
GRU + att + clause (2) 70.20 60.01
GRU + att + clause (3) 69.64 37.29
GRU + att + pLab (1) 69.20 61.95
GRU + att + pLab (2) 69.37 62.13
GRU + att + pLab (3) 68.77 60.85
GRU + att + pLab (4) 68.05 59.31
GRU + att + pLab (5) 68.11 60.75
GRU + att + pLab + genre (1) 71.59 64.94
GRU + att + pLab + genre (2) 71.61 64.28
GRU + att + pLab + genre (3) 70.37 63.55
GRU + att + pLab + genre (4) 70.96 63.74
GRU + att + pLab + genre (5) 70.57 63.65
</p>
<p>Table 3: SE-type classification on English test set,
with context as predicted labels (pLab).
</p>
<p>type labeler for different feature sets, with 10-fold
cross validation and on a held-out test set. To test
if the context is useful they extend their classifier
with a CRF that includes the predicted label of the
preceding clause. In the oracle setting it includes
the gold label of the previous clause.
</p>
<p>Feature set A consists of standard NLP features
including POS tags and Brown clusters. Feature
set B includes more detailed features such as tense,
lemma, negation, modality, WordNet sense, Word-
Net supersense and WordNet hypernym sense. We
presume that some of the information captured by
feature set B, particularly sense and hypernym in-
formation, may not be captured in the word em-
beddings we use in our approach.
</p>
<p>Evaluation of our neural systems. Our local
system (cf. Section 4.1) achieves an accuracy of
66.55 (Table 3). Adding genre information does
not help, but adding attention within the local
clause yields an improvement of 2.44 percentage
points (pp). Using both attention and genre infor-
mation leads to a 2.13 pp increase over the model
that uses only attention. Adding context infor-
mation beyond the local clause – a window of
up to three previous clauses – improves the word-
based attention models slightly, but a wider win-
dow (four or more clauses) causes a major drop
</p>
<p>235</p>
<p />
</div>
<div class="page"><p />
<p>Acc F1
GRU + att + gLab (1) 72.71 65.37
GRU + att + gLab (2) 72.68 66.51
GRU + att + gLab (3) 72.66 65.03
GRU + att + gLab (4) 72.61 64.33
GRU + att + gLab (5) 73.40 66.39
GRU + att + gLab + genre (1) 73.44 66.76
GRU + att + gLab + genre (2) 73.45 66.51
GRU + att + gLab + genre (3) 72.84 66.29
GRU + att + gLab + genre (4) 73.12 66.21
GRU + att + gLab + genre (5) 73.34 66.13
</p>
<p>Table 4: SE-type classification on English test set,
sequence oracle model using gold labels (gLab).
</p>
<p>in accuracy.9 Using context as predicted labels of
previous clauses improves the model slightly (up
to 0.38 pp), but adding genre on top of that im-
proves the model by up to 2.62 pp compared to the
basic model with attention. The oracle model (cf.
Table 4), which uses the gold labels of previous
clauses, gives an upper bound for the impact of
sequence information: 73.40% accuracy for pre-
vious 5 gold labels. Combined with genre infor-
mation, the upper bound reaches 73.45% accuracy
when using the previous 2 gold labels.
</p>
<p>The best accuracy on the English data (ignor-
ing the oracle) is achieved by the model that uses
2 previous predicted labels plus genre information
(71.61%). This model outperforms Friedrich et al.
(2016)’s results when using standard NLP features
(feature set A) and their model using feature set B
separately. Our model comes close to Friedrich et
al.’s best results obtained by applying their entire
set of features, particularly considering that our
system only uses generic word embeddings.
</p>
<p>Window size as hyper-parameter? We achieve
best results when incorporating two previous la-
bels or two previous clauses (cf. Table 3). This
is in line with Palmer et al. (2007) who report that
in most cases performance starts to degrade as the
model incorporates more than two previous labels.
A window size of two does not always lead to
best performance on the German dataset (cf. Sec-
tion 7), where the model using predicted labels
from the maximum window size (5) performs best.
When adding genre information, we achieve best
results with window size two (cf. Table 5 and 6).
This inconsistency can possibly be traced back to
the fact that we applied the best-performing vari-
</p>
<p>9We achieve 36.24 acc for 4 and 36.17 acc for 5 clauses.
</p>
<p>Figure 3: Visualization of attention for ST, GS,
GEN, and REP.
</p>
<p>Figure 4: Mean attention scores per POS tags on
English dataset. POS tags from PTB.
</p>
<p>ations of our system developed on English data to
our German dataset without further hyperparame-
ter tuning.
</p>
<p>Results for single classes. Fig. 6 shows macro-
average F1 scores of our best performing system
for the single SE classes. The scores are very sim-
ilar to the results of Friedrich et al. (2016). Scores
for GENERALIZING SENTENCE are the lowest as
this class is very infrequent in the data set, while
scores for the classes STATE, EVENT, and RE-
PORT are the highest. In addition, we explored
our system’s performance for binary classification
(Fig. 6): here we classified STATE vs. the re-
maining classes, EVENT vs. the remaining classes
etc. Binary classification achieves better perfor-
mance and can be helpful for downstream applica-
tions which only need information about specific
</p>
<p>236</p>
<p />
</div>
<div class="page"><p />
<p>Figure 5: Position of words with maximum atten-
tion within clauses. x-axis represents the normal-
ized position within the clause, y-axis the number
of words with maximum attention at that position.
</p>
<p>Figure 6: Macro-average F1 scores of our best per-
forming system for single SE classes, multiclass
vs. binary classification.
</p>
<p>SE types, for example for distinguishing generic
from non-generic sentences.
</p>
<p>Analysis of attention. Attention is not only an
effective mechanism that allows models to focus
on specific parts of the input, but it may also en-
able interesting linguistic insights: (1) the atten-
tion to specific words or POS for specific SE types,
(2) the overall distribution of attention weights
among POS tag labels and SE types, and (3) the
position of words with maximum/high attention
scores within a clause.
</p>
<p>Fig. 3 shows example clauses with their at-
tention weights. In the first clause, a STATE,
the model attends most to the nouns “China” and
“Japan”. In the next clause, a GENERALIZING
SENTENCE, the noun “system” is assigned the
highest attention weight. The highest weighted
word in the GENERIC SENTENCE is the pronoun
“their”, and in REPORT it is the verb “answered”.
</p>
<p>Fig. 4 visualizes the mean attention score per
POS tag for all SE types (gold labels).10 Inter-
estingly, attention seems to be especially impor-
tant for classes that are rare, such as IMPERA-
</p>
<p>10We post-process our data with POS tags using spaCy11
</p>
<p>with the PTB tagset (Marcus et al., 1993).
</p>
<p>TIVE or REPORT, each less than 5% of the English
dataset. The heat map indicates that the model
especially attends to verbs when classifying the
SE type REPORT. This is not surprising, since
REPORT clauses are signaled by verbs of speech.
GENERALIZING SENTENCE attend to symbols,
mainly punctuation, and genitive markers such as
“’s”. The OTHER class, which includes clauses
without an assigned SE type label, attends mostly
to interjections. Indeed, OTHER is frequent in gen-
res with fragmented sentences (emails, blogs), and
numerous interjections such as “wow” or “um”.
</p>
<p>Fig. 5 shows the relative positions of words with
maximum attention within clauses. The model
mostly attends to words at the end of clauses and
almost never to words in the first half of clauses.
This distribution shifts to the left when consider-
ing more words with high attention scores instead
of only the word with maximum attention – words
with 2nd (3rd, 4th, 5th) highest attention score can
often be found at the beginning of clauses. The
model seems to draw information from a broad
range of positions.
</p>
<p>We explored the impact of the attention vec-
tors as inputs to a sequence labeling model –
each clause is described through the words with
the highest attention weights and these weights,
and used in a conditional random field system
(CRF++12). The best performance was obtained
when using the attention vector of the current
clause (and no additional context) – 61.68% ac-
curacy (47.18% F1 score). CRF++ maps the at-
tention information to binary features, and as such
cannot take advantage of information captured in
the numerical values of the attention weights, or
the embeddings of the given words.
</p>
<p>7 Porting the System to German
</p>
<p>One advantage for developing NN-based systems
that do not rely on hand-crafted features is that
they can be used with different language data.
We use the system described above with German
data, only adjusting the size of the input embed-
dings.13 Compared to the English dataset, the Ger-
man dataset is smaller (44% in size) and less di-
verse with respect to genre (7 genres). The gen-
res in the German dataset (argumentative texts,
wikipedia, commentary, news, fiction, report, talk)
</p>
<p>12https://taku910.github.io/crfpp/
13The different size of the embeddings (for English and
</p>
<p>German cf. section 4.4, may have an impact on the results.
</p>
<p>237</p>
<p />
</div>
<div class="page"><p />
<p>Acc F1
Basic GRU 72.67 61.55
Basic GRU + genre 72.08 66.33
GRU + attention 72.31 72.23
GRU + attention + genre 73.75 65.69
GRU + att + clause (1) 73.49 63.99
GRU + att + clause (2) 70.21 58.66
GRU + att + clause (3) 49.31 47.01
GRU + att + pLab (1) 69.83 44.31
GRU + att + pLab (2) 70.12 44.33
GRU + att + pLab (3) 70.50 44.91
GRU + att + pLab (4) 72.16 45.12
GRU + att + pLab (5) 72.85 45.52
GRU + att + pLab + genre (1) 72.19 53.22
GRU + att + pLab + genre (2) 73.98 54.78
GRU + att + pLab + genre (3) 70.78 46.25
GRU + att + pLab + genre (4) 72.88 48.94
GRU + att + pLab + genre (5) 72.60 45.98
</p>
<p>Table 5: SE-type classification on German test set.
</p>
<p>Acc F1
GRU + att + gLab (1) 71.33 58.32
GRU + att + gLab (2) 72.23 59.43
GRU + att + gLab (3) 73.81 59.12
GRU + att + gLab (4) 75.74 60.39
GRU + att + gLab (5) 76.32 61.01
GRU + att + gLab + genre (1) 74.79 59.34
GRU + att + gLab + genre (2) 77.97 61.47
GRU + att + gLab + genre (3) 74.28 59.84
GRU + att + gLab + genre (4) 74.10 59.70
GRU + att + gLab + genre (5) 74.96 58.18
</p>
<p>Table 6: SE-type classification on German test set,
sequence oracle model .
</p>
<p>are more similar to one another than the ones in the
English dataset. The results comparing the effec-
tiveness of integrating context and genre informa-
tion are in Table 5. The results of the oracle model
using gold labels for previous clauses are in Ta-
ble 6. Compared to English, the models achieve
higher performance, but attention by itself does
not improve the results, and neither does the inclu-
sion of genre information. Used jointly, attention
and genre information yield a moderate increase
of 1.06 pp. accuracy compared to the basic GRU.
Attention may need more data and possibly more
diversity to be learned effectively, and we will ex-
plore this in future work.
</p>
<p>Modeling context seems to have a larger impact:
</p>
<p>compared to the basic GRU using attention, infor-
mation about the current and the previous clauses
improves the model by up to 1.67 pp. More con-
textual information leads to higher accuracy.
</p>
<p>8 Conclusion
</p>
<p>We presented an RNN-based approach to situation
entity classification that bears clear advantages
compared to previous classifier models that rely
on carefully hand-engineered features and lexical
semantic resources: it is easily transferable to
other languages as it can tune pre-trained embed-
dings to encode semantic information relevant
for the task, and can develop attention models to
capture – and reveal – relevant information from
the larger context. We designed and compared
several GRU-based RNN models that jointly
model local and contextual information in a
unified architecture. Genre information was
added to model common properties of specific
textual genres. What makes our work interesting
for linguistically informed semantic models
is the exploration of different model variants
that combine local classification with sequence
information gained from the contextual history,
and how these properties interact with genre
characteristics. We specifically explore attention
mechanisms that help our models focus on
specific characteristics of the local and non-local
contexts. Attention models jointly using genre
and context information in the form of previous
predicted labels perform best for our task, for
both languages. The performance results of
our best models outperform the state of the art
models of Fried16 for English when using either
off-the-shelf NLP features (set A) or, separately,
hand-crafted features based on lexical resources
(set B). A small margin of ca. 3 pp accuracy is
left to achieve in future work to compete with the
knowledge-rich models of (Friedrich et al., 2016).
</p>
<p>Acknowledgments. We thank Sabrina Effen-
berger, Jesper Klein, Sarina Meyer, and Rebekka
Sons for the annotations, and the reviewers for
their insightful comments. This research is funded
by the Leibniz Science Campus Empirical Lin-
guistics &amp; Computational Language Modeling,
supported by Leibniz Association grant no. SAS-
2015-IDS-LWC and by the Ministry of Science,
Research, and Art of Baden-Württemberg.
</p>
<p>238</p>
<p />
</div>
<div class="page"><p />
<p>References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
</p>
<p>gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.
</p>
<p>Maria Becker, Alexis Palmer, and Anette Frank. 2016a.
Argumentative texts and clause types. In Proceed-
ings of the 3rd Workshop on Argument Mining.
pages 21–30.
</p>
<p>Maria Becker, Alexis Palmer, and Anette Frank. 2016b.
Clause Types and Modality in Argumentative Mi-
crotexts. In Workshop on Foundations of the
Language of Argumentation (in conjunction with
COMMA). Potsdam, Germany, pages 1–9.
</p>
<p>Y. Bengio, P. Simard, and P. Frasconi. 1994. Learn-
ing long-term dependencies with gradient descent
is difficult. Trans. Neur. Netw. 5(2):157–166.
https://doi.org/10.1109/72.279181.
</p>
<p>James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research 13(Feb):281–305.
</p>
<p>Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics 18(4):467479.
</p>
<p>Kyunghyun Cho, B van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches.
</p>
<p>Li Dong and Mirella Lapata. 2016. Language
to logical form with neural attention. In Pro-
ceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 33–43.
http://www.aclweb.org/anthology/P16-1004.
</p>
<p>John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research 12(Jul):2121–2159.
</p>
<p>Annemarie Friedrich and Alexis Palmer. 2014. Situ-
ation entity annotation. In Proceedings of the Lin-
guistic Annotation Workshop VIII.
</p>
<p>Annemarie Friedrich, Alexis Palmer, and Manfred
Pinkal. 2016. Situation entity types: automatic
classification of clause-level aspect. In Proceed-
ings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1:
Long Papers). Berlin, Germany, pages 1757–1768.
http://www.aclweb.org/anthology/P16-1166.
</p>
<p>Annemarie Friedrich, Alexis Palmer, Melissa Peate
Sørensen, and Manfred Pinkal. 2015. Annotating
genericity: a survey, a scheme, and a corpus. In The
9th Linguistic Annotation Workshop held in conjun-
cion with NAACL 2015. page 21.
</p>
<p>Annemarie Friedrich and Manfred Pinkal. 2015.
Discourse-sensitive Automatic Identification of
Generic Expressions. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). Beijing, China.
</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.
</p>
<p>Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The Manually Annotated
Sub-Corpus: A community resource for and by the
people. In Proceedings of the ACL2010 Conference
Short Papers. pages 68–73.
</p>
<p>Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics. Baltimore, Maryland, pages 655–
665.
</p>
<p>Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP). Doha, Qatar, page
17461751.
</p>
<p>Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer.
2016. Neural architectures for named entity recog-
nition. In Proceedings of the 2016 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 260–270.
http://www.aclweb.org/anthology/N16-1030.
</p>
<p>Sharid Loaiciga, Thomas Meyer, and Andrei Popescu-
Belis. 2014. English-french verb phrase alignment
in europarl for tense translation modeling. In Pro-
ceedings of The Ninth Language Resources and
Evaluation Conference (LREC).
</p>
<p>Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics.
</p>
<p>Kleio-Isidora Mavridou, Annemarie Friedrich, Melissa
Peate Sorensen, Alexis Palmer, and Manfred Pinkal.
2015. Linking discourse modes and situation enti-
ties in a cross-linguistic corpus study. In Proceed-
ings of the EMNLP Workshop LSDSem 2015: Link-
ing Models of Lexical, Sentential and Discourse-
level Semantics.
</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.
</p>
<p>239</p>
<p />
</div>
<div class="page"><p />
<p>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 746–751.
http://www.aclweb.org/anthology/N13-1090.
</p>
<p>Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). Berlin, Germany, pages
1105–1116. http://www.aclweb.org/anthology/P16-
1105.
</p>
<p>Alexis Palmer and Annemarie Friedrich. 2014. Genre
distinctions and discourse modes: Text types differ
in their situation type distributions. In Proceedings
of the Workshop on Frontiers and Connections be-
tween Argumentation Theory and NLP.
</p>
<p>Alexis Palmer, Elias Ponvert, Jason Baldridge, and
Carlota Smith. 2007. A sequencing model for sit-
uation entity classification. In Proceedings of ACL.
</p>
<p>Andreas Peldszus and Manfred Stede. 2015. An an-
notated corpus of argumentative microtexts. In Pro-
ceedings of the First European Conference on Argu-
mentation.
</p>
<p>Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers). Association for
Computational Linguistics, Berlin, Germany, pages
412–418. http://anthology.aclweb.org/P16-2067.
</p>
<p>Nils Reimers, Judith Eckle-Kohler, Carsten Schnober,
Jungi Kim, and Iryna Gurevych. 2014. Germeval-
2014: Nested Named Entity Recognition with neural
networks. In Proceedings of the 12th Edition of the
KONVENS Conference. page 117120.
</p>
<p>Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In Proceedings of the 4th International Confer-
ence on Learning Representations (ICLR). San Juan,
Puerto Rico.
</p>
<p>Uladzimir Sidarenka, Andreas Peldszus, and Manfred
Stede. 2015. Discourse Segmentation of German
Texts. In Journal for Language Technology and
Computational Linguistics. volume 30, pages 71–
98.
</p>
<p>Carlota S Smith. 2003. Modes of discourse: The local
structure of texts, volume 103. Cambridge Univer-
sity Press.
</p>
<p>Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology..
</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term mem-
ory networks. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1556–
1566. http://www.aclweb.org/anthology/P15-1150.
</p>
<p>Ngoc Thang Vu, Heike Adel, Pankaj Gupta, and Hin-
rich Schütze. 2016. Combining recurrent and con-
volutional neural networks for relation classifica-
tion. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. San Diego, California, pages 534–539.
http://www.aclweb.org/anthology/N16-1065.
</p>
<p>Yequan Wang, Minlie Huang, xiaoyan zhu, and
Li Zhao. 2016. Attention-based LSTM for Aspect-
level Sentiment Classification. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics, Austin, Texas, pages 606–615.
https://aclweb.org/anthology/D16-1058.
</p>
<p>Wenpeng Yin, Katharina Kann, Mo Yu, and Hin-
rich Schütze. 2017. Comparative study of cnn
and rnn for natural language processing. CoRR
abs/1702.01923.
</p>
<p>Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei
Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. Trans-
actions of the Association for Computational Lin-
guistics pages 371–383.
</p>
<p>Jie Zhou and Wei Xu. 2015. End-to-end learn-
ing of semantic role labeling using recurrent neu-
ral networks. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1127–
1137. http://www.aclweb.org/anthology/P15-1109.
</p>
<p>240</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 241–250,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Predictive Linguistic Features of Schizophrenia
</p>
<p>Efsun Sarioglu Kayi1, Mona Diab1, Luca Pauselli2, Michael Compton2, Glen Coppersmith3
1Department of Computer Science, George Washington University
</p>
<p>{efsun, mtdiab}@gwu.edu
2Medical Center, Columbia University
</p>
<p>{mtc2176@cumc.columbia.edu, pausell@nyspi.columbia.edu}
3Qntfy
</p>
<p>glen@qntfy.com
</p>
<p>Abstract
</p>
<p>Schizophrenia is one of the most disabling
and difficult to treat of all human medi-
cal/health conditions, ranking in the top
ten causes of disability worldwide. It
has been a puzzle in part due to diffi-
culty in identifying its basic, fundamental
components. Several studies have shown
that some manifestations of schizophrenia
(e.g., the negative symptoms that include
blunting of speech prosody, as well as the
disorganization symptoms that lead to dis-
ordered language) can be understood from
the perspective of linguistics. However,
schizophrenia research has not kept pace
with technologies in computational lin-
guistics, especially in semantics and prag-
matics. As such, we examine the writ-
ings of schizophrenia patients analyzing
their syntax, semantics and pragmatics. In
addition, we analyze tweets of (self pro-
claimed) schizophrenia patients who pub-
licly discuss their diagnoses. For writ-
ing samples dataset, syntactic features are
found to be the most successful in classifi-
cation whereas for the less structured Twit-
ter dataset, a combination of features per-
formed the best.
</p>
<p>1 Introduction
</p>
<p>Schizophrenia is an etiologically complex, hetero-
geneous, and chronic disorder. It imposes major
impairments on affected individuals, can be devas-
tating to families, and it diminishes the productiv-
ity of communities. Furthermore, schizophrenia is
associated with remarkably high direct and indi-
rect health care costs. Persons with schizophrenia
often have multiple medical comorbidities, have
a tragically reduced life expectancy, and are of-
ten treated without the benefits of sophisticated
measurement-based care.
</p>
<p>Similar to other psychoses, schizophrenia has
been studied extensively on the neurological and
behavioral levels. Covington et al. (2005) note
the existence of many language abnormalities (in
syntactic, semantic, pragmatic, and phonetic do-
mains of linguistics) comparing patients to con-
trols. They observed the following:
</p>
<p>• reduction in syntax complexity (Fraser et al.,
1986);
</p>
<p>• impaired semantics, such as the organization
of individual propositions into larger struc-
tures (Rodriguez-Ferrera et al., 2001);
</p>
<p>• abnormalities in pragmatics which is a level
obviously disordered in schizophrenia (Cov-
ington et al., 2005);
</p>
<p>• phonetic anomalies like flattened intonation
(aprosody), more pauses, and constricted
pitch/timbre (Stein, 1993).
</p>
<p>A few studies have used computational meth-
ods to assess acoustic parameters (e.g., pauses,
prosody) that correlate with negative symptoms,
but schizophrenia research has not kept pace with
technologies in computational linguistics, espe-
cially in semantics and pragmatics. Accordingly,
we analyze the predictive power of linguistic fea-
tures in a comprehensive manner by computing
and analyzing many syntactic, semantic and prag-
matic features. This sort of analysis is particularly
useful for finding meaningful signals that help us
better understand the mental health conditions. To
this end, we compute part-of-speech (POS) tags
and dependency parses to capture the syntactic in-
formation in patients’ writings. For semantics, we
derive topic based representations and semantic
role labels of writings. In addition, we add more
semantics by adding dense features using clusters
that are trained on online resources. For pragmat-
ics, we consider the sentiment that exists in writ-
ings, i.e. positive vs. negative and its intensity. To
</p>
<p>241</p>
<p />
</div>
<div class="page"><p />
<p>the best of our knowledge, no previous work has
conducted comprehensive analysis of schizophre-
nia patients’ writings from the perspective of syn-
tax, semantics and pragmatics, collectively.
</p>
<p>2 Predictive Linguistic Features of
Schizophrenia
</p>
<p>2.1 Dataset
</p>
<p>The first dataset called LabWriting consists of 93
patients with schizophrenia who were recruited
from sites in both Washington, D.C. and New
York City. This includes patients that have a
diagnosis of schizophreniform disorder or first-
episode or early-course patients with a psychotic
disorder not otherwise specified. All patients
were native English-speaking patients, aged 18-50
years and cognitively intact enough to understand
and participate in the study. A total of 95 eligible
controls were also native English speakers aged
18-50. Patients and controls did not differ by
age, race, or marital status, however, patients
were more likely to be male and had completed
fewer years of education. All study participants
were assessed for their ability to give consent,
and written informed consent was obtained using
Institutional Review Board-approved processes.
Patients and controls were asked to write two
paragraph-length essays: one about their average
Sunday and the second about what makes them
the angriest. The total number of writing samples
collected from both patients and controls is 373.
Below is a sample response from this dataset (text
from patients rendered verbatim as is including
typos):
</p>
<p>The one thing that probably makes me the
most angry is when good people receive the bad
end of the draw. This includes a child being struck
for no good reason. A person who is killed but
was an innocent bystander. Or even when people
pour their heart and soul into a job which pays
them peanurs but they cannot sustain themselves
without this income. Just in generul a good person
getting the raw end of deal. For instance people
getting laid off because their company made
bad investments. the Higher ups keep their jobs
while the worker ants get disposed of. How about
people who take advantage of others and build an
Empire off it like insurance or drug companies.
All these good decent people not getting what they
deserved. Yup that makes me angry.
</p>
<p>In addition, we evaluated social media mes-
sages with self-reported diagnoses of schizophre-
nia using the Twitter API. This dataset includes
174 users with apparently genuine self-stated
diagnosis of a schizophrenia-related condition
and 174 age and gender matched controls.
Schizophrenia users were selected via regular
expression on schizo for a close phonetic ap-
proximation. Each diagnosis was examined by a
human annotator to verify that it seems genuine.
For each schizophrenia user, a control that had
the same gender label and was closest in age
was selected. The average number of tweets
per user is around 2,800. Detailed information
on this dataset can be found in (Mitchell et al.,
2015). Below are some tweets from this dataset
(they have been rephrased to preserve anonymity):
</p>
<p>this is my first time being unemployed. please
forgive me. i’m crazy. #schizophrenia
</p>
<p>i’m in my late 50s. i worry if i have much
time left as they say people with #schizophrenia
die 15-20 years younger
</p>
<p>#schizophrenia takes me to devil-like places
in my mind
</p>
<p>2.2 Approach and Experimental Design
</p>
<p>We cast the problem as a supervised binary clas-
sification task where a system should discrimi-
nate between a patient and a control. To classify
schizophrenia patients from controls, we trained
support vector machines (SVM) with linear ker-
nel and Random Forest classifiers. We used Weka
(Hall et al., 2009) to conduct the experiments with
10-fold stratified cross validation. We report Pre-
cision, Recall, F-Score, and Area Under Curve
(AUC) value which is the area under receiver op-
erating characteristics curve (ROC).
</p>
<p>2.2.1 Syntactic Features
To capture the syntactic information from writ-
ings, we produce the POS tags and dependency
parse trees using Stanford Core NLP (Manning
et al., 2014). To use these as features to the clas-
sifier, we calculate the frequency of each POS tag
and dependencies from parse trees. For the Twit-
ter dataset, we use a parser (Kong et al., 2014) and
POS tagger (Gimpel et al., 2011) that are specifi-
cally trained for social media data.
</p>
<p>242</p>
<p />
</div>
<div class="page"><p />
<p>2.2.2 Semantic Features
To analyze the semantics of the writings, we con-
sider several sources of information. As a first
approach, we use semantic role labeling (SRL).
Specifically, we use Semafor (Das et al., 2010)
tool to generate semantic role labels of the writ-
ings and then calculate the frequency of the labels
as features for the classifier. For Twitter dataset,
due to its short form and poor syntax, we were not
able to compute SRL features.
</p>
<p>In addition to SRL, we analyzed the topic dis-
tribution of writings using Latent Dirichlet Allo-
cation (LDA) (Blei et al., 2003). With this ap-
proach, we want to see the possibility of differ-
ent themes emerging in the writings of patients vs.
controls. Using LDA, we represent each writing
as a topic distribution where each topic is auto-
matically learned as a distribution over the words
of the vocabulary. We use the MALLET tool (Mc-
Callum, 2002) to train the topic model and empiri-
cally choose number of topics based on best classi-
fication performance on a validation set. The best
performing number of topics is 20 for LabWriting
dataset and 40 for Twitter dataset.
</p>
<p>Finally, we compute dense semantic features
by computing clusters based on global word vec-
tors. Specifically, for LabWriting dataset, we use
word vectors trained on Wikipedia 2014 dump and
Gigaword 5 (Parker, 2011) which are generated
based on global word-word co-occurrence statis-
tics (Pennington et al., 2014). For Twitter dataset,
we use Twitter models trained on 2 billion tweets.1
</p>
<p>We, then, create clusters of these word vectors us-
ing the K-means algorithm (K= 100, empirically
chosen) for both datasets. Then, for each writ-
ing, we calculate the frequency of each cluster by
checking the existence of each word of the docu-
ment in the cluster. With this cluster based repre-
sentation, we aim to capture the effect of semanti-
cally related words on the classification.
</p>
<p>2.2.3 Pragmatic Features
For pragmatics, we wanted to see whether patients
exhibit more negative sentiment than controls. For
that purpose, we use the Stanford Sentiment Anal-
ysis tool (Socher et al., 2013). Given a sentence, it
predicts its sentiment at five possible levels: very
negative, negative, neutral, positive, and very pos-
itive. For each writing, we calculate the frequency
of sentiment levels. Additionally, sentiment inten-
</p>
<p>1http://nlp.stanford.edu/projects/glove/
</p>
<p>sities are produced at the phrase level. Rather than
categorical values, this intensity encodes the mag-
nitude of the sentiment more explicitly. As such,
we calculate the total intensity for each document
as sum of its phrases’ intensities at each level. For
Twitter dataset, we use a sentiment classifier that
was trained for social media data (Radeva et al.,
2016). Its output includes three levels of sentiment
negative, neutral, and positive without intensity
information.
</p>
<p>2.2.4 Feature Analysis
To be able to better evaluate best performing fea-
tures, we analyze them based on two feature se-
lection algorithms: Information Gain (IG) for
Random Forest and Recursive Feature Elimina-
tion (RFE) algorithm for SVM (Guyon et al.,
2002). The Information Gain measure selects the
attributes that decrease the entropy the most. The
RFE algorithm, on the other hand, selects features
based on their weights based on the fact that the
larger weights correspond to the more informative
features.
</p>
<p>3 Results
</p>
<p>The list of syntactic, semantic and pragmatic fea-
tures are presented in Table 1 for both datasets. Ta-
bles 2 and 3 illustrate our results for the LabWrit-
ing dataset and Twitter dataset, respectively. The
majority baseline F-Score is 34.39 for the Lab-
Writing and 32.11 for Twitter dataset. The top per-
formance for each dataset and classifier is shown
in bold. The corresponding ROC plots for features
are shown in Figures 1 and 2 for LabWriting and
Twitter datasets respectively. In each ROC plot,
true positive rate (recall) is plotted against true
negative rate where SVM is shown in magenta and
Random Forest is shown in blue. The diagonal
line from bottom left to upper right represents ran-
dom guess and better performing results are closer
to upper left corner. Overall, Random Forest per-
forms better than SVM even though for some fea-
ture combinations, SVM’s performance is higher.
This could be due to bootstrapping of samples that
takes place in Random Forest since both of the
datasets are on the smaller side. For LabWrit-
ing dataset, the best performing features according
to F-Score are syntactic: POS+Parse (syntax) for
SVM and syntax + pragmatics features for Ran-
dom Forest. According to AUC, best performing
feature is POS for both classifiers. For Twitter
dataset, the best performing features according to
</p>
<p>243</p>
<p />
</div>
<div class="page"><p />
<p>(a) POS (b) Parse
</p>
<p>(c) SRL (d) Topics (e) Clusters
</p>
<p>(f) Sentiment (g) Sentiment Intensity
</p>
<p>Figure 1: ROC Plots for LabWriting Dataset
</p>
<p>244</p>
<p />
</div>
<div class="page"><p />
<p>(a) POS (b) Parse
</p>
<p>(c) Topics (d) Clusters
</p>
<p>(e) Sentiment
</p>
<p>Figure 2: ROC Plots for Twitter Dataset
</p>
<p>245</p>
<p />
</div>
<div class="page"><p />
<p>Table 1: Feature Categories
</p>
<p>Category Writing Samples Twitter
Syntactic POS, Dependency Parse POS, Dependency Parse
Semantic (Sem.) SRL, Topics, Clusters Topics , Clusters
Pragmatic (Prag.) Sentiment, Sentiment Intensity Sentiment
</p>
<p>Table 2: Classification Performance of LabWriting Dataset
</p>
<p>SVM Random Forest
Features AUC Precision Recall F-Score AUC Precision Recall F-Score
POS 75.72 68.89 68.07 68.48 78.92 70.11 69.41 69.76
Parse 65.34 60.15 59.23 59.69 66.68 66.74 65.16 65.94
SRL 64.25 58.65 58.24 58.44 70.62 65.22 64.64 64.93
Topics 66.49 63.52 63.17 63.34 68.26 62.77 62.34 62.55
Clusters 69.68 65.12 64.85 64.98 68.43 65.38 64.62 65.00
Sentiment 60.23 54.99 53.83 54.40 56.27 57.97 57.66 57.81
Sentiment Intensity 69.98 64.20 64.07 64.13 69.39 65.31 64.62 64.96
Syntax 74.17 69.38 68.38 68.88 75.78 69.25 68.09 68.67
Semantics 66.46 61.59 61.05 61.32 69.16 64.39 63.72 64.05
Pragmatics 68.95 62.67 62.45 62.56 69.59 67.57 66.78 67.17
Syntax + Sem. 68.24 66.36 66.12 66.24 76.60 71.24 69.36 70.29
Syntax + Prag. 73.43 68.14 67.28 67.71 78.75 71.74 70.52 71.12
Sem.+ Prag. 68.01 63.98 63.46 63.72 72.09 65.75 64.81 65.28
All 70.11 66.94 66.66 66.80 77.57 71.18 69.91 70.54
</p>
<p>both F-Score and AUC are the ones that include
most of the combination of features: semantics +
pragmatics for Random Forest and all features for
SVM. Typically, essays, such as the ones in Lab-
Writing dataset, are expected to have better syntax
than informal tweets and as such syntactic features
were not as predictive for tweets. We also analyze
top performing features according to Information
Gain measure and SVM RFE algorithm in Sec-
tions 3.1, 3.3, 3.2 and explain the differences of
results for the two datasets in Section 3.4.
</p>
<p>3.1 Top Syntactic Features
</p>
<p>Syntactic features perform well mainly for Lab-
Writing dataset. Between POS tags and depen-
dence parses, the former perform better for both
datasets. For LabWriting dataset, the top POS
tag is FW, (Foreign Word). When we look at
the words that were tagged FW, they correspond
to misspelled words. Even though this could be
considered a criterion for schizophrenia patients,
it may also depend on patients and controls’ ed-
ucation and language skills which we expect it to
be similar but it may still show some differences.
Another top POS tag is LS, (List item marker),
</p>
<p>which was assigned to small case i which in re-
ality refers to pronoun I. This could imply that
the patients prefer to talk about themselves. This
coincides with several other studies (Rude et al.,
2004; Chung and Pennebaker, 2007) which found
that use of first person singular is associated with
negative affective states such as depression. Be-
cause of the likelihood of comorbidity of mental
illnesses, this requires further investigation as to
whether this is specific to schizophrenia patients or
not. Finally, another top POS tag is RP, adverbial
particle and top parse tag is advmod, adverb mod-
ifier. This could mean the ratio of adverbs used
could be a characteristic of patients. Finally for
Twitter dataset, the top POS tag is # correspond-
ing to hash tags. This could be an important dis-
criminative feature between patients and controls
as patients use less hashtags than controls.
</p>
<p>3.2 Top Semantic Features
</p>
<p>For classification using semantic features, clusters,
topics and SRL perform comparably. For Lab-
Writing dataset, top SRL features consist of gen-
eral categories and some specific ones that could
be relevant for schizophrenia patients. General la-
</p>
<p>246</p>
<p />
</div>
<div class="page"><p />
<p>Table 3: Classification Performance of Twitter Dataset
</p>
<p>SVM Random Forest
Features AUC Precision Recall F-Score AUC Precision Recall F-Score
POS 69.34 67.83 59.14 63.19 75.17 69.48 68.92 69.20
Parse 58.63 44.76 54.06 48.97 63.72 60.94 60.63 60.78
Topics 79.88 74.85 73.66 74.25 83.48 79.38 78.77 79.07
Clusters 78.02 73.87 71.27 72.55 82.54 74.80 73.76 74.28
Sentiment 75.79 65.15 60.50 62.74 85.28 80.00 79.50 79.75
Syntax 74.87 68.16 62.13 65.01 74.35 67.72 67.47 67.59
Semantics 80.29 70.81 70.34 70.57 85.62 75.00 74.72 74.86
Syntax+Sem. 81.47 73.02 72.55 72.78 86.35 78.30 78.02 78.16
Syntax+Prag. 82.16 75.61 72.22 73.88 83.55 75.89 75.52 75.70
Sem.+Prag. 80.99 74.52 74.05 74.28 88.98 82.01 81.30 81.65
All 82.58 75.18 74.39 74.78 88.01 78.81 78.40 78.60
</p>
<p>bels are Quantity and Social Event. More specific
labels are Morality Evaluation, Catastrophe, Ma-
nipulate into Doing and Being Obligated. Words
that are labeled as such are listed in Table 4. These
two different sets of labels could be due to the type
of questions asked to the patients. One question
is neutral in nature talking about their daily life
whereas the other is about the things that make
them angry and more emotionally charged. A sec-
ond semantic feature is the topic distributions of
writings. The top words from the most informa-
tive topics are listed in Table 5. For LabWrit-
ing dataset, one of the top topics consist of words
about typical Sunday activities corresponding to
one of the questions asked. The second top topic,
on the other hand, consist of words that show the
anger of the author. For Twitter dataset, one of
the topics consist of schizophrenia-related words
and the other consist of hate words. Again, the top
topics seem to contain relevant information in ana-
lyzing schizophrenia patients’ writings and classi-
fication using topic features perform comparably
well. As a final semantic feature, we use dense
cluster features. The classification performance
of cluster features is similar to classification per-
formance using topics. However, cluster features’
analysis is not as interpretable as topics, since they
are formed from massive online data resources.
</p>
<p>3.3 Top Pragmatic Features
</p>
<p>When it comes to pragmatic features, top senti-
ment features are neutral, negative and very neg-
ative (LabWriting only). For sentiment intensity,
neutral intensity, negative intensity and very neg-
ative intensity are more informative which is con-
</p>
<p>sistent with sentiment categorical analysis. In gen-
eral, neutral sentiment is the most common for a
given text and for patients, we would expect to
see more negative sentiment and this was con-
firmed by this analysis. However, negative sen-
timent could also be prominent in other psychi-
atric diseases such as post-traumatic stress disor-
der (PTSD)(Coppersmith et al., 2015), as such,
by itself, it may not be a discriminatory feature
for schizophrenia patients. For classification pur-
poses, sentiment intensity features performed bet-
ter than sentiment features. This could be due to
the fact that intensity values are more specific and
collected at word/phrase level in contrast to sen-
tence level.
</p>
<p>3.4 Effect of Datasets’ Characteristics
</p>
<p>The two datasets have some commonalities and
differences and present different challenges. The
LabWriting dataset was collected in a more con-
trolled manner and follows a structure that can be
expected from a short essay. Accordingly, NLP
tools applied to these writings are successful. On
the other hand, the Twitter dataset consists of com-
binations of short text that include many abbrevi-
ations that are not standard, e.g. users’ own so-
lutions to fixed length limit imposed by Twitter.
It is also very informal in nature and thus lacks
proper grammar and syntax more frequently than
LabWriting. Hence, some machine learning ap-
proaches for NLP analysis of these tweets are lim-
ited even though social media specific tools were
used such as POS tagger (Gimpel et al., 2011),
dependency parser (Kong et al., 2014), sentiment
analysis tool (Radeva et al., 2016), and Twitter
</p>
<p>247</p>
<p />
</div>
<div class="page"><p />
<p>Table 4: Top Discriminative SRL Features
</p>
<p>Label Sample Words/Phrases
Quantity several, both, all, a lot, many, a little, a few, lots
Social Event social, dinner, picnics, hosting, dance
Morality Evaluation wrong, depraved, foul, evil, moral
Catastrophe incident,tragedy, suffer
Manipulate into Doing harassing, bullying
Being Obligated duty, job, have to, had to, should, must, responsibility, entangled, task
</p>
<p>Table 5: Discriminative Topics’ Top Words
</p>
<p>Method Dataset Top Words
IG Writing church sunday wake god service pray sing worship bible spending thanking
IG&amp;RFE Writing i’m can’t trust upset person lie real feel honest lied lies judge lying steal
IG&amp;RFE Twitter god jesus mental schizophrenic schizophrenia illness paranoid medical evil
IG&amp;RFE Twitter don love people fuck life feel fucking hate shit stop god person sleep bad die
</p>
<p>models for dense clusters. For instance, even
though we were able to compute POS tags and
parse trees for tweets, the tag set is much smaller
than PennTree Bank tags set. Similarly, some
approaches such as SRL were not successful on
tweets. On the other hand, both datasets consist
of patients and controls with similar demographics
(age, gender, etc), thus we largely expect patients
and controls to have similar linguistic capabilities.
In addition, for LabWriting dataset, patients and
controls were recruited from the same neighbor-
hoods. We have no such explicit guarantees for
the Twitter dataset, though they were excluded if
they did not primarily tweet in English. Accord-
ingly, any differentiation these classification meth-
ods found can largely be attributed to the illness.
Finally, LabWriting dataset had many spelling er-
rors. We elected not to employ any spelling cor-
rection techniques (since misspelling may very
well be a feature meaningful to schizophrenia).
This likely negatively influenced the calculation
of some of the features which depend on correct
spelling such as SRL.
</p>
<p>4 Related Work
</p>
<p>To date, some studies have investigated apply-
ing Latent Semantic Analysis (LSA) to the prob-
lem (Elvevag et al., 2007) of lexical coherence
and they found significant distinctions between
schizophrenia patients and controls. The work of
(Bedi et al., 2015) extends this approach by incor-
porating syntax, i.e., phrase level LSA measures
</p>
<p>and POS tags. In the latter related work, several
measures based on LSA representation were de-
veloped to capture the possible incoherence in pa-
tients. In our study, we used LDA to capture pos-
sible differences in themes between patients and
controls. LDA is a more descriptive technique
than LSA since topics are represented as distribu-
tions over vocabulary and top words for topics pro-
vide a way to understand the theme that they rep-
resent. We also incorporated syntax to our anal-
ysis with POS tags and additionally dependency
parses. Another work by (Howes et al., 2013) pre-
dicts outcomes by analyzing doctor-patient com-
munication in therapy using LDA. Even though
manual analysis of LDA topics with manual top-
ics seems promising, classification using topics
does not perform as successful unless otherwise
additional features are incorporated such as doc-
tors’ and patients’ information. Although, we had
detailed demographic information for LabWriting
dataset and derived age and sex information for
Twitter dataset, we chose not to incorporate them
to the classification process be able focus solely on
writings’ characteristics.
</p>
<p>The work of Mitchell et al. (2015) is, in many
respects, similar to ours by examining schizophre-
nia using LDA, clustering and sentiment anal-
ysis. Their sentiment analysis is lexicon-based
using Linguistic Inquiry Word Count (LIWC)
(Tausczik and Pennebaker, 2010) categories. In
our approach to sentiment analysis, we utilized
a machine learning approach. Lexicon-based ap-
</p>
<p>248</p>
<p />
</div>
<div class="page"><p />
<p>proaches generally have higher precision at the
cost of lower recall. Having coverage of more of
the content may be beneficial for analysis and in-
terpretation, so we opt to use a more generalizable
machine learning approach. For clustering, they
used Brown clustering; whereas, we used clusters
trained on global word vectors which were learned
from large amounts of online data. This has the
advantage that we could capture words and/or se-
mantics that may not be learned from our dataset.
Finally, their use of LDA is similar to our ap-
proach, i.e. representing documents as topic distri-
butions, and their analysis does not include syntac-
tic and dense cluster features. They had their best
performance with an accuracy value of 82.3 using
a combination of topic based representation and
their version of sentiment features. In our analysis,
combination of semantic and pragmatic features
performed the best with an accuracy value of 81.7.
Due to possible differences in preprocessing, pa-
rameter selection, and randomness that exist in the
experiments, the results are not directly compara-
ble, however, this also shows that the difficulty of
applying more advanced machine learning based
NLP techniques for Twitter dataset.
</p>
<p>5 Conclusion
</p>
<p>Computational assessment models of schizophre-
nia may provide ways for clinicians to monitor
symptoms more effectively and a deeper under-
standing of schizophrenia and the underpinning
cognitive biases could benefit affected individu-
als, families, and society at large. Objective and
passive assessment of schizophrenia symptoms
(e.g., delusion or paranoia) may provide clarity to
clinical assessments, which currently rely on pa-
tients’ self-reporting symptoms. Furthermore, the
techniques discussed here hold some potential for
early detection of schizophrenia. This would be
greatly beneficial to young people and first-degree
relatives of schizophrenia patients who are pro-
dromal (clinically appearing to be at high risk for
schizophrenia) but not yet delusional/psychotic,
since it would allow targeted early interventions.
</p>
<p>Among the linguistic features considered for
this study, syntactic fetures provide the biggest
boost in classification performance for LabWriting
dataset. For Twitter dataset, combination of fea-
tures such as semantics and pragmatics for SVM
and syntax, semantics and pragmatics for Random
Forest have the best performance.
</p>
<p>In the future, we will be focusing on the fea-
tures that showed the most promise in this study
and also add new features such as level of com-
mitted belief for pragmatics. Finally, we are col-
lecting more data and we will expand our analysis
to more mental health datasets.
</p>
<p>References
Gillinder Bedi, Facundo Carrillo, Guillermo A Cec-
</p>
<p>chi, Diego Fernández Slezak, Mariano Sigman,
Natália B Mota, Sidarta Ribeiro, Daniel C Javitt,
Mauro Copelli, and Cheryl M Corcoran. 2015. Au-
tomated Analysis of Free Speech Predicts Psychosis
Onset in High-Risk Youths. Npj Schizophrenia 1.
</p>
<p>David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res. 3:993–1022.
</p>
<p>Cindy K Chung and James W Pennebaker. 2007. So-
cial Communication .
</p>
<p>Glen Coppersmith, Mark Dredze, Craig Harman, and
Kristy Hollingshead. 2015. From ADHD to SAD:
Analyzing the Language of Mental Health on Twit-
ter through Self-Reported Diagnoses. In NAACL
Workshop on Computational Linguistics and Clin-
ical Psychology.
</p>
<p>Michael A Covington, Congzhou He, Cati Brown,
Lorina Naci, Jonathan T McClain, Bess Sirmon
Fjordbak, James Semple, and John Brown. 2005.
Schizophrenia and the Structure of Language: The
Linguist’s View. Schizophr Res 77(1):85–98.
</p>
<p>Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-semantic
Parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics. pages 948–956.
</p>
<p>Brita Elvevag, Peter W Foltz, Daniel R Weinberger,
and Terry E Goldberg. 2007. Quantifying Incoher-
ence in Speech: An Automated Methodology and
Novel Application to Schizophrenia. Schizophr Res
93(1-3):304–316.
</p>
<p>W I Fraser, K M King, P Thomas, and R E Kendell.
1986. The Diagnosis of Schizophrenia by Language
Analysis. Br J Psychiatry 148:275–278.
</p>
<p>Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech Tagging
for Twitter: Annotation, Features, and Experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2.
Association for Computational Linguistics, Strouds-
burg, PA, USA, HLT ’11, pages 42–47.
</p>
<p>249</p>
<p />
</div>
<div class="page"><p />
<p>Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene Selection for Can-
cer Classification Using Support Vector Machines.
Mach. Learn. 46(1-3):389–422.
</p>
<p>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explor. Newsl. 11(1):10–18.
</p>
<p>Christine Howes, Matthew Purver, and Rose Mc-
Cabe. 2013. Using Conversation Topics for
Predicting Therapy Outcomes in Schizophre-
nia. Biomed Inform Insights 6(Suppl 1):39–50.
https://doi.org/10.4137/BII.S11661.
</p>
<p>Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A Smith. 2014. A Dependency Parser for
Tweets .
</p>
<p>Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit. In Association for Com-
putational Linguistics (ACL) System Demonstra-
tions. pages 55–60.
</p>
<p>Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
Http://mallet.cs.umass.edu.
</p>
<p>Margaret Mitchell, Kristy Hollingshead, and Glen
Coppersmith. 2015. Quantifying the Language of
Schizophrenia in Social Media. In Proceedings
of the 2nd Workshop on Computational Linguistics
and Clinical Psychology: From Linguistic Signal to
Clinical Reality. pages 11–20.
</p>
<p>Robert et al. Parker. 2011. English Gigaword Fifth Edi-
tion LDC2011T07.
</p>
<p>Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543.
</p>
<p>Axinia Radeva, Mohammad Rasooli, and Kathleen
McKeown. 2016. Columbia Language Independent
Sentiment System. Technical report, Columbia Uni-
versity.
</p>
<p>S Rodriguez-Ferrera, R A McCarthy, and P J
McKenna. 2001. Language in Schizophrenia and Its
Relationship to Formal Thought Disorder. Psychol
Med 31(2):197–205.
</p>
<p>Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language Use of Depressed and
Depression-Vulnerable College Students. Cognition
and Emotion 18(8):1121–1133.
</p>
<p>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
</p>
<p>and Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. Conference on Empirical Methods in
Natural Language Processing (EMNLP) .
</p>
<p>J Stein. 1993. Vocal Alterations in Schizophrenic
Speech. J Nerv Ment Dis 181(1):59–62.
</p>
<p>Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Com-
puterized Text Analysis Methods. Journal of Lan-
guage and Social Psychology pages 24–54.
</p>
<p>250</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 251–261,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Learning to Solve Geometry Problems from Natural Language
Demonstrations in Textbooks
</p>
<p>Mrinmaya Sachan Eric P. Xing
School of Computer Science
Carnegie Mellon University
</p>
<p>{mrinmays, epxing}@cs.cmu.edu
</p>
<p>Abstract
</p>
<p>Humans as well as animals are good at
imitation. Inspired by this, the learning
by demonstration view of machine learn-
ing learns to perform a task from detailed
example demonstrations. In this paper,
we introduce the task of question answer-
ing using natural language demonstra-
tions where the question answering system
is provided with detailed demonstrative
solutions to questions in natural language.
As a case study, we explore the task of
learning to solve geometry problems using
demonstrative solutions available in text-
books. We collect a new dataset of demon-
strative geometry solutions from textbooks
and explore approaches that learn to in-
terpret these demonstrations as well as to
use these interpretations to solve geometry
problems. Our approaches show improve-
ments over the best previously published
system for solving geometry problems.
</p>
<p>1 Introduction
</p>
<p>Cognitive science emphasizes the importance of
imitation or learning by example (Meltzoff and
Moore, 1977; Meltzoff, 1995) in human learn-
ing. When a teacher signals a pedagogical inten-
tion, children tend to imitate the teacher’s actions
(Buchsbaum et al., 2011; Butler and Markman,
2014). Inspired by this phenomenon, the learn-
ing by demonstration view of machine learning
(Schaal, 1997; Argall et al., 2009; Goldwasser and
Roth, 2014) assumes training data in the form of
example demonstrations. A task is demonstrated
by a teacher and the learner generalizes from these
demonstrations in order to execute the task.
</p>
<p>In this paper, we introduce the novel task
of question answering using natural language
</p>
<p>Text Description:
</p>
<p>measure(   MAO, 30o)
isCircle(O)
</p>
<p>radius(O, 4 cm)
?x
</p>
<p>Diagram:
</p>
<p>liesOn( A, circle O), liesOn( B, circle O), 
liesOn( C, circle O), liesOn( D, circle O)
</p>
<p>isLine(AB), isLine(BC), isLine(CA), isLine(BD), isLine(DA)
isTriangle(ABC), isTriangle(ABD), isTriangle(AOM)
</p>
<p>measure(   ADB, x), measure(   MAO, 30o)
measure(   AMO, 90o)
</p>
<p>…
</p>
<p>Figure 1: Above: An example SAT style geometry problem
with the text description, corresponding diagram and (option-
ally) answer candidates. Below: A logical expression that
represents the meaning of the text description and the dia-
gram in the problem. GEOS derives a weighted logical ex-
pression where each predicates also carry a weighted score
but we do not show them here for clarity.
</p>
<p>demonstrations. Research in question answer-
ing has traditionally focused on learning from
question-answer pairs (Burger et al., 2001). How-
ever, it is well-established in the educational psy-
chology literature (Allington and Cunningham,
2010; Felder et al., 2000) that children tend to
learn better and faster from concrete illustrations
and demonstrations. In this paper, we raise the
question – “Can we leverage demonstrative solu-
tions for questions as provided by a teacher to im-
prove our question answering systems?”
</p>
<p>As a case study, we propose the task of learn-
ing to solve SAT geometry problems (such as the
one in Figure 1) using demonstrative solutions
to these problems (such as the one in Figure 2).
Such demonstrations are common in textbooks as
they help students learn how to solve geometry
problems effectively. We build a new dataset of
demonstrative solutions of geometry problems and
show that it can be used to improve GEOS (Seo
et al., 2015), the state-of-the-art in solving geom-
</p>
<p>251</p>
<p />
</div>
<div class="page"><p />
<p>1.	Sum	of	interior	angles	of	a	triangle	is	
1800	
</p>
<p>=&gt;	 OAM	+	 AMO	+	 MOA	=	1800	
</p>
<p>=&gt;	 MOA	=	600	
	
</p>
<p>2.	Similar	triangle	theorem	
=&gt;	 MOB	~	 MOA	
</p>
<p>=&gt;	 MOB	=	 MOA	=	600	
	
</p>
<p>3.	 AOB	=	 MOB	+	 MOA	
=&gt; AOB	=	1200	
	
</p>
<p>4.	Angle	subtended	by	a	chord	at	the	
center	is	twice	the	angle	subtended	at	
the	circumference	
=&gt;	 ADB	=	0.5	x	 AOB	
	 						=	600	
</p>
<p>Figure 2: An example demonstration on how to solve the
problem in Figure 1: (1) Use the theorem that the sum of in-
terior angles of a triangle is 180◦and additionally the fact that
∠AMO is 90◦to conclude that ∠MOA is 60◦. (2) Conclude
that4MOA∼4MOB (using a similar triangle theorem) and
then, conclude that ∠MOB = ∠MOA = 60◦(using the theo-
rem that corresponding angles of similar triangles are equal).
(3) Use angle sum rule to conclude that ∠AOB = ∠MOB +
∠MOA = 120◦. (4) Use the theorem that the angle subtended
by an arc of a circle at the centre is double the angle sub-
tended by it at any point on the circle to conclude that ∠ADB
= 0.5×∠AOB = 60◦.
</p>
<p>etry problems.
We also present a technique inspired from re-
</p>
<p>cent work in situated question answering (Krish-
namurthy et al., 2016) that jointly learns how to
interpret the demonstration and use this interpre-
tation to solve geometry problems. We model the
interpretation task (the task of recognizing various
states in the demonstration) as a semantic parsing
task. We model state transitions in the demonstra-
tion via a deduction model that treats each appli-
cation of a theorem of geometry as a state tran-
sition. We describe techniques to learn the two
models separately as well as jointly from various
kinds of supervision: (a) when we only have a set
of question-answer pairs as supervision, (b) when
we have a set of questions and demonstrative so-
lutions for them, and (c) when we have a set of
question-answer pairs and a set of demonstrations.
</p>
<p>An important benefit of our approach is ‘in-
terpretability’. While GEOS is uninterpretable,
our approach utilizes known theorems of geom-
etry to deductively solve geometry problems. Our
approach also generates demonstrative solutions
(like Figure 2) as a by-product which can be pro-
</p>
<p>vided to students on educational platforms such as
MOOCs to assist in their learning.
</p>
<p>We present an experimental evaluation of our
approach on the two datasets previously intro-
duced in Seo et al. (2015) and a new dataset col-
lected by us from a number of math textbooks in
India. Our experiments show that our approach
of leveraging demonstrations improves GEOS. We
also performed user studies with a number of
school students studying geometry, who found that
our approach is more interpretable as well as more
useful in comparison to GEOS.
</p>
<p>2 Background: GEOS
</p>
<p>GEOS solves geometry problems via a multi-stage
approach. It first learns to parse the problem text
and the diagram to a formal problem description
compatible with both of them. The problem de-
scription is a first-order logic expression (see Fig-
ure 1) that includes known numbers or geometrical
entities (e.g. 4 cm) as constants, unknown num-
bers or geometrical entities (e.g. O) as variables,
geometric or arithmetic relations (e.g. isLine, is-
Triangle) as predicates and properties of geomet-
rical entities (e.g. measure, liesOn) as functions.
The parser first learns a set of relations that poten-
tially correspond to the problem text (or diagram)
along with confidence scores. Then, a subset of
relations that maximize the joint text and diagram
score are picked as the problem description.
</p>
<p>For diagram parsing, GEOS uses a publicly
available diagram parser for geometry problems
(Seo et al., 2014) that provides confidence scores
for each literal to be true in the diagram. We use
the diagram parser from GEOS to handle in our
work too.
</p>
<p>Text parsing is performed in three stages. The
parser first maps words or phrases in the text to
their corresponding concepts. Then, it identifies
relations between identified concepts. Finally, it
performs relation completion which handles im-
plications and coordinating conjunctions.
</p>
<p>Finally, GEOS uses a numerical approach to
check the satisfiablity of literals, and to answer
the multiple-choice question. While this solver
is grounded in coordinate geometry and indeed
works well, it has some issues: GEOS requires
an explicit mapping of each predicate to a set of
constraints over point coordinates. For example,
the predicate isPerpendicular(AB, CD) is mapped
to the constraint yB−yAxB−xA ×
</p>
<p>yD−yC
xD−xC = −1. These con-
</p>
<p>252</p>
<p />
</div>
<div class="page"><p />
<p>Axiom Premise Conclusion
Midpoint Definition midpoint(M, AB) length(AM) = length(MB)
</p>
<p>Angle Addition interior(D, ABC) angle(ABC) = angle(ABD) + angle(DBC)
Supplementary Angles perpendicular(AB,CD) ∧ liesOn(C,AB) angle(ACD) + angle(DCB) = 180◦
Vertically Opp. Angles intersectAt(AB, CD, M) angle(AMC) = angle(BMD)
</p>
<p>Table 1: Examples of geometry theorems as horn clause rules.
</p>
<p>straints can be non-trivial to write and often re-
quire manual engineering. As a result, GEOS’s
constraint set is incomplete and it cannot solve a
number of SAT style geometry problems. Further-
more, this solver is not interpretable. As our user
studies show, it is not natural for a student to un-
derstand the solution of these geometry problems
in terms of satisfiability of constraints over coor-
dinates. A more natural way for students to under-
stand and reason about these problems is through
deductive reasoning using well-known axioms and
theorems of geometry. This kind of deductive rea-
soning is used in explanations in textbooks. In
contrast to GEOS which uses supervised learning,
our approach learns to solve geometry problems
by interpreting natural language demonstrations of
the solution. These demonstrations illustrate the
process of solving the geometry problem via step-
wise application of geometry theorems.
</p>
<p>3 Theorems as Horn Clause Rules
</p>
<p>We represent theorems as horn clause rules that
map a premise in the logical language to a conclu-
sion in the same language. Table 1 gives some
examples of geometry theorems written as horn
clause rules. The free variables in the theorems
are universally quantified. The variables are also
typed. For example, ABC can be of type triangle
or angle but not line. Let T be the set of theo-
rems. Formally, each theorem t ∈ T maps a log-
ical formula l(pr)t corresponding to the premise to
a logical formula l(co)t corresponding to the con-
clusion. The demonstration can be seen as a pro-
gram – a sequence of horn clause rule applications
that lead to the solution of the geometry problem.
Given a current state, theorem t can be applied
to the state if there exists an assignment to free
variables in l(pr)t that is true in the state. Each
theorem application also has a probability asso-
ciated with it; in our case, these probabilities are
learned by a trained model. The state diagram for
the demonstration in Figure 2 is shown in Figure
3. Now, we describe the various components of
our learning from demonstrations approach: a se-
</p>
<p>Figure 3: State sequence corresponding to the demonstration
in Figure 2. Theorems applied are marked in green and the
state information is marked in red. Here S0 corresponds to the
state derived from question interpretation and each theorem
application subsequently adds new predicates to the logical
formula corresponding to S0. The final state contains the an-
swer: measure(ADB, 60◦). This annotation of states and the-
orem applications is provided only for illustrative purposes.
It is not required by our model.
</p>
<p>mantic parser to interpret the demonstration and a
deductive solver that learns to chain theorems.
</p>
<p>4 Approach
</p>
<p>4.1 Interpretation via Semantic Parsing
</p>
<p>We first describe a semantic parser that maps a
piece of text (in the geometry question or a demon-
stration) to a logical expression such as the one
shown in Figure 1. Our semantic parser uses
a part-based log-linear model inspired from the
multi-step approach taken in GEOS, which, in-
turn is closely related to prior work in relation ex-
traction and semantic role labeling. However, un-
like GEOS, our parser combines the various steps
in a joint model. Our parser first maps words or
phrases in the input text x to corresponding con-
cepts in the geometry language. Then, it identi-
fies relations between identified concepts. Finally,
it performs relation completion to handle implica-
tions and coordinating conjunctions. We choose
a log-linear model over the parses which decom-
poses into two parts. Let p = {p1, p2} where p1
denotes the concepts identified in p and p2 de-
notes the identified relations. The relation com-
pletion is performed by using a similar rule-based
approach as in GEOS. The log-linear model also
</p>
<p>253</p>
<p />
</div>
<div class="page"><p />
<p>factorizes into two components for concept and re-
lation identification:
</p>
<p>P(p|x;θθθ p) = 1Z(x;θθθ p) exp
(
θθθ Tpφφφ(p,x)
</p>
<p>)
θθθ Tpφφφ(p,x) = θθθ
</p>
<p>T
p1φ1φ1φ1(p1,x)+θθθ
</p>
<p>T
p2φφφ 2(p2,x)
</p>
<p>Z(x;θθθ p) is the partition function of the log-linear
model and φφφ is the concatenation [φφφ 1 φφφ 2]. The
complexity of searching for the highest scoring
latent parse is exponential. Hence, we use beam
search with a fixed beam size (100) for inference.
That is, in each step, we only expand the ten most
promising candidates so far given by the current
score. We first infer p1 to identify a beam of
concepts. Then, we infer p2 to identify relations
among candidate concepts. We find the optimal
parameters θθθ p using maximum-likelihood estima-
tion with L2 regularization:
</p>
<p>θθθ ∗p = argmax
θθθ p
</p>
<p>∑
(x,p)∈Train
</p>
<p>logP(p|x;θθθ p)−λ ||θθθ p||22
</p>
<p>We use L-BFGS to optimize the objective. Fi-
nally, relation completion is performed using a de-
terministic rule-based approach as in GEOS which
handles implicit concepts like the “Equals” rela-
tion in the sentence “Circle O has a radius of 5”
and coordinating conjunctions like “bisect” be-
tween the two lines and two angles in “AM and
CM bisect BAC and BCA”. We refer the interested
reader to section 4.3 in Seo et al. (2015) for details.
</p>
<p>This semantic parser is used to identify program
states in demonstrations as well as to map geome-
try questions to logical expressions.
</p>
<p>4.1.1 State and Axiom Identification
Given a demonstrative solution of a geometry
problem in natural language such as the one shown
in Figure 2, we identify theorem applications by
two simple heuristics. Often, theorem mentions
in demonstrations collected from textbooks are la-
beled as references to theorems previously intro-
duced in the textbook (for example, “Theorem
3.1”). In this case, we simply label the theo-
rem application as the referenced theorem. Some-
times, the theorems are mentioned verbosely in
the demonstration. To identify these mentions, we
collect a set of theorem mentions from textbooks.
Each theorem is also represented as a set of the-
orem mentions. Then, we use an off-the-shelf se-
mantic text similarity system (Šarić et al., 2012)
and check if a contiguous sequence of sentences
</p>
<p>in the demonstration is a paraphrase of any of the
gold theorem mentions. If the degree of similar-
ity of a contiguous sequence of sentences in the
demonstration with any of the gold theorem men-
tions is above a threshold, our system labels the se-
quence of sentences as the theorem. The text sim-
ilarity system is tuned on the training dataset and
the threshold is tuned on the development set. This
heuristic works well and has a small error (&lt; 10%)
on our development set.
</p>
<p>For state identification, we use our semantic
parser. The initial state corresponds to the logical
expression corresponding to the question. Subse-
quent states are derived by parsing sentences in the
demonstration. The identified state sequences are
used to train our deductive solver.
</p>
<p>4.2 Deductive Solver
</p>
<p>Our deductive solver, inspired from Krishna-
murthy et al. (2016), uses the parsed state and
axiom information (when provided) and learns to
score the sequence of axiom applications which
can lead to the solution of the problem. Our solver
uses a log-linear model over the space of possible
axiom applications. Given a set of theorems T
and optionally demonstration d, we assume T =
[t1, t2, . . . tk] to be a sequence of theorem applica-
tions. Each theorem application leads to a change
in state. Let s0 be the initial state determined by
the logical formula derived from the question text
and the diagram. Let s = [s1,s2, . . .sk] be the se-
quence of program states after corresponding the-
orem applications. The final state sk contains the
answer to the question. We define the model score
of the deduction as:
</p>
<p>P(s|T,d;θθθ ex) =
1
</p>
<p>Z(T,d;θθθ ex)
</p>
<p>k
</p>
<p>∏
i=1
</p>
<p>exp
(
θθθ Texψψψ(si−1,si, ti,d)
</p>
<p>)
Here, θθθ ex represents the model parameters and ψψψ
represents the feature vector that depends on the
successive states si−1 and si, the demonstration
d and the corresponding theorem application ti.
We find optimal parameters θθθ ex using maximum-
likelihood estimation with L2 regularization:
</p>
<p>θθθ ∗ex = argmax
θθθ ex
</p>
<p>∑
s∈Train
</p>
<p>logP(s|T,d;θθθ ex)−µ||θθθ ex||22
</p>
<p>We use beam search for inference and L-BFGS to
optimize the objective.
</p>
<p>254</p>
<p />
</div>
<div class="page"><p />
<p>4.3 Joint Semantic Parsing and Deduction
</p>
<p>Finally, we describe a joint model for semantic
parsing and problem solving that parses the geom-
etry problem text, the demonstration when avail-
able, and learns a sequence of theorem applica-
tions that can solve the problem.
</p>
<p>In this case, we use a joint log-linear model for
semantic parsing and deduction. The model com-
prises of factors that scores semantic parses of the
question and the demonstration (when provided)
and the other that scores various possible theo-
rem applications. The model predicts the answer
a given the question q (and possibly demonstra-
tion d) using two latent variables: p represents
the latent semantic parse of the question and the
demonstration which involves identifying the log-
ical formula for the question (and for every state
in the demonstration when provided) and s repre-
sents the (possibly latent) program.
</p>
<p>P(p,s|q,a,d;θθθ) ∝ fp(p|{q,a,d};θθθ p)
× fs(s|T,d, ;θθθ s)
</p>
<p>Here, θθθ = {θθθ p,θθθ ex}. fp and fs represent
the factors for semantic parsing and deduc-
tion. fp(p|{q,a,d};θθθ p) ∝ exp
</p>
<p>(
θθθ Tpφφφ(p,{q,a,d})
</p>
<p>)
and fs(s|T,d, ;θθθ s) ∝
</p>
<p>k
∏
i=1
</p>
<p>exp
(
θθθ Texψψψ(si−1,si, ti,d)
</p>
<p>)
as defined in Sections 4.1 and 4.2. Next, we de-
scribe approaches to learn the joint model with
various kinds of supervision.
</p>
<p>4.4 Learning from Types of Supervision
</p>
<p>Our joint model for parsing and deduction can be
learned using various kinds of supervision. We
provide a learning algorithm when (a) we only
have geometry question-answer pairs as supervi-
sion, (b) when we have geometry questions and
demonstrations for solving them, and (c) mixed
supervision: when we have a set of geometry
question-answer pairs in addition to some geom-
etry questions and demonstrations. To do this,
we implement two supervision schemes (Krishna-
murthy et al., 2016). The first supervision scheme
only verifies the answer and treats other states in
the supervision as latent. The second scheme ver-
ifies every state in the program. We combine both
kinds of supervision when provided. Given super-
vision {qi,ai}ni=1 and {qi,ai.di}mi=1, we define the
</p>
<p>following L2 regularized objective:
</p>
<p>J (θθθ) = ν
n
</p>
<p>∑
i=1
</p>
<p>log ∑
p,s
</p>
<p>P(p,s|qi,ai;θθθ)×1exec(s)=ai
</p>
<p>+(1−ν)
m
</p>
<p>∑
i=1
</p>
<p>log ∑
p,s
</p>
<p>P(p,s|qi,ai,di;θθθ)×1s(di)=s
</p>
<p>−λ ||θθθ p||22−µ||θθθ ex||22
For learning from answers, we set ν = 1. For
learning from demonstrations, we set ν = 0. We
tune hyperparameters λ , µ and ν on a held out
dev set. We use L-BFGS, using beam search for
inference for training all our models. To avoid re-
peated usage of unnecessary theorems in the so-
lution, we constrain the next theorem application
to be distinct from previous theorem applications
during beam search.
</p>
<p>4.5 Features
Next, we define our feature set: φφφ 1, φφφ 2 for learn-
ing the semantic parser and ψψψ for learning the de-
duction model. Semantic parser features φφφ 1 and
φφφ 2 are inspired from GEOS. The deduction model
features ψψψ score consecutive states in the deduc-
tion si−1, si and the theorem ti which when applied
to si−1 leads to si. ψψψ comprises of features that
score if theorem ti is applicable on state si−1 and if
the application of ti on state si−1 leads to state si.
Table 2 lists the feature set.
</p>
<p>5 Demonstrations Dataset
</p>
<p>We collect a new dataset of demonstrations for
solving geometry problems from a set of grade 6-
10 Indian high school math textbooks by four pub-
lishers/authors – NCERT1, R S Aggarwal2, R D
Sharma3 and M L Aggarwal4 – a total of 5× 4 =
20 textbooks as well as a set of online geometry
problems and solutions from three popular edu-
cational portals: Tiwari Academy5, School Lamp6
</p>
<p>and Oswaal Books7 for grade 6-10 students in In-
dia. Millions of students in India study geome-
try from these books and portals every year and
these materials are available online. We manually
</p>
<p>1http://epathshala.nic.in/
e-pathshala-4/flipbook/
</p>
<p>2http://www.amazon.in/
Books-R-S-Aggarwal/
</p>
<p>3http://www.amazon.in/Books-R-Sharma/
4http://www.amazon.in/
</p>
<p>Books-Aggarwal-M-L/
5http://www.tiwariacademy.com/
6http://www.schoollamp.com
7http://www.oswaalbooks.com
</p>
<p>255</p>
<p />
</div>
<div class="page"><p />
<p>φφ φ
1
</p>
<p>Lexicon Map Indicator that the word or phrase maps to a predicate in a lexicon created in
GEOS. GEOS derives correspondences between words/phrases and geometry
keywords and concepts in the geometry language using manual annotations
in its training data. For instance, the lexicon contains (“square”, square, Is-
Square) including all possible concepts for the phrase “square”.
</p>
<p>Regex for num-
bers and explicit
variables
</p>
<p>Indicator that the word or phrase satisfies a regular expression to detect num-
bers or explicit variables (e.g. “5”, “AB”, “O”). These regular expressions
were built as a part of GEOS.
</p>
<p>φφ φ
2
</p>
<p>Dependency
tree distance
</p>
<p>Shortest distance between the words of the concept nodes in the dependency
tree. We use indicator features for distances of -3 to 3. Positive distance
shows if the child word is at the right of the parentâĂŹs in the sentence, and
negative otherwise.
</p>
<p>Word distance Distance between the words of the concept nodes in the sentence.
Dependency
edge
</p>
<p>Indicator functions for outgoing edges of the parent and child for the shortest
path between them.
</p>
<p>Part of speech
tag
</p>
<p>Indicator functions for the POS tags of the parent and the child
</p>
<p>Relation type Indicator functions for unary / binary parent and child nodes.
Return type Indicator functions for the return types of the parent and the child nodes. For
</p>
<p>example, return type of Equals is boolean, and that of LengthOf is numeric.
</p>
<p>ψψ ψ
</p>
<p>State and the-
orem premise
predicates
</p>
<p>Treat the state si−1 and theorem premise l
(pr)
ti as multi-sets of predicates. The
</p>
<p>feature is given by div(si−1||l(pr)ti ), the divergence between the two multi-
sets. div(A,B), the divergence between multi-sets A and B is given by
∑k
</p>
<p>min(Ak,Bk)
Bk
</p>
<p>which measures the degree to which the elements in A satisfy
the pre-condition in B.
</p>
<p>State and the-
orem premise
predicate-
arguments
</p>
<p>Now treat the state si−1 and theorem premise l
(pr)
ti as two multi-sets over
</p>
<p>predicate-arguments. The feature is given by div(si−1||l(pr)ti ), the divergence
between the two multi-sets.
</p>
<p>State and theo-
rem conclusion
predicates
</p>
<p>Now treat the state si and theorem conclusion l
(co)
ti as two multi-sets over
</p>
<p>predicate-arguments. The feature is given by div(si||l(co)ti ), the divergence
between the two multi-sets.
</p>
<p>State and theo-
rem conclusion
predicate-
arguments
</p>
<p>Now treat the state si and theorem conclusion l
(co)
ti as two multi-sets over
</p>
<p>predicate-arguments. The feature is given by div(si||l(co)ti ), the divergence
between the two multi-sets.
</p>
<p>State and theo-
rem conclusion
predicates
</p>
<p>Treat the state si and theorem conclusion l
(co)
ti as two distributions over predi-
</p>
<p>cates. The feature is the total variation distance between the two distributions.
</p>
<p>State and theo-
rem conclusion
predicate-
arguments
</p>
<p>Now treat the state ei and theorem conclusion l
(co)
ti as two distributions over
</p>
<p>predicate-arguments. The feature is the total variation distance between the
two distributions.
</p>
<p>Product Fea-
tures
</p>
<p>We additionally use three product features: ψψψ1ψψψ3ψψψ5, ψψψ2ψψψ4ψψψ6 and
ψψψ1ψψψ2ψψψ3ψψψ4ψψψ5ψψψ6
</p>
<p>Table 2: The feature set for our joint semantic-parsing and deduction model. Features φφφ 1 and φφφ 2 are motivated from GEOS
</p>
<p>256</p>
<p />
</div>
<div class="page"><p />
<p>marked chapters relevant for geometry in these
books and then parsed them using Adobe Acro-
bat’s pdf2xml parser. Then, we manually extracted
example problems leading to a total of 2235 geom-
etry problems with demonstrations. We also an-
notated 1000 demonstrations by labeling the var-
ious states and theorem applications. We manu-
ally collected a set of theorems of geometry by go-
ing through the textbooks, and wrote them as horn
clause rules. A total of 293 unique theorems were
collected. Then, we marked contiguous sentences
in the demonstration texts as one of these 293 the-
orems or as states. An example annotation for the
running example in Figures 1 and 2 is provided in
Figure 3. Note that the annotation of states and
theorem applications is not used in training our
models and is only used for testing the accuracy
of the programs induced by our model.
</p>
<p>6 Experiments
</p>
<p>We use three geometry question datasets for eval-
uating our system: practice and official SAT style
geometry questions used in GEOS, and an ad-
ditional dataset of geometry questions collected
from the aforementioned textbooks. We selected
a total of 1406 SAT style questions across grades
6-10. This dataset is approximately 7.5 times
the size of the datasets used in Seo et al. (2015).
We split the dataset into training (350 questions),
development (150 questions) and test (906 ques-
tions) with equal proportion of grade 6-10 ques-
tions. We also annotated the training and de-
velopment set questions with ground-truth logi-
cal forms. GEOS used 13 types of entities, 94
functions and predicates. We added some more
entities, functions and predicates to cover other
more complex concepts in geometry not covered
in GEOS. Thus, we obtained a final set of 19 en-
tity types and 115 functions and predicates. We
use the training set to train our semantic parser
with expanded set of entity types, functions and
predicates. We used Stanford CoreNLP (Manning
et al., 2014) for linguistic pre-processing. We also
adapted the GEOS solver to the expanded set of
entities, functions and predicates for comparison
purposes. We call this system GEOS++.
</p>
<p>6.1 Quantitative Results
</p>
<p>We evaluated our joint model of semantic parsing
and deduction with various settings for training:
training on question-answer pairs or demonstra-
</p>
<p>P O T
GEOS 61 49 32
</p>
<p>GEOS++ 62 49 44
O.S. (QA Pairs) 63 52 47
</p>
<p>O.S. (Demonstrations) 66 55 56
O.S. (QA + Demonstrations) 67 57 58
</p>
<p>Table 3: Scores of various approaches on the SAT practice
(P) and official (O) datasets and a dataset of questions from
the 20 textbooks (T). We use SATâĂŹs grading scheme that
rewards a correct answer with a score of 1.0 and penalizes a
wrong answer with a negative score of 0.25. O.S. represents
our system trained on question-answer (QA) pairs, demon-
strations, or a combination of QA pairs and demonstrations.
</p>
<p>tions alone, or with a combination of question-
answer pairs and demonstrations. We compare
our joint semantic parsing and deduction models
against GEOS and GEOS++.
</p>
<p>In the first setting, we only use question-answer
pairs as supervision. We compare our seman-
tic parsing and deduction model to GEOS and
GEOS++ on practice and official SAT style ge-
ometry questions from Seo et al. (2015) as well
as the dataset of geometry questions collected
from the 20 textbooks (see Table 3). On all the
three datasets, our system outperforms GEOS and
GEOS++. Especially on the dataset from the 20
textbooks (which is a harder dataset and includes
more problems which require complex reasoning
supported by our deduction model), GEOS and
GEOS++ do not perform very well whereas our
system achieves a very good score.
</p>
<p>Next, we only use demonstrations to train our
joint model (see Table 3). We test this model
on the aforementioned datasets and compare it
to GEOS and GEOS++ trained on respective
datasets. Again, our system outperforms GEOS
and GEOS++ on all three datasets. Especially on
the textbook dataset, this model trained on demon-
strations has significant improvements as our se-
mantic parsing and deduction model trains the de-
duction model as well and learns to reason about
geometry using axiomatic knowledge.
</p>
<p>Finally, we train our semantic parsing and
deduction model on a combination of question
answer-pairs and demonstrations. This model
trained on question-answer pairs and demonstra-
tions leads to further improvements over mod-
els trained only question-answer pairs or only on
demonstrations. These results (shown in Table 3)
hold on all the three datasets.
</p>
<p>We tested the correctness of the parses and the
</p>
<p>257</p>
<p />
</div>
<div class="page"><p />
<p>P R F1
GEOS 0.82 0.63 0.71
</p>
<p>O.S. (Parser) 0.88 0.75 0.81
O.S. (Joint) 0.89 0.80 0.84
</p>
<p>Table 4: Precision, Recall and F1 scores of the parses induced
by GEOS and our models when only the parsing model or the
joint model is used.
</p>
<p>Deduction Joint
QA Pairs 0.56 0.61
</p>
<p>Demonstrations 0.64 0.68
QA + Demonstrations 0.68 0.70
</p>
<p>Table 5: Accuracy of the programs induced by various ver-
sions of our joint model trained on question-answer pairs,
demonstrations or a combination of the two. We provide re-
sults when we use the deduction model or the joint model.
</p>
<p>deductive programs induced by our models. First,
we compared the parses induced by our models
with gold parses on the development set. Table
4 reports the Precision, Recall and F1 scores of
the parses induced by our models when only the
parsing model or when the joint model is used and
compares it with GEOS. We conclude that both
our models perform better as compared to GEOS
in parsing. Furthermore, our joint model of pars-
ing and deduction further improves the parsing ac-
curacy. Then, we compared the programs induced
by the aforementioned models with gold program
annotations on the textbook dataset. Table 5 re-
ports the accuracy of programs induced by var-
ious versions of our models. Our models when
trained on demonstrations induces more accurate
programs as compared to the semantic parsing and
deduction model when trained on question-answer
pairs. Moreover, the semantic parsing and deduc-
tion model when trained on question-answer pairs
as well as demonstrations achieves an even better
accuracy. Our joint model of parsing and deduc-
tion induces more accurate programs as compared
to the deduction model alone.
</p>
<p>6.2 User Study on Interpretability
</p>
<p>A key benefit of our axiomatic solver is that it
provides an easy-to-understand student-friendly
demonstrative solution to geometry problems.
This is important because students typically learn
geometry by rigorous deduction whereas numeri-
cal solvers do not provide such interpretability.
</p>
<p>To test the interpretability of our axiomatic
solver, we asked 50 grade 6-10 students (10 stu-
</p>
<p>Interpretability Usefulness
GEOS++ O.S. GEOS++ O.S.
</p>
<p>Grade 6 2.7 3.0 2.9 3.2
Grade 7 3.0 3.7 3.3 3.6
Grade 8 2.7 3.6 3.1 3.5
Grade 9 2.4 3.4 3.0 3.6
Grade 10 2.8 3.1 3.2 3.7
Overall 2.7 3.4 3.1 3.5
</p>
<p>Table 6: User study ratings for GEOS++ and our system
(O.S.) trained on question-answer pairs and demonstrations
by a number of grade 6-10 student subjects. Ten students in
each grade were asked to rate the two systems on a scale of
1-5 on two facets: ‘interpretability’ and ‘usefulness’. Each
cell shows the mean rating computed over ten students in that
grade for that facet.
</p>
<p>dents in each grade) to use GEOS++ and our best
performing system trained on question-answer
pairs and demonstrations as a web-based assistive
tool. They were each asked to rate how ‘inter-
pretable’ and ‘useful’ the two systems were for
their studies on a scale of 1-5. Table 6 shows
the mean rating by students in each grade on the
two facets. We can observe that students of each
grade found our system to be more interpretable as
well as more useful to them than GEOS++. This
study supports the need and the efficacy of an in-
terpretable solution for geometry problems. Our
solution can be used as an assistive tool for help-
ing students learn geometry on MOOCs.
</p>
<p>7 Related Work
</p>
<p>Solving Geometry Problems: Standardized tests
have been recently proposed as ‘drivers for
progress in AI’ (Clark and Etzioni, 2016). These
tests are easily accessible, and measurable, and
hence have attracted several NLP researchers.
There is a growing body of work on solving stan-
dardized tests such as reading comprehensions
(Richardson et al., 2013, inter alia), science ques-
tion answering (Clark, 2015; Schoenick et al.,
2016, inter alia), algebra word problems (Kush-
man et al., 2014; Roy and Roth, 2015, inter alia),
geometry problems (Seo et al., 2014, 2015) and
pre-university entrance exams (Fujita et al., 2014;
Arai and Matsuzaki, 2014).
</p>
<p>While the problem of using computers to
solve geometry questions is old (Feigenbaum and
Feldman, 1963; Schattschneider and King, 1997;
Davis, 2006), NLP and vision techniques were
first used to solve geometry problems in Seo et al.
(2015). While Seo et al. (2014) only aligned ge-
ometric shapes with their textual mentions, Seo
</p>
<p>258</p>
<p />
</div>
<div class="page"><p />
<p>et al. (2015) also extracted geometric relations and
built GEOS. We improve GEOS by building an ax-
iomatic solver that performs deductive reasoning
by learning from demonstrative problem solutions.
</p>
<p>Learning from Demonstration: Our work fol-
lows the learning from demonstration view of ma-
chine learning (Schaal, 1997) which stems from
the work on social learning in developmental psy-
chology (Meltzoff and Moore, 1977; Meltzoff,
1995). Learning from demonstration is a popu-
lar way of learning policies from example state
to action mappings in robotics applications. Im-
itation learning (Schaal, 1999; Abbeel and Ng,
2004; Ross et al., 2011) is a popular instance of
learning from demonstration where the algorithm
observes a human expert perform a series of ac-
tions to accomplish the task and learns a policy
that “imitates” the expert with the purpose of gen-
eralizing to unseen data. Imitation learning is in-
creasingly being used in NLP (Vlachos and Clark,
2014; Berant and Liang, 2015; Augenstein et al.,
2015; Beck et al., 2016; Goodman et al., 2016a,b).
However, all these models focus on learning re-
spective NLP models from the final supervision
e.g. semantic parses or denotations. However,
we provide a technique to learn from demonstra-
tions by learning a joint semantic parsing and de-
duction model. Another related line of work is
Hixon et al. (2015) who acquire knowledge in the
form of knowledge graphs for question answering
from natural language dialogs and (Goldwasser
and Roth, 2014) who propose a technique called
learning from natural instructions. Learning from
natural instructions allows human teachers to in-
teract with an automated learner using natural in-
structions, allowing the teacher to communicate
the domain expertise to the learner via natural lan-
guage. However, this work was evaluated on a
very simple Freecell game with a very small num-
ber of concepts (3). On the other hand, our model
is evaluated on a real task of solving SAT style ge-
ometry problems.
</p>
<p>Semantic Parsing: Semantic parsing is the
NLP task of learning to map language to a formal
meaning representation. Early semantic parsers
learnt the parsing model from natural language
utterances paired with logical forms (Zelle and
Mooney, 1993, 1996; Kate et al., 2005, inter alia).
However, recently indirect supervision, such as
denotations (Liang et al., 2011; Berant et al., 2013,
inter alia) and natural language directions for robot
</p>
<p>navigation (Shimizu and Haas, 2009; Matuszek
et al., 2010; Chen and Mooney, 2011, inter alia)
are being used to train these semantic parsers. In
most of the above examples, the execution model
is fairly simple (e.g. execution of a SQL query
in a database, or binary feedback for interaction
of the robot with the environment). However, our
work uses demonstrations such as those given in
textbooks for learning a semantic parser. Further-
more, our work learns the semantic parser along
with the execution model. In our case, the exe-
cution model is a program sequence constructed
from a set of theorem applications. Thus, our
work provides a way to integrate semantic pars-
ing with probabilistic programming. This integra-
tion has been pursued before for science diagram
question-answering on food-web networks (Krish-
namurthy et al., 2016) – which is closely related
to our work. Technically, our deductive solver and
the approach of learning from different kinds of
supervision are the same as the execution model
in Krishnamurthy et al. (2016). While Krishna-
murthy et al. (2016) only has two program encod-
ings, our work involves a much larger number of
programs. We also provide an approach for learn-
ing from demonstrations.
</p>
<p>8 Conclusion
</p>
<p>We described an approach that learns to solve SAT
style geometry problems using detailed demon-
strative solutions in natural language. The ap-
proach learns to jointly interpret demonstrations
as well as how to use this interpretation to deduc-
tively solve geometry problems using axiomatic
knowledge. Our approach showed significant im-
provements over the best previously published
work on a number of datasets. A user-study con-
ducted on a number of school students studying
geometry found our approach to be more inter-
pretable and useful than its predecessors. In the
future, we would like to extend our work in other
domains such as science QA (Jansen et al., 2016)
and use our work to assist student learning on plat-
forms such as MOOCs.
</p>
<p>Acknowledgments
</p>
<p>We thank the anonymous reviewers for their valu-
able comments and suggestions. This work was
supported by the following research grants: NSF
IIS1447676, ONR N000141410684 and ONR
N000141712463.
</p>
<p>259</p>
<p />
</div>
<div class="page"><p />
<p>References
Pieter Abbeel and Andrew Y Ng. 2004. Apprentice-
</p>
<p>ship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international confer-
ence on Machine learning. ACM, page 1.
</p>
<p>R.L. Allington and P.M. Cunningham. 2010. Children
benefit from modeling, demonstration, and explana-
tion .
</p>
<p>Noriko H Arai and Takuya Matsuzaki. 2014. The im-
pact of ai on education–can a robot get into the uni-
versity of tokyo? In Proc. ICCE. pages 1034–1042.
</p>
<p>Brenna D Argall, Sonia Chernova, Manuela Veloso,
and Brett Browning. 2009. A survey of robot learn-
ing from demonstration. Robotics and autonomous
systems 57(5):469–483.
</p>
<p>Isabelle Augenstein, Andreas Vlachos, and Diana
Maynard. 2015. Extracting relations between non-
standard entities using distant supervision and im-
itation learning. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, pages 747–757.
</p>
<p>Daniel Beck, Andreas Vlachos, Gustavo Paetzold, and
Lucia Specia. 2016. SHEF-MIME: word-level qual-
ity estimation using imitation learning. In Proceed-
ings of the First Conference on Machine Translation,
WMT 2016, colocated with ACL 2016, August 11-
12, Berlin, Germany. pages 772–776.
</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2013, 18-21 October
2013, Grand Hyatt Seattle, Seattle, Washington,
USA, A meeting of SIGDAT, a Special Interest Group
of the ACL. pages 1533–1544.
</p>
<p>Jonathan Berant and Percy Liang. 2015. Imitation
learning of agenda-based semantic parsers. Trans-
actions of the Association for Computational Lin-
guistics 3:545–558.
</p>
<p>Daphna Buchsbaum, Alison Gopnik, Thomas L Grif-
fiths, and Patrick Shafto. 2011. ChildrenâĂŹs im-
itation of causal action sequences is influenced by
statistical and pedagogical evidence. Cognition
120(3):331–340.
</p>
<p>John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
et al. 2001. Issues, tasks and program structures to
roadmap research in question &amp; answering (q&amp;a) .
</p>
<p>Lucas P Butler and Ellen M Markman. 2014.
Preschoolers use pedagogical cues to guide radical
reorganization of category knowledge. Cognition
130(1):116–127.
</p>
<p>David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011). pages 859–865.
</p>
<p>Peter Clark. 2015. Elementary School Science and
Math Tests as a Driver for AI:Take the Aristo Chal-
lenge! In Proceedings of IAAI.
</p>
<p>Peter Clark and Oren Etzioni. 2016. My computer is
an honor student - but how intelligent is it? stan-
dardized tests as a measure of ai. In Proceedings of
AI Magazine.
</p>
<p>Tom Davis. 2006. Geometry with computers. Techni-
cal report.
</p>
<p>Edward A Feigenbaum and Julian Feldman. 1963.
Computers and thought. The AAAI Press.
</p>
<p>Richard M Felder, Donald R Woods, James E Stice,
and Armando Rugarcia. 2000. The future of en-
gineering education ii. teaching methods that work.
Chemical Engineering Education pages 26–39.
</p>
<p>Akira Fujita, Akihiro Kameda, Ai Kawazoe, and
Yusuke Miyao. 2014. Overview of todai robot
project and evaluation framework of its nlp-based
problem solving. World History 36:36.
</p>
<p>Dan Goldwasser and Dan Roth. 2014. Learning from
natural instructions. Machine Learning 94(2):205–
232.
</p>
<p>James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016a. Noise reduction and targeted explo-
ration in imitation learning for abstract meaning rep-
resentation parsing. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers).
</p>
<p>James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016b. Ucl+ sheffield at semeval-2016 task
8: Imitation learning for amr parsing with an α-
bound. Proceedings of SemEval pages 1167–1172.
</p>
<p>Ben Hixon, Peter Clark, and Hannaneh Hajishirzi.
2015. Learning knowledge graphs for question an-
swering through conversational dialog. In NAACL
HLT 2015, The 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Den-
ver, Colorado, USA, May 31 - June 5, 2015. pages
851–861. http://aclweb.org/anthology/N/N15/N15-
1086.pdf.
</p>
<p>Peter Jansen, Niranjan Balasubramanian, Mihai Sur-
deanu, and Peter Clark. 2016. What’s in an
explanation? characterizing knowledge and in-
ference requirements for elementary science ex-
ams. In COLING 2016, 26th International Con-
ference on Computational Linguistics, Proceed-
ings of the Conference: Technical Papers, Decem-
ber 11-16, 2016, Osaka, Japan. pages 2956–2965.
http://aclweb.org/anthology/C/C16/C16-1278.pdf.
</p>
<p>260</p>
<p />
</div>
<div class="page"><p />
<p>Rohit J Kate, Yuk Wah, Wong Raymond, and
J Mooney. 2005. Learning to transform natural to
formal languages. In Proceedings of AAAI-05. Cite-
seer.
</p>
<p>Jayant Krishnamurthy, Oyvind Tafjord, and Aniruddha
Kembhavi. 2016. Semantic parsing to probabilistic
programs for situated question answering. In Jian
Su, Xavier Carreras, and Kevin Duh, editors, Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016. The As-
sociation for Computational Linguistics, pages 160–
170.
</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.
</p>
<p>Percy Liang, Michael I Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1. Association
for Computational Linguistics, pages 590–599.
</p>
<p>Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard,
and David McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit.
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.
</p>
<p>Cynthia Matuszek, Dieter Fox, and Karl Koscher.
2010. Following directions using statistical ma-
chine translation. In 2010 5th ACM/IEEE Inter-
national Conference on Human-Robot Interaction
(HRI). IEEE, pages 251–258.
</p>
<p>Andrew N Meltzoff. 1995. Understanding the inten-
tions of others: re-enactment of intended acts by
18-month-old children. Developmental psychology
31(5):838.
</p>
<p>Andrew N. Meltzoff and M. Keith Moore. 1977.
Imitation of facial and manual gestures by
human neonates. Science 198(4312):75–78.
https://doi.org/10.1126/science.198.4312.75.
</p>
<p>Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP).
</p>
<p>Stéphane Ross, Geoffrey J. Gordon, and Drew Bag-
nell. 2011. A reduction of imitation learning and
structured prediction to no-regret online learning. In
Proceedings of the Fourteenth International Confer-
ence on Artificial Intelligence and Statistics, AIS-
TATS 2011, Fort Lauderdale, USA, April 11-13,
2011. pages 627–635.
</p>
<p>Subhro Roy and Dan Roth. 2015. Solving gen-
eral arithmetic word problems. In Proceedings of
EMNLP.
</p>
<p>Stefan Schaal. 1997. Learning from demonstration.
In M. I. Jordan and T. Petsche, editors, Advances
in Neural Information Processing Systems 9, MIT
Press, pages 1040–1046.
</p>
<p>Stefan Schaal. 1999. Is imitation learning the route
to humanoid robots? Trends in cognitive sciences
3(6):233–242.
</p>
<p>Doris Schattschneider and James King. 1997. Geom-
etry Turned On: Dynamic Software in Learning,
Teaching, and Research. Mathematical Association
of America Notes.
</p>
<p>Carissa Schoenick, Peter Clark, Oyvind Tafjord,
Peter D. Turney, and Oren Etzioni. 2016.
Moving beyond the turing test with the allen
AI science challenge. CoRR abs/1604.04315.
http://arxiv.org/abs/1604.04315.
</p>
<p>Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, and
Oren Etzioni. 2014. Diagram understanding in ge-
ometry questions. In Proceedings of AAAI.
</p>
<p>Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren
Etzioni, and Clint Malcolm. 2015. Solving geome-
try problems: combining text and diagram interpre-
tation. In Proceedings of EMNLP.
</p>
<p>Nobuyuki Shimizu and Andrew R. Haas. 2009. Learn-
ing to follow navigational route instructions. In
IJCAI 2009, Proceedings of the 21st Interna-
tional Joint Conference on Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009. pages
1488–1493.
</p>
<p>Andreas Vlachos and Stephen Clark. 2014. A new cor-
pus and imitation learning framework for context-
dependent semantic parsing. Transactions of the As-
sociation for Computational Linguistics 2:547–559.
</p>
<p>Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šna-
jder, and Bojana Dalbelo Bašić. 2012. Takelab:
Systems for measuring semantic text similar-
ity. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval
2012). Association for Computational Lin-
guistics, Montréal, Canada, pages 441–448.
http://www.aclweb.org/anthology/S12-1060.
</p>
<p>John M. Zelle and Raymond J. Mooney. 1993. Learn-
ing semantic grammars with constructive inductive
logic programming. In Proceedings of the 11th Na-
tional Conference on Artificial Intelligence. Wash-
ington, DC, USA, July 11-15, 1993.. pages 817–822.
</p>
<p>John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In In Proceedings of the Thirteenth
National Conference on Artificial Intelligence.
</p>
<p>261</p>
<p />
</div>
<div class="page"><p />
<p>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 262–270,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics
</p>
<p>Ways of Asking and Replying in Duplicate Question Detection
</p>
<p>João Rodrigues, Chakaveh Saedi, Vladislav Maraev, João Silva, António Branco
University of Lisbon
</p>
<p>{joao.rodrigues,chakaveh.saedi,vlad.maraev,
jsilva,antonio.branco}@di.fc.ul.pt
</p>
<p>Abstract
</p>
<p>This paper presents the results of sys-
tematic experimentation on the impact in
duplicate question detection of different
types of questions across both a number of
established approaches and a novel, supe-
rior one used to address this language pro-
cessing task. This study permits to gain
a novel insight on the different levels of
robustness of the diverse detection meth-
ods with respect to different conditions of
their application, including the ones that
approximate real usage scenarios.
</p>
<p>1 Introduction
</p>
<p>Automatic detection of semantically equivalent
questions is a language processing task of the ut-
most importance given the upsurge of interest in
conversational interfaces. It is a key procedure
in finding answers to questions. For instance, in
a context of customer support via a chat channel,
with the help of duplicate question detection, pre-
vious interactions between customers and human
operators can be explored to provide an increas-
ingly automatic question answering service. If a
new input question is equivalent to a question al-
ready stored, it can be replied automatically with
the answer stored with its recorded duplicate.
</p>
<p>Though it has been less researched than simi-
lar tasks, duplicate question detection (DQD) is
attracting an increasing interest. It can be seen
as belonging to a family of semantic text sim-
ilarity tasks, which have been addressed in Se-
mEval challenges since 2012, and which in the last
SemEval2016, for instance, included also tasks
like plagiarism detection or degree of similarity
between machine translation output and its post-
edited version, among others. Semantic textual
similarity assesses the degree to which two tex-
</p>
<p>tual segments are semantically equivalent to each
other, which is typically scored on an ordinal scale
ranging from semantic equivalence to complete
semantic dissimilarity.
</p>
<p>Paraphrase detection can be seen as a special
case of semantic textual similarity, where the scale
is reduced to its two extremes and the outcome for
an input pair is yes/no. DQD, in turn, could be
seen as a special case of paraphrase detection that
is restricted to interrogative expressions.
</p>
<p>While SemEval2016 had no task on yes/no
DQD, it had a “Question-Question” graded sim-
ilarity subtask of Task 1. The top performing
system in this subtask (0.74 Pearson correlation)
scored below the best result when all subtasks of
Task 1 are considered (0.77), and also below the
best scores of many of the other subtasks (e.g. 0.84
in plagiarism detection (Agirre et al., 2016)).
</p>
<p>While scores obtained for different tasks by sys-
tems trained and evaluated over different datasets
cannot be compared, those results nonetheless
lead one to ponder whether focusing on pairs of
interrogatives may be a task that is harder than
paraphrase detection that focuses on pairs of non-
interrogatives (e.g. plagiarism pairs), or at least
whether it needs different and specific approaches
for similar levels of performance to be attained.
</p>
<p>When checking for other research results
specifically addressing DQD, pretty competitive
results can be found, however, as in Bogdanova
et al. (2015). These authors used a dataset that
included a dump from the Meta forum in Stack-
Exchange (a source that would be explored also in
SemEval2016) and a dump from the AskUbuntu
forum, and reported over 92% accuracy.
</p>
<p>The pairs in these datasets are made of the tex-
tual segments that are submitted by the users of the
forums to elicit some feedback from other users
that may be of help, and that will pile up in threads
of reactions. They have two parts, known as “ti-
</p>
<p>262</p>
<p />
</div>
<div class="page"><p />
<p>tle” and “body”. The title tends to be a short seg-
ment identifying the issue being addressed, and
the body is where that issue is expanded, and can
be several paragraphs long.
</p>
<p>To avoid a maze of exactly duplicate questions,
and thus of duplicate threads, which would ham-
per the usability of the forums, for the same is-
sue, all duplicates except one are removed, leaving
only near duplicates—that are marked as such and
cross-linked to each other, and may be of help in
addressing the same topic from a different angle.
</p>
<p>The pairs of duplicate segments included in the
experimental datasets mentioned above are the ti-
tles and bodies of nearly duplicate threads. The
pairs of non-duplicate segments are made of titles
and bodies that are not near duplicate.
</p>
<p>While these “real life” data are important for
the development of DQD solutions that support
the management of these community forums, their
textual segments are quite far from expressions in
clean and clear interrogative form. The short sup-
ply of this sort of datasets has been perhaps part
of the reason why the DQD has not been more
researched. This may help to explain also the
lack of further studies so far on how the nature of
the questions and the data may impact the perfor-
mance of the systems on this task.
</p>
<p>The experiments reported in this paper aim to
address this issue and help to advance our under-
standing of the nature of DQD and to improve its
application. We will resort to previous datasets
used in the literature, just mentioned above, but
we will seek to explore also a new dataset from
Quora, released recently, in January 2017.
</p>
<p>The pairs of segments in this Quora dataset con-
cern any subject and are thus not restricted to any
domain. The segments are typically one sentence
long, clean and clear interrogative expressions.
Their grammatical well-formedness is ensured by
the volunteer experts that answer them and that,
before writing their replies, can use the editing fa-
cility to adjust the wording of the question entered
by the user if needed.
</p>
<p>This is in clear contrast with the other datasets
extracted from community forums. The forums
are organized by specific domains. The segments
may be several sentences long and are typically of-
fered in a sloppy wording, with non-standard ex-
pressions and suboptimal grammaticality.
</p>
<p>By resorting only to data of the latter type,
Bogdanova et al. (2015) confirmed that systems
</p>
<p>trained (and evaluated) on a smaller dataset that
is domain specific can perform substantially bet-
ter than when they are trained (and evaluated) on
a larger dataset from a generic domain.
</p>
<p>In this paper, we seek to further advance the un-
derstanding of DQD and possible constraints on
their development and application. We assess the
level of impact of the length of the segments in
the pairs, and study whether there is a difference
when systems handle well-edited, generic domain
segments, versus domain specific and sloppy ones.
</p>
<p>As the datasets with labeled pairs of segments
are scarce, to develop a system to a new specific
domain lacking a training dataset, the natural way
to go is to train it on a generic domain dataset. We
also study the eventual loss of performance in this
real usage scenario.
</p>
<p>These empirical contrasts may have a differ-
ent impact in different types of approaches to
DQD. The present study will be undertaken across
a range of different techniques, encompassing a
rule-based baseline, a classifier-based system and
solutions based on neural networks.
</p>
<p>To secure comparability of the individual re-
sults, the experimental datasets used are organized
along common settings. They have the same vol-
ume (30K pairs), the same training vs. testing
split rate (80%/20%), and the same class balance
(50%/50% of duplicates and non-duplicates).
</p>
<p>This paper is organized as follows. In Section
2, the datasets used are described. Sections 3, 4
and 5 present the experimental results of a range of
different detection techniques, respectively, rule-
based, supervised classifiers and neural networks.
In section 6, the results obtained are discussed, and
further experiments are reported in Section 7, ap-
proximating a real usage scenarios of application.
Sections 8 and 9 present the related work and the
conclusions.
</p>
<p>2 Datasets
</p>
<p>We used two datasets, from two sources:1 (i) from
the AskUbuntu online community forum where
a query entered by a user (in the form of a title
followed by a body) is answered with contribu-
tions from any other user (which are piled up in a
thread); and (ii) from Quora, an online moderated
question answering site where each query intro-
duced by a user, typically in a grammatical inter-
</p>
<p>1Datasets are available from https://github.com/nlx-
group/dqd
</p>
<p>263</p>
<p />
</div>
<div class="page"><p />
<p>1 Q How is the new Harry Potter book ’Harry Potter
and the Cursed Child’?
Q How bad is the new book by J.K Rowling?
</p>
<p>0 Q Should the toothbrush be wet or dry before apply-
ing the toothpaste?
Q What is the cheapest toothpaste?
</p>
<p>1 Q Can I install Ubuntu and Windows side by side?
Q How do I dual boot Windows along side Ubuntu?
</p>
<p>Figure 1: Three example question pairs and their
labels from the Quora dataset
</p>
<p>1 Q Why is more than 3GB of RAM not recognised
when using amd64?
Q Ubuntu 10.04 LTS 64bit only showing 2.9GB of
memory
</p>
<p>1 Q How can I fix a 404 Error when updating pack-
ages?
Q What does this mean &amp; what impact does it have:
Failed to download repository information
</p>
<p>0 Q hiphop, nginx, spdy
Q print xlsx file from command line using
ghostscript and libreoffice
</p>
<p>Figure 2: Three example segment pairs (titles
only) and their labels from the AskUbuntu dataset
</p>
<p>rogative sentence, receives an answer often from a
volunteer expert. For either dataset, the language
of the textual segments is English.
</p>
<p>We resorted to the first Quora dataset, released
by the end of January 2017.2 It consists of over
400k pairs of questions labeled 1 in case they are
duplicates of each other, or 0 otherwise. The pairs
in the dataset released were collected with sam-
pling techniques and their labeling may not be
fully correct, and are not restricted to any subject
(Iyer et al., 2017).
</p>
<p>The other dataset used here is similar to one of
the datasets used by Bogdanova et al. (2015). It
is made of queries from the AskUbuntu forum,3
</p>
<p>which are thus on a specific domain, namely from
the IT area, in particular about the Ubuntu opera-
tive system. We used AskUbuntu dump available,
from September 2014,4 containing 167,765 ques-
tions, of which 17,115 were labeled as a duplicate.
</p>
<p>A portion with 30k randomly selected pairs of
title+body was extracted, the same size as the por-
tion used by Bogdanova et al. (2015). This por-
tion is balanced, thus with an identical number of
duplicate and non-duplicate pairs. To support the
experiments described below, it was divided into
24k/6k for training/testing, an 80%/20% split.
</p>
<p>2https://data.quora.com/First-Quora-Dataset-Release-
Question-Pairs.
</p>
<p>3https://askubuntu.com/
4https://meta.stackexchange.com/a/224922.
</p>
<p>The textual segments in this dataset contain
both the title and the body of the query in the
corresponding thread, and this dataset is referred
to as AskUbuntuTB, while its counterpart with ti-
tles only—obtained by removing the bodies—is
referred to as AskUbuntuTO.
</p>
<p>To support comparison, a portion with 30k ran-
domly selected pairs was extracted also from the
Quora release, with the same duplicate vs. non-
duplicate balance and the same training vs. test
split rates as for the AskUbuntu dataset.
</p>
<p>The average length of the segments in number
of words is 84 in AskUbuntuTB. Its counterpart
AskUbuntuTO, with titles only, represent a very
substantial (10 times) drop to 8 words per segment
on average, which is similar to the 10 words per
segment in the Quora dataset.
</p>
<p>The vocabularies sizes of AskUbuntuTB,
AskUbuntuTO and Quora are 45k, 16k and 24k
items, respectively, and their volumes are 5M,
500k and 650k tokens, respectively. Concerning
the 400k pair Quora release, in turn, it contains
9M tokens and a 125k item vocabulary.
</p>
<p>3 Rule-based
</p>
<p>As a first approach experimented with, inspired by
(Wu et al., 2011), we resorted to the Jaccard Coef-
ficient over n-grams with n ranging 1 to 4.
</p>
<p>Before applying this technique, the textual seg-
ments were preprocessed by undertaking (i) to-
kenization and stemming, using the NLTK tok-
enizer and Porter stemmer(Bird, 2006); and (ii)
markup cleaning, whereby markup tags for refer-
ences, links, snippets of code, etc. were removed.
</p>
<p>To find the best threshold, we used the train-
ing set in a series of trials and applied the best
results for the test sets. This led to the thresh-
olds 0.1, 0.016, 0.03 for Quora, AskUbuntuTO
and AskUbuntuTB, respectively.
</p>
<p>This approach obtains 72.91% accuracy when
applied over AskUbuntuTB.5
</p>
<p>When running over AskUbuntuTO, its perfor-
mance seems not to be dramatically affected by the
much shorter segment length, suffering a slight de-
crease to 72.35%. Interestingly, a clear drop of the
accuracy of over 3 percentage points is observed
when it is run over Quora, scoring 69.53%.
</p>
<p>These results seem to indicate that while this
technique is quite robust with respect to the short-
</p>
<p>5This is in line with the accuracy score of 72.65% reported
by Bogdanova et al. (2015) with similar settings.
</p>
<p>264</p>
<p />
</div>
<div class="page"><p />
<p>Title: vsftpd not listing large directory from WAN interface
Body: I have vsftpd running on my Ubuntu server, which is behind an Asus RT-N66U router. Port 21 is forwarded to the
server. I can connect via my public IP address to the server (81 more words omitted)
Title: hiphop, nginx, spdy
Body: I’m about a month young at linux and brand new to ubuntu. I can do this to install hiphop
https://github.com/facebook/hiphop-php/wiki/Building-and-installing-HHVM-on-Ubuntu-12.04 (69 more words omitted)
Title: No wireless ubuntu13.10
Body: Installed ubuntu 13.10 yesterday no internet connection.12.10 ok and dongle ok,13.4 no dongle, nothing now. Compac
mini 110c, broadcom 4313 (AR8132 Q.Atheros.) Only have ubuntu on notebook.
</p>
<p>Figure 3: Three example segments (titles and bodies) from AskUbuntu dataset
</p>
<p>ening of the length of the segments, it is less robust
when its application changes from a specific to an
unconstrained domain.
</p>
<p>4 Classifier
</p>
<p>4.1 Basic features
To set up a DQD system resorting to an approach
based on a supervised machine learning classi-
fier, we resorted to supporting vector machines
(SVM), following its acknowledged good perfor-
mance in this sort of tasks and following an option
also taken by Bogdanova et al. (2015). We em-
ployed SVC (Support Vector Classification) im-
plementation from the sklearn support vector ma-
chine toolkit (Pedregosa et al., 2011)
</p>
<p>For the first version of the classifier, a basic fea-
ture set (FS) was adopted. N -grams, with n from
1 to 4, were extracted from the training set and the
ones with at least 10 occurrences6 were selected to
support the FS
</p>
<p>For each textual segment in a pair, a vector of
size k was generated, where k is the number of n-
grams included in the FS. Each vector encodes
the occurrences of the n-grams in the correspond-
ing segment, where vector position i will be 1 if
the i-th n-gram occurs in the segment, and 0 oth-
erwise. Then a feature vector of size 2k is created
by concatenating the vectors of the two segments.
This vector is further extended with the scores of
the Jaccard coefficient determined over 1, 2, 3 and
4-grams. Hence, the final feature vector represent-
ing the pair to the classifier has the length 2k + 4.
</p>
<p>This system achieves 70.25% accuracy7 when
trained over the AskUbuntuTB. Its accuracy drops
some 1.5 percentage points, to 68.88%, when
trained with the shorter segments of AskUbun-
tuTO, and drops over 5 points, to 64.93%, when
</p>
<p>6We tried thresholds ranging from 5 to 15.
7We tried also with another implementation of SVM,
</p>
<p>namely SVM-light (Joachims, 2006), and the same score
70.25 was achieved.
</p>
<p>trained with Quora, also with shorter segments
than AskUbuntuTB but from a broader, all-
encompassing domain.
</p>
<p>4.2 Advanced features
</p>
<p>To have an insight on how strong an SVM-based
DQD resolver resorting to a basic FS like the one
described above may be, we proceeded with fur-
ther experiments, by adding more advanced fea-
tures. We used Princeton WordNet (Fellbaum,
1998) to bring semantic knowledge to the system
and used further text preprocessing to have more
explicit lexical information, namely the text was
normalized, e.g. “n’t” was replaced with “not”,
etc., and POS tagged, with NLTK.
</p>
<p>Lexical features The vector of each segment
was extended with an extra feature, namely the
number of negative words (e.g. nothing, never,
etc.) occurring in it. And, to the concatenation
of segment vectors, one further feature was added,
the number of nouns that are common to both seg-
ments, provided they are not already included in
the FS. Any pair was then represented by a vec-
tor of size 2(k + 1) + 4 + 1.
</p>
<p>Semantic features Eventually, any pair was rep-
resented by a vector of size 2(k + 1) + 4 + 2, with
its length being extended with yet an extra feature,
namely the value of the cosine similarity between
the embeddings of the segments in the pair.
</p>
<p>For a given segment, its embedding, or distribu-
tional semantic vector, was obtained by summing
up the embeddings of the nouns and verbs occur-
ring in it, as these showed to support the best per-
formance after experiments have been undertaken
with all parts-of-speech and their subsets.
</p>
<p>The embeddings were based on WordNet
synsets, rather than on words, as these were shown
to lead to better results after experimenting with
both options. We employed word2vec word em-
beddings (Mikolov et al., 2013) and used Autoex-
</p>
<p>265</p>
<p />
</div>
<div class="page"><p />
<p>tend (Rothe and Schütze, 2015) to extract synset
embeddings with the support of WordNet. We
adopted the same configuration as in that paper
and used version 3 of WordNet, which contains
over 120k concepts, represented by synsets. The
main advantage of synset embeddings over word
embeddings in duplicate detection is the fact that
synonyms receive exactly the same distributional
vectors, which helps to appropriately take into ac-
count words in the segments of the pair that are
different in linguistic form but are synonyms.
</p>
<p>Results The resulting system permitted an im-
provement of over 5 percentage points with re-
spect to its previous version trained with basic fea-
tures, scoring 75.87% accuracy when running over
AskUbuntuTB.
</p>
<p>This advantage is not so large when it is run
over the datasets with shorter segments. It scored
70.87% with AskUbuntuTO (positive delta of al-
most 2 points relative to the previous basic ver-
sion), and 68.56% with Quora (over 3.5 points bet-
ter).8
</p>
<p>5 Neural Networks
</p>
<p>We experimented with three different architectures
for DQD resolvers based on neural networks. The
first experiment adopts the architecture explored
in one of the papers reporting the most com-
petitive results for DQD, and the second adopts
the neural architecture of the top performing sys-
tem in the “Question-Question” subtask of Se-
mEval2016. The third system adopts a hybrid ar-
chitecture combining key ingredients of the previ-
ous two.
</p>
<p>5.1 Convolutional
</p>
<p>The architecture of convolutional neural network
(CNN) to address DQD was introduced by Bog-
danova et al. (2015). First, the CNN obtains the
vectorial representations of the words, also known
as word embeddings, in the two input segments.
Next, a convolutional layer constructs a vectorial
representation for each one of the two segments.
Finally, the two representations are compared us-
ing cosine similarity, whose value if above an em-
pirically estimated threshold, determines that the
two segments are duplicate (diagram in Figure 4).
</p>
<p>8This score was obtained resorting to 1- to 4-grams. Ex-
periments with 1- to 3-grams and with 1- to 5-grams delivered
worst scores, respectively 68.38% and 68.42%.
</p>
<p>WR CONV POOL cosine
similarity
</p>
<p>Figure 4: CNN architecture: word representation
(WR), convolution (CONV), pooling (POOL) and
cosine similarity measurement layers.
</p>
<p>To replicate this approach, we resorted to Keras
(Chollet, 2015) with Tensorflow (Abadi et al.,
2015) back-end for training and evaluating the
neural network. The hyper-parameters either
replicate the ones reported by Bogdanova et al.
(2015) or are taken from vanilla CNN architecture
as it is implemented in the above libraries.
</p>
<p>The DeepLearning4j9 toolkit was used for cre-
ating the initial word representations. Bogdanova
et al. (2015) specify only the skip-gram neural
network architecture and the embeddings dimen-
sionality of 200 as training parameters for their
best run. In our experiment, besides these param-
eters, all the other hyper-parameters were taken
from a vanilla version of word2vec implemented
in DeepLearning4j. In this experiment, to train
word embeddings, we used the 38 million tokens
of the September 2014 AskUbuntu dump avail-
able.10
</p>
<p>When trained over AskUbuntuTB, the system
performs with 73.40% accuracy. An improvement
of over 1 point, to 74.50%, was obtained with a
slight variant where the CNN was run without pre-
trained word embeddings, and with a random ini-
tialization of the embeddings using uniform distri-
bution.
</p>
<p>The drop in performance observed in the sys-
tems presented above when moving to shorter
segments is also observed here, with a much
greater impact with Quora, coming down almost
15 points, to 59.90%, than with AskUbuntuTO,
which comes down less than half a point, to
74.10%. This seems to indicate that the CNN is
less robust than previous approaches when mov-
ing from a specific to a generic domain.
</p>
<p>The score of 73.40%, obtained with settings
similar to Bogdanova et al. (2015), is inferior in
almost 20 percentage points to the score reported
in that paper. This led us to look more carefully in
the two experiments.
</p>
<p>As indicated in previous sections, in our experi-
9http://deeplearning4j.org
</p>
<p>10Bogdanova et al. (2015) used 121 million tokens from
the May 2014 dump available to them.
</p>
<p>266</p>
<p />
</div>
<div class="page"><p />
<p>ments the datasets were submitted to a preprocess-
ing phase, including markup cleaning by means of
which tags for references, links, snippets of code,
etc. were removed. One of these tags is rendered
to the reader of a thread in the AskUbuntu forum
as “Possible duplicate: &lt;title&gt;”, where &lt;title&gt; is
instantiated with the title of the other thread that
the present one is a possible duplicate of, and is
linked to the page containing that other thread.
</p>
<p>As we hypothesized that this might be a reason
for the 20 point delta observed, we retrained our
CNN-based system over AskUbuntuTB slightly
modified just to keep that “Possible duplicate
&lt;title&gt;” phrase. Accuracy of 94.20% was ob-
tained, in the same range of the 92.9% score re-
ported by Bogdanova et al. (2015).11
</p>
<p>5.2 Deep
</p>
<p>MayoNLP (Afzal et al., 2016) was the top per-
forming system in the “Question-Question” sub-
task of SemEval 2016 Task 1 (Agirre et al., 2016).
</p>
<p>Its architecture is based on Deep Structured Se-
mantic Models, introduced by Huang et al. (2013),
whose first layer is a 30k dense neural network fol-
lowed by two hidden multi-layers with 300 neu-
rons each and finally a 128 neuron output layer.
All the layers are feed-forward and fully con-
nected (diagram in Figure 5).
</p>
<p>This neural network was used to process text
and given the huge dimension of the input text
(around 500k tokens), a word hashing method was
used that creates trigrams for every word in the
input sentence: for instance, the word girl would
be represented as the trigrams #gi, gir, irl and rl#,
including the beginning and end marks. This per-
mitted to reduce the dimension of the input text to
30k, which is represented in the first neural layer.
</p>
<p>The MayoNLP system adopts this architecture
with the difference that the two hidden layer be-
come a 1k neuron layer and the output layer is
adapted to the SemEval2016 subtask, which is a
graded textual similarity classification.
</p>
<p>We resorted to the Keras deep learning library to
replicate this architecture. Given that the dimen-
sion of the input in our task was smaller, we used
one neuron for each word in our vocabulary and
it was not necessary to resort to word hashing for
dimensionality reduction. Hence, an input layer
with approximately the same size of neurons was
</p>
<p>11Our attempt to reach the authors to obtain a copy of the
dataset used in their paper remained unreplied.
</p>
<p>WR FC cosine
similarity
</p>
<p>FC
</p>
<p>multi-layer deep network
</p>
<p>Figure 5: DNN architecture: word representation
layer (WR), fully connected layers (FC) and cosine
similarity measurement layer.
</p>
<p>created: 63k for the AskUbuntuTB dataset, 16k
neurons for AskUbuntuTO and 24k for Quora.
</p>
<p>When evaluating the resulting system, the
same overall pattern as with previous approaches
emerges. The best accuracy is obtained with
AskUbuntuTB, 78.65%, which has a slight drop
with AskUbuntuTO, to 78.40%.
</p>
<p>These scores are in contrast with the accuracy of
69.53% obtained with Quora, indicating that also
here moving to a generic domain imposes a sub-
stantial loss of accuracy, of over 8 points.12
</p>
<p>5.3 Hybrid DCNN
</p>
<p>We also experimented with a novel architecture we
developed by combining the convoluted and deep
models discussed in the previous sections. By re-
sorting to the Keras deep learning library, the key
ingredients of the convoluted and the deep net-
works (DCNN) were implemented together.
</p>
<p>The hybrid DCNN starts with the same input
structure as the CNN, obtaining the vectorial rep-
resentations of words in two input segments. It
then connects each of them to a shared convo-
lutional layer followed by three hidden and fully
connected layers, whose output is finally com-
pared using the cosine distance. Both the convolu-
tional and the deep layers share the same weights
for the two sentences input, in a siamese network
(diagram in Figure 6).
</p>
<p>The vectorial representation uses an embedding
layer of 300 randomly initiated neurons with uni-
form distribution which are trainable. The con-
volution layer uses 300 neurons for the output of
filters with a kernel size of 15 units, and each deep
layer has 50 neurons.
</p>
<p>Differently, from previous approaches, the re-
sulting DQD resolver scores better over AskUbun-
tuTO, scoring 79.67%, than over AskUbuntuTB,
for which it gets a 79.00% accuracy score. This
</p>
<p>12In the “Question-Question” subtask of SemEval 2016,
thus with different datasets and for the different task of 0 to
5 graded similarity classification task, the MayoNLP system
scored 0.73035 in terms of Pearson correlation coefficient
(Agirre et al., 2016).
</p>
<p>267</p>
<p />
</div>
<div class="page"><p />
<p>WR CONV POOL cosine
similarity
</p>
<p>multi-layer deep network
</p>
<p>FC FCFC
</p>
<p>Figure 6: DCNN architecture.
</p>
<p>may be an indicator that, when using the title and
body, the neural network could perform better but
may be failing due to the sparseness of the data,
which requires possibly a higher number of neu-
rons in the deep layer.
</p>
<p>As for the result with Quora, in turn, the same
pattern is observed as in previous systems. There
is a substantial drop of over 8 points, to 71.48%.
</p>
<p>6 Discussion
</p>
<p>The experimental results reported in the previous
sections are summarized in Table 1. The perfor-
mance of each approach or architecture for DQD
was assessed in the respective section. Putting all
results side by side, some patterns emerge.
</p>
<p>Shortening the length of the segments (from 84
words per segment on average, with AskUbun-
tuTB, to 8 or 10 words, respectively with
AskUbuntuTO or Quora) has an overall negative
impact on the accuracy of the systems, except
for DCNN. For AskUbuntuTO, the negative delta
ranges from 0.25 points, with DNN, to over 5
points, with SVM-adv.
</p>
<p>NN-based solutions seem thus to be more ro-
bust to the shortening of the length of the segments
than SVM-based ones, even to the point where the
more sophisticated DCNN approach inverts this
pattern, and performs better for shorter segments
than for longer ones with AskUbuntu.
</p>
<p>As the average length of segments in AskUbun-
tuTO and Quora are similar, the contrast between
their scores permits to uncover yet another pattern.
Moving from a specific to a generic domain has an
overall negative impact on the accuracy of the sys-
tems, which is wider than with the shortening of
the segments. The negative delta ranges from less
than 3 points, with Jaccard or SVM-base, to over
14 points, with DNN.
</p>
<p>The level of the impact seems to be inverted
here. It is the non NN-based solutions that appear
as more robust to the generalization of the domain
than the NN-based ones, to the point that the supe-
riority shown by NN-based ones with the specific
domain is reduced or even canceled with the gen-
eral domain.
</p>
<p>It is interesting to note that, for the generic do-
main, the CNN approach offers the worst result.
The DNN overcomes the best SVM approach by
less than 2 points. And only the DCNN overcomes
the overall second-best, but also by a modest mar-
gin.
</p>
<p>It is also very interesting to note that, for gen-
eral domain, the rule-based approach is one of the
two second best, thus challenging the immense so-
phistication of any other approach, including the
NN-based ones.
</p>
<p>7 Cross-domain application
</p>
<p>Given the scarcity of labeled datasets of pairs
of interrogative segments, in real usage scenar-
ios systems tend to be trained on as much data as
possible from all sources and different domains.
We experimentally approximated this scenario by
training the best DQD system of each approach
over the generic dataset (Quora) and evaluating
them over the focused dataset (AskUbuntuTO).
</p>
<p>The rule-based, the advanced SVM and the
DCNN perform, respectively, with accuracies of
57.63% (dropping almost 15 points), 53.50%
(dropping over 17 points) and 56.42% (dropping
over 15 points).
</p>
<p>Interestingly, leaner systems seem to be more
robust in this approximation to its application in
real-usage scenarios than more sophisticated ones.
Importantly, however, for all types of systems, ac-
curacy is observed to degrade and comes close to
random decision performance.13
</p>
<p>Given these results, it is interesting to check,
for the generic domain, how this challenge may
evolve when the training set is substantially en-
larged, namely to the largest dataset available at
present, that is to the over 400K pairs of the full
Quora release. We picked the more lean technique
(rule-based) and more sophisticated one (DCNN).
</p>
<p>Interestingly their accuracy scores evolved to-
wards opposite directions when compared to the
scores obtained with the (over 13 times) smaller
experimental Quora dataset, with 30k pairs. The
rule-based solution scored 66.74% accuracy, drop-
ping over 2 points, while the DCNN scored
79.36%, progressing almost 8 points.14
</p>
<p>13Recall that in our experimental conditions, the 30k
datasets are balanced with 50% duplicate pairs and 50% non-
duplicate, but the 400k Quora release is not. To confirm this
trend, we collected yet another data point for CNN, which
scored 50.20%, in line with the other systems.
</p>
<p>14 Given the huge volume of the Quora release, we imple-
</p>
<p>268</p>
<p />
</div>
<div class="page"><p />
<p>Jcrd SVM-bas SVM-adv CNN DNN DCNN
AskUbuntu
</p>
<p>title and body 72.91 70.25 75.87 74.50 78.65 79.00
title only 72.35 68.88 70.87 74.12 78.40 79.67
</p>
<p>Quora 69.53 64.93 68.56 59.90 69.53 71.48
</p>
<p>Table 1: Accuracy of the 6 systems (columns) over the 3 datasets (lines)
</p>
<p>8 Related work
</p>
<p>An interesting approach to DQD was introduced
by Wu et al. (2011). It resorts to the Jaccard co-
efficient to measure similarities between two seg-
ments in the pair. Separate coefficients are calcu-
lated, and assigned different weights, for the seg-
ments. A threshold is empirically estimated and
used to determine whether two threads are dupli-
cates. An f-score of 60.29 is obtained for the titles
only, trained with 3M questions and tested against
2k pairs taken from a dataset obtained from Baidu
Zhidao, in Chinese. This approach is used as a
baseline by Bogdanova et al. (2015). This system
inspired one of the architectures used in our exper-
iments, presented in detail in Section 5.1.
</p>
<p>The recent SemEval-2016 Task 1 included a
“Question-Question” subtask to determine the de-
gree of similarity between two interrogative seg-
ments. The MayoNLP system (Afzal et al., 2016)
obtained the best accuracy in this task. This sys-
tem inspired one of the systems used in our exper-
iments, presented in detail in Section 5.2.
</p>
<p>Regarding the Quora dataset released 2 months
ago, to the best of our knowledge, up to now
there is only one unpublished paper concerning
that task (Wang et al., 2017). It proposes a multi-
perspective matching (BiMPM) model and evalu-
ates it upon a 96%/2%/2% train/dev/test split. This
system is reported to reach an accuracy of 88.17%.
</p>
<p>Other draft results concerning Quora dataset are
available only as blog posts15,16 and are based on
the model for natural language inference proposed
by Parikh et al. (2016).
</p>
<p>mented a lean version of DCNN to run this experiment, which
used a vectorial representation of 25 neurons randomly initi-
ated, followed by a convolution layer which uses 10 neurons
for the output of filters and with a 5 kernel size. The deep
layers were reduced to two layers each with 10 neurons. A
70%/30% randomly extracted for training/testing was used
both for the experiment with the DCNN and with the rule-
based approach.
</p>
<p>15https://engineering.quora.com/Semantic-Question-
Matching-with-Deep-Learning
</p>
<p>16https://explosion.ai/blog/quora-deep-text-pair-
classification
</p>
<p>9 Conclusions
</p>
<p>The experiments reported in this paper permit-
ted to advance the understanding of the duplicate
question detection task and improve its applica-
tion. There is consistent progress in terms of the
accuracy of the systems as one moves from less to
more sophisticated approaches, from rule-based to
support vector machines, and from these to neu-
ral networks, when its application is over a nar-
row, specific domain. The same trend is observed
for the range of support vector machines solutions,
with better results obtained for resolvers resort-
ing to more advanced features. And it is observed
also for the range of neural network architectures
experimented with, from convoluted to deep net-
works, and from these to hybrid convoluted deep
ones. Overall, the novel neural network architec-
ture we propose presents the best performance of
all resolvers tested.
</p>
<p>The rate of this progress is however mitigated
or even gets close to be canceled when one moves
from a narrow and specific to broad and all-
encompassing domain. Under our experimental
conditions, the gap of over 11 points from the
worst to the best performing solution with a nar-
row domain is cut to almost half, and the more
sophisticated solution, with the best score, over-
comes the leanest one just by less than 2 points
when running over a generic domain.
</p>
<p>Interestingly, when one moves, in turn, from
longer to (eight times) shorter segments, only mi-
nor drops in performance are registered.
</p>
<p>Given the scarcity of labeled datasets of pairs
of interrogative segments, in real usage scenarios,
systems are trained on as much data as possible
from all sources and different domains and eventu-
ally applied over narrow domains. We experimen-
tally approximated this scenario, where the accu-
racy of the systems was observed to degrade and
come close to random decision performance.
</p>
<p>In future work, we will extend our experimental
space to further systems and conditions, including
larger datasets, and languages other than English.
</p>
<p>269</p>
<p />
</div>
<div class="page"><p />
<p>Acknowledgements
</p>
<p>The present research was partly supported by the
CLARIN and ANI/3279/2016 grants.
</p>
<p>References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene
</p>
<p>Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.
</p>
<p>Naveed Afzal, Yanshan Wang, and Hongfang Liu.
2016. MayoNLP at SemEval-2016 task 1: Semantic
textual similarity based on lexical semantic net and
deep learning semantic model. In Proceedings of
the 10th International Workshop on Semantic Eval-
uation (SemEval-2016). pages 1258–1263.
</p>
<p>Eneko Agirre, Carmen Banea, Daniel M. Cer, Mona T.
Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-
man Rigau, and Janyce Wiebe. 2016. Semeval-
2016 task 1: Semantic textual similarity, mono-
lingual and cross-lingual evaluation. In Steven
Bethard, Daniel M. Cer, Marine Carpuat, David Ju-
rgens, Preslav Nakov, and Torsten Zesch, editors,
Proceedings of the 10th International Workshop on
Semantic Evaluation, SemEval@NAACL-HLT 2016,
San Diego, CA, USA, June 16–17, 2016. The As-
sociation for Computer Linguistics, pages 497–511.
http://aclweb.org/anthology/S/S16/S16-1081.pdf.
</p>
<p>Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interac-
tive presentation sessions. Association for Compu-
tational Linguistics, pages 69–72.
</p>
<p>Dasha Bogdanova, Cı́cero Nogueira dos Santos, Lu-
ciano Barbosa, and Bianca Zadrozny. 2015. Detect-
ing semantically equivalent questions in online user
forums. In Afra Alishahi and Alessandro Moschitti,
editors, Proceedings of the 19th Conference on
Computational Natural Language Learning, CoNLL
2015, Beijing, China, July 30–31, 2015. ACL, pages
123–131. http://aclweb.org/anthology/K/K15/K15-
1013.pdf.
</p>
<p>François Chollet. 2015. Keras.
https://github.com/fchollet/keras.
</p>
<p>Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.
</p>
<p>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
International Conference on Information &amp; Knowl-
edge Management. ACM, pages 2333–2338.
</p>
<p>Shankar Iyer, Nikhil Dandekar, and Kornél Cser-
nai. 2017. First quora dataset release: Question
pairs. https://data.quora.com/First-Quora-Dataset-
Release-Question-Pairs.
</p>
<p>Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining. pages 217–226.
</p>
<p>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous
space word representations. In Lucy Vander-
wende, Hal Daumé III, and Katrin Kirchhoff, ed-
itors, Human Language Technologies: Confer-
ence of the North American Chapter of the As-
sociation of Computational Linguistics, Proceed-
ings, June 9-14, 2013, Westin Peachtree Plaza
Hotel, Atlanta, Georgia, USA. The Association
for Computational Linguistics, pages 746–751.
http://aclweb.org/anthology/N/N13/N13-1090.pdf.
</p>
<p>Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Jian Su,
Xavier Carreras, and Kevin Duh, editors, Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1–4, 2016. The Asso-
ciation for Computational Linguistics, pages 2249–
2255. http://aclweb.org/anthology/D/D16/D16-
1244.pdf.
</p>
<p>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research
12:2825–2830.
</p>
<p>Sascha Rothe and Hinrich Schütze. 2015. Au-
toextend: Extending word embeddings to embed-
dings for synsets and lexemes. arXiv preprint
arXiv:1507.01127 .
</p>
<p>Zhiguo Wang, Wael Hamza, and Radu Florian.
2017. Bilateral multi-perspective matching for nat-
ural language sentences. CoRR abs/1702.03814.
http://arxiv.org/abs/1702.03814.
</p>
<p>Yan Wu, Qi Zhang, and Xuanjing Huang. 2011. Ef-
ficient near-duplicate detection for Q&amp;A forum.
In Fifth International Joint Conference on Natu-
ral Language Processing, IJCNLP 2011, Chiang
Mai, Thailand, November 8–13, 2011. The Associ-
ation for Computer Linguistics, pages 1001–1009.
http://aclweb.org/anthology/I/I11/I11-1112.pdf.
</p>
<p>270</p>
<p />
</div>
<div class="page"><p />
<p>Author Index
</p>
<p>António Rodrigues, João, 262
Apidianaki, Marianna, 12, 84
Asghar, Nabiha, 78
</p>
<p>Becker, Maria, 230
Berant, Jonathan, 161
Bhatia, Archna, 178
Bhavsar, Virendrakumar, 54
Blache, Philippe, 168
Boleda, Gemma, 104
Branco, António, 262
Bravo-Marquez, Felipe, 65
</p>
<p>Callison-Burch, Chris, 12, 84
Carpuat, Marine, 33
Chen, Ao, 44
Chersoni, Emmanuele, 168
Cocos, Anne, 84
Compton, Michael, 241
Cook, Paul, 54
Coppersmith, Glen, 241
Cotterell, Ryan, 97
</p>
<p>Dagan, Ido, 155
Diab, Mona, 241
Drozd, Aleksandr, 135
</p>
<p>Eichler, Kathrin, 220
</p>
<p>Farmer, Stephanie, 1
Feng, Yukun, 91
Ferraro, Francis, 97
Finley, Gregory, 1
Frank, Anette, 230
</p>
<p>Geva, Mor, 161
Gharbieh, Waseem, 54
Gilroy, Sorcha, 199
Gupta, Abhijeet, 104
</p>
<p>Han, Na-Rae, 178
Hirschberg, Julia, 110
Hovy, Eduard, 209
Husic, Halima, 189
Hwang, Jena D., 178
</p>
<p>Jauhar, Sujay Kumar, 209
Jiang, Xin, 78
</p>
<p>Kiss, Tibor, 189
Korhonen, Anna, 22
Krause, Sebastian, 220
</p>
<p>Lenci, Alessandro, 168
Levitan, Sarah Ita, 110
Li, Bofang, 135
Li, Hang, 78
Liu, Chunhua, 91
Lopez, Adam, 199
</p>
<p>Maneth, Sebastian, 199
Maraev, Vladislav, 262
Maredia, Angel, 110
Medić, Zoran, 115
Modi, Ashutosh, 121
Mohammad, Saif, 65
</p>
<p>Nastase, Vivi, 230
Nguyen, Dai Quoc, 121
Nguyen, Dat Quoc, 121
</p>
<p>O’Gorman, Tim, 178
Ostermann, Simon, 128
</p>
<p>Padó, Sebastian, 104, 115
Pakhomov, Serguei, 1
Palmer, Alexis, 230
Pauselli, Luca, 241
Pelletier, Francis Jeffry, 189
Pinkal, Manfred, 121, 128
Poliak, Adam, 97
Ponti, Edoardo Maria, 22
Poppek, Johanna, 189
Poupart, Pascal, 78
</p>
<p>Rajana, Sneha, 12
Rogers, Anna, 135
Roth, Michael, 128
</p>
<p>Sachan, Mrinmaya, 251
Saedi, Chakaveh, 262
Sarioglu Kayi, Efsun, 241
</p>
<p>271</p>
<p />
</div>
<div class="page"><p />
<p>Schechtman, Kara, 110
Schneider, Nathan, 178
Shutova, Ekaterina, 149
Shwartz, Vered, 12, 155
Silva, João, 262
Šnajder, Jan, 115
Srikumar, Vivek, 178
Staniek, Michael, 230
Stanovsky, Gabriel, 155
Sun, Maosong, 44
</p>
<p>Talmor, Alon, 161
Thater, Stefan, 121, 128
</p>
<p>Uszkoreit, Hans, 220
</p>
<p>Van Durme, Benjamin, 97
Vulić, Ivan, 22
Vyas, Yogarshi, 33
</p>
<p>Wundsam, Andreas, 149
</p>
<p>Xing, Eric, 251
Xu, Feiyu, 220
Xu, Jian, 91
</p>
<p>Yannakoudakis, Helen, 149
Yu, Dong, 91</p>
<p />
</div>
<ul>	<li>Program</li>
	<li>What Analogies Reveal about Word Vectors and their Compositionality</li>
	<li>Learning Antonyms with Paraphrases and a Morphology-Aware Neural Network</li>
	<li>Decoding Sentiment from Distributed Representations of Sentences</li>
	<li>Detecting Asymmetric Semantic Relations in Context: A Case-Study on Hypernymy Detection</li>
	<li>Domain-Specific New Words Detection in Chinese</li>
	<li>Deep Learning Models For Multiword Expression Identification</li>
	<li>Emotion Intensities in Tweets</li>
	<li>Deep Active Learning for Dialogue Generation</li>
	<li>Mapping the Paraphrase Database to WordNet</li>
	<li>Semantic Frame Labeling with Target-based Neural Model</li>
	<li>Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles</li>
	<li>Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible</li>
	<li>Comparing Approaches for Automatic Question Identification</li>
	<li>Does Free Word Order Hurt? Assessing the Practical Lexical Function Model for Croatian</li>
	<li>A Mixture Model for Learning Multi-Sense Word Embeddings</li>
	<li>Aligning Script Events with Narrative Texts</li>
	<li>The (too Many) Problems of Analogical Reasoning with Word Vectors</li>
	<li>Semantic Frames and Visual Scenes: Learning Semantic Role Inventories from Image and Video Descriptions</li>
	<li>Acquiring Predicate Paraphrases from News Tweets</li>
	<li>Evaluating Semantic Parsing against a Simple Web-based Question Answering Model</li>
	<li>Logical Metonymy in a Distributional Model of Sentence Comprehension</li>
	<li>Double Trouble: The Problem of Construal in Semantic Annotation of Adpositions</li>
	<li>Issues of Mass and Count: Dealing with `Dual-Life' Nouns</li>
	<li>Parsing Graphs with Regular Graph Grammars</li>
	<li>Embedded Semantic Lexicon Induction with Joint Global and Local Optimization</li>
	<li>Generating Pattern-Based Entailment Graphs for Relation Extraction</li>
	<li>Classifying Semantic Clause Types: Modeling Context and Genre Characteristics with Recurrent Neural Networks and Attention</li>
	<li>Predictive Linguistic Features of Schizophrenia</li>
	<li>Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks</li>
	<li>Ways of Asking and Replying in Duplicate Question Detection</li>
</ul>
</body></html>